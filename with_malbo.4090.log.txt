import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

from malbo import compute_malbo_parameters

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

import flash_attn

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        @torch._dynamo.disable
        def call_flash_varlen(*args, **kwargs):
            return flash_attn.flash_attn_varlen_func(
                *args, **kwargs
            )

        y = call_flash_varlen(
            q[0],           # 1. q
            k[0],           # 2. k
            v[0],           # 3. v
            seqlens,        # 4. cu_seqlens_q
            seqlens,        # 5. cu_seqlens_k
            max_len,        # 6. max_seqlen_q
            max_len,        # 7. max_seqlen_k
            causal=True,
            softmax_scale=yarn.attn_scale,
            window_size=(-1, -1) if bm_size <= 0 else (bm_size, -1)
        )
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        @torch._dynamo.disable
        def call_flash_varlen(*args, **kwargs):
            return flash_attn.flash_attn_varlen_func(
                *args, **kwargs
            )

        y = call_flash_varlen(
            q[0],           # 1. q
            k[0],           # 2. k
            v[0],           # 3. v
            seqlens,        # 4. cu_seqlens_q
            seqlens,        # 5. cu_seqlens_k
            max_len,        # 6. max_seqlen_q
            max_len,        # 7. max_seqlen_k
            causal=True,
            softmax_scale=yarn.attn_scale,
            window_size=(-1, -1) if bm_size <= 0 else (bm_size, -1)
        )
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int, use_malbo: bool):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

        self.use_malbo = use_malbo

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0

            loss = (cross_entropy * mtp_weights).sum()
            if self.use_malbo:
                T, K = logits_flat.shape
                with torch.no_grad():
                    vhat, kappa, gamma = compute_malbo_parameters(cross_entropy.T, K)
                    weights_transposed = kappa * gamma

                malbo_loss = T * (cross_entropy * weights_transposed.T * mtp_weights).sum()
            else:
                malbo_loss = loss
        elif self.training:
            if self.use_malbo:
                logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
                T, K = logits_flat.shape
                cross_entropy = F.cross_entropy(logits_flat, target_seq, reduction="none")
                loss = cross_entropy.sum()

                with torch.no_grad():
                    vhat, kappa, gamma = compute_malbo_parameters(cross_entropy.unsqueeze(0), K)
                    weights = (kappa * gamma).squeeze(0)

                malbo_loss = T * (weights * cross_entropy).sum()
            else:
                loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
                malbo_loss = loss
        else:
            if self.use_malbo:
                K = logits_for_loss.size(-1)
                cross_entropy = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="none")
                loss = cross_entropy.mean()

                with torch.no_grad():
                    vhat, kappa, gamma = compute_malbo_parameters(cross_entropy.unsqueeze(0), K)
                    weights = (kappa * gamma).squeeze(0)

                malbo_loss = (weights * cross_entropy).sum()
            else:
                loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
                malbo_loss = loss

        return loss, malbo_loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

# NB: modified for 4090
@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (4 * 2048 * 1, 8 * 2048 * 1, 12 * 2048 * 1)
    train_bs_extension: int = 12 * 2048 * 1
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 1 * 64 * 1024 * 1
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 100  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size),
    use_malbo=os.environ.get('use_malbo', 'True') == 'True',
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=False)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args())[1] / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss, val_malbo_loss = 0, 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                this_val_loss, this_val_malbo_loss = model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
                val_loss += this_val_loss
                val_malbo_loss += this_val_malbo_loss
        val_loss /= val_steps
        val_malbo_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        dist.reduce(val_malbo_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} val_malbo_loss:{val_malbo_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args())[1] / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.19 (main, Oct 21 2025, 16:43:05) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Wed Jan 14 03:34:40 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 4090        On  |   00000000:C2:00.0 Off |                  Off |
|  0%   40C    P2             35W /  450W |     911MiB /  24564MiB |      3%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           18358      C   .../envs/speedrun/bin/python3.10        902MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8294 val_malbo_loss:10.8293 train_time:0ms step_avg:0.04ms
step:1/1775 train_time:759ms step_avg:758.67ms
step:2/1775 train_time:1430ms step_avg:715.07ms
step:3/1775 train_time:2110ms step_avg:703.17ms
step:4/1775 train_time:2788ms step_avg:696.94ms
step:5/1775 train_time:3461ms step_avg:692.10ms
step:6/1775 train_time:4137ms step_avg:689.54ms
step:7/1775 train_time:4813ms step_avg:687.61ms
step:8/1775 train_time:5492ms step_avg:686.53ms
step:9/1775 train_time:6165ms step_avg:685.00ms
step:10/1775 train_time:6842ms step_avg:684.17ms
step:11/1775 train_time:7529ms step_avg:684.45ms
step:12/1775 train_time:8410ms step_avg:700.79ms
step:13/1775 train_time:9095ms step_avg:699.59ms
step:14/1775 train_time:9777ms step_avg:698.36ms
step:15/1775 train_time:10453ms step_avg:696.85ms
step:16/1775 train_time:11132ms step_avg:695.74ms
step:17/1775 train_time:11806ms step_avg:694.49ms
step:18/1775 train_time:12482ms step_avg:693.44ms
step:19/1775 train_time:13155ms step_avg:692.39ms
step:20/1775 train_time:13833ms step_avg:691.66ms
step:21/1775 train_time:14570ms step_avg:693.82ms
step:22/1775 train_time:15257ms step_avg:693.52ms
step:23/1775 train_time:15931ms step_avg:692.63ms
step:24/1775 train_time:16609ms step_avg:692.03ms
step:25/1775 train_time:17254ms step_avg:690.15ms
step:26/1775 train_time:17899ms step_avg:688.42ms
step:27/1775 train_time:18544ms step_avg:686.80ms
step:28/1775 train_time:19192ms step_avg:685.41ms
step:29/1775 train_time:19833ms step_avg:683.89ms
step:30/1775 train_time:20478ms step_avg:682.60ms
step:31/1775 train_time:21122ms step_avg:681.35ms
step:32/1775 train_time:21768ms step_avg:680.26ms
step:33/1775 train_time:22408ms step_avg:679.03ms
step:34/1775 train_time:23055ms step_avg:678.08ms
step:35/1775 train_time:23698ms step_avg:677.08ms
step:36/1775 train_time:24343ms step_avg:676.19ms
step:37/1775 train_time:24985ms step_avg:675.26ms
step:38/1775 train_time:25632ms step_avg:674.52ms
step:39/1775 train_time:26275ms step_avg:673.72ms
step:40/1775 train_time:26921ms step_avg:673.02ms
step:41/1775 train_time:27565ms step_avg:672.32ms
step:42/1775 train_time:28200ms step_avg:671.42ms
step:43/1775 train_time:28841ms step_avg:670.73ms
step:44/1775 train_time:29488ms step_avg:670.18ms
step:45/1775 train_time:30132ms step_avg:669.60ms
step:46/1775 train_time:30777ms step_avg:669.06ms
step:47/1775 train_time:31419ms step_avg:668.48ms
step:48/1775 train_time:32066ms step_avg:668.03ms
step:49/1775 train_time:32706ms step_avg:667.47ms
step:50/1775 train_time:33350ms step_avg:667.01ms
step:51/1775 train_time:33993ms step_avg:666.52ms
step:52/1775 train_time:34647ms step_avg:666.28ms
step:53/1775 train_time:35287ms step_avg:665.79ms
step:54/1775 train_time:35931ms step_avg:665.38ms
step:55/1775 train_time:36574ms step_avg:664.98ms
step:56/1775 train_time:37220ms step_avg:664.64ms
step:57/1775 train_time:37858ms step_avg:664.18ms
step:58/1775 train_time:38506ms step_avg:663.90ms
step:59/1775 train_time:39147ms step_avg:663.51ms
step:60/1775 train_time:39790ms step_avg:663.16ms
step:61/1775 train_time:40427ms step_avg:662.74ms
step:62/1775 train_time:41224ms step_avg:664.90ms
step:63/1775 train_time:41860ms step_avg:664.44ms
step:64/1775 train_time:42504ms step_avg:664.12ms
step:65/1775 train_time:43141ms step_avg:663.71ms
step:66/1775 train_time:43784ms step_avg:663.40ms
step:67/1775 train_time:44422ms step_avg:663.02ms
step:68/1775 train_time:45066ms step_avg:662.73ms
step:69/1775 train_time:45705ms step_avg:662.39ms
step:70/1775 train_time:46348ms step_avg:662.12ms
step:71/1775 train_time:46986ms step_avg:661.78ms
step:72/1775 train_time:47629ms step_avg:661.51ms
step:73/1775 train_time:48267ms step_avg:661.20ms
step:74/1775 train_time:48910ms step_avg:660.95ms
step:75/1775 train_time:49550ms step_avg:660.66ms
step:76/1775 train_time:50194ms step_avg:660.45ms
step:77/1775 train_time:50834ms step_avg:660.18ms
step:78/1775 train_time:51478ms step_avg:659.97ms
step:79/1775 train_time:52117ms step_avg:659.71ms
step:80/1775 train_time:52761ms step_avg:659.52ms
step:81/1775 train_time:53401ms step_avg:659.27ms
step:82/1775 train_time:54044ms step_avg:659.07ms
step:83/1775 train_time:54684ms step_avg:658.84ms
step:84/1775 train_time:55327ms step_avg:658.66ms
step:85/1775 train_time:55966ms step_avg:658.42ms
step:86/1775 train_time:56610ms step_avg:658.26ms
step:87/1775 train_time:57250ms step_avg:658.05ms
step:88/1775 train_time:57894ms step_avg:657.89ms
step:89/1775 train_time:58536ms step_avg:657.71ms
step:90/1775 train_time:59179ms step_avg:657.54ms
step:91/1775 train_time:59819ms step_avg:657.35ms
step:92/1775 train_time:60464ms step_avg:657.22ms
step:93/1775 train_time:61104ms step_avg:657.03ms
step:94/1775 train_time:61758ms step_avg:656.99ms
step:95/1775 train_time:62412ms step_avg:656.97ms
step:96/1775 train_time:63082ms step_avg:657.11ms
step:97/1775 train_time:63752ms step_avg:657.24ms
step:98/1775 train_time:64479ms step_avg:657.95ms
step:99/1775 train_time:65157ms step_avg:658.15ms
step:100/1775 train_time:65828ms step_avg:658.28ms
step:100/1775 val_loss:6.5200 val_malbo_loss:6.4115 train_time:65837ms step_avg:658.37ms
step:101/1775 train_time:66385ms step_avg:657.28ms
step:102/1775 train_time:67057ms step_avg:657.42ms
step:103/1775 train_time:67724ms step_avg:657.51ms
step:104/1775 train_time:68395ms step_avg:657.64ms
step:105/1775 train_time:69060ms step_avg:657.72ms
step:106/1775 train_time:69731ms step_avg:657.84ms
step:107/1775 train_time:70260ms step_avg:656.63ms
step:108/1775 train_time:70748ms step_avg:655.07ms
step:109/1775 train_time:71298ms step_avg:654.11ms
step:110/1775 train_time:71968ms step_avg:654.26ms
step:111/1775 train_time:72635ms step_avg:654.37ms
step:112/1775 train_time:73305ms step_avg:654.51ms
step:113/1775 train_time:73933ms step_avg:654.27ms
step:114/1775 train_time:74485ms step_avg:653.38ms
step:115/1775 train_time:74909ms step_avg:651.38ms
step:116/1775 train_time:75306ms step_avg:649.19ms
step:117/1775 train_time:75651ms step_avg:646.59ms
step:118/1775 train_time:75997ms step_avg:644.05ms
step:119/1775 train_time:76341ms step_avg:641.52ms
step:120/1775 train_time:76688ms step_avg:639.06ms
step:121/1775 train_time:77033ms step_avg:636.63ms
step:122/1775 train_time:77379ms step_avg:634.26ms
step:123/1775 train_time:77723ms step_avg:631.89ms
step:124/1775 train_time:78070ms step_avg:629.60ms
step:125/1775 train_time:78415ms step_avg:627.32ms
step:126/1775 train_time:78761ms step_avg:625.08ms
step:127/1775 train_time:79107ms step_avg:622.89ms
step:128/1775 train_time:79455ms step_avg:620.75ms
step:129/1775 train_time:79800ms step_avg:618.60ms
step:130/1775 train_time:80147ms step_avg:616.51ms
step:131/1775 train_time:80491ms step_avg:614.43ms
step:132/1775 train_time:80838ms step_avg:612.41ms
step:133/1775 train_time:81181ms step_avg:610.38ms
step:134/1775 train_time:81533ms step_avg:608.46ms
step:135/1775 train_time:81881ms step_avg:606.52ms
step:136/1775 train_time:82227ms step_avg:604.61ms
step:137/1775 train_time:82574ms step_avg:602.73ms
step:138/1775 train_time:82920ms step_avg:600.87ms
step:139/1775 train_time:83265ms step_avg:599.03ms
step:140/1775 train_time:83611ms step_avg:597.22ms
step:141/1775 train_time:84067ms step_avg:596.22ms
step:142/1775 train_time:84728ms step_avg:596.68ms
step:143/1775 train_time:85384ms step_avg:597.09ms
step:144/1775 train_time:86056ms step_avg:597.61ms
step:145/1775 train_time:86722ms step_avg:598.08ms
step:146/1775 train_time:87392ms step_avg:598.57ms
step:147/1775 train_time:88058ms step_avg:599.03ms
step:148/1775 train_time:88729ms step_avg:599.52ms
step:149/1775 train_time:89396ms step_avg:599.98ms
step:150/1775 train_time:90068ms step_avg:600.45ms
step:151/1775 train_time:90735ms step_avg:600.89ms
step:152/1775 train_time:91408ms step_avg:601.37ms
step:153/1775 train_time:92075ms step_avg:601.80ms
step:154/1775 train_time:92748ms step_avg:602.26ms
step:155/1775 train_time:93414ms step_avg:602.67ms
step:156/1775 train_time:94088ms step_avg:603.13ms
step:157/1775 train_time:94757ms step_avg:603.55ms
step:158/1775 train_time:95432ms step_avg:604.00ms
step:159/1775 train_time:96099ms step_avg:604.40ms
step:160/1775 train_time:96575ms step_avg:603.59ms
step:161/1775 train_time:96971ms step_avg:602.30ms
step:162/1775 train_time:97318ms step_avg:600.73ms
step:163/1775 train_time:97664ms step_avg:599.17ms
step:164/1775 train_time:98010ms step_avg:597.62ms
step:165/1775 train_time:98357ms step_avg:596.10ms
step:166/1775 train_time:98705ms step_avg:594.61ms
step:167/1775 train_time:99048ms step_avg:593.10ms
step:168/1775 train_time:99396ms step_avg:591.64ms
step:169/1775 train_time:99739ms step_avg:590.17ms
step:170/1775 train_time:100085ms step_avg:588.73ms
step:171/1775 train_time:100429ms step_avg:587.31ms
step:172/1775 train_time:100777ms step_avg:585.91ms
step:173/1775 train_time:101133ms step_avg:584.58ms
step:174/1775 train_time:101480ms step_avg:583.22ms
step:175/1775 train_time:101823ms step_avg:581.85ms
step:176/1775 train_time:102168ms step_avg:580.50ms
step:177/1775 train_time:102514ms step_avg:579.17ms
step:178/1775 train_time:102860ms step_avg:577.86ms
step:179/1775 train_time:103205ms step_avg:576.56ms
step:180/1775 train_time:103550ms step_avg:575.28ms
step:181/1775 train_time:103892ms step_avg:573.99ms
step:182/1775 train_time:104239ms step_avg:572.74ms
step:183/1775 train_time:104583ms step_avg:571.49ms
step:184/1775 train_time:104930ms step_avg:570.27ms
step:185/1775 train_time:105275ms step_avg:569.05ms
step:186/1775 train_time:105621ms step_avg:567.85ms
step:187/1775 train_time:106015ms step_avg:566.92ms
step:188/1775 train_time:106658ms step_avg:567.33ms
step:189/1775 train_time:107218ms step_avg:567.29ms
step:190/1775 train_time:107851ms step_avg:567.64ms
step:191/1775 train_time:108529ms step_avg:568.22ms
step:192/1775 train_time:109212ms step_avg:568.81ms
step:193/1775 train_time:109884ms step_avg:569.35ms
step:194/1775 train_time:110564ms step_avg:569.92ms
step:195/1775 train_time:111243ms step_avg:570.48ms
step:196/1775 train_time:111926ms step_avg:571.05ms
step:197/1775 train_time:112602ms step_avg:571.58ms
step:198/1775 train_time:113291ms step_avg:572.18ms
step:199/1775 train_time:113941ms step_avg:572.57ms
step:200/1775 train_time:114608ms step_avg:573.04ms
step:200/1775 val_loss:6.0054 val_malbo_loss:5.8835 train_time:114617ms step_avg:573.09ms
step:201/1775 train_time:115176ms step_avg:573.02ms
step:202/1775 train_time:115849ms step_avg:573.51ms
step:203/1775 train_time:116518ms step_avg:573.98ms
step:204/1775 train_time:117189ms step_avg:574.46ms
step:205/1775 train_time:117856ms step_avg:574.91ms
step:206/1775 train_time:118703ms step_avg:576.23ms
step:207/1775 train_time:119370ms step_avg:576.67ms
step:208/1775 train_time:120042ms step_avg:577.13ms
step:209/1775 train_time:120711ms step_avg:577.56ms
step:210/1775 train_time:121384ms step_avg:578.02ms
step:211/1775 train_time:121996ms step_avg:578.18ms
step:212/1775 train_time:122665ms step_avg:578.61ms
step:213/1775 train_time:123333ms step_avg:579.03ms
step:214/1775 train_time:124005ms step_avg:579.46ms
step:215/1775 train_time:124667ms step_avg:579.85ms
step:216/1775 train_time:125310ms step_avg:580.14ms
step:217/1775 train_time:125856ms step_avg:579.98ms
step:218/1775 train_time:126516ms step_avg:580.35ms
step:219/1775 train_time:127174ms step_avg:580.71ms
step:220/1775 train_time:127843ms step_avg:581.10ms
step:221/1775 train_time:128484ms step_avg:581.38ms
step:222/1775 train_time:129132ms step_avg:581.67ms
step:223/1775 train_time:129649ms step_avg:581.39ms
step:224/1775 train_time:130216ms step_avg:581.32ms
step:225/1775 train_time:130714ms step_avg:580.95ms
step:226/1775 train_time:131239ms step_avg:580.70ms
step:227/1775 train_time:131739ms step_avg:580.35ms
step:228/1775 train_time:132328ms step_avg:580.39ms
step:229/1775 train_time:132827ms step_avg:580.03ms
step:230/1775 train_time:133397ms step_avg:579.99ms
step:231/1775 train_time:133895ms step_avg:579.63ms
step:232/1775 train_time:134422ms step_avg:579.40ms
step:233/1775 train_time:134920ms step_avg:579.06ms
step:234/1775 train_time:135571ms step_avg:579.36ms
step:235/1775 train_time:136243ms step_avg:579.76ms
step:236/1775 train_time:136916ms step_avg:580.15ms
step:237/1775 train_time:137583ms step_avg:580.52ms
step:238/1775 train_time:138265ms step_avg:580.94ms
step:239/1775 train_time:138932ms step_avg:581.31ms
step:240/1775 train_time:139606ms step_avg:581.69ms
step:241/1775 train_time:140273ms step_avg:582.04ms
step:242/1775 train_time:140947ms step_avg:582.43ms
step:243/1775 train_time:141614ms step_avg:582.78ms
step:244/1775 train_time:142288ms step_avg:583.15ms
step:245/1775 train_time:142955ms step_avg:583.49ms
step:246/1775 train_time:143628ms step_avg:583.85ms
step:247/1775 train_time:144296ms step_avg:584.19ms
step:248/1775 train_time:144974ms step_avg:584.57ms
step:249/1775 train_time:145630ms step_avg:584.86ms
step:250/1775 train_time:146290ms step_avg:585.16ms
step:251/1775 train_time:146963ms step_avg:585.51ms
step:252/1775 train_time:147637ms step_avg:585.86ms
step:253/1775 train_time:148307ms step_avg:586.19ms
step:254/1775 train_time:148980ms step_avg:586.54ms
step:255/1775 train_time:149650ms step_avg:586.86ms
step:256/1775 train_time:150323ms step_avg:587.20ms
step:257/1775 train_time:150883ms step_avg:587.09ms
step:258/1775 train_time:151323ms step_avg:586.52ms
step:259/1775 train_time:151880ms step_avg:586.41ms
step:260/1775 train_time:152552ms step_avg:586.74ms
step:261/1775 train_time:153219ms step_avg:587.04ms
step:262/1775 train_time:153890ms step_avg:587.37ms
step:263/1775 train_time:154558ms step_avg:587.67ms
step:264/1775 train_time:155230ms step_avg:587.99ms
step:265/1775 train_time:155897ms step_avg:588.29ms
step:266/1775 train_time:156570ms step_avg:588.61ms
step:267/1775 train_time:157208ms step_avg:588.80ms
step:268/1775 train_time:157839ms step_avg:588.95ms
step:269/1775 train_time:158473ms step_avg:589.12ms
step:270/1775 train_time:159151ms step_avg:589.45ms
step:271/1775 train_time:159803ms step_avg:589.68ms
step:272/1775 train_time:160408ms step_avg:589.73ms
step:273/1775 train_time:160943ms step_avg:589.53ms
step:274/1775 train_time:161603ms step_avg:589.79ms
step:275/1775 train_time:162274ms step_avg:590.09ms
step:276/1775 train_time:162950ms step_avg:590.40ms
step:277/1775 train_time:163627ms step_avg:590.71ms
step:278/1775 train_time:164276ms step_avg:590.92ms
step:279/1775 train_time:164776ms step_avg:590.59ms
step:280/1775 train_time:165278ms step_avg:590.28ms
step:281/1775 train_time:165779ms step_avg:589.96ms
step:282/1775 train_time:166354ms step_avg:589.91ms
step:283/1775 train_time:166837ms step_avg:589.53ms
step:284/1775 train_time:167342ms step_avg:589.23ms
step:285/1775 train_time:168068ms step_avg:589.71ms
step:286/1775 train_time:168606ms step_avg:589.53ms
step:287/1775 train_time:169149ms step_avg:589.37ms
step:288/1775 train_time:169661ms step_avg:589.10ms
step:289/1775 train_time:170161ms step_avg:588.79ms
step:290/1775 train_time:170661ms step_avg:588.49ms
step:291/1775 train_time:171161ms step_avg:588.18ms
step:292/1775 train_time:171810ms step_avg:588.39ms
step:293/1775 train_time:172464ms step_avg:588.61ms
step:294/1775 train_time:173151ms step_avg:588.95ms
step:295/1775 train_time:173759ms step_avg:589.01ms
step:296/1775 train_time:174435ms step_avg:589.31ms
step:297/1775 train_time:175098ms step_avg:589.56ms
step:298/1775 train_time:175623ms step_avg:589.34ms
step:299/1775 train_time:176145ms step_avg:589.11ms
step:300/1775 train_time:176703ms step_avg:589.01ms
step:300/1775 val_loss:5.7152 val_malbo_loss:5.5927 train_time:176713ms step_avg:589.04ms
step:301/1775 train_time:177116ms step_avg:588.42ms
step:302/1775 train_time:177584ms step_avg:588.03ms
step:303/1775 train_time:178157ms step_avg:587.98ms
step:304/1775 train_time:178663ms step_avg:587.71ms
step:305/1775 train_time:179207ms step_avg:587.56ms
step:306/1775 train_time:179735ms step_avg:587.37ms
step:307/1775 train_time:180274ms step_avg:587.21ms
step:308/1775 train_time:180780ms step_avg:586.95ms
step:309/1775 train_time:181328ms step_avg:586.82ms
step:310/1775 train_time:181849ms step_avg:586.61ms
step:311/1775 train_time:182495ms step_avg:586.80ms
step:312/1775 train_time:183158ms step_avg:587.05ms
step:313/1775 train_time:183840ms step_avg:587.35ms
step:314/1775 train_time:184434ms step_avg:587.37ms
step:315/1775 train_time:185040ms step_avg:587.43ms
step:316/1775 train_time:185545ms step_avg:587.17ms
step:317/1775 train_time:186103ms step_avg:587.08ms
step:318/1775 train_time:186607ms step_avg:586.81ms
step:319/1775 train_time:187146ms step_avg:586.67ms
step:320/1775 train_time:187649ms step_avg:586.40ms
step:321/1775 train_time:188170ms step_avg:586.20ms
step:322/1775 train_time:188682ms step_avg:585.97ms
step:323/1775 train_time:189309ms step_avg:586.10ms
step:324/1775 train_time:189959ms step_avg:586.29ms
step:325/1775 train_time:190567ms step_avg:586.36ms
step:326/1775 train_time:191142ms step_avg:586.33ms
step:327/1775 train_time:191727ms step_avg:586.32ms
step:328/1775 train_time:192271ms step_avg:586.19ms
step:329/1775 train_time:192784ms step_avg:585.97ms
step:330/1775 train_time:193308ms step_avg:585.78ms
step:331/1775 train_time:193830ms step_avg:585.59ms
step:332/1775 train_time:194462ms step_avg:585.73ms
step:333/1775 train_time:195075ms step_avg:585.81ms
step:334/1775 train_time:195577ms step_avg:585.56ms
step:335/1775 train_time:196101ms step_avg:585.38ms
step:336/1775 train_time:196605ms step_avg:585.14ms
step:337/1775 train_time:197147ms step_avg:585.00ms
step:338/1775 train_time:197650ms step_avg:584.76ms
step:339/1775 train_time:198172ms step_avg:584.58ms
step:340/1775 train_time:198683ms step_avg:584.36ms
step:341/1775 train_time:199218ms step_avg:584.22ms
step:342/1775 train_time:199765ms step_avg:584.11ms
step:343/1775 train_time:200287ms step_avg:583.93ms
step:344/1775 train_time:200910ms step_avg:584.04ms
step:345/1775 train_time:201409ms step_avg:583.79ms
step:346/1775 train_time:201937ms step_avg:583.63ms
step:347/1775 train_time:202439ms step_avg:583.40ms
step:348/1775 train_time:202975ms step_avg:583.26ms
step:349/1775 train_time:203535ms step_avg:583.19ms
step:350/1775 train_time:204123ms step_avg:583.21ms
step:351/1775 train_time:204731ms step_avg:583.28ms
step:352/1775 train_time:205276ms step_avg:583.17ms
step:353/1775 train_time:205776ms step_avg:582.93ms
step:354/1775 train_time:206498ms step_avg:583.33ms
step:355/1775 train_time:207094ms step_avg:583.36ms
step:356/1775 train_time:207604ms step_avg:583.16ms
step:357/1775 train_time:208151ms step_avg:583.06ms
step:358/1775 train_time:208721ms step_avg:583.02ms
step:359/1775 train_time:209306ms step_avg:583.03ms
step:360/1775 train_time:209965ms step_avg:583.24ms
step:361/1775 train_time:210467ms step_avg:583.01ms
step:362/1775 train_time:210995ms step_avg:582.86ms
step:363/1775 train_time:211497ms step_avg:582.64ms
step:364/1775 train_time:212036ms step_avg:582.52ms
step:365/1775 train_time:212537ms step_avg:582.29ms
step:366/1775 train_time:213102ms step_avg:582.25ms
step:367/1775 train_time:213601ms step_avg:582.02ms
step:368/1775 train_time:214189ms step_avg:582.03ms
step:369/1775 train_time:214920ms step_avg:582.44ms
step:370/1775 train_time:215475ms step_avg:582.36ms
step:371/1775 train_time:216139ms step_avg:582.58ms
step:372/1775 train_time:216812ms step_avg:582.83ms
step:373/1775 train_time:217333ms step_avg:582.66ms
step:374/1775 train_time:217858ms step_avg:582.51ms
step:375/1775 train_time:218359ms step_avg:582.29ms
step:376/1775 train_time:218924ms step_avg:582.25ms
step:377/1775 train_time:219425ms step_avg:582.03ms
step:378/1775 train_time:220032ms step_avg:582.10ms
step:379/1775 train_time:220691ms step_avg:582.30ms
step:380/1775 train_time:221358ms step_avg:582.52ms
step:381/1775 train_time:222004ms step_avg:582.69ms
step:382/1775 train_time:222605ms step_avg:582.74ms
step:383/1775 train_time:223250ms step_avg:582.90ms
step:384/1775 train_time:223899ms step_avg:583.07ms
step:385/1775 train_time:224547ms step_avg:583.24ms
step:386/1775 train_time:225194ms step_avg:583.40ms
step:387/1775 train_time:225838ms step_avg:583.56ms
step:388/1775 train_time:226483ms step_avg:583.72ms
step:389/1775 train_time:227123ms step_avg:583.86ms
step:390/1775 train_time:227772ms step_avg:584.03ms
step:391/1775 train_time:228412ms step_avg:584.17ms
step:392/1775 train_time:229067ms step_avg:584.35ms
step:393/1775 train_time:229742ms step_avg:584.58ms
step:394/1775 train_time:230387ms step_avg:584.74ms
step:395/1775 train_time:231028ms step_avg:584.88ms
step:396/1775 train_time:231674ms step_avg:585.04ms
step:397/1775 train_time:232326ms step_avg:585.21ms
step:398/1775 train_time:232991ms step_avg:585.40ms
step:399/1775 train_time:233632ms step_avg:585.55ms
step:400/1775 train_time:234277ms step_avg:585.69ms
step:400/1775 val_loss:5.5252 val_malbo_loss:5.3993 train_time:234288ms step_avg:585.72ms
step:401/1775 train_time:234851ms step_avg:585.66ms
step:402/1775 train_time:235527ms step_avg:585.89ms
step:403/1775 train_time:236116ms step_avg:585.89ms
step:404/1775 train_time:236807ms step_avg:586.16ms
step:405/1775 train_time:237418ms step_avg:586.22ms
step:406/1775 train_time:237890ms step_avg:585.94ms
step:407/1775 train_time:238577ms step_avg:586.18ms
step:408/1775 train_time:239259ms step_avg:586.42ms
step:409/1775 train_time:239934ms step_avg:586.64ms
step:410/1775 train_time:240613ms step_avg:586.86ms
step:411/1775 train_time:241293ms step_avg:587.09ms
step:412/1775 train_time:241983ms step_avg:587.34ms
step:413/1775 train_time:242666ms step_avg:587.57ms
step:414/1775 train_time:243352ms step_avg:587.81ms
step:415/1775 train_time:244027ms step_avg:588.02ms
step:416/1775 train_time:244707ms step_avg:588.24ms
step:417/1775 train_time:245381ms step_avg:588.44ms
step:418/1775 train_time:246063ms step_avg:588.67ms
step:419/1775 train_time:246735ms step_avg:588.87ms
step:420/1775 train_time:247414ms step_avg:589.08ms
step:421/1775 train_time:248085ms step_avg:589.28ms
step:422/1775 train_time:248760ms step_avg:589.48ms
step:423/1775 train_time:249431ms step_avg:589.67ms
step:424/1775 train_time:250107ms step_avg:589.88ms
step:425/1775 train_time:250781ms step_avg:590.07ms
step:426/1775 train_time:251456ms step_avg:590.27ms
step:427/1775 train_time:252134ms step_avg:590.48ms
step:428/1775 train_time:252811ms step_avg:590.68ms
step:429/1775 train_time:253484ms step_avg:590.87ms
step:430/1775 train_time:254151ms step_avg:591.05ms
step:431/1775 train_time:254838ms step_avg:591.27ms
step:432/1775 train_time:255524ms step_avg:591.49ms
step:433/1775 train_time:256209ms step_avg:591.71ms
step:434/1775 train_time:256889ms step_avg:591.91ms
step:435/1775 train_time:257568ms step_avg:592.11ms
step:436/1775 train_time:258246ms step_avg:592.31ms
step:437/1775 train_time:258916ms step_avg:592.49ms
step:438/1775 train_time:259593ms step_avg:592.68ms
step:439/1775 train_time:260264ms step_avg:592.86ms
step:440/1775 train_time:260939ms step_avg:593.04ms
step:441/1775 train_time:261626ms step_avg:593.26ms
step:442/1775 train_time:262355ms step_avg:593.56ms
step:443/1775 train_time:263144ms step_avg:594.00ms
step:444/1775 train_time:263819ms step_avg:594.19ms
step:445/1775 train_time:264491ms step_avg:594.36ms
step:446/1775 train_time:265167ms step_avg:594.55ms
step:447/1775 train_time:265841ms step_avg:594.72ms
step:448/1775 train_time:266524ms step_avg:594.92ms
step:449/1775 train_time:267219ms step_avg:595.14ms
step:450/1775 train_time:267912ms step_avg:595.36ms
step:451/1775 train_time:268593ms step_avg:595.55ms
step:452/1775 train_time:269269ms step_avg:595.73ms
step:453/1775 train_time:269940ms step_avg:595.89ms
step:454/1775 train_time:270614ms step_avg:596.07ms
step:455/1775 train_time:271284ms step_avg:596.23ms
step:456/1775 train_time:271954ms step_avg:596.39ms
step:457/1775 train_time:272622ms step_avg:596.55ms
step:458/1775 train_time:273297ms step_avg:596.72ms
step:459/1775 train_time:273967ms step_avg:596.88ms
step:460/1775 train_time:274642ms step_avg:597.05ms
step:461/1775 train_time:275322ms step_avg:597.23ms
step:462/1775 train_time:275999ms step_avg:597.40ms
step:463/1775 train_time:276670ms step_avg:597.56ms
step:464/1775 train_time:277646ms step_avg:598.37ms
step:465/1775 train_time:278316ms step_avg:598.53ms
step:466/1775 train_time:278991ms step_avg:598.69ms
step:467/1775 train_time:279659ms step_avg:598.84ms
step:468/1775 train_time:280330ms step_avg:598.99ms
step:469/1775 train_time:281002ms step_avg:599.15ms
step:470/1775 train_time:281685ms step_avg:599.33ms
step:471/1775 train_time:282354ms step_avg:599.48ms
step:472/1775 train_time:283030ms step_avg:599.64ms
step:473/1775 train_time:283700ms step_avg:599.79ms
step:474/1775 train_time:284384ms step_avg:599.97ms
step:475/1775 train_time:285055ms step_avg:600.12ms
step:476/1775 train_time:285734ms step_avg:600.28ms
step:477/1775 train_time:286405ms step_avg:600.43ms
step:478/1775 train_time:287080ms step_avg:600.59ms
step:479/1775 train_time:287750ms step_avg:600.73ms
step:480/1775 train_time:288423ms step_avg:600.88ms
step:481/1775 train_time:289095ms step_avg:601.03ms
step:482/1775 train_time:289772ms step_avg:601.19ms
step:483/1775 train_time:290449ms step_avg:601.34ms
step:484/1775 train_time:291134ms step_avg:601.52ms
step:485/1775 train_time:291808ms step_avg:601.67ms
step:486/1775 train_time:292485ms step_avg:601.82ms
step:487/1775 train_time:293154ms step_avg:601.96ms
step:488/1775 train_time:293830ms step_avg:602.11ms
step:489/1775 train_time:294500ms step_avg:602.25ms
step:490/1775 train_time:295194ms step_avg:602.44ms
step:491/1775 train_time:296016ms step_avg:602.88ms
step:492/1775 train_time:296692ms step_avg:603.03ms
step:493/1775 train_time:297364ms step_avg:603.17ms
step:494/1775 train_time:298040ms step_avg:603.32ms
step:495/1775 train_time:298711ms step_avg:603.46ms
step:496/1775 train_time:299385ms step_avg:603.60ms
step:497/1775 train_time:300057ms step_avg:603.74ms
step:498/1775 train_time:300733ms step_avg:603.88ms
step:499/1775 train_time:301428ms step_avg:604.06ms
step:500/1775 train_time:302106ms step_avg:604.21ms
step:500/1775 val_loss:5.3853 val_malbo_loss:5.2509 train_time:302115ms step_avg:604.23ms
step:501/1775 train_time:302640ms step_avg:604.07ms
step:502/1775 train_time:303288ms step_avg:604.16ms
step:503/1775 train_time:303929ms step_avg:604.23ms
step:504/1775 train_time:304577ms step_avg:604.32ms
step:505/1775 train_time:305222ms step_avg:604.40ms
step:506/1775 train_time:305871ms step_avg:604.49ms
step:507/1775 train_time:306515ms step_avg:604.57ms
step:508/1775 train_time:307164ms step_avg:604.65ms
step:509/1775 train_time:307809ms step_avg:604.73ms
step:510/1775 train_time:308527ms step_avg:604.95ms
step:511/1775 train_time:309180ms step_avg:605.05ms
step:512/1775 train_time:309829ms step_avg:605.13ms
step:513/1775 train_time:310471ms step_avg:605.21ms
step:514/1775 train_time:311121ms step_avg:605.29ms
step:515/1775 train_time:311766ms step_avg:605.37ms
step:516/1775 train_time:312416ms step_avg:605.46ms
step:517/1775 train_time:313076ms step_avg:605.56ms
step:518/1775 train_time:313746ms step_avg:605.69ms
step:519/1775 train_time:314417ms step_avg:605.81ms
step:520/1775 train_time:315091ms step_avg:605.94ms
step:521/1775 train_time:315755ms step_avg:606.05ms
step:522/1775 train_time:316428ms step_avg:606.18ms
step:523/1775 train_time:317098ms step_avg:606.31ms
step:524/1775 train_time:317806ms step_avg:606.50ms
step:525/1775 train_time:318617ms step_avg:606.89ms
step:526/1775 train_time:319290ms step_avg:607.02ms
step:527/1775 train_time:319959ms step_avg:607.13ms
step:528/1775 train_time:320634ms step_avg:607.26ms
step:529/1775 train_time:321303ms step_avg:607.38ms
step:530/1775 train_time:321976ms step_avg:607.50ms
step:531/1775 train_time:322645ms step_avg:607.62ms
step:532/1775 train_time:323320ms step_avg:607.74ms
step:533/1775 train_time:323985ms step_avg:607.85ms
step:534/1775 train_time:324628ms step_avg:607.92ms
step:535/1775 train_time:325294ms step_avg:608.03ms
step:536/1775 train_time:325964ms step_avg:608.14ms
step:537/1775 train_time:326632ms step_avg:608.25ms
step:538/1775 train_time:327301ms step_avg:608.37ms
step:539/1775 train_time:327970ms step_avg:608.48ms
step:540/1775 train_time:328659ms step_avg:608.63ms
step:541/1775 train_time:329326ms step_avg:608.74ms
step:542/1775 train_time:330000ms step_avg:608.86ms
step:543/1775 train_time:330664ms step_avg:608.96ms
step:544/1775 train_time:331337ms step_avg:609.08ms
step:545/1775 train_time:332011ms step_avg:609.19ms
step:546/1775 train_time:332690ms step_avg:609.32ms
step:547/1775 train_time:333366ms step_avg:609.44ms
step:548/1775 train_time:334042ms step_avg:609.57ms
step:549/1775 train_time:334711ms step_avg:609.67ms
step:550/1775 train_time:335387ms step_avg:609.79ms
step:551/1775 train_time:336059ms step_avg:609.91ms
step:552/1775 train_time:336736ms step_avg:610.03ms
step:553/1775 train_time:337404ms step_avg:610.13ms
step:554/1775 train_time:338087ms step_avg:610.27ms
step:555/1775 train_time:338763ms step_avg:610.38ms
step:556/1775 train_time:339436ms step_avg:610.50ms
step:557/1775 train_time:340108ms step_avg:610.61ms
step:558/1775 train_time:340782ms step_avg:610.72ms
step:559/1775 train_time:341449ms step_avg:610.82ms
step:560/1775 train_time:342123ms step_avg:610.93ms
step:561/1775 train_time:342790ms step_avg:611.03ms
step:562/1775 train_time:343461ms step_avg:611.14ms
step:563/1775 train_time:344128ms step_avg:611.24ms
step:564/1775 train_time:344800ms step_avg:611.35ms
step:565/1775 train_time:345467ms step_avg:611.45ms
step:566/1775 train_time:346139ms step_avg:611.55ms
step:567/1775 train_time:346806ms step_avg:611.65ms
step:568/1775 train_time:347478ms step_avg:611.76ms
step:569/1775 train_time:348163ms step_avg:611.89ms
step:570/1775 train_time:348835ms step_avg:611.99ms
step:571/1775 train_time:349501ms step_avg:612.09ms
step:572/1775 train_time:350175ms step_avg:612.19ms
step:573/1775 train_time:350994ms step_avg:612.55ms
step:574/1775 train_time:351692ms step_avg:612.70ms
step:575/1775 train_time:352365ms step_avg:612.81ms
step:576/1775 train_time:353036ms step_avg:612.91ms
step:577/1775 train_time:353703ms step_avg:613.00ms
step:578/1775 train_time:354384ms step_avg:613.12ms
step:579/1775 train_time:355051ms step_avg:613.21ms
step:580/1775 train_time:355730ms step_avg:613.33ms
step:581/1775 train_time:356401ms step_avg:613.43ms
step:582/1775 train_time:357079ms step_avg:613.54ms
step:583/1775 train_time:357712ms step_avg:613.57ms
step:584/1775 train_time:358230ms step_avg:613.41ms
step:585/1775 train_time:358770ms step_avg:613.28ms
step:586/1775 train_time:359286ms step_avg:613.12ms
step:587/1775 train_time:359830ms step_avg:613.00ms
step:588/1775 train_time:360355ms step_avg:612.85ms
step:589/1775 train_time:360904ms step_avg:612.74ms
step:590/1775 train_time:361422ms step_avg:612.58ms
step:591/1775 train_time:361964ms step_avg:612.46ms
step:592/1775 train_time:362487ms step_avg:612.31ms
step:593/1775 train_time:363030ms step_avg:612.19ms
step:594/1775 train_time:363606ms step_avg:612.13ms
step:595/1775 train_time:364122ms step_avg:611.97ms
step:596/1775 train_time:364661ms step_avg:611.85ms
step:597/1775 train_time:365177ms step_avg:611.69ms
step:598/1775 train_time:365717ms step_avg:611.57ms
step:599/1775 train_time:366231ms step_avg:611.40ms
step:600/1775 train_time:366806ms step_avg:611.34ms
step:600/1775 val_loss:5.3467 val_malbo_loss:5.2105 train_time:366820ms step_avg:611.37ms
step:601/1775 train_time:367288ms step_avg:611.13ms
step:602/1775 train_time:367882ms step_avg:611.10ms
step:603/1775 train_time:368396ms step_avg:610.94ms
step:604/1775 train_time:368960ms step_avg:610.86ms
step:605/1775 train_time:369629ms step_avg:610.96ms
step:606/1775 train_time:370169ms step_avg:610.84ms
step:607/1775 train_time:370740ms step_avg:610.77ms
step:608/1775 train_time:371331ms step_avg:610.74ms
step:609/1775 train_time:371940ms step_avg:610.74ms
step:610/1775 train_time:372631ms step_avg:610.87ms
step:611/1775 train_time:373201ms step_avg:610.80ms
step:612/1775 train_time:373843ms step_avg:610.85ms
step:613/1775 train_time:374475ms step_avg:610.89ms
step:614/1775 train_time:375050ms step_avg:610.83ms
step:615/1775 train_time:375612ms step_avg:610.75ms
step:616/1775 train_time:376201ms step_avg:610.72ms
step:617/1775 train_time:376852ms step_avg:610.78ms
step:618/1775 train_time:377489ms step_avg:610.82ms
step:619/1775 train_time:378150ms step_avg:610.90ms
step:620/1775 train_time:378804ms step_avg:610.97ms
step:621/1775 train_time:379381ms step_avg:610.92ms
step:622/1775 train_time:380048ms step_avg:611.01ms
step:623/1775 train_time:380724ms step_avg:611.11ms
step:624/1775 train_time:381402ms step_avg:611.22ms
step:625/1775 train_time:382077ms step_avg:611.32ms
step:626/1775 train_time:382758ms step_avg:611.44ms
step:627/1775 train_time:383432ms step_avg:611.53ms
step:628/1775 train_time:384107ms step_avg:611.64ms
step:629/1775 train_time:384780ms step_avg:611.73ms
step:630/1775 train_time:385454ms step_avg:611.83ms
step:631/1775 train_time:386126ms step_avg:611.93ms
step:632/1775 train_time:386817ms step_avg:612.05ms
step:633/1775 train_time:387512ms step_avg:612.18ms
step:634/1775 train_time:388207ms step_avg:612.31ms
step:635/1775 train_time:388917ms step_avg:612.47ms
step:636/1775 train_time:389597ms step_avg:612.57ms
step:637/1775 train_time:390264ms step_avg:612.66ms
step:638/1775 train_time:390992ms step_avg:612.84ms
step:639/1775 train_time:391701ms step_avg:612.99ms
step:640/1775 train_time:392418ms step_avg:613.15ms
step:641/1775 train_time:393118ms step_avg:613.29ms
step:642/1775 train_time:393828ms step_avg:613.44ms
step:643/1775 train_time:394542ms step_avg:613.60ms
step:644/1775 train_time:395281ms step_avg:613.79ms
step:645/1775 train_time:396024ms step_avg:613.99ms
step:646/1775 train_time:396745ms step_avg:614.16ms
step:647/1775 train_time:397455ms step_avg:614.30ms
step:648/1775 train_time:398206ms step_avg:614.52ms
step:649/1775 train_time:398902ms step_avg:614.64ms
step:650/1775 train_time:399619ms step_avg:614.80ms
step:651/1775 train_time:400350ms step_avg:614.98ms
step:652/1775 train_time:401027ms step_avg:615.07ms
step:653/1775 train_time:401761ms step_avg:615.25ms
step:654/1775 train_time:402503ms step_avg:615.45ms
step:655/1775 train_time:403234ms step_avg:615.62ms
step:656/1775 train_time:403937ms step_avg:615.76ms
step:657/1775 train_time:404633ms step_avg:615.88ms
step:658/1775 train_time:405344ms step_avg:616.02ms
step:659/1775 train_time:406000ms step_avg:616.09ms
step:660/1775 train_time:406731ms step_avg:616.26ms
step:661/1775 train_time:407399ms step_avg:616.34ms
step:662/1775 train_time:408117ms step_avg:616.49ms
step:663/1775 train_time:408822ms step_avg:616.63ms
step:664/1775 train_time:409546ms step_avg:616.79ms
step:665/1775 train_time:410216ms step_avg:616.87ms
step:666/1775 train_time:410910ms step_avg:616.98ms
step:667/1775 train_time:411606ms step_avg:617.10ms
step:668/1775 train_time:412240ms step_avg:617.13ms
step:669/1775 train_time:412843ms step_avg:617.10ms
step:670/1775 train_time:413440ms step_avg:617.08ms
step:671/1775 train_time:414034ms step_avg:617.04ms
step:672/1775 train_time:414700ms step_avg:617.11ms
step:673/1775 train_time:415315ms step_avg:617.11ms
step:674/1775 train_time:416002ms step_avg:617.21ms
step:675/1775 train_time:416696ms step_avg:617.33ms
step:676/1775 train_time:417378ms step_avg:617.42ms
step:677/1775 train_time:418055ms step_avg:617.51ms
step:678/1775 train_time:418749ms step_avg:617.62ms
step:679/1775 train_time:419437ms step_avg:617.73ms
step:680/1775 train_time:420056ms step_avg:617.73ms
step:681/1775 train_time:420623ms step_avg:617.65ms
step:682/1775 train_time:421189ms step_avg:617.58ms
step:683/1775 train_time:421670ms step_avg:617.38ms
step:684/1775 train_time:422149ms step_avg:617.18ms
step:685/1775 train_time:422589ms step_avg:616.92ms
step:686/1775 train_time:423067ms step_avg:616.72ms
step:687/1775 train_time:423490ms step_avg:616.43ms
step:688/1775 train_time:423927ms step_avg:616.17ms
step:689/1775 train_time:424368ms step_avg:615.92ms
step:690/1775 train_time:424789ms step_avg:615.64ms
step:691/1775 train_time:425208ms step_avg:615.35ms
step:692/1775 train_time:425644ms step_avg:615.09ms
step:693/1775 train_time:426085ms step_avg:614.84ms
step:694/1775 train_time:426540ms step_avg:614.61ms
step:695/1775 train_time:427052ms step_avg:614.46ms
step:696/1775 train_time:427574ms step_avg:614.33ms
step:697/1775 train_time:428255ms step_avg:614.43ms
step:698/1775 train_time:428838ms step_avg:614.38ms
step:699/1775 train_time:429403ms step_avg:614.31ms
step:700/1775 train_time:429903ms step_avg:614.15ms
step:700/1775 val_loss:5.1543 val_malbo_loss:5.0138 train_time:429915ms step_avg:614.16ms
step:701/1775 train_time:430477ms step_avg:614.09ms
step:702/1775 train_time:431137ms step_avg:614.16ms
step:703/1775 train_time:431801ms step_avg:614.23ms
step:704/1775 train_time:432475ms step_avg:614.31ms
step:705/1775 train_time:433145ms step_avg:614.39ms
step:706/1775 train_time:433818ms step_avg:614.47ms
step:707/1775 train_time:434486ms step_avg:614.55ms
step:708/1775 train_time:435161ms step_avg:614.63ms
step:709/1775 train_time:435802ms step_avg:614.67ms
step:710/1775 train_time:436460ms step_avg:614.73ms
step:711/1775 train_time:437129ms step_avg:614.81ms
step:712/1775 train_time:437805ms step_avg:614.89ms
step:713/1775 train_time:438475ms step_avg:614.97ms
step:714/1775 train_time:439170ms step_avg:615.08ms
step:715/1775 train_time:439834ms step_avg:615.15ms
step:716/1775 train_time:440531ms step_avg:615.27ms
step:717/1775 train_time:441196ms step_avg:615.34ms
step:718/1775 train_time:441863ms step_avg:615.41ms
step:719/1775 train_time:442532ms step_avg:615.48ms
step:720/1775 train_time:443205ms step_avg:615.56ms
step:721/1775 train_time:443875ms step_avg:615.64ms
step:722/1775 train_time:444551ms step_avg:615.72ms
step:723/1775 train_time:445228ms step_avg:615.81ms
step:724/1775 train_time:445901ms step_avg:615.89ms
step:725/1775 train_time:446572ms step_avg:615.96ms
step:726/1775 train_time:447246ms step_avg:616.04ms
step:727/1775 train_time:447916ms step_avg:616.12ms
step:728/1775 train_time:448590ms step_avg:616.19ms
step:729/1775 train_time:449259ms step_avg:616.27ms
step:730/1775 train_time:449934ms step_avg:616.35ms
step:731/1775 train_time:450605ms step_avg:616.42ms
step:732/1775 train_time:451279ms step_avg:616.50ms
step:733/1775 train_time:451948ms step_avg:616.57ms
step:734/1775 train_time:452622ms step_avg:616.65ms
step:735/1775 train_time:453222ms step_avg:616.63ms
step:736/1775 train_time:453898ms step_avg:616.71ms
step:737/1775 train_time:454568ms step_avg:616.78ms
step:738/1775 train_time:455189ms step_avg:616.79ms
step:739/1775 train_time:455823ms step_avg:616.81ms
step:740/1775 train_time:456468ms step_avg:616.85ms
step:741/1775 train_time:457111ms step_avg:616.88ms
step:742/1775 train_time:457757ms step_avg:616.92ms
step:743/1775 train_time:458399ms step_avg:616.96ms
step:744/1775 train_time:459044ms step_avg:616.99ms
step:745/1775 train_time:459685ms step_avg:617.03ms
step:746/1775 train_time:460340ms step_avg:617.08ms
step:747/1775 train_time:460991ms step_avg:617.12ms
step:748/1775 train_time:461635ms step_avg:617.16ms
step:749/1775 train_time:462275ms step_avg:617.19ms
step:750/1775 train_time:462922ms step_avg:617.23ms
step:751/1775 train_time:463563ms step_avg:617.26ms
step:752/1775 train_time:464208ms step_avg:617.30ms
step:753/1775 train_time:464848ms step_avg:617.33ms
step:754/1775 train_time:465492ms step_avg:617.36ms
step:755/1775 train_time:466131ms step_avg:617.39ms
step:756/1775 train_time:466776ms step_avg:617.43ms
step:757/1775 train_time:467414ms step_avg:617.46ms
step:758/1775 train_time:468061ms step_avg:617.49ms
step:759/1775 train_time:468701ms step_avg:617.52ms
step:760/1775 train_time:469343ms step_avg:617.56ms
step:761/1775 train_time:469983ms step_avg:617.59ms
step:762/1775 train_time:470648ms step_avg:617.65ms
step:763/1775 train_time:471288ms step_avg:617.68ms
step:764/1775 train_time:472074ms step_avg:617.90ms
step:765/1775 train_time:472749ms step_avg:617.97ms
step:766/1775 train_time:473427ms step_avg:618.05ms
step:767/1775 train_time:474104ms step_avg:618.13ms
step:768/1775 train_time:474779ms step_avg:618.20ms
step:769/1775 train_time:475454ms step_avg:618.28ms
step:770/1775 train_time:476127ms step_avg:618.35ms
step:771/1775 train_time:476787ms step_avg:618.40ms
step:772/1775 train_time:477455ms step_avg:618.47ms
step:773/1775 train_time:478126ms step_avg:618.53ms
step:774/1775 train_time:478799ms step_avg:618.60ms
step:775/1775 train_time:479431ms step_avg:618.62ms
step:776/1775 train_time:480052ms step_avg:618.62ms
step:777/1775 train_time:480715ms step_avg:618.68ms
step:778/1775 train_time:481366ms step_avg:618.72ms
step:779/1775 train_time:482014ms step_avg:618.76ms
step:780/1775 train_time:482685ms step_avg:618.83ms
step:781/1775 train_time:483357ms step_avg:618.90ms
step:782/1775 train_time:484003ms step_avg:618.93ms
step:783/1775 train_time:484642ms step_avg:618.95ms
step:784/1775 train_time:485285ms step_avg:618.99ms
step:785/1775 train_time:485931ms step_avg:619.02ms
step:786/1775 train_time:486579ms step_avg:619.06ms
step:787/1775 train_time:487223ms step_avg:619.09ms
step:788/1775 train_time:487870ms step_avg:619.12ms
step:789/1775 train_time:488515ms step_avg:619.16ms
step:790/1775 train_time:489166ms step_avg:619.20ms
step:791/1775 train_time:489808ms step_avg:619.23ms
step:792/1775 train_time:490458ms step_avg:619.26ms
step:793/1775 train_time:491104ms step_avg:619.30ms
step:794/1775 train_time:491750ms step_avg:619.33ms
step:795/1775 train_time:492392ms step_avg:619.36ms
step:796/1775 train_time:493036ms step_avg:619.39ms
step:797/1775 train_time:493678ms step_avg:619.42ms
step:798/1775 train_time:494325ms step_avg:619.46ms
step:799/1775 train_time:494966ms step_avg:619.48ms
step:800/1775 train_time:495620ms step_avg:619.52ms
step:800/1775 val_loss:5.0298 val_malbo_loss:4.8874 train_time:495630ms step_avg:619.54ms
step:801/1775 train_time:496154ms step_avg:619.42ms
step:802/1775 train_time:496803ms step_avg:619.46ms
step:803/1775 train_time:497453ms step_avg:619.49ms
step:804/1775 train_time:498102ms step_avg:619.53ms
step:805/1775 train_time:498771ms step_avg:619.59ms
step:806/1775 train_time:499431ms step_avg:619.64ms
step:807/1775 train_time:500111ms step_avg:619.72ms
step:808/1775 train_time:500806ms step_avg:619.81ms
step:809/1775 train_time:501492ms step_avg:619.89ms
step:810/1775 train_time:502175ms step_avg:619.97ms
step:811/1775 train_time:502853ms step_avg:620.04ms
step:812/1775 train_time:503549ms step_avg:620.13ms
step:813/1775 train_time:504222ms step_avg:620.20ms
step:814/1775 train_time:504892ms step_avg:620.26ms
step:815/1775 train_time:505562ms step_avg:620.32ms
step:816/1775 train_time:506238ms step_avg:620.39ms
step:817/1775 train_time:506907ms step_avg:620.45ms
step:818/1775 train_time:507579ms step_avg:620.51ms
step:819/1775 train_time:508248ms step_avg:620.57ms
step:820/1775 train_time:508924ms step_avg:620.64ms
step:821/1775 train_time:509592ms step_avg:620.70ms
step:822/1775 train_time:510267ms step_avg:620.76ms
step:823/1775 train_time:510937ms step_avg:620.82ms
step:824/1775 train_time:511617ms step_avg:620.89ms
step:825/1775 train_time:512286ms step_avg:620.95ms
step:826/1775 train_time:512958ms step_avg:621.01ms
step:827/1775 train_time:513627ms step_avg:621.07ms
step:828/1775 train_time:514300ms step_avg:621.14ms
step:829/1775 train_time:514970ms step_avg:621.19ms
step:830/1775 train_time:515643ms step_avg:621.26ms
step:831/1775 train_time:516313ms step_avg:621.32ms
step:832/1775 train_time:516990ms step_avg:621.38ms
step:833/1775 train_time:517659ms step_avg:621.44ms
step:834/1775 train_time:518332ms step_avg:621.50ms
step:835/1775 train_time:519001ms step_avg:621.56ms
step:836/1775 train_time:519675ms step_avg:621.62ms
step:837/1775 train_time:520345ms step_avg:621.68ms
step:838/1775 train_time:521020ms step_avg:621.74ms
step:839/1775 train_time:521689ms step_avg:621.80ms
step:840/1775 train_time:522363ms step_avg:621.86ms
step:841/1775 train_time:523036ms step_avg:621.92ms
step:842/1775 train_time:523708ms step_avg:621.98ms
step:843/1775 train_time:524383ms step_avg:622.04ms
step:844/1775 train_time:525058ms step_avg:622.11ms
step:845/1775 train_time:525726ms step_avg:622.16ms
step:846/1775 train_time:526530ms step_avg:622.38ms
step:847/1775 train_time:527199ms step_avg:622.43ms
step:848/1775 train_time:527871ms step_avg:622.49ms
step:849/1775 train_time:528539ms step_avg:622.54ms
step:850/1775 train_time:529207ms step_avg:622.60ms
step:851/1775 train_time:529837ms step_avg:622.61ms
step:852/1775 train_time:530517ms step_avg:622.67ms
step:853/1775 train_time:531188ms step_avg:622.73ms
step:854/1775 train_time:531865ms step_avg:622.79ms
step:855/1775 train_time:532535ms step_avg:622.85ms
step:856/1775 train_time:533210ms step_avg:622.91ms
step:857/1775 train_time:533874ms step_avg:622.96ms
step:858/1775 train_time:534511ms step_avg:622.97ms
step:859/1775 train_time:535048ms step_avg:622.87ms
step:860/1775 train_time:535538ms step_avg:622.72ms
step:861/1775 train_time:536016ms step_avg:622.55ms
step:862/1775 train_time:536456ms step_avg:622.34ms
step:863/1775 train_time:536898ms step_avg:622.13ms
step:864/1775 train_time:537316ms step_avg:621.89ms
step:865/1775 train_time:537737ms step_avg:621.66ms
step:866/1775 train_time:538179ms step_avg:621.45ms
step:867/1775 train_time:538607ms step_avg:621.23ms
step:868/1775 train_time:539041ms step_avg:621.02ms
step:869/1775 train_time:539444ms step_avg:620.76ms
step:870/1775 train_time:539897ms step_avg:620.57ms
step:871/1775 train_time:540354ms step_avg:620.38ms
step:872/1775 train_time:540809ms step_avg:620.19ms
step:873/1775 train_time:541271ms step_avg:620.01ms
step:874/1775 train_time:541725ms step_avg:619.82ms
step:875/1775 train_time:542147ms step_avg:619.60ms
step:876/1775 train_time:542601ms step_avg:619.41ms
step:877/1775 train_time:543027ms step_avg:619.19ms
step:878/1775 train_time:543440ms step_avg:618.95ms
step:879/1775 train_time:543870ms step_avg:618.74ms
step:880/1775 train_time:544323ms step_avg:618.55ms
step:881/1775 train_time:544790ms step_avg:618.38ms
step:882/1775 train_time:545250ms step_avg:618.20ms
step:883/1775 train_time:545710ms step_avg:618.02ms
step:884/1775 train_time:546250ms step_avg:617.93ms
step:885/1775 train_time:546805ms step_avg:617.86ms
step:886/1775 train_time:547354ms step_avg:617.78ms
step:887/1775 train_time:547877ms step_avg:617.67ms
step:888/1775 train_time:548335ms step_avg:617.49ms
step:889/1775 train_time:548777ms step_avg:617.30ms
step:890/1775 train_time:549179ms step_avg:617.05ms
step:891/1775 train_time:549590ms step_avg:616.82ms
step:892/1775 train_time:550040ms step_avg:616.64ms
step:893/1775 train_time:550493ms step_avg:616.45ms
step:894/1775 train_time:550950ms step_avg:616.28ms
step:895/1775 train_time:551435ms step_avg:616.13ms
step:896/1775 train_time:551903ms step_avg:615.96ms
step:897/1775 train_time:552361ms step_avg:615.79ms
step:898/1775 train_time:552806ms step_avg:615.60ms
step:899/1775 train_time:553262ms step_avg:615.42ms
step:900/1775 train_time:553707ms step_avg:615.23ms
step:900/1775 val_loss:4.9069 val_malbo_loss:4.7477 train_time:553718ms step_avg:615.24ms
step:901/1775 train_time:554168ms step_avg:615.06ms
step:902/1775 train_time:554618ms step_avg:614.88ms
step:903/1775 train_time:555069ms step_avg:614.69ms
step:904/1775 train_time:555529ms step_avg:614.52ms
step:905/1775 train_time:555989ms step_avg:614.35ms
step:906/1775 train_time:556438ms step_avg:614.17ms
step:907/1775 train_time:556879ms step_avg:613.98ms
step:908/1775 train_time:557356ms step_avg:613.83ms
step:909/1775 train_time:557892ms step_avg:613.74ms
step:910/1775 train_time:558409ms step_avg:613.64ms
step:911/1775 train_time:558943ms step_avg:613.55ms
step:912/1775 train_time:559446ms step_avg:613.43ms
step:913/1775 train_time:559976ms step_avg:613.34ms
step:914/1775 train_time:560450ms step_avg:613.18ms
step:915/1775 train_time:560893ms step_avg:613.00ms
step:916/1775 train_time:561357ms step_avg:612.83ms
step:917/1775 train_time:561859ms step_avg:612.71ms
step:918/1775 train_time:562303ms step_avg:612.53ms
step:919/1775 train_time:562776ms step_avg:612.38ms
step:920/1775 train_time:563256ms step_avg:612.24ms
step:921/1775 train_time:563716ms step_avg:612.07ms
step:922/1775 train_time:564189ms step_avg:611.92ms
step:923/1775 train_time:564709ms step_avg:611.82ms
step:924/1775 train_time:565174ms step_avg:611.66ms
step:925/1775 train_time:565635ms step_avg:611.50ms
step:926/1775 train_time:566098ms step_avg:611.34ms
step:927/1775 train_time:566540ms step_avg:611.15ms
step:928/1775 train_time:567009ms step_avg:611.00ms
step:929/1775 train_time:567481ms step_avg:610.85ms
step:930/1775 train_time:567959ms step_avg:610.71ms
step:931/1775 train_time:568427ms step_avg:610.55ms
step:932/1775 train_time:569397ms step_avg:610.94ms
step:933/1775 train_time:569871ms step_avg:610.79ms
step:934/1775 train_time:570341ms step_avg:610.64ms
step:935/1775 train_time:570806ms step_avg:610.49ms
step:936/1775 train_time:571258ms step_avg:610.32ms
step:937/1775 train_time:571730ms step_avg:610.17ms
step:938/1775 train_time:572187ms step_avg:610.01ms
step:939/1775 train_time:572651ms step_avg:609.85ms
step:940/1775 train_time:573109ms step_avg:609.69ms
step:941/1775 train_time:573563ms step_avg:609.52ms
step:942/1775 train_time:574032ms step_avg:609.38ms
step:943/1775 train_time:574460ms step_avg:609.18ms
step:944/1775 train_time:574904ms step_avg:609.01ms
step:945/1775 train_time:575408ms step_avg:608.90ms
step:946/1775 train_time:575859ms step_avg:608.73ms
step:947/1775 train_time:576306ms step_avg:608.56ms
step:948/1775 train_time:576776ms step_avg:608.41ms
step:949/1775 train_time:577274ms step_avg:608.30ms
step:950/1775 train_time:577742ms step_avg:608.15ms
step:951/1775 train_time:578220ms step_avg:608.01ms
step:952/1775 train_time:578679ms step_avg:607.86ms
step:953/1775 train_time:579123ms step_avg:607.68ms
step:954/1775 train_time:579550ms step_avg:607.49ms
step:955/1775 train_time:579989ms step_avg:607.32ms
step:956/1775 train_time:580515ms step_avg:607.23ms
step:957/1775 train_time:581058ms step_avg:607.17ms
step:958/1775 train_time:581591ms step_avg:607.09ms
step:959/1775 train_time:582096ms step_avg:606.98ms
step:960/1775 train_time:582621ms step_avg:606.90ms
step:961/1775 train_time:583112ms step_avg:606.78ms
step:962/1775 train_time:583569ms step_avg:606.62ms
step:963/1775 train_time:584011ms step_avg:606.45ms
step:964/1775 train_time:584486ms step_avg:606.31ms
step:965/1775 train_time:584964ms step_avg:606.18ms
step:966/1775 train_time:585433ms step_avg:606.04ms
step:967/1775 train_time:585904ms step_avg:605.90ms
step:968/1775 train_time:586433ms step_avg:605.82ms
step:969/1775 train_time:586928ms step_avg:605.71ms
step:970/1775 train_time:587452ms step_avg:605.62ms
step:971/1775 train_time:587920ms step_avg:605.48ms
step:972/1775 train_time:588367ms step_avg:605.32ms
step:973/1775 train_time:588862ms step_avg:605.20ms
step:974/1775 train_time:589397ms step_avg:605.13ms
step:975/1775 train_time:589955ms step_avg:605.08ms
step:976/1775 train_time:590484ms step_avg:605.00ms
step:977/1775 train_time:591028ms step_avg:604.94ms
step:978/1775 train_time:591530ms step_avg:604.84ms
step:979/1775 train_time:591994ms step_avg:604.69ms
step:980/1775 train_time:592467ms step_avg:604.56ms
step:981/1775 train_time:592937ms step_avg:604.42ms
step:982/1775 train_time:593467ms step_avg:604.35ms
step:983/1775 train_time:593994ms step_avg:604.27ms
step:984/1775 train_time:594540ms step_avg:604.21ms
step:985/1775 train_time:595078ms step_avg:604.14ms
step:986/1775 train_time:595617ms step_avg:604.07ms
step:987/1775 train_time:596148ms step_avg:604.00ms
step:988/1775 train_time:596680ms step_avg:603.93ms
step:989/1775 train_time:597173ms step_avg:603.81ms
step:990/1775 train_time:597640ms step_avg:603.68ms
step:991/1775 train_time:598086ms step_avg:603.52ms
step:992/1775 train_time:598556ms step_avg:603.38ms
step:993/1775 train_time:599019ms step_avg:603.24ms
step:994/1775 train_time:599499ms step_avg:603.12ms
step:995/1775 train_time:599961ms step_avg:602.98ms
step:996/1775 train_time:600418ms step_avg:602.83ms
step:997/1775 train_time:600867ms step_avg:602.68ms
step:998/1775 train_time:601317ms step_avg:602.52ms
step:999/1775 train_time:601757ms step_avg:602.36ms
step:1000/1775 train_time:602192ms step_avg:602.19ms
step:1000/1775 val_loss:4.7832 val_malbo_loss:4.6262 train_time:602206ms step_avg:602.21ms
step:1001/1775 train_time:602649ms step_avg:602.05ms
step:1002/1775 train_time:603088ms step_avg:601.88ms
step:1003/1775 train_time:603514ms step_avg:601.71ms
step:1004/1775 train_time:603954ms step_avg:601.55ms
step:1005/1775 train_time:604400ms step_avg:601.39ms
step:1006/1775 train_time:604847ms step_avg:601.24ms
step:1007/1775 train_time:605276ms step_avg:601.07ms
step:1008/1775 train_time:605756ms step_avg:600.95ms
step:1009/1775 train_time:606276ms step_avg:600.87ms
step:1010/1775 train_time:606800ms step_avg:600.79ms
step:1011/1775 train_time:607381ms step_avg:600.77ms
step:1012/1775 train_time:607956ms step_avg:600.75ms
step:1013/1775 train_time:608386ms step_avg:600.58ms
step:1014/1775 train_time:608857ms step_avg:600.45ms
step:1015/1775 train_time:609295ms step_avg:600.29ms
step:1016/1775 train_time:609746ms step_avg:600.14ms
step:1017/1775 train_time:610149ms step_avg:599.95ms
step:1018/1775 train_time:610591ms step_avg:599.80ms
step:1019/1775 train_time:611059ms step_avg:599.67ms
step:1020/1775 train_time:611511ms step_avg:599.52ms
step:1021/1775 train_time:611958ms step_avg:599.37ms
step:1022/1775 train_time:612372ms step_avg:599.19ms
step:1023/1775 train_time:612818ms step_avg:599.04ms
step:1024/1775 train_time:613239ms step_avg:598.87ms
step:1025/1775 train_time:613688ms step_avg:598.72ms
step:1026/1775 train_time:614128ms step_avg:598.57ms
step:1027/1775 train_time:614551ms step_avg:598.39ms
step:1028/1775 train_time:614975ms step_avg:598.22ms
step:1029/1775 train_time:615445ms step_avg:598.10ms
step:1030/1775 train_time:615965ms step_avg:598.02ms
step:1031/1775 train_time:616486ms step_avg:597.95ms
step:1032/1775 train_time:617001ms step_avg:597.87ms
step:1033/1775 train_time:617531ms step_avg:597.80ms
step:1034/1775 train_time:618072ms step_avg:597.75ms
step:1035/1775 train_time:618598ms step_avg:597.68ms
step:1036/1775 train_time:619124ms step_avg:597.61ms
step:1037/1775 train_time:619665ms step_avg:597.56ms
step:1038/1775 train_time:620181ms step_avg:597.48ms
step:1039/1775 train_time:620702ms step_avg:597.40ms
step:1040/1775 train_time:621219ms step_avg:597.33ms
step:1041/1775 train_time:621747ms step_avg:597.26ms
step:1042/1775 train_time:622199ms step_avg:597.12ms
step:1043/1775 train_time:622713ms step_avg:597.04ms
step:1044/1775 train_time:623197ms step_avg:596.93ms
step:1045/1775 train_time:623710ms step_avg:596.85ms
step:1046/1775 train_time:624176ms step_avg:596.73ms
step:1047/1775 train_time:624704ms step_avg:596.66ms
step:1048/1775 train_time:625231ms step_avg:596.59ms
step:1049/1775 train_time:625678ms step_avg:596.45ms
step:1050/1775 train_time:626113ms step_avg:596.30ms
step:1051/1775 train_time:626548ms step_avg:596.14ms
step:1052/1775 train_time:626973ms step_avg:595.98ms
step:1053/1775 train_time:627404ms step_avg:595.83ms
step:1054/1775 train_time:627831ms step_avg:595.67ms
step:1055/1775 train_time:628261ms step_avg:595.51ms
step:1056/1775 train_time:628675ms step_avg:595.34ms
step:1057/1775 train_time:629093ms step_avg:595.17ms
step:1058/1775 train_time:629500ms step_avg:594.99ms
step:1059/1775 train_time:629915ms step_avg:594.82ms
step:1060/1775 train_time:630329ms step_avg:594.65ms
step:1061/1775 train_time:630755ms step_avg:594.49ms
step:1062/1775 train_time:631185ms step_avg:594.34ms
step:1063/1775 train_time:631614ms step_avg:594.18ms
step:1064/1775 train_time:632029ms step_avg:594.01ms
step:1065/1775 train_time:632459ms step_avg:593.86ms
step:1066/1775 train_time:632885ms step_avg:593.70ms
step:1067/1775 train_time:633317ms step_avg:593.55ms
step:1068/1775 train_time:633745ms step_avg:593.39ms
step:1069/1775 train_time:634168ms step_avg:593.23ms
step:1070/1775 train_time:634599ms step_avg:593.08ms
step:1071/1775 train_time:635028ms step_avg:592.93ms
step:1072/1775 train_time:635531ms step_avg:592.85ms
step:1073/1775 train_time:636048ms step_avg:592.78ms
step:1074/1775 train_time:636566ms step_avg:592.71ms
step:1075/1775 train_time:637080ms step_avg:592.63ms
step:1076/1775 train_time:637573ms step_avg:592.54ms
step:1077/1775 train_time:638017ms step_avg:592.40ms
step:1078/1775 train_time:638461ms step_avg:592.26ms
step:1079/1775 train_time:638879ms step_avg:592.10ms
step:1080/1775 train_time:639360ms step_avg:592.00ms
step:1081/1775 train_time:639862ms step_avg:591.92ms
step:1082/1775 train_time:640378ms step_avg:591.85ms
step:1083/1775 train_time:640887ms step_avg:591.77ms
step:1084/1775 train_time:641452ms step_avg:591.75ms
step:1085/1775 train_time:641954ms step_avg:591.66ms
step:1086/1775 train_time:642456ms step_avg:591.58ms
step:1087/1775 train_time:642952ms step_avg:591.49ms
step:1088/1775 train_time:643475ms step_avg:591.43ms
step:1089/1775 train_time:643961ms step_avg:591.33ms
step:1090/1775 train_time:644466ms step_avg:591.25ms
step:1091/1775 train_time:644954ms step_avg:591.16ms
step:1092/1775 train_time:645475ms step_avg:591.09ms
step:1093/1775 train_time:645938ms step_avg:590.98ms
step:1094/1775 train_time:646443ms step_avg:590.90ms
step:1095/1775 train_time:646940ms step_avg:590.81ms
step:1096/1775 train_time:647422ms step_avg:590.71ms
step:1097/1775 train_time:647907ms step_avg:590.62ms
step:1098/1775 train_time:648412ms step_avg:590.54ms
step:1099/1775 train_time:648896ms step_avg:590.44ms
step:1100/1775 train_time:649380ms step_avg:590.35ms
step:1100/1775 val_loss:4.6248 val_malbo_loss:4.4737 train_time:649391ms step_avg:590.36ms
step:1101/1775 train_time:649888ms step_avg:590.27ms
step:1102/1775 train_time:650535ms step_avg:590.32ms
step:1103/1775 train_time:651181ms step_avg:590.37ms
step:1104/1775 train_time:651830ms step_avg:590.43ms
step:1105/1775 train_time:652476ms step_avg:590.48ms
step:1106/1775 train_time:653126ms step_avg:590.53ms
step:1107/1775 train_time:653774ms step_avg:590.58ms
step:1108/1775 train_time:654423ms step_avg:590.63ms
step:1109/1775 train_time:655069ms step_avg:590.68ms
step:1110/1775 train_time:655742ms step_avg:590.76ms
step:1111/1775 train_time:656392ms step_avg:590.81ms
step:1112/1775 train_time:657043ms step_avg:590.87ms
step:1113/1775 train_time:657691ms step_avg:590.92ms
step:1114/1775 train_time:658340ms step_avg:590.97ms
step:1115/1775 train_time:658741ms step_avg:590.80ms
step:1116/1775 train_time:659119ms step_avg:590.61ms
step:1117/1775 train_time:659501ms step_avg:590.42ms
step:1118/1775 train_time:659878ms step_avg:590.23ms
step:1119/1775 train_time:660249ms step_avg:590.04ms
step:1120/1775 train_time:660629ms step_avg:589.85ms
step:1121/1775 train_time:661020ms step_avg:589.67ms
step:1122/1775 train_time:661428ms step_avg:589.51ms
step:1123/1775 train_time:661812ms step_avg:589.32ms
step:1124/1775 train_time:662339ms step_avg:589.27ms
step:1125/1775 train_time:662970ms step_avg:589.31ms
step:1126/1775 train_time:663632ms step_avg:589.37ms
step:1127/1775 train_time:664338ms step_avg:589.47ms
step:1128/1775 train_time:665030ms step_avg:589.57ms
step:1129/1775 train_time:665675ms step_avg:589.61ms
step:1130/1775 train_time:666273ms step_avg:589.62ms
step:1131/1775 train_time:666930ms step_avg:589.68ms
step:1132/1775 train_time:667581ms step_avg:589.74ms
step:1133/1775 train_time:668218ms step_avg:589.78ms
step:1134/1775 train_time:668858ms step_avg:589.82ms
step:1135/1775 train_time:669504ms step_avg:589.87ms
step:1136/1775 train_time:670154ms step_avg:589.92ms
step:1137/1775 train_time:670801ms step_avg:589.97ms
step:1138/1775 train_time:671452ms step_avg:590.03ms
step:1139/1775 train_time:672096ms step_avg:590.08ms
step:1140/1775 train_time:672747ms step_avg:590.13ms
step:1141/1775 train_time:673393ms step_avg:590.18ms
step:1142/1775 train_time:674043ms step_avg:590.23ms
step:1143/1775 train_time:674687ms step_avg:590.28ms
step:1144/1775 train_time:675340ms step_avg:590.33ms
step:1145/1775 train_time:675752ms step_avg:590.18ms
step:1146/1775 train_time:676137ms step_avg:590.00ms
step:1147/1775 train_time:676589ms step_avg:589.88ms
step:1148/1775 train_time:677240ms step_avg:589.93ms
step:1149/1775 train_time:677883ms step_avg:589.98ms
step:1150/1775 train_time:678549ms step_avg:590.04ms
step:1151/1775 train_time:679195ms step_avg:590.09ms
step:1152/1775 train_time:679845ms step_avg:590.14ms
step:1153/1775 train_time:680497ms step_avg:590.20ms
step:1154/1775 train_time:681041ms step_avg:590.16ms
step:1155/1775 train_time:681428ms step_avg:589.98ms
step:1156/1775 train_time:681808ms step_avg:589.80ms
step:1157/1775 train_time:682388ms step_avg:589.79ms
step:1158/1775 train_time:682859ms step_avg:589.69ms
step:1159/1775 train_time:683385ms step_avg:589.63ms
step:1160/1775 train_time:683971ms step_avg:589.63ms
step:1161/1775 train_time:684549ms step_avg:589.62ms
step:1162/1775 train_time:685132ms step_avg:589.61ms
step:1163/1775 train_time:685710ms step_avg:589.60ms
step:1164/1775 train_time:686299ms step_avg:589.60ms
step:1165/1775 train_time:686628ms step_avg:589.38ms
step:1166/1775 train_time:686934ms step_avg:589.14ms
step:1167/1775 train_time:687242ms step_avg:588.90ms
step:1168/1775 train_time:687705ms step_avg:588.79ms
step:1169/1775 train_time:688290ms step_avg:588.79ms
step:1170/1775 train_time:688871ms step_avg:588.78ms
step:1171/1775 train_time:689465ms step_avg:588.78ms
step:1172/1775 train_time:689814ms step_avg:588.58ms
step:1173/1775 train_time:690117ms step_avg:588.33ms
step:1174/1775 train_time:690422ms step_avg:588.09ms
step:1175/1775 train_time:690726ms step_avg:587.85ms
step:1176/1775 train_time:691035ms step_avg:587.61ms
step:1177/1775 train_time:691346ms step_avg:587.38ms
step:1178/1775 train_time:691664ms step_avg:587.15ms
step:1179/1775 train_time:691977ms step_avg:586.92ms
step:1180/1775 train_time:692290ms step_avg:586.69ms
step:1181/1775 train_time:692596ms step_avg:586.45ms
step:1182/1775 train_time:692910ms step_avg:586.22ms
step:1183/1775 train_time:693229ms step_avg:585.99ms
step:1184/1775 train_time:693535ms step_avg:585.76ms
step:1185/1775 train_time:693928ms step_avg:585.59ms
step:1186/1775 train_time:694494ms step_avg:585.58ms
step:1187/1775 train_time:695041ms step_avg:585.54ms
step:1188/1775 train_time:695577ms step_avg:585.50ms
step:1189/1775 train_time:696153ms step_avg:585.49ms
step:1190/1775 train_time:696733ms step_avg:585.49ms
step:1191/1775 train_time:697337ms step_avg:585.51ms
step:1192/1775 train_time:697878ms step_avg:585.47ms
step:1193/1775 train_time:698234ms step_avg:585.28ms
step:1194/1775 train_time:698791ms step_avg:585.25ms
step:1195/1775 train_time:699366ms step_avg:585.24ms
step:1196/1775 train_time:699946ms step_avg:585.24ms
step:1197/1775 train_time:700540ms step_avg:585.25ms
step:1198/1775 train_time:701119ms step_avg:585.24ms
step:1199/1775 train_time:701693ms step_avg:585.23ms
step:1200/1775 train_time:702273ms step_avg:585.23ms
step:1200/1775 val_loss:4.5037 val_malbo_loss:4.3658 train_time:702281ms step_avg:585.23ms
step:1201/1775 train_time:702772ms step_avg:585.16ms
step:1202/1775 train_time:703280ms step_avg:585.09ms
step:1203/1775 train_time:703788ms step_avg:585.03ms
step:1204/1775 train_time:704387ms step_avg:585.04ms
step:1205/1775 train_time:704911ms step_avg:584.99ms
step:1206/1775 train_time:705474ms step_avg:584.97ms
step:1207/1775 train_time:706047ms step_avg:584.96ms
step:1208/1775 train_time:706611ms step_avg:584.94ms
step:1209/1775 train_time:707233ms step_avg:584.97ms
step:1210/1775 train_time:707823ms step_avg:584.98ms
step:1211/1775 train_time:708371ms step_avg:584.95ms
step:1212/1775 train_time:708976ms step_avg:584.96ms
step:1213/1775 train_time:709503ms step_avg:584.92ms
step:1214/1775 train_time:710089ms step_avg:584.92ms
step:1215/1775 train_time:710547ms step_avg:584.81ms
step:1216/1775 train_time:710935ms step_avg:584.65ms
step:1217/1775 train_time:711310ms step_avg:584.48ms
step:1218/1775 train_time:711730ms step_avg:584.34ms
step:1219/1775 train_time:712318ms step_avg:584.35ms
step:1220/1775 train_time:712939ms step_avg:584.38ms
step:1221/1775 train_time:713471ms step_avg:584.33ms
step:1222/1775 train_time:714004ms step_avg:584.29ms
step:1223/1775 train_time:714567ms step_avg:584.27ms
step:1224/1775 train_time:715161ms step_avg:584.28ms
step:1225/1775 train_time:715700ms step_avg:584.24ms
step:1226/1775 train_time:716237ms step_avg:584.21ms
step:1227/1775 train_time:716792ms step_avg:584.18ms
step:1228/1775 train_time:717431ms step_avg:584.23ms
step:1229/1775 train_time:717990ms step_avg:584.21ms
step:1230/1775 train_time:718579ms step_avg:584.21ms
step:1231/1775 train_time:719158ms step_avg:584.21ms
step:1232/1775 train_time:719759ms step_avg:584.22ms
step:1233/1775 train_time:720364ms step_avg:584.24ms
step:1234/1775 train_time:720962ms step_avg:584.25ms
step:1235/1775 train_time:721680ms step_avg:584.36ms
step:1236/1775 train_time:722224ms step_avg:584.32ms
step:1237/1775 train_time:722784ms step_avg:584.30ms
step:1238/1775 train_time:723363ms step_avg:584.30ms
step:1239/1775 train_time:723884ms step_avg:584.25ms
step:1240/1775 train_time:724374ms step_avg:584.17ms
step:1241/1775 train_time:724931ms step_avg:584.15ms
step:1242/1775 train_time:725538ms step_avg:584.17ms
step:1243/1775 train_time:725964ms step_avg:584.04ms
step:1244/1775 train_time:726566ms step_avg:584.06ms
step:1245/1775 train_time:727017ms step_avg:583.95ms
step:1246/1775 train_time:727583ms step_avg:583.93ms
step:1247/1775 train_time:728144ms step_avg:583.92ms
step:1248/1775 train_time:728742ms step_avg:583.93ms
step:1249/1775 train_time:729197ms step_avg:583.82ms
step:1250/1775 train_time:729754ms step_avg:583.80ms
step:1251/1775 train_time:730320ms step_avg:583.79ms
step:1252/1775 train_time:730902ms step_avg:583.79ms
step:1253/1775 train_time:731405ms step_avg:583.72ms
step:1254/1775 train_time:731955ms step_avg:583.70ms
step:1255/1775 train_time:732520ms step_avg:583.68ms
step:1256/1775 train_time:733032ms step_avg:583.62ms
step:1257/1775 train_time:733580ms step_avg:583.60ms
step:1258/1775 train_time:734172ms step_avg:583.60ms
step:1259/1775 train_time:734772ms step_avg:583.62ms
step:1260/1775 train_time:735377ms step_avg:583.63ms
step:1261/1775 train_time:735935ms step_avg:583.61ms
step:1262/1775 train_time:736464ms step_avg:583.57ms
step:1263/1775 train_time:736974ms step_avg:583.51ms
step:1264/1775 train_time:737536ms step_avg:583.49ms
step:1265/1775 train_time:738095ms step_avg:583.47ms
step:1266/1775 train_time:738713ms step_avg:583.50ms
step:1267/1775 train_time:739296ms step_avg:583.50ms
step:1268/1775 train_time:739933ms step_avg:583.54ms
step:1269/1775 train_time:740508ms step_avg:583.54ms
step:1270/1775 train_time:741075ms step_avg:583.52ms
step:1271/1775 train_time:741648ms step_avg:583.52ms
step:1272/1775 train_time:742168ms step_avg:583.47ms
step:1273/1775 train_time:742708ms step_avg:583.43ms
step:1274/1775 train_time:743283ms step_avg:583.42ms
step:1275/1775 train_time:743815ms step_avg:583.38ms
step:1276/1775 train_time:744326ms step_avg:583.33ms
step:1277/1775 train_time:744890ms step_avg:583.31ms
step:1278/1775 train_time:745379ms step_avg:583.24ms
step:1279/1775 train_time:745965ms step_avg:583.24ms
step:1280/1775 train_time:746566ms step_avg:583.25ms
step:1281/1775 train_time:747159ms step_avg:583.26ms
step:1282/1775 train_time:747748ms step_avg:583.27ms
step:1283/1775 train_time:748322ms step_avg:583.26ms
step:1284/1775 train_time:748902ms step_avg:583.26ms
step:1285/1775 train_time:749246ms step_avg:583.07ms
step:1286/1775 train_time:749611ms step_avg:582.90ms
step:1287/1775 train_time:750100ms step_avg:582.83ms
step:1288/1775 train_time:750627ms step_avg:582.78ms
step:1289/1775 train_time:751220ms step_avg:582.79ms
step:1290/1775 train_time:751827ms step_avg:582.81ms
step:1291/1775 train_time:752323ms step_avg:582.74ms
step:1292/1775 train_time:752912ms step_avg:582.75ms
step:1293/1775 train_time:753550ms step_avg:582.79ms
step:1294/1775 train_time:754161ms step_avg:582.81ms
step:1295/1775 train_time:754761ms step_avg:582.83ms
step:1296/1775 train_time:755386ms step_avg:582.86ms
step:1297/1775 train_time:756030ms step_avg:582.91ms
step:1298/1775 train_time:756542ms step_avg:582.85ms
step:1299/1775 train_time:756969ms step_avg:582.73ms
step:1300/1775 train_time:757502ms step_avg:582.69ms
step:1300/1775 val_loss:4.3947 val_malbo_loss:4.2615 train_time:757509ms step_avg:582.70ms
step:1301/1775 train_time:757990ms step_avg:582.62ms
step:1302/1775 train_time:758587ms step_avg:582.63ms
step:1303/1775 train_time:759174ms step_avg:582.64ms
step:1304/1775 train_time:759706ms step_avg:582.60ms
step:1305/1775 train_time:760205ms step_avg:582.53ms
step:1306/1775 train_time:760739ms step_avg:582.50ms
step:1307/1775 train_time:761318ms step_avg:582.49ms
step:1308/1775 train_time:761920ms step_avg:582.51ms
step:1309/1775 train_time:762459ms step_avg:582.47ms
step:1310/1775 train_time:763054ms step_avg:582.48ms
step:1311/1775 train_time:763647ms step_avg:582.49ms
step:1312/1775 train_time:764245ms step_avg:582.50ms
step:1313/1775 train_time:764815ms step_avg:582.49ms
step:1314/1775 train_time:765417ms step_avg:582.51ms
step:1315/1775 train_time:766000ms step_avg:582.51ms
step:1316/1775 train_time:766540ms step_avg:582.48ms
step:1317/1775 train_time:767074ms step_avg:582.44ms
step:1318/1775 train_time:767614ms step_avg:582.41ms
step:1319/1775 train_time:768151ms step_avg:582.37ms
step:1320/1775 train_time:768681ms step_avg:582.33ms
step:1321/1775 train_time:769245ms step_avg:582.32ms
step:1322/1775 train_time:769846ms step_avg:582.33ms
step:1323/1775 train_time:770443ms step_avg:582.35ms
step:1324/1775 train_time:771021ms step_avg:582.34ms
step:1325/1775 train_time:771605ms step_avg:582.34ms
step:1326/1775 train_time:772168ms step_avg:582.33ms
step:1327/1775 train_time:772697ms step_avg:582.29ms
step:1328/1775 train_time:773290ms step_avg:582.30ms
step:1329/1775 train_time:773885ms step_avg:582.31ms
step:1330/1775 train_time:774484ms step_avg:582.32ms
step:1331/1775 train_time:775059ms step_avg:582.31ms
step:1332/1775 train_time:775615ms step_avg:582.29ms
step:1333/1775 train_time:776207ms step_avg:582.30ms
step:1334/1775 train_time:776868ms step_avg:582.36ms
step:1335/1775 train_time:777560ms step_avg:582.44ms
step:1336/1775 train_time:778165ms step_avg:582.46ms
step:1337/1775 train_time:778762ms step_avg:582.47ms
step:1338/1775 train_time:779363ms step_avg:582.48ms
step:1339/1775 train_time:779971ms step_avg:582.50ms
step:1340/1775 train_time:780546ms step_avg:582.50ms
step:1341/1775 train_time:781174ms step_avg:582.53ms
step:1342/1775 train_time:781817ms step_avg:582.58ms
step:1343/1775 train_time:782419ms step_avg:582.59ms
step:1344/1775 train_time:782984ms step_avg:582.58ms
step:1345/1775 train_time:783345ms step_avg:582.41ms
step:1346/1775 train_time:783703ms step_avg:582.25ms
step:1347/1775 train_time:784057ms step_avg:582.08ms
step:1348/1775 train_time:784440ms step_avg:581.93ms
step:1349/1775 train_time:784863ms step_avg:581.81ms
step:1350/1775 train_time:785311ms step_avg:581.71ms
step:1351/1775 train_time:785664ms step_avg:581.54ms
step:1352/1775 train_time:786029ms step_avg:581.38ms
step:1353/1775 train_time:786389ms step_avg:581.22ms
step:1354/1775 train_time:786768ms step_avg:581.07ms
step:1355/1775 train_time:787168ms step_avg:580.94ms
step:1356/1775 train_time:787639ms step_avg:580.85ms
step:1357/1775 train_time:788051ms step_avg:580.73ms
step:1358/1775 train_time:788424ms step_avg:580.58ms
step:1359/1775 train_time:788827ms step_avg:580.45ms
step:1360/1775 train_time:789306ms step_avg:580.37ms
step:1361/1775 train_time:789770ms step_avg:580.29ms
step:1362/1775 train_time:790248ms step_avg:580.21ms
step:1363/1775 train_time:790655ms step_avg:580.08ms
step:1364/1775 train_time:791037ms step_avg:579.94ms
step:1365/1775 train_time:791407ms step_avg:579.79ms
step:1366/1775 train_time:791784ms step_avg:579.64ms
step:1367/1775 train_time:792169ms step_avg:579.49ms
step:1368/1775 train_time:792549ms step_avg:579.35ms
step:1369/1775 train_time:792930ms step_avg:579.20ms
step:1370/1775 train_time:793316ms step_avg:579.06ms
step:1371/1775 train_time:793694ms step_avg:578.92ms
step:1372/1775 train_time:794078ms step_avg:578.77ms
step:1373/1775 train_time:794447ms step_avg:578.62ms
step:1374/1775 train_time:794783ms step_avg:578.44ms
step:1375/1775 train_time:795239ms step_avg:578.36ms
step:1376/1775 train_time:795836ms step_avg:578.37ms
step:1377/1775 train_time:796427ms step_avg:578.38ms
step:1378/1775 train_time:797024ms step_avg:578.39ms
step:1379/1775 train_time:797615ms step_avg:578.40ms
step:1380/1775 train_time:798205ms step_avg:578.41ms
step:1381/1775 train_time:798729ms step_avg:578.37ms
step:1382/1775 train_time:799305ms step_avg:578.37ms
step:1383/1775 train_time:799879ms step_avg:578.36ms
step:1384/1775 train_time:800464ms step_avg:578.37ms
step:1385/1775 train_time:801041ms step_avg:578.37ms
step:1386/1775 train_time:801628ms step_avg:578.38ms
step:1387/1775 train_time:802206ms step_avg:578.38ms
step:1388/1775 train_time:802801ms step_avg:578.39ms
step:1389/1775 train_time:803369ms step_avg:578.38ms
step:1390/1775 train_time:803915ms step_avg:578.36ms
step:1391/1775 train_time:804492ms step_avg:578.36ms
step:1392/1775 train_time:805078ms step_avg:578.36ms
step:1393/1775 train_time:805654ms step_avg:578.36ms
step:1394/1775 train_time:806235ms step_avg:578.36ms
step:1395/1775 train_time:806812ms step_avg:578.36ms
step:1396/1775 train_time:807396ms step_avg:578.36ms
step:1397/1775 train_time:807972ms step_avg:578.36ms
step:1398/1775 train_time:808557ms step_avg:578.37ms
step:1399/1775 train_time:809133ms step_avg:578.37ms
step:1400/1775 train_time:809711ms step_avg:578.36ms
step:1400/1775 val_loss:4.3046 val_malbo_loss:4.1746 train_time:809724ms step_avg:578.37ms
step:1401/1775 train_time:810166ms step_avg:578.28ms
step:1402/1775 train_time:810760ms step_avg:578.29ms
step:1403/1775 train_time:811350ms step_avg:578.30ms
step:1404/1775 train_time:811945ms step_avg:578.31ms
step:1405/1775 train_time:812534ms step_avg:578.32ms
step:1406/1775 train_time:813131ms step_avg:578.33ms
step:1407/1775 train_time:814134ms step_avg:578.63ms
step:1408/1775 train_time:814608ms step_avg:578.56ms
step:1409/1775 train_time:815190ms step_avg:578.56ms
step:1410/1775 train_time:815786ms step_avg:578.57ms
step:1411/1775 train_time:816378ms step_avg:578.58ms
step:1412/1775 train_time:816963ms step_avg:578.59ms
step:1413/1775 train_time:817538ms step_avg:578.58ms
step:1414/1775 train_time:818118ms step_avg:578.58ms
step:1415/1775 train_time:818694ms step_avg:578.58ms
step:1416/1775 train_time:819276ms step_avg:578.59ms
step:1417/1775 train_time:819852ms step_avg:578.58ms
step:1418/1775 train_time:820432ms step_avg:578.58ms
step:1419/1775 train_time:821005ms step_avg:578.58ms
step:1420/1775 train_time:821584ms step_avg:578.58ms
step:1421/1775 train_time:822159ms step_avg:578.58ms
step:1422/1775 train_time:822739ms step_avg:578.58ms
step:1423/1775 train_time:823326ms step_avg:578.58ms
step:1424/1775 train_time:823908ms step_avg:578.59ms
step:1425/1775 train_time:824338ms step_avg:578.48ms
step:1426/1775 train_time:824765ms step_avg:578.38ms
step:1427/1775 train_time:825219ms step_avg:578.29ms
step:1428/1775 train_time:825727ms step_avg:578.24ms
step:1429/1775 train_time:826282ms step_avg:578.22ms
step:1430/1775 train_time:826764ms step_avg:578.16ms
step:1431/1775 train_time:827159ms step_avg:578.03ms
step:1432/1775 train_time:827558ms step_avg:577.90ms
step:1433/1775 train_time:828053ms step_avg:577.85ms
step:1434/1775 train_time:828603ms step_avg:577.83ms
step:1435/1775 train_time:828994ms step_avg:577.70ms
step:1436/1775 train_time:829391ms step_avg:577.57ms
step:1437/1775 train_time:829785ms step_avg:577.44ms
step:1438/1775 train_time:830190ms step_avg:577.32ms
step:1439/1775 train_time:830690ms step_avg:577.27ms
step:1440/1775 train_time:831086ms step_avg:577.14ms
step:1441/1775 train_time:831590ms step_avg:577.09ms
step:1442/1775 train_time:831983ms step_avg:576.96ms
step:1443/1775 train_time:832385ms step_avg:576.84ms
step:1444/1775 train_time:832822ms step_avg:576.75ms
step:1445/1775 train_time:833319ms step_avg:576.69ms
step:1446/1775 train_time:833738ms step_avg:576.58ms
step:1447/1775 train_time:834302ms step_avg:576.57ms
step:1448/1775 train_time:834850ms step_avg:576.55ms
step:1449/1775 train_time:835415ms step_avg:576.55ms
step:1450/1775 train_time:836003ms step_avg:576.55ms
step:1451/1775 train_time:836576ms step_avg:576.55ms
step:1452/1775 train_time:837158ms step_avg:576.56ms
step:1453/1775 train_time:837732ms step_avg:576.55ms
step:1454/1775 train_time:838311ms step_avg:576.55ms
step:1455/1775 train_time:838884ms step_avg:576.55ms
step:1456/1775 train_time:839372ms step_avg:576.49ms
step:1457/1775 train_time:839946ms step_avg:576.49ms
step:1458/1775 train_time:840530ms step_avg:576.49ms
step:1459/1775 train_time:841103ms step_avg:576.49ms
step:1460/1775 train_time:841684ms step_avg:576.50ms
step:1461/1775 train_time:842259ms step_avg:576.49ms
step:1462/1775 train_time:842839ms step_avg:576.50ms
step:1463/1775 train_time:843414ms step_avg:576.50ms
step:1464/1775 train_time:843995ms step_avg:576.50ms
step:1465/1775 train_time:844563ms step_avg:576.49ms
step:1466/1775 train_time:845074ms step_avg:576.45ms
step:1467/1775 train_time:845651ms step_avg:576.45ms
step:1468/1775 train_time:846232ms step_avg:576.45ms
step:1469/1775 train_time:846807ms step_avg:576.45ms
step:1470/1775 train_time:847387ms step_avg:576.45ms
step:1471/1775 train_time:847932ms step_avg:576.43ms
step:1472/1775 train_time:848324ms step_avg:576.31ms
step:1473/1775 train_time:848900ms step_avg:576.31ms
step:1474/1775 train_time:849483ms step_avg:576.31ms
step:1475/1775 train_time:850062ms step_avg:576.31ms
step:1476/1775 train_time:850573ms step_avg:576.27ms
step:1477/1775 train_time:851147ms step_avg:576.27ms
step:1478/1775 train_time:851725ms step_avg:576.27ms
step:1479/1775 train_time:852298ms step_avg:576.27ms
step:1480/1775 train_time:852875ms step_avg:576.27ms
step:1481/1775 train_time:853447ms step_avg:576.26ms
step:1482/1775 train_time:854025ms step_avg:576.27ms
step:1483/1775 train_time:854597ms step_avg:576.26ms
step:1484/1775 train_time:855175ms step_avg:576.26ms
step:1485/1775 train_time:855748ms step_avg:576.26ms
step:1486/1775 train_time:856252ms step_avg:576.21ms
step:1487/1775 train_time:856824ms step_avg:576.21ms
step:1488/1775 train_time:857402ms step_avg:576.21ms
step:1489/1775 train_time:857974ms step_avg:576.21ms
step:1490/1775 train_time:858552ms step_avg:576.21ms
step:1491/1775 train_time:859126ms step_avg:576.21ms
step:1492/1775 train_time:859704ms step_avg:576.21ms
step:1493/1775 train_time:860276ms step_avg:576.21ms
step:1494/1775 train_time:860856ms step_avg:576.21ms
step:1495/1775 train_time:861427ms step_avg:576.21ms
step:1496/1775 train_time:861871ms step_avg:576.12ms
step:1497/1775 train_time:862438ms step_avg:576.11ms
step:1498/1775 train_time:863017ms step_avg:576.11ms
step:1499/1775 train_time:863589ms step_avg:576.11ms
step:1500/1775 train_time:864168ms step_avg:576.11ms
step:1500/1775 val_loss:4.2157 val_malbo_loss:4.0879 train_time:864175ms step_avg:576.12ms
step:1501/1775 train_time:864548ms step_avg:575.98ms
step:1502/1775 train_time:865112ms step_avg:575.97ms
step:1503/1775 train_time:865692ms step_avg:575.98ms
step:1504/1775 train_time:866283ms step_avg:575.99ms
step:1505/1775 train_time:866865ms step_avg:575.99ms
step:1506/1775 train_time:867444ms step_avg:575.99ms
step:1507/1775 train_time:867986ms step_avg:575.97ms
step:1508/1775 train_time:868497ms step_avg:575.93ms
step:1509/1775 train_time:869021ms step_avg:575.89ms
step:1510/1775 train_time:869599ms step_avg:575.89ms
step:1511/1775 train_time:870172ms step_avg:575.89ms
step:1512/1775 train_time:870749ms step_avg:575.89ms
step:1513/1775 train_time:871322ms step_avg:575.89ms
step:1514/1775 train_time:871902ms step_avg:575.89ms
step:1515/1775 train_time:872474ms step_avg:575.89ms
step:1516/1775 train_time:873052ms step_avg:575.89ms
step:1517/1775 train_time:873625ms step_avg:575.89ms
step:1518/1775 train_time:874205ms step_avg:575.89ms
step:1519/1775 train_time:874780ms step_avg:575.89ms
step:1520/1775 train_time:875367ms step_avg:575.90ms
step:1521/1775 train_time:875947ms step_avg:575.90ms
step:1522/1775 train_time:876536ms step_avg:575.91ms
step:1523/1775 train_time:877111ms step_avg:575.91ms
step:1524/1775 train_time:877697ms step_avg:575.92ms
step:1525/1775 train_time:878245ms step_avg:575.90ms
step:1526/1775 train_time:878836ms step_avg:575.91ms
step:1527/1775 train_time:879409ms step_avg:575.91ms
step:1528/1775 train_time:879988ms step_avg:575.91ms
step:1529/1775 train_time:880562ms step_avg:575.91ms
step:1530/1775 train_time:881140ms step_avg:575.91ms
step:1531/1775 train_time:881714ms step_avg:575.91ms
step:1532/1775 train_time:882292ms step_avg:575.91ms
step:1533/1775 train_time:882868ms step_avg:575.91ms
step:1534/1775 train_time:883447ms step_avg:575.91ms
step:1535/1775 train_time:884019ms step_avg:575.91ms
step:1536/1775 train_time:884595ms step_avg:575.91ms
step:1537/1775 train_time:885111ms step_avg:575.87ms
step:1538/1775 train_time:885535ms step_avg:575.77ms
step:1539/1775 train_time:886099ms step_avg:575.76ms
step:1540/1775 train_time:886724ms step_avg:575.79ms
step:1541/1775 train_time:887357ms step_avg:575.83ms
step:1542/1775 train_time:887988ms step_avg:575.87ms
step:1543/1775 train_time:888532ms step_avg:575.85ms
step:1544/1775 train_time:889040ms step_avg:575.80ms
step:1545/1775 train_time:889574ms step_avg:575.78ms
step:1546/1775 train_time:890087ms step_avg:575.74ms
step:1547/1775 train_time:890622ms step_avg:575.71ms
step:1548/1775 train_time:891170ms step_avg:575.69ms
step:1549/1775 train_time:891697ms step_avg:575.66ms
step:1550/1775 train_time:892193ms step_avg:575.61ms
step:1551/1775 train_time:892686ms step_avg:575.56ms
step:1552/1775 train_time:893184ms step_avg:575.51ms
step:1553/1775 train_time:893673ms step_avg:575.45ms
step:1554/1775 train_time:894181ms step_avg:575.41ms
step:1555/1775 train_time:894682ms step_avg:575.36ms
step:1556/1775 train_time:895174ms step_avg:575.30ms
step:1557/1775 train_time:895645ms step_avg:575.24ms
step:1558/1775 train_time:896231ms step_avg:575.24ms
step:1559/1775 train_time:896696ms step_avg:575.17ms
step:1560/1775 train_time:897169ms step_avg:575.11ms
step:1561/1775 train_time:897686ms step_avg:575.07ms
step:1562/1775 train_time:898234ms step_avg:575.05ms
step:1563/1775 train_time:898766ms step_avg:575.03ms
step:1564/1775 train_time:899345ms step_avg:575.03ms
step:1565/1775 train_time:899922ms step_avg:575.03ms
step:1566/1775 train_time:900502ms step_avg:575.03ms
step:1567/1775 train_time:901079ms step_avg:575.03ms
step:1568/1775 train_time:901662ms step_avg:575.04ms
step:1569/1775 train_time:902236ms step_avg:575.04ms
step:1570/1775 train_time:902816ms step_avg:575.04ms
step:1571/1775 train_time:903391ms step_avg:575.04ms
step:1572/1775 train_time:903973ms step_avg:575.05ms
step:1573/1775 train_time:904547ms step_avg:575.05ms
step:1574/1775 train_time:905126ms step_avg:575.05ms
step:1575/1775 train_time:905700ms step_avg:575.05ms
step:1576/1775 train_time:906279ms step_avg:575.05ms
step:1577/1775 train_time:906852ms step_avg:575.05ms
step:1578/1775 train_time:907432ms step_avg:575.05ms
step:1579/1775 train_time:908005ms step_avg:575.05ms
step:1580/1775 train_time:908584ms step_avg:575.05ms
step:1581/1775 train_time:909164ms step_avg:575.06ms
step:1582/1775 train_time:909744ms step_avg:575.06ms
step:1583/1775 train_time:910319ms step_avg:575.06ms
step:1584/1775 train_time:910899ms step_avg:575.06ms
step:1585/1775 train_time:911473ms step_avg:575.06ms
step:1586/1775 train_time:912053ms step_avg:575.06ms
step:1587/1775 train_time:912626ms step_avg:575.06ms
step:1588/1775 train_time:913205ms step_avg:575.07ms
step:1589/1775 train_time:913779ms step_avg:575.07ms
step:1590/1775 train_time:914357ms step_avg:575.07ms
step:1591/1775 train_time:914930ms step_avg:575.07ms
step:1592/1775 train_time:915510ms step_avg:575.07ms
step:1593/1775 train_time:916083ms step_avg:575.07ms
step:1594/1775 train_time:916661ms step_avg:575.07ms
step:1595/1775 train_time:917234ms step_avg:575.07ms
step:1596/1775 train_time:917813ms step_avg:575.07ms
step:1597/1775 train_time:918386ms step_avg:575.07ms
step:1598/1775 train_time:918964ms step_avg:575.07ms
step:1599/1775 train_time:919537ms step_avg:575.07ms
step:1600/1775 train_time:920116ms step_avg:575.07ms
step:1600/1775 val_loss:4.1431 val_malbo_loss:4.0174 train_time:920124ms step_avg:575.08ms
step:1601/1775 train_time:920600ms step_avg:575.02ms
step:1602/1775 train_time:920995ms step_avg:574.90ms
step:1603/1775 train_time:921476ms step_avg:574.84ms
step:1604/1775 train_time:921930ms step_avg:574.77ms
step:1605/1775 train_time:922391ms step_avg:574.70ms
step:1606/1775 train_time:922895ms step_avg:574.65ms
step:1607/1775 train_time:923268ms step_avg:574.53ms
step:1608/1775 train_time:923649ms step_avg:574.41ms
step:1609/1775 train_time:924020ms step_avg:574.28ms
step:1610/1775 train_time:924399ms step_avg:574.16ms
step:1611/1775 train_time:924796ms step_avg:574.05ms
step:1612/1775 train_time:925295ms step_avg:574.00ms
step:1613/1775 train_time:925740ms step_avg:573.92ms
step:1614/1775 train_time:926229ms step_avg:573.87ms
step:1615/1775 train_time:926659ms step_avg:573.78ms
step:1616/1775 train_time:927052ms step_avg:573.67ms
step:1617/1775 train_time:927509ms step_avg:573.60ms
step:1618/1775 train_time:927912ms step_avg:573.49ms
step:1619/1775 train_time:928346ms step_avg:573.41ms
step:1620/1775 train_time:928858ms step_avg:573.37ms
step:1621/1775 train_time:929374ms step_avg:573.33ms
step:1622/1775 train_time:929902ms step_avg:573.31ms
step:1623/1775 train_time:930429ms step_avg:573.28ms
step:1624/1775 train_time:930972ms step_avg:573.26ms
step:1625/1775 train_time:931491ms step_avg:573.23ms
step:1626/1775 train_time:932000ms step_avg:573.19ms
step:1627/1775 train_time:932496ms step_avg:573.14ms
step:1628/1775 train_time:933023ms step_avg:573.11ms
step:1629/1775 train_time:933524ms step_avg:573.07ms
step:1630/1775 train_time:934056ms step_avg:573.04ms
step:1631/1775 train_time:934587ms step_avg:573.01ms
step:1632/1775 train_time:935139ms step_avg:573.00ms
step:1633/1775 train_time:935703ms step_avg:573.00ms
step:1634/1775 train_time:936236ms step_avg:572.97ms
step:1635/1775 train_time:936775ms step_avg:572.95ms
step:1636/1775 train_time:937270ms step_avg:572.90ms
step:1637/1775 train_time:937743ms step_avg:572.84ms
step:1638/1775 train_time:938277ms step_avg:572.82ms
step:1639/1775 train_time:938838ms step_avg:572.81ms
step:1640/1775 train_time:939284ms step_avg:572.73ms
step:1641/1775 train_time:939850ms step_avg:572.73ms
step:1642/1775 train_time:940404ms step_avg:572.72ms
step:1643/1775 train_time:940960ms step_avg:572.71ms
step:1644/1775 train_time:941508ms step_avg:572.69ms
step:1645/1775 train_time:942068ms step_avg:572.69ms
step:1646/1775 train_time:942607ms step_avg:572.67ms
step:1647/1775 train_time:943146ms step_avg:572.64ms
step:1648/1775 train_time:943704ms step_avg:572.64ms
step:1649/1775 train_time:944249ms step_avg:572.62ms
step:1650/1775 train_time:944794ms step_avg:572.60ms
step:1651/1775 train_time:945332ms step_avg:572.58ms
step:1652/1775 train_time:945869ms step_avg:572.56ms
step:1653/1775 train_time:946452ms step_avg:572.57ms
step:1654/1775 train_time:947054ms step_avg:572.58ms
step:1655/1775 train_time:947617ms step_avg:572.58ms
step:1656/1775 train_time:948163ms step_avg:572.56ms
step:1657/1775 train_time:948710ms step_avg:572.55ms
step:1658/1775 train_time:949267ms step_avg:572.54ms
step:1659/1775 train_time:949820ms step_avg:572.53ms
step:1660/1775 train_time:950385ms step_avg:572.52ms
step:1661/1775 train_time:950950ms step_avg:572.52ms
step:1662/1775 train_time:951486ms step_avg:572.49ms
step:1663/1775 train_time:952034ms step_avg:572.48ms
step:1664/1775 train_time:952598ms step_avg:572.47ms
step:1665/1775 train_time:953137ms step_avg:572.45ms
step:1666/1775 train_time:953683ms step_avg:572.44ms
step:1667/1775 train_time:954245ms step_avg:572.43ms
step:1668/1775 train_time:954794ms step_avg:572.42ms
step:1669/1775 train_time:955344ms step_avg:572.41ms
step:1670/1775 train_time:955861ms step_avg:572.37ms
step:1671/1775 train_time:956412ms step_avg:572.36ms
step:1672/1775 train_time:956984ms step_avg:572.36ms
step:1673/1775 train_time:957523ms step_avg:572.34ms
step:1674/1775 train_time:958062ms step_avg:572.32ms
step:1675/1775 train_time:958561ms step_avg:572.28ms
step:1676/1775 train_time:959019ms step_avg:572.21ms
step:1677/1775 train_time:959597ms step_avg:572.21ms
step:1678/1775 train_time:960179ms step_avg:572.22ms
step:1679/1775 train_time:960756ms step_avg:572.22ms
step:1680/1775 train_time:961337ms step_avg:572.22ms
step:1681/1775 train_time:961914ms step_avg:572.23ms
step:1682/1775 train_time:962496ms step_avg:572.23ms
step:1683/1775 train_time:963071ms step_avg:572.23ms
step:1684/1775 train_time:963653ms step_avg:572.24ms
step:1685/1775 train_time:964229ms step_avg:572.24ms
step:1686/1775 train_time:964811ms step_avg:572.25ms
step:1687/1775 train_time:965387ms step_avg:572.25ms
step:1688/1775 train_time:965969ms step_avg:572.26ms
step:1689/1775 train_time:966547ms step_avg:572.26ms
step:1690/1775 train_time:966976ms step_avg:572.18ms
step:1691/1775 train_time:967534ms step_avg:572.17ms
step:1692/1775 train_time:968117ms step_avg:572.17ms
step:1693/1775 train_time:968694ms step_avg:572.18ms
step:1694/1775 train_time:969275ms step_avg:572.18ms
step:1695/1775 train_time:969852ms step_avg:572.18ms
step:1696/1775 train_time:970437ms step_avg:572.19ms
step:1697/1775 train_time:971014ms step_avg:572.19ms
step:1698/1775 train_time:971595ms step_avg:572.20ms
step:1699/1775 train_time:972171ms step_avg:572.20ms
step:1700/1775 train_time:972752ms step_avg:572.21ms
step:1700/1775 val_loss:4.0841 val_malbo_loss:3.9578 train_time:972760ms step_avg:572.21ms
step:1701/1775 train_time:973207ms step_avg:572.14ms
step:1702/1775 train_time:973791ms step_avg:572.14ms
step:1703/1775 train_time:974370ms step_avg:572.15ms
step:1704/1775 train_time:974953ms step_avg:572.16ms
step:1705/1775 train_time:975529ms step_avg:572.16ms
step:1706/1775 train_time:976113ms step_avg:572.16ms
step:1707/1775 train_time:976690ms step_avg:572.17ms
step:1708/1775 train_time:977270ms step_avg:572.17ms
step:1709/1775 train_time:977846ms step_avg:572.17ms
step:1710/1775 train_time:978428ms step_avg:572.18ms
step:1711/1775 train_time:979004ms step_avg:572.18ms
step:1712/1775 train_time:979586ms step_avg:572.19ms
step:1713/1775 train_time:980162ms step_avg:572.19ms
step:1714/1775 train_time:980744ms step_avg:572.20ms
step:1715/1775 train_time:981320ms step_avg:572.20ms
step:1716/1775 train_time:981905ms step_avg:572.21ms
step:1717/1775 train_time:982472ms step_avg:572.20ms
step:1718/1775 train_time:983056ms step_avg:572.21ms
step:1719/1775 train_time:983640ms step_avg:572.22ms
step:1720/1775 train_time:984238ms step_avg:572.23ms
step:1721/1775 train_time:984812ms step_avg:572.23ms
step:1722/1775 train_time:985392ms step_avg:572.24ms
step:1723/1775 train_time:985967ms step_avg:572.24ms
step:1724/1775 train_time:986555ms step_avg:572.25ms
step:1725/1775 train_time:987110ms step_avg:572.24ms
step:1726/1775 train_time:987655ms step_avg:572.22ms
step:1727/1775 train_time:988190ms step_avg:572.20ms
step:1728/1775 train_time:988718ms step_avg:572.17ms
step:1729/1775 train_time:989099ms step_avg:572.06ms
step:1730/1775 train_time:989548ms step_avg:571.99ms
step:1731/1775 train_time:990066ms step_avg:571.96ms
step:1732/1775 train_time:990603ms step_avg:571.94ms
step:1733/1775 train_time:991168ms step_avg:571.94ms
step:1734/1775 train_time:991646ms step_avg:571.88ms
step:1735/1775 train_time:992136ms step_avg:571.84ms
step:1736/1775 train_time:992712ms step_avg:571.84ms
step:1737/1775 train_time:993199ms step_avg:571.79ms
step:1738/1775 train_time:993786ms step_avg:571.80ms
step:1739/1775 train_time:994360ms step_avg:571.80ms
step:1740/1775 train_time:994912ms step_avg:571.79ms
step:1741/1775 train_time:995483ms step_avg:571.79ms
step:1742/1775 train_time:996012ms step_avg:571.76ms
step:1743/1775 train_time:996452ms step_avg:571.69ms
step:1744/1775 train_time:996825ms step_avg:571.57ms
step:1745/1775 train_time:997231ms step_avg:571.48ms
step:1746/1775 train_time:997667ms step_avg:571.40ms
step:1747/1775 train_time:998157ms step_avg:571.36ms
step:1748/1775 train_time:998671ms step_avg:571.32ms
step:1749/1775 train_time:999238ms step_avg:571.32ms
step:1750/1775 train_time:999808ms step_avg:571.32ms
step:1751/1775 train_time:1000433ms step_avg:571.35ms
step:1752/1775 train_time:1001022ms step_avg:571.36ms
step:1753/1775 train_time:1001522ms step_avg:571.32ms
step:1754/1775 train_time:1002117ms step_avg:571.33ms
step:1755/1775 train_time:1002660ms step_avg:571.32ms
step:1756/1775 train_time:1003202ms step_avg:571.30ms
step:1757/1775 train_time:1003782ms step_avg:571.30ms
step:1758/1775 train_time:1004286ms step_avg:571.27ms
step:1759/1775 train_time:1004833ms step_avg:571.25ms
step:1760/1775 train_time:1005319ms step_avg:571.20ms
step:1761/1775 train_time:1005751ms step_avg:571.13ms
step:1762/1775 train_time:1006310ms step_avg:571.12ms
step:1763/1775 train_time:1006813ms step_avg:571.08ms
step:1764/1775 train_time:1007398ms step_avg:571.09ms
step:1765/1775 train_time:1007988ms step_avg:571.10ms
step:1766/1775 train_time:1008550ms step_avg:571.09ms
step:1767/1775 train_time:1009137ms step_avg:571.10ms
step:1768/1775 train_time:1009587ms step_avg:571.03ms
step:1769/1775 train_time:1010098ms step_avg:571.00ms
step:1770/1775 train_time:1010643ms step_avg:570.98ms
step:1771/1775 train_time:1011167ms step_avg:570.96ms
step:1772/1775 train_time:1011702ms step_avg:570.94ms
step:1773/1775 train_time:1012264ms step_avg:570.93ms
step:1774/1775 train_time:1012815ms step_avg:570.92ms
step:1775/1775 train_time:1013246ms step_avg:570.84ms
step:1775/1775 val_loss:4.0541 val_malbo_loss:3.9277 train_time:1013255ms step_avg:570.85ms
peak memory allocated: 9762 MiB reserved: 11408 MiB
