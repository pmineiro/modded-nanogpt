import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

from malbo import compute_malbo_parameters

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from triton.tools.tensor_descriptor import TensorDescriptor
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

@triton.jit
def linear_relu_square_kernel(a_desc, b_desc, c_desc, aux_desc,
                                 M, N, K,
                                 BLOCK_SIZE_M: tl.constexpr,
                                 BLOCK_SIZE_N: tl.constexpr,
                                 BLOCK_SIZE_K: tl.constexpr,
                                 GROUP_SIZE_M: tl.constexpr,
                                 NUM_SMS: tl.constexpr,
                                 FORWARD: tl.constexpr,
                                 ):
    dtype = tl.bfloat16
    start_pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)
    num_tiles = num_pid_m * num_pid_n

    tile_id_c = start_pid - NUM_SMS
    num_pid_in_group = GROUP_SIZE_M * num_pid_n

    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True):
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am = pid_m * BLOCK_SIZE_M
        offs_bn = pid_n * BLOCK_SIZE_N

        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for ki in range(k_tiles):
            offs_k = ki * BLOCK_SIZE_K
            a = a_desc.load([offs_am, offs_k])
            b = b_desc.load([offs_bn, offs_k])
            accumulator = tl.dot(a, b.T, accumulator)

        tile_id_c += NUM_SMS
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am_c = pid_m * BLOCK_SIZE_M
        offs_bn_c = pid_n * BLOCK_SIZE_N

        acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))
        acc = tl.permute(acc, (0, 2, 1))
        acc0, acc1 = tl.split(acc)

        c0 = acc0.to(dtype)
        if not FORWARD:
            c0_pre = aux_desc.load([offs_am_c, offs_bn_c])
            c0 = 2 * c0 * tl.where(c0_pre > 0, c0_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c], c0)

        if FORWARD:
            c0_post = tl.maximum(c0, 0)
            c0_post = c0_post * c0_post
            aux_desc.store([offs_am_c, offs_bn_c], c0_post)

        c1 = acc1.to(dtype)
        if not FORWARD:
            c1_pre = aux_desc.load([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2])
            c1 = 2 * c1 * tl.where(c1_pre > 0, c1_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1)

        if FORWARD:
            c1_post = tl.maximum(c1, 0)
            c1_post = c1_post * c1_post
            aux_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1_post)


def linear_relu_square(a, b, aux=None):
    M, K = a.shape
    N, K = b.shape
    dtype = a.dtype

    c = torch.empty((M, N), device=a.device, dtype=dtype)

    FORWARD = False
    if aux is None:
        FORWARD = True
        aux = torch.empty((M, N), device=a.device, dtype=dtype)

    NUM_SMS = torch.cuda.get_device_properties("cuda").multi_processor_count

    BLOCK_SIZE_M = 128
    BLOCK_SIZE_N = 256
    BLOCK_SIZE_K = 64
    num_stages = 4 if FORWARD else 3
    num_warps = 8

    a_desc = TensorDescriptor.from_tensor(a, [BLOCK_SIZE_M, BLOCK_SIZE_K])
    b_desc = TensorDescriptor.from_tensor(b, [BLOCK_SIZE_N, BLOCK_SIZE_K])
    c_desc = TensorDescriptor.from_tensor(c, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])
    aux_desc = TensorDescriptor.from_tensor(aux, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])

    def grid(META):
        return (min(
            NUM_SMS,
            triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),
        ), )

    linear_relu_square_kernel[grid](
        a_desc, b_desc, c_desc, aux_desc,
        M, N, K,
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=1,
        NUM_SMS=NUM_SMS,
        FORWARD=FORWARD,
        num_stages=num_stages,
        num_warps=num_warps
    )

    if FORWARD:
        return c, aux
    else:
        return c

class FusedLinearReLUSquareFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, W1, W2):
        pre, post = linear_relu_square(x.view((-1, x.shape[-1])), W1)
        x3 = post @ W2
        ctx.save_for_backward(x, W1, W2, pre, post)
        return x3.view(x.shape)

    @staticmethod
    def backward(ctx, grad_output):
        x, W1, W2, pre, post = ctx.saved_tensors
        dW2 = post.T @ grad_output
        dpre = linear_relu_square(grad_output.view((-1, grad_output.shape[-1])), W2, aux=pre)
        dW1 = dpre.T @ x
        dx = dpre @ W1
        return dx.view(x.shape), dW1, dW2

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        # relu(x)^2:
        # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977

        # This call computes relu(x @ W1.T)^2 @ W2.T
        return FusedLinearReLUSquareFunction.apply(x, self.c_fc, self.c_proj)


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# Fused Softcapped Cross Entropy

@triton.jit
def fused_softcapped_entropy_fwd_kernel(
    logits_ptr, losses_ptr, lse_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)
    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    
    max_val = -float('inf')
    sum_exp = 0.0
    
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=-float('inf')).to(tl.float32)
        z = A * tl.sigmoid((val + B) / C)
        z = tl.where(mask, z, -float('inf'))
        curr_max = tl.max(z, axis=0)
        new_max = tl.maximum(max_val, curr_max)
        sum_exp = sum_exp * tl.exp(max_val - new_max) + tl.sum(tl.exp(z - new_max), axis=0)
        max_val = new_max
    
    lse = max_val + tl.log(sum_exp)
    tl.store(lse_ptr + row_idx, lse)
    
    total_loss = 0.0
    for k in range(n_predict):
        target_idx = row_idx + k
        if target_idx < n_rows:
            weight = tl.load(mtp_weights_ptr + k)
            if weight > 0:
                target = tl.load(targets_ptr + target_idx).to(tl.int32)
                if target >= 0 and target < n_cols:
                    val_target = tl.load(logits_row_ptr + target).to(tl.float32)
                    z_target = A * tl.sigmoid((val_target + B) / C)
                    total_loss += weight * (lse - z_target)
    
    tl.store(losses_ptr + row_idx, total_loss)

@triton.jit
def fused_softcapped_entropy_bwd_kernel(
    grad_input_ptr, grad_output_ptr, lse_ptr, logits_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v, stride_grad_n, stride_grad_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)

    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    grad_row_ptr = grad_input_ptr + row_idx * stride_grad_n
    
    lse = tl.load(lse_ptr + row_idx)
    grad_loss = tl.load(grad_output_ptr + row_idx)
    
    S_w = 0.0
    for k in range(n_predict):
        if row_idx + k < n_rows:
            S_w += tl.load(mtp_weights_ptr + k)
            
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=0.0).to(tl.float32)
        u = (val + B) / C
        sigmoid_u = tl.sigmoid(u)
        z = A * sigmoid_u
        p = tl.exp(z - lse)
        
        term1 = S_w * p
        term2 = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
        for k in range(n_predict):
            if row_idx + k < n_rows:
                target = tl.load(targets_ptr + row_idx + k).to(tl.int32)
                weight = tl.load(mtp_weights_ptr + k)
                term2 += tl.where(cols == target, weight, 0.0)
        
        grad_z = grad_loss * (term1 - term2)
        dz_dx = (1.0 / C) * z * (1.0 - sigmoid_u)
        grad_x = grad_z * dz_dx
        tl.store(grad_row_ptr + cols, grad_x.to(tl.bfloat16), mask=mask)

class FusedSoftcappedCrossEntropy(torch.autograd.Function):
    @staticmethod
    def forward(ctx, logits, targets, mtp_weights, A=23.0, B=5.0, C=7.5):
        n_rows, n_cols = logits.shape
        if mtp_weights is None:
             mtp_weights = torch.tensor([1.0], device=logits.device, dtype=torch.float32)
        n_predict = mtp_weights.shape[0]

        losses = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        lse = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        
        logits = logits.contiguous()
        targets = targets.contiguous()
        mtp_weights = mtp_weights.contiguous()

        grid = (n_rows,)
        fused_softcapped_entropy_fwd_kernel[grid](
            logits, losses, lse, targets, mtp_weights,
            logits.stride(0), logits.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        
        ctx.save_for_backward(logits, targets, mtp_weights, lse)
        ctx.params = (A, B, C)
        return losses

    @staticmethod
    def backward(ctx, grad_output):
        logits, targets, mtp_weights, lse = ctx.saved_tensors
        A, B, C = ctx.params
        n_rows, n_cols = logits.shape
        n_predict = mtp_weights.shape[0]
        
        grad_input = torch.empty((n_rows, n_cols), dtype=torch.bfloat16, device=logits.device)
        grad_output = grad_output.contiguous()
        
        grid = (n_rows,)
        fused_softcapped_entropy_bwd_kernel[grid](
            grad_input, grad_output, lse, logits, targets, mtp_weights,
            logits.stride(0), logits.stride(1), grad_input.stride(0), grad_input.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        return grad_input, None, None, None, None, None

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int, use_malbo: bool):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

        self.use_malbo = use_malbo

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        if self.training:
            losses = FusedSoftcappedCrossEntropy.apply(logits.view(-1, logits.size(-1)), target_seq, mtp_weights)
            loss = losses.sum()
            if self.use_malbo:
                T, K = logits.view(-1, logits.size(-1)).shape
                with torch.no_grad():
                    vhat, kappa, gamma = compute_malbo_parameters((-losses).float().exp().unsqueeze(0), K)
                    weights = (vhat * kappa * gamma).squeeze(0)
                malbo_loss = T * (weights * losses).sum()
            else:
                malbo_loss = loss
        else:
            logits = 23 * torch.sigmoid((logits + 5) / 7.5)
            logits_for_loss = logits.float()
            losses = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="none")
            loss = losses.mean()
            if self.use_malbo:
                T, K = logits.view(-1, logits.size(-1)).shape
                with torch.no_grad():
                    vhat, kappa, gamma = compute_malbo_parameters((-losses).float().exp().unsqueeze(0), K)
                    weights = (vhat * kappa * gamma).squeeze(0)
                malbo_loss = (weights * losses).sum()
            else:
                malbo_loss = loss
        return loss, malbo_loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 1 # scales with world size
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size),
    use_malbo=os.environ.get('use_malbo', 'True') == 'True',
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args())[1] / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
    break
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss, val_malbo_loss = 0, 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                this_val_loss, this_val_malbo_loss = model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
                val_loss += this_val_loss
                val_malbo_loss += this_val_malbo_loss
        val_loss /= val_steps
        val_malbo_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        dist.reduce(val_malbo_loss, 0, op=dist.ReduceOp.AVG)
        n_predict = training_manager.mtp_weights_schedule[step].size(0)
        lr = get_lr(step)
        bs = get_bs(step)
        print0(f"step:{step}/{train_steps} {lr=:.4f} {bs=} {n_predict=} val_loss:{val_loss:.4f} val_malbo_loss:{val_malbo_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        loss, malbo_loss = model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
        malbo_loss /= grad_accum_steps
        loss /= grad_accum_steps
        malbo_loss.backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    n_predict = training_manager.mtp_weights_schedule[step].size(0)
    lr = get_lr(step)
    bs = get_bs(step)
    print0(f"step:{step+1}/{train_steps} {loss.item()=} {n_predict=} {lr=:.4f} {bs=} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.19 (main, Oct 21 2025, 16:43:05) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Fri Jan 23 03:22:40 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:A1:00.0 Off |                    0 |
| N/A   41C    P0             82W /  310W |    1103MiB /  81559MiB |     14%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           14701      C   .../envs/speedrun/bin/python3.10       1094MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 lr=1.0000 bs=131072 n_predict=3 val_loss:10.8314 val_malbo_loss:10.8186 train_time:0ms step_avg:0.04ms
step:1/1775 loss.item()=38783.10546875 n_predict=3 lr=1.0000 bs=131072 train_time:458ms step_avg:457.64ms
step:2/1775 loss.item()=38569.3203125 n_predict=3 lr=1.0000 bs=131072 train_time:1734ms step_avg:866.98ms
step:3/1775 loss.item()=32861.875 n_predict=3 lr=1.0000 bs=131072 train_time:2103ms step_avg:700.86ms
step:4/1775 loss.item()=31558.529296875 n_predict=3 lr=1.0000 bs=131072 train_time:2505ms step_avg:626.22ms
step:5/1775 loss.item()=31607.3671875 n_predict=3 lr=1.0000 bs=131072 train_time:2905ms step_avg:581.01ms
step:6/1775 loss.item()=30794.89453125 n_predict=3 lr=1.0000 bs=131072 train_time:3307ms step_avg:551.16ms
step:7/1775 loss.item()=30619.1328125 n_predict=3 lr=1.0000 bs=131072 train_time:3709ms step_avg:529.82ms
step:8/1775 loss.item()=28111.763671875 n_predict=3 lr=1.0000 bs=131072 train_time:4113ms step_avg:514.14ms
step:9/1775 loss.item()=28874.49609375 n_predict=3 lr=1.0000 bs=131072 train_time:4514ms step_avg:501.54ms
step:10/1775 loss.item()=26721.376953125 n_predict=3 lr=1.0000 bs=131072 train_time:4919ms step_avg:491.87ms
step:11/1775 loss.item()=27370.44140625 n_predict=3 lr=1.0000 bs=131072 train_time:5321ms step_avg:483.77ms
step:12/1775 loss.item()=26872.42578125 n_predict=3 lr=1.0000 bs=131072 train_time:5728ms step_avg:477.31ms
step:13/1775 loss.item()=26787.77734375 n_predict=3 lr=1.0000 bs=131072 train_time:6132ms step_avg:471.67ms
step:14/1775 loss.item()=26300.19140625 n_predict=3 lr=1.0000 bs=131072 train_time:6538ms step_avg:466.99ms
step:15/1775 loss.item()=26216.794921875 n_predict=3 lr=1.0000 bs=131072 train_time:6942ms step_avg:462.77ms
step:16/1775 loss.item()=25543.953125 n_predict=3 lr=1.0000 bs=131072 train_time:7348ms step_avg:459.23ms
step:17/1775 loss.item()=25106.84375 n_predict=3 lr=1.0000 bs=131072 train_time:7753ms step_avg:456.06ms
step:18/1775 loss.item()=26510.388671875 n_predict=3 lr=1.0000 bs=131072 train_time:8161ms step_avg:453.38ms
step:19/1775 loss.item()=26316.423828125 n_predict=3 lr=1.0000 bs=131072 train_time:8562ms step_avg:450.62ms
step:20/1775 loss.item()=26555.83203125 n_predict=3 lr=1.0000 bs=131072 train_time:8967ms step_avg:448.37ms
step:21/1775 loss.item()=25653.07421875 n_predict=3 lr=1.0000 bs=131072 train_time:9373ms step_avg:446.32ms
step:22/1775 loss.item()=25495.4609375 n_predict=3 lr=1.0000 bs=131072 train_time:9779ms step_avg:444.49ms
step:23/1775 loss.item()=24840.423828125 n_predict=3 lr=1.0000 bs=131072 train_time:10184ms step_avg:442.77ms
step:24/1775 loss.item()=25142.623046875 n_predict=3 lr=1.0000 bs=131072 train_time:10588ms step_avg:441.19ms
step:25/1775 loss.item()=25797.75 n_predict=3 lr=1.0000 bs=131072 train_time:10992ms step_avg:439.69ms
step:26/1775 loss.item()=25145.798828125 n_predict=3 lr=1.0000 bs=131072 train_time:11400ms step_avg:438.45ms
step:27/1775 loss.item()=24883.357421875 n_predict=3 lr=1.0000 bs=131072 train_time:11803ms step_avg:437.16ms
step:28/1775 loss.item()=24803.83984375 n_predict=3 lr=1.0000 bs=131072 train_time:12208ms step_avg:436.01ms
step:29/1775 loss.item()=25640.1015625 n_predict=3 lr=1.0000 bs=131072 train_time:12613ms step_avg:434.93ms
step:30/1775 loss.item()=24697.03125 n_predict=3 lr=1.0000 bs=131072 train_time:13024ms step_avg:434.15ms
step:31/1775 loss.item()=23936.296875 n_predict=3 lr=1.0000 bs=131072 train_time:13425ms step_avg:433.08ms
step:32/1775 loss.item()=24037.53515625 n_predict=3 lr=1.0000 bs=131072 train_time:13832ms step_avg:432.24ms
step:33/1775 loss.item()=24711.5390625 n_predict=3 lr=1.0000 bs=131072 train_time:14236ms step_avg:431.39ms
step:34/1775 loss.item()=24071.83984375 n_predict=3 lr=1.0000 bs=131072 train_time:14642ms step_avg:430.64ms
step:35/1775 loss.item()=24444.1953125 n_predict=3 lr=1.0000 bs=131072 train_time:15045ms step_avg:429.87ms
step:36/1775 loss.item()=24363.41796875 n_predict=3 lr=1.0000 bs=131072 train_time:15451ms step_avg:429.19ms
step:37/1775 loss.item()=24000.0078125 n_predict=3 lr=1.0000 bs=131072 train_time:15858ms step_avg:428.59ms
step:38/1775 loss.item()=23482.62109375 n_predict=3 lr=1.0000 bs=131072 train_time:16263ms step_avg:427.97ms
step:39/1775 loss.item()=24096.744140625 n_predict=3 lr=1.0000 bs=131072 train_time:16666ms step_avg:427.33ms
step:40/1775 loss.item()=23589.990234375 n_predict=3 lr=1.0000 bs=131072 train_time:17072ms step_avg:426.79ms
step:41/1775 loss.item()=23667.107421875 n_predict=3 lr=1.0000 bs=131072 train_time:17474ms step_avg:426.19ms
step:42/1775 loss.item()=23816.166015625 n_predict=3 lr=1.0000 bs=131072 train_time:17880ms step_avg:425.72ms
step:43/1775 loss.item()=23816.796875 n_predict=3 lr=1.0000 bs=131072 train_time:18286ms step_avg:425.25ms
step:44/1775 loss.item()=23307.994140625 n_predict=3 lr=1.0000 bs=131072 train_time:18688ms step_avg:424.72ms
step:45/1775 loss.item()=22662.62890625 n_predict=3 lr=1.0000 bs=131072 train_time:19093ms step_avg:424.29ms
step:46/1775 loss.item()=23185.998046875 n_predict=3 lr=1.0000 bs=131072 train_time:19500ms step_avg:423.91ms
step:47/1775 loss.item()=23060.45703125 n_predict=3 lr=1.0000 bs=131072 train_time:19904ms step_avg:423.48ms
step:48/1775 loss.item()=22497.97265625 n_predict=3 lr=1.0000 bs=131072 train_time:20309ms step_avg:423.11ms
step:49/1775 loss.item()=24088.720703125 n_predict=3 lr=1.0000 bs=131072 train_time:20714ms step_avg:422.74ms
step:50/1775 loss.item()=22641.958984375 n_predict=3 lr=1.0000 bs=131072 train_time:21120ms step_avg:422.40ms
step:51/1775 loss.item()=22393.2421875 n_predict=3 lr=1.0000 bs=131072 train_time:21523ms step_avg:422.02ms
step:52/1775 loss.item()=22404.9609375 n_predict=3 lr=1.0000 bs=131072 train_time:21927ms step_avg:421.68ms
step:53/1775 loss.item()=22581.365234375 n_predict=3 lr=1.0000 bs=131072 train_time:22334ms step_avg:421.40ms
step:54/1775 loss.item()=23032.369140625 n_predict=3 lr=1.0000 bs=131072 train_time:22740ms step_avg:421.11ms
step:55/1775 loss.item()=22422.87109375 n_predict=3 lr=1.0000 bs=131072 train_time:23143ms step_avg:420.79ms
step:56/1775 loss.item()=22736.693359375 n_predict=3 lr=1.0000 bs=131072 train_time:23549ms step_avg:420.52ms
step:57/1775 loss.item()=22859.4375 n_predict=3 lr=1.0000 bs=131072 train_time:23954ms step_avg:420.24ms
step:58/1775 loss.item()=22329.83984375 n_predict=3 lr=1.0000 bs=131072 train_time:24360ms step_avg:420.01ms
step:59/1775 loss.item()=22742.75390625 n_predict=3 lr=1.0000 bs=131072 train_time:24764ms step_avg:419.73ms
step:60/1775 loss.item()=23231.51953125 n_predict=3 lr=1.0000 bs=131072 train_time:25168ms step_avg:419.47ms
step:61/1775 loss.item()=21168.330078125 n_predict=3 lr=1.0000 bs=131072 train_time:25574ms step_avg:419.24ms
step:62/1775 loss.item()=21958.94921875 n_predict=3 lr=1.0000 bs=131072 train_time:25980ms step_avg:419.04ms
step:63/1775 loss.item()=22507.30078125 n_predict=3 lr=1.0000 bs=131072 train_time:26384ms step_avg:418.79ms
step:64/1775 loss.item()=22688.48828125 n_predict=3 lr=1.0000 bs=131072 train_time:26788ms step_avg:418.57ms
step:65/1775 loss.item()=22572.265625 n_predict=3 lr=1.0000 bs=131072 train_time:27195ms step_avg:418.38ms
step:66/1775 loss.item()=22449.6796875 n_predict=3 lr=1.0000 bs=131072 train_time:27600ms step_avg:418.18ms
step:67/1775 loss.item()=22386.70703125 n_predict=3 lr=1.0000 bs=131072 train_time:28003ms step_avg:417.95ms
step:68/1775 loss.item()=23088.509765625 n_predict=3 lr=1.0000 bs=131072 train_time:28407ms step_avg:417.76ms
step:69/1775 loss.item()=21865.10546875 n_predict=3 lr=1.0000 bs=131072 train_time:28811ms step_avg:417.55ms
step:70/1775 loss.item()=21590.140625 n_predict=3 lr=1.0000 bs=131072 train_time:29218ms step_avg:417.41ms
step:71/1775 loss.item()=21899.646484375 n_predict=3 lr=1.0000 bs=131072 train_time:29621ms step_avg:417.20ms
step:72/1775 loss.item()=21367.421875 n_predict=3 lr=1.0000 bs=131072 train_time:30026ms step_avg:417.03ms
step:73/1775 loss.item()=21255.8046875 n_predict=3 lr=1.0000 bs=131072 train_time:30430ms step_avg:416.85ms
step:74/1775 loss.item()=21386.806640625 n_predict=3 lr=1.0000 bs=131072 train_time:30836ms step_avg:416.70ms
step:75/1775 loss.item()=21605.484375 n_predict=3 lr=1.0000 bs=131072 train_time:31240ms step_avg:416.53ms
step:76/1775 loss.item()=21015.17578125 n_predict=3 lr=1.0000 bs=131072 train_time:31646ms step_avg:416.39ms
step:77/1775 loss.item()=21223.861328125 n_predict=3 lr=1.0000 bs=131072 train_time:32050ms step_avg:416.23ms
step:78/1775 loss.item()=21800.0625 n_predict=3 lr=1.0000 bs=131072 train_time:32457ms step_avg:416.11ms
step:79/1775 loss.item()=21221.673828125 n_predict=3 lr=1.0000 bs=131072 train_time:32858ms step_avg:415.92ms
step:80/1775 loss.item()=21495.42578125 n_predict=3 lr=1.0000 bs=131072 train_time:33264ms step_avg:415.80ms
step:81/1775 loss.item()=21876.23828125 n_predict=3 lr=1.0000 bs=131072 train_time:33667ms step_avg:415.64ms
step:82/1775 loss.item()=21902.689453125 n_predict=3 lr=1.0000 bs=131072 train_time:34075ms step_avg:415.55ms
step:83/1775 loss.item()=20982.9140625 n_predict=3 lr=1.0000 bs=131072 train_time:34482ms step_avg:415.45ms
step:84/1775 loss.item()=20637.171875 n_predict=3 lr=1.0000 bs=131072 train_time:34889ms step_avg:415.34ms
step:85/1775 loss.item()=20922.03125 n_predict=3 lr=1.0000 bs=131072 train_time:35290ms step_avg:415.18ms
step:86/1775 loss.item()=21397.109375 n_predict=3 lr=1.0000 bs=131072 train_time:35695ms step_avg:415.06ms
step:87/1775 loss.item()=21073.134765625 n_predict=3 lr=1.0000 bs=131072 train_time:36099ms step_avg:414.93ms
step:88/1775 loss.item()=21181.505859375 n_predict=3 lr=1.0000 bs=131072 train_time:36506ms step_avg:414.84ms
step:89/1775 loss.item()=21060.935546875 n_predict=3 lr=1.0000 bs=131072 train_time:36908ms step_avg:414.70ms
step:90/1775 loss.item()=20974.560546875 n_predict=3 lr=1.0000 bs=131072 train_time:37315ms step_avg:414.62ms
step:91/1775 loss.item()=20954.96484375 n_predict=3 lr=1.0000 bs=131072 train_time:37718ms step_avg:414.49ms
step:92/1775 loss.item()=20674.5234375 n_predict=3 lr=1.0000 bs=131072 train_time:38123ms step_avg:414.38ms
step:93/1775 loss.item()=21233.24609375 n_predict=3 lr=1.0000 bs=131072 train_time:38525ms step_avg:414.25ms
step:94/1775 loss.item()=20949.96875 n_predict=3 lr=1.0000 bs=131072 train_time:38933ms step_avg:414.18ms
step:95/1775 loss.item()=21214.65234375 n_predict=3 lr=1.0000 bs=131072 train_time:39337ms step_avg:414.07ms
step:96/1775 loss.item()=21011.3359375 n_predict=3 lr=1.0000 bs=131072 train_time:39743ms step_avg:413.99ms
step:97/1775 loss.item()=21186.751953125 n_predict=3 lr=1.0000 bs=131072 train_time:40146ms step_avg:413.88ms
step:98/1775 loss.item()=20002.447265625 n_predict=3 lr=1.0000 bs=131072 train_time:40554ms step_avg:413.81ms
step:99/1775 loss.item()=20455.681640625 n_predict=3 lr=1.0000 bs=131072 train_time:40957ms step_avg:413.71ms
step:100/1775 loss.item()=21293.22265625 n_predict=3 lr=1.0000 bs=131072 train_time:41362ms step_avg:413.62ms
step:101/1775 loss.item()=20683.6796875 n_predict=3 lr=1.0000 bs=131072 train_time:41765ms step_avg:413.51ms
step:102/1775 loss.item()=19880.845703125 n_predict=3 lr=1.0000 bs=131072 train_time:42169ms step_avg:413.42ms
step:103/1775 loss.item()=20531.21484375 n_predict=3 lr=1.0000 bs=131072 train_time:42574ms step_avg:413.34ms
step:104/1775 loss.item()=21476.546875 n_predict=3 lr=1.0000 bs=131072 train_time:42982ms step_avg:413.29ms
step:105/1775 loss.item()=20232.37890625 n_predict=3 lr=1.0000 bs=131072 train_time:43384ms step_avg:413.18ms
step:106/1775 loss.item()=20215.359375 n_predict=3 lr=1.0000 bs=131072 train_time:43790ms step_avg:413.11ms
step:107/1775 loss.item()=20898.572265625 n_predict=3 lr=1.0000 bs=131072 train_time:44194ms step_avg:413.03ms
step:108/1775 loss.item()=20853.8125 n_predict=3 lr=1.0000 bs=131072 train_time:44601ms step_avg:412.97ms
step:109/1775 loss.item()=21131.328125 n_predict=3 lr=1.0000 bs=131072 train_time:45004ms step_avg:412.88ms
step:110/1775 loss.item()=20322.6484375 n_predict=3 lr=1.0000 bs=131072 train_time:45408ms step_avg:412.80ms
step:111/1775 loss.item()=20320.75390625 n_predict=3 lr=1.0000 bs=131072 train_time:45813ms step_avg:412.73ms
step:112/1775 loss.item()=20804.3515625 n_predict=3 lr=1.0000 bs=131072 train_time:46218ms step_avg:412.66ms
step:113/1775 loss.item()=20728.162109375 n_predict=3 lr=1.0000 bs=131072 train_time:46623ms step_avg:412.59ms
step:114/1775 loss.item()=19810.41796875 n_predict=3 lr=1.0000 bs=131072 train_time:47028ms step_avg:412.52ms
step:115/1775 loss.item()=19903.89453125 n_predict=3 lr=1.0000 bs=131072 train_time:47433ms step_avg:412.46ms
step:116/1775 loss.item()=20033.623046875 n_predict=3 lr=1.0000 bs=131072 train_time:47839ms step_avg:412.41ms
step:117/1775 loss.item()=20232.9453125 n_predict=3 lr=1.0000 bs=131072 train_time:48242ms step_avg:412.33ms
step:118/1775 loss.item()=21072.31640625 n_predict=3 lr=1.0000 bs=131072 train_time:48648ms step_avg:412.28ms
step:119/1775 loss.item()=20515.125 n_predict=3 lr=1.0000 bs=131072 train_time:49054ms step_avg:412.21ms
step:120/1775 loss.item()=20105.3984375 n_predict=3 lr=1.0000 bs=131072 train_time:49459ms step_avg:412.16ms
step:121/1775 loss.item()=19827.34765625 n_predict=3 lr=1.0000 bs=131072 train_time:49862ms step_avg:412.08ms
step:122/1775 loss.item()=20314.4921875 n_predict=3 lr=1.0000 bs=131072 train_time:50267ms step_avg:412.02ms
step:123/1775 loss.item()=19654.140625 n_predict=3 lr=1.0000 bs=131072 train_time:50670ms step_avg:411.95ms
step:124/1775 loss.item()=20457.7421875 n_predict=3 lr=1.0000 bs=131072 train_time:51078ms step_avg:411.92ms
step:125/1775 loss.item()=19801.099609375 n_predict=3 lr=1.0000 bs=131072 train_time:51481ms step_avg:411.85ms
step:126/1775 loss.item()=19982.2578125 n_predict=3 lr=1.0000 bs=131072 train_time:51886ms step_avg:411.80ms
step:127/1775 loss.item()=20075.58984375 n_predict=3 lr=1.0000 bs=131072 train_time:52292ms step_avg:411.75ms
step:128/1775 loss.item()=19339.94140625 n_predict=3 lr=1.0000 bs=131072 train_time:52699ms step_avg:411.71ms
step:129/1775 loss.item()=19258.80859375 n_predict=3 lr=1.0000 bs=131072 train_time:53102ms step_avg:411.64ms
step:130/1775 loss.item()=19812.888671875 n_predict=3 lr=1.0000 bs=131072 train_time:53507ms step_avg:411.60ms
step:131/1775 loss.item()=20642.4921875 n_predict=3 lr=1.0000 bs=131072 train_time:53910ms step_avg:411.53ms
step:132/1775 loss.item()=20108.765625 n_predict=3 lr=1.0000 bs=131072 train_time:54318ms step_avg:411.50ms
step:133/1775 loss.item()=20425.939453125 n_predict=3 lr=1.0000 bs=131072 train_time:54720ms step_avg:411.43ms
step:134/1775 loss.item()=19624.171875 n_predict=3 lr=1.0000 bs=131072 train_time:55126ms step_avg:411.39ms
step:135/1775 loss.item()=19319.4140625 n_predict=3 lr=1.0000 bs=131072 train_time:55529ms step_avg:411.32ms
step:136/1775 loss.item()=19911.5859375 n_predict=3 lr=1.0000 bs=131072 train_time:55935ms step_avg:411.29ms
step:137/1775 loss.item()=19340.154296875 n_predict=3 lr=1.0000 bs=131072 train_time:56340ms step_avg:411.24ms
step:138/1775 loss.item()=19550.22265625 n_predict=3 lr=1.0000 bs=131072 train_time:56746ms step_avg:411.20ms
step:139/1775 loss.item()=19333.13671875 n_predict=3 lr=1.0000 bs=131072 train_time:57150ms step_avg:411.15ms
step:140/1775 loss.item()=19941.154296875 n_predict=3 lr=1.0000 bs=131072 train_time:57557ms step_avg:411.12ms
step:141/1775 loss.item()=19752.87890625 n_predict=3 lr=1.0000 bs=131072 train_time:57960ms step_avg:411.06ms
step:142/1775 loss.item()=19643.126953125 n_predict=3 lr=1.0000 bs=131072 train_time:58364ms step_avg:411.02ms
step:143/1775 loss.item()=19258.80078125 n_predict=3 lr=1.0000 bs=131072 train_time:58767ms step_avg:410.96ms
step:144/1775 loss.item()=19659.650390625 n_predict=3 lr=1.0000 bs=131072 train_time:59172ms step_avg:410.92ms
step:145/1775 loss.item()=19314.4296875 n_predict=3 lr=1.0000 bs=131072 train_time:59577ms step_avg:410.87ms
step:146/1775 loss.item()=19489.72265625 n_predict=3 lr=1.0000 bs=131072 train_time:59982ms step_avg:410.84ms
step:147/1775 loss.item()=19174.255859375 n_predict=3 lr=1.0000 bs=131072 train_time:60384ms step_avg:410.77ms
step:148/1775 loss.item()=19832.62890625 n_predict=3 lr=1.0000 bs=131072 train_time:60790ms step_avg:410.74ms
step:149/1775 loss.item()=19630.0703125 n_predict=3 lr=1.0000 bs=131072 train_time:61193ms step_avg:410.69ms
step:150/1775 loss.item()=19418.6484375 n_predict=3 lr=1.0000 bs=131072 train_time:61601ms step_avg:410.67ms
step:151/1775 loss.item()=19126.890625 n_predict=3 lr=1.0000 bs=131072 train_time:62002ms step_avg:410.61ms
step:152/1775 loss.item()=19167.333984375 n_predict=3 lr=1.0000 bs=131072 train_time:62409ms step_avg:410.58ms
step:153/1775 loss.item()=18974.90625 n_predict=3 lr=1.0000 bs=131072 train_time:62812ms step_avg:410.54ms
step:154/1775 loss.item()=19455.068359375 n_predict=3 lr=1.0000 bs=131072 train_time:63218ms step_avg:410.51ms
step:155/1775 loss.item()=19436.71875 n_predict=3 lr=1.0000 bs=131072 train_time:63620ms step_avg:410.45ms
step:156/1775 loss.item()=19614.421875 n_predict=3 lr=1.0000 bs=131072 train_time:64025ms step_avg:410.42ms
step:157/1775 loss.item()=19765.955078125 n_predict=3 lr=1.0000 bs=131072 train_time:64427ms step_avg:410.36ms
step:158/1775 loss.item()=19089.13671875 n_predict=3 lr=1.0000 bs=131072 train_time:64834ms step_avg:410.34ms
step:159/1775 loss.item()=19441.302734375 n_predict=3 lr=1.0000 bs=131072 train_time:65238ms step_avg:410.30ms
step:160/1775 loss.item()=18880.591796875 n_predict=3 lr=1.0000 bs=131072 train_time:65643ms step_avg:410.27ms
step:161/1775 loss.item()=19239.498046875 n_predict=3 lr=1.0000 bs=131072 train_time:66046ms step_avg:410.22ms
step:162/1775 loss.item()=19053.126953125 n_predict=3 lr=1.0000 bs=131072 train_time:66451ms step_avg:410.19ms
step:163/1775 loss.item()=19419.90625 n_predict=3 lr=1.0000 bs=131072 train_time:66855ms step_avg:410.16ms
step:164/1775 loss.item()=19451.1640625 n_predict=3 lr=1.0000 bs=131072 train_time:67260ms step_avg:410.12ms
step:165/1775 loss.item()=19132.94921875 n_predict=3 lr=1.0000 bs=131072 train_time:67662ms step_avg:410.08ms
step:166/1775 loss.item()=19033.90625 n_predict=3 lr=1.0000 bs=131072 train_time:68067ms step_avg:410.04ms
step:167/1775 loss.item()=19300.75 n_predict=3 lr=1.0000 bs=131072 train_time:68472ms step_avg:410.01ms
step:168/1775 loss.item()=19620.041015625 n_predict=3 lr=1.0000 bs=131072 train_time:68878ms step_avg:409.99ms
step:169/1775 loss.item()=18920.671875 n_predict=3 lr=1.0000 bs=131072 train_time:69281ms step_avg:409.94ms
step:170/1775 loss.item()=18869.951171875 n_predict=3 lr=1.0000 bs=131072 train_time:69686ms step_avg:409.92ms
step:171/1775 loss.item()=18452.4140625 n_predict=3 lr=1.0000 bs=131072 train_time:70090ms step_avg:409.88ms
step:172/1775 loss.item()=18963.095703125 n_predict=3 lr=1.0000 bs=131072 train_time:70498ms step_avg:409.87ms
step:173/1775 loss.item()=19592.91796875 n_predict=3 lr=1.0000 bs=131072 train_time:70900ms step_avg:409.83ms
step:174/1775 loss.item()=19172.67578125 n_predict=3 lr=1.0000 bs=131072 train_time:71305ms step_avg:409.80ms
step:175/1775 loss.item()=19396.939453125 n_predict=3 lr=1.0000 bs=131072 train_time:71710ms step_avg:409.77ms
step:176/1775 loss.item()=18863.171875 n_predict=3 lr=1.0000 bs=131072 train_time:72116ms step_avg:409.75ms
step:177/1775 loss.item()=19635.623046875 n_predict=3 lr=1.0000 bs=131072 train_time:72518ms step_avg:409.71ms
step:178/1775 loss.item()=18627.6953125 n_predict=3 lr=1.0000 bs=131072 train_time:72923ms step_avg:409.68ms
step:179/1775 loss.item()=18908.51171875 n_predict=3 lr=1.0000 bs=131072 train_time:73324ms step_avg:409.63ms
step:180/1775 loss.item()=18628.34375 n_predict=3 lr=1.0000 bs=131072 train_time:73730ms step_avg:409.61ms
step:181/1775 loss.item()=18946.04296875 n_predict=3 lr=1.0000 bs=131072 train_time:74134ms step_avg:409.58ms
step:182/1775 loss.item()=18663.5703125 n_predict=3 lr=1.0000 bs=131072 train_time:74539ms step_avg:409.56ms
step:183/1775 loss.item()=18985.96484375 n_predict=3 lr=1.0000 bs=131072 train_time:74942ms step_avg:409.52ms
step:184/1775 loss.item()=18321.015625 n_predict=3 lr=1.0000 bs=131072 train_time:75347ms step_avg:409.50ms
step:185/1775 loss.item()=18422.66796875 n_predict=3 lr=1.0000 bs=131072 train_time:75755ms step_avg:409.49ms
step:186/1775 loss.item()=18872.17578125 n_predict=3 lr=1.0000 bs=131072 train_time:76160ms step_avg:409.46ms
step:187/1775 loss.item()=19290.505859375 n_predict=3 lr=1.0000 bs=131072 train_time:76562ms step_avg:409.42ms
step:188/1775 loss.item()=18531.50390625 n_predict=3 lr=1.0000 bs=131072 train_time:76967ms step_avg:409.40ms
step:189/1775 loss.item()=18559.505859375 n_predict=3 lr=1.0000 bs=131072 train_time:77371ms step_avg:409.37ms
step:190/1775 loss.item()=18585.3203125 n_predict=3 lr=1.0000 bs=131072 train_time:77779ms step_avg:409.36ms
step:191/1775 loss.item()=18659.26171875 n_predict=3 lr=1.0000 bs=131072 train_time:78182ms step_avg:409.33ms
step:192/1775 loss.item()=18965.5703125 n_predict=3 lr=1.0000 bs=131072 train_time:78587ms step_avg:409.31ms
step:193/1775 loss.item()=18480.7734375 n_predict=3 lr=1.0000 bs=131072 train_time:78993ms step_avg:409.29ms
step:194/1775 loss.item()=19621.4609375 n_predict=3 lr=1.0000 bs=131072 train_time:79400ms step_avg:409.28ms
step:195/1775 loss.item()=18627.748046875 n_predict=3 lr=1.0000 bs=131072 train_time:79804ms step_avg:409.25ms
step:196/1775 loss.item()=18406.181640625 n_predict=3 lr=1.0000 bs=131072 train_time:80209ms step_avg:409.23ms
step:197/1775 loss.item()=17986.609375 n_predict=3 lr=1.0000 bs=131072 train_time:80613ms step_avg:409.20ms
step:198/1775 loss.item()=18761.7109375 n_predict=3 lr=1.0000 bs=131072 train_time:81020ms step_avg:409.19ms
step:199/1775 loss.item()=19079.73828125 n_predict=3 lr=1.0000 bs=131072 train_time:81422ms step_avg:409.15ms
step:200/1775 loss.item()=18783.490234375 n_predict=3 lr=1.0000 bs=131072 train_time:81827ms step_avg:409.13ms
step:201/1775 loss.item()=18377.6640625 n_predict=3 lr=1.0000 bs=131072 train_time:82232ms step_avg:409.12ms
step:202/1775 loss.item()=18502.46484375 n_predict=3 lr=1.0000 bs=131072 train_time:82638ms step_avg:409.10ms
step:203/1775 loss.item()=18911.283203125 n_predict=3 lr=1.0000 bs=131072 train_time:83045ms step_avg:409.09ms
step:204/1775 loss.item()=18329.078125 n_predict=3 lr=1.0000 bs=131072 train_time:83451ms step_avg:409.07ms
step:205/1775 loss.item()=18272.85546875 n_predict=3 lr=1.0000 bs=131072 train_time:83855ms step_avg:409.05ms
step:206/1775 loss.item()=19123.42578125 n_predict=3 lr=1.0000 bs=131072 train_time:84262ms step_avg:409.04ms
step:207/1775 loss.item()=18565.69140625 n_predict=3 lr=1.0000 bs=131072 train_time:84663ms step_avg:409.00ms
step:208/1775 loss.item()=17897.021484375 n_predict=3 lr=1.0000 bs=131072 train_time:85070ms step_avg:408.99ms
step:209/1775 loss.item()=19770.490234375 n_predict=3 lr=1.0000 bs=131072 train_time:85473ms step_avg:408.96ms
step:210/1775 loss.item()=18103.62890625 n_predict=3 lr=1.0000 bs=131072 train_time:85880ms step_avg:408.95ms
step:211/1775 loss.item()=18510.59375 n_predict=3 lr=1.0000 bs=131072 train_time:86281ms step_avg:408.91ms
step:212/1775 loss.item()=18442.638671875 n_predict=3 lr=1.0000 bs=131072 train_time:86688ms step_avg:408.91ms
step:213/1775 loss.item()=18398.5390625 n_predict=3 lr=1.0000 bs=131072 train_time:87088ms step_avg:408.86ms
step:214/1775 loss.item()=18798.82421875 n_predict=3 lr=1.0000 bs=131072 train_time:87494ms step_avg:408.85ms
step:215/1775 loss.item()=18098.296875 n_predict=3 lr=1.0000 bs=131072 train_time:87895ms step_avg:408.82ms
step:216/1775 loss.item()=18269.2421875 n_predict=3 lr=1.0000 bs=131072 train_time:88304ms step_avg:408.82ms
step:217/1775 loss.item()=18434.953125 n_predict=3 lr=1.0000 bs=131072 train_time:88708ms step_avg:408.79ms
step:218/1775 loss.item()=19140.751953125 n_predict=3 lr=1.0000 bs=131072 train_time:89113ms step_avg:408.78ms
step:219/1775 loss.item()=19239.0234375 n_predict=3 lr=1.0000 bs=131072 train_time:89514ms step_avg:408.74ms
step:220/1775 loss.item()=18642.875 n_predict=3 lr=1.0000 bs=131072 train_time:89921ms step_avg:408.73ms
step:221/1775 loss.item()=18662.6640625 n_predict=3 lr=1.0000 bs=131072 train_time:90324ms step_avg:408.71ms
step:222/1775 loss.item()=19490.89453125 n_predict=3 lr=1.0000 bs=131072 train_time:90729ms step_avg:408.69ms
step:223/1775 loss.item()=18060.611328125 n_predict=3 lr=1.0000 bs=131072 train_time:91130ms step_avg:408.65ms
step:224/1775 loss.item()=17804.650390625 n_predict=3 lr=1.0000 bs=131072 train_time:91538ms step_avg:408.65ms
step:225/1775 loss.item()=18886.19921875 n_predict=3 lr=1.0000 bs=131072 train_time:91941ms step_avg:408.63ms
step:226/1775 loss.item()=18137.99609375 n_predict=3 lr=1.0000 bs=131072 train_time:92345ms step_avg:408.61ms
step:227/1775 loss.item()=18194.9375 n_predict=3 lr=1.0000 bs=131072 train_time:92748ms step_avg:408.58ms
step:228/1775 loss.item()=18069.453125 n_predict=3 lr=1.0000 bs=131072 train_time:93156ms step_avg:408.58ms
step:229/1775 loss.item()=18275.85546875 n_predict=3 lr=1.0000 bs=131072 train_time:93559ms step_avg:408.55ms
step:230/1775 loss.item()=17848.44921875 n_predict=3 lr=1.0000 bs=131072 train_time:93964ms step_avg:408.54ms
step:231/1775 loss.item()=17861.37109375 n_predict=3 lr=1.0000 bs=131072 train_time:94367ms step_avg:408.51ms
step:232/1775 loss.item()=17842.36328125 n_predict=3 lr=1.0000 bs=131072 train_time:94774ms step_avg:408.51ms
step:233/1775 loss.item()=17294.1171875 n_predict=3 lr=1.0000 bs=131072 train_time:95176ms step_avg:408.48ms
step:234/1775 loss.item()=18055.3515625 n_predict=3 lr=1.0000 bs=131072 train_time:95581ms step_avg:408.47ms
step:235/1775 loss.item()=17903.92578125 n_predict=3 lr=1.0000 bs=131072 train_time:95984ms step_avg:408.44ms
step:236/1775 loss.item()=18635.41796875 n_predict=3 lr=1.0000 bs=131072 train_time:96388ms step_avg:408.42ms
step:237/1775 loss.item()=17922.9296875 n_predict=3 lr=1.0000 bs=131072 train_time:96794ms step_avg:408.41ms
step:238/1775 loss.item()=18163.84375 n_predict=3 lr=1.0000 bs=131072 train_time:97199ms step_avg:408.40ms
step:239/1775 loss.item()=18216.08984375 n_predict=3 lr=1.0000 bs=131072 train_time:97603ms step_avg:408.38ms
step:240/1775 loss.item()=18108.1171875 n_predict=3 lr=1.0000 bs=131072 train_time:98008ms step_avg:408.37ms
step:241/1775 loss.item()=18391.630859375 n_predict=3 lr=1.0000 bs=131072 train_time:98412ms step_avg:408.35ms
step:242/1775 loss.item()=18723.544921875 n_predict=3 lr=1.0000 bs=131072 train_time:98818ms step_avg:408.34ms
step:243/1775 loss.item()=17852.1953125 n_predict=3 lr=1.0000 bs=131072 train_time:99220ms step_avg:408.31ms
step:244/1775 loss.item()=17990.4453125 n_predict=3 lr=1.0000 bs=131072 train_time:99623ms step_avg:408.29ms
step:245/1775 loss.item()=18554.94140625 n_predict=3 lr=1.0000 bs=131072 train_time:100026ms step_avg:408.27ms
step:246/1775 loss.item()=18204.125 n_predict=3 lr=1.0000 bs=131072 train_time:100433ms step_avg:408.26ms
step:247/1775 loss.item()=18309.439453125 n_predict=3 lr=1.0000 bs=131072 train_time:100836ms step_avg:408.24ms
step:248/1775 loss.item()=18041.841796875 n_predict=3 lr=1.0000 bs=131072 train_time:101241ms step_avg:408.23ms
step:249/1775 loss.item()=18586.0078125 n_predict=3 lr=1.0000 bs=131072 train_time:101643ms step_avg:408.21ms
step:250/1775 loss.item()=17759.916015625 n_predict=3 lr=1.0000 bs=131072 train_time:102049ms step_avg:408.20ms
step:250/1775 lr=1.0000 bs=131072 n_predict=3 val_loss:4.6121 val_malbo_loss:4.6820 train_time:102084ms step_avg:408.33ms
step:251/1775 loss.item()=17926.81640625 n_predict=3 lr=1.0000 bs=131072 train_time:102454ms step_avg:408.18ms
step:252/1775 loss.item()=17761.20703125 n_predict=3 lr=1.0000 bs=131072 train_time:102861ms step_avg:408.18ms
step:253/1775 loss.item()=17465.978515625 n_predict=3 lr=1.0000 bs=131072 train_time:103265ms step_avg:408.16ms
step:254/1775 loss.item()=17547.8125 n_predict=3 lr=1.0000 bs=131072 train_time:103671ms step_avg:408.15ms
step:255/1775 loss.item()=18162.291015625 n_predict=3 lr=1.0000 bs=131072 train_time:104073ms step_avg:408.13ms
step:256/1775 loss.item()=17688.66796875 n_predict=3 lr=1.0000 bs=131072 train_time:104479ms step_avg:408.12ms
step:257/1775 loss.item()=18265.767578125 n_predict=3 lr=1.0000 bs=131072 train_time:104880ms step_avg:408.09ms
step:258/1775 loss.item()=18261.6953125 n_predict=3 lr=1.0000 bs=131072 train_time:105289ms step_avg:408.10ms
step:259/1775 loss.item()=17490.775390625 n_predict=3 lr=1.0000 bs=131072 train_time:105691ms step_avg:408.07ms
step:260/1775 loss.item()=17923.197265625 n_predict=3 lr=1.0000 bs=131072 train_time:106094ms step_avg:408.06ms
step:261/1775 loss.item()=17628.0234375 n_predict=3 lr=1.0000 bs=131072 train_time:106496ms step_avg:408.03ms
step:262/1775 loss.item()=17673.650390625 n_predict=3 lr=1.0000 bs=131072 train_time:106901ms step_avg:408.02ms
step:263/1775 loss.item()=17574.1015625 n_predict=3 lr=1.0000 bs=131072 train_time:107305ms step_avg:408.00ms
step:264/1775 loss.item()=18160.376953125 n_predict=3 lr=1.0000 bs=131072 train_time:107711ms step_avg:408.00ms
step:265/1775 loss.item()=17601.7265625 n_predict=3 lr=1.0000 bs=131072 train_time:108113ms step_avg:407.97ms
step:266/1775 loss.item()=17896.26953125 n_predict=3 lr=1.0000 bs=131072 train_time:108518ms step_avg:407.96ms
step:267/1775 loss.item()=18064.61328125 n_predict=3 lr=1.0000 bs=131072 train_time:108920ms step_avg:407.94ms
step:268/1775 loss.item()=17962.46875 n_predict=3 lr=1.0000 bs=131072 train_time:109329ms step_avg:407.94ms
step:269/1775 loss.item()=18051.56640625 n_predict=3 lr=1.0000 bs=131072 train_time:109730ms step_avg:407.92ms
step:270/1775 loss.item()=17558.55859375 n_predict=3 lr=1.0000 bs=131072 train_time:110137ms step_avg:407.91ms
step:271/1775 loss.item()=17247.6484375 n_predict=3 lr=1.0000 bs=131072 train_time:110537ms step_avg:407.89ms
step:272/1775 loss.item()=18029.55078125 n_predict=3 lr=1.0000 bs=131072 train_time:110943ms step_avg:407.88ms
step:273/1775 loss.item()=17408.87890625 n_predict=3 lr=1.0000 bs=131072 train_time:111346ms step_avg:407.86ms
step:274/1775 loss.item()=18035.009765625 n_predict=3 lr=1.0000 bs=131072 train_time:111752ms step_avg:407.85ms
step:275/1775 loss.item()=16833.107421875 n_predict=3 lr=1.0000 bs=131072 train_time:112154ms step_avg:407.83ms
step:276/1775 loss.item()=17978.54296875 n_predict=3 lr=1.0000 bs=131072 train_time:112558ms step_avg:407.82ms
step:277/1775 loss.item()=17993.0234375 n_predict=3 lr=1.0000 bs=131072 train_time:112959ms step_avg:407.79ms
step:278/1775 loss.item()=17436.1171875 n_predict=3 lr=1.0000 bs=131072 train_time:113367ms step_avg:407.79ms
step:279/1775 loss.item()=17356.560546875 n_predict=3 lr=1.0000 bs=131072 train_time:113770ms step_avg:407.78ms
step:280/1775 loss.item()=17651.8515625 n_predict=3 lr=1.0000 bs=131072 train_time:114174ms step_avg:407.76ms
step:281/1775 loss.item()=17842.072265625 n_predict=3 lr=1.0000 bs=131072 train_time:114576ms step_avg:407.74ms
step:282/1775 loss.item()=17751.966796875 n_predict=3 lr=1.0000 bs=131072 train_time:114981ms step_avg:407.73ms
step:283/1775 loss.item()=17998.744140625 n_predict=3 lr=1.0000 bs=131072 train_time:115385ms step_avg:407.72ms
step:284/1775 loss.item()=17955.4921875 n_predict=3 lr=1.0000 bs=131072 train_time:115791ms step_avg:407.71ms
step:285/1775 loss.item()=17160.0546875 n_predict=3 lr=1.0000 bs=131072 train_time:116193ms step_avg:407.69ms
step:286/1775 loss.item()=18419.2109375 n_predict=3 lr=1.0000 bs=131072 train_time:116599ms step_avg:407.69ms
step:287/1775 loss.item()=17055.1328125 n_predict=3 lr=1.0000 bs=131072 train_time:117001ms step_avg:407.67ms
step:288/1775 loss.item()=17495.69921875 n_predict=3 lr=1.0000 bs=131072 train_time:117410ms step_avg:407.67ms
step:289/1775 loss.item()=17924.7265625 n_predict=3 lr=1.0000 bs=131072 train_time:117811ms step_avg:407.65ms
step:290/1775 loss.item()=17647.099609375 n_predict=3 lr=1.0000 bs=131072 train_time:118218ms step_avg:407.65ms
step:291/1775 loss.item()=16973.58984375 n_predict=3 lr=1.0000 bs=131072 train_time:118620ms step_avg:407.63ms
step:292/1775 loss.item()=17447.53125 n_predict=3 lr=1.0000 bs=131072 train_time:119026ms step_avg:407.62ms
step:293/1775 loss.item()=17479.552734375 n_predict=3 lr=1.0000 bs=131072 train_time:119430ms step_avg:407.61ms
step:294/1775 loss.item()=17774.3828125 n_predict=3 lr=1.0000 bs=131072 train_time:119834ms step_avg:407.60ms
step:295/1775 loss.item()=17261.337890625 n_predict=3 lr=1.0000 bs=131072 train_time:120234ms step_avg:407.57ms
step:296/1775 loss.item()=17571.484375 n_predict=3 lr=1.0000 bs=131072 train_time:120641ms step_avg:407.57ms
step:297/1775 loss.item()=17358.564453125 n_predict=3 lr=1.0000 bs=131072 train_time:121043ms step_avg:407.55ms
step:298/1775 loss.item()=17600.619140625 n_predict=3 lr=1.0000 bs=131072 train_time:121449ms step_avg:407.55ms
step:299/1775 loss.item()=17414.576171875 n_predict=3 lr=1.0000 bs=131072 train_time:121851ms step_avg:407.53ms
step:300/1775 loss.item()=17438.09375 n_predict=3 lr=1.0000 bs=131072 train_time:122255ms step_avg:407.52ms
step:301/1775 loss.item()=18151.0 n_predict=3 lr=1.0000 bs=131072 train_time:122655ms step_avg:407.49ms
step:302/1775 loss.item()=17581.2265625 n_predict=3 lr=1.0000 bs=131072 train_time:123061ms step_avg:407.49ms
step:303/1775 loss.item()=17067.8125 n_predict=3 lr=1.0000 bs=131072 train_time:123463ms step_avg:407.47ms
step:304/1775 loss.item()=17784.34765625 n_predict=3 lr=1.0000 bs=131072 train_time:123873ms step_avg:407.48ms
step:305/1775 loss.item()=17013.787109375 n_predict=3 lr=1.0000 bs=131072 train_time:124274ms step_avg:407.46ms
step:306/1775 loss.item()=17407.28515625 n_predict=3 lr=1.0000 bs=131072 train_time:124677ms step_avg:407.44ms
step:307/1775 loss.item()=17645.923828125 n_predict=3 lr=1.0000 bs=131072 train_time:125084ms step_avg:407.44ms
step:308/1775 loss.item()=17148.6796875 n_predict=3 lr=1.0000 bs=131072 train_time:125489ms step_avg:407.43ms
step:309/1775 loss.item()=17428.0859375 n_predict=3 lr=1.0000 bs=131072 train_time:125891ms step_avg:407.41ms
step:310/1775 loss.item()=17692.1796875 n_predict=3 lr=1.0000 bs=131072 train_time:126297ms step_avg:407.41ms
step:311/1775 loss.item()=16966.50390625 n_predict=3 lr=1.0000 bs=131072 train_time:126703ms step_avg:407.40ms
step:312/1775 loss.item()=17430.697265625 n_predict=3 lr=1.0000 bs=131072 train_time:127109ms step_avg:407.40ms
step:313/1775 loss.item()=17659.796875 n_predict=3 lr=1.0000 bs=131072 train_time:127513ms step_avg:407.39ms
step:314/1775 loss.item()=17229.787109375 n_predict=3 lr=1.0000 bs=131072 train_time:127919ms step_avg:407.38ms
step:315/1775 loss.item()=17784.56640625 n_predict=3 lr=1.0000 bs=131072 train_time:128322ms step_avg:407.37ms
step:316/1775 loss.item()=17499.056640625 n_predict=3 lr=1.0000 bs=131072 train_time:128729ms step_avg:407.37ms
step:317/1775 loss.item()=17522.2109375 n_predict=3 lr=1.0000 bs=131072 train_time:129133ms step_avg:407.36ms
step:318/1775 loss.item()=17843.44140625 n_predict=3 lr=1.0000 bs=131072 train_time:129537ms step_avg:407.35ms
step:319/1775 loss.item()=17245.560546875 n_predict=3 lr=1.0000 bs=131072 train_time:129941ms step_avg:407.34ms
step:320/1775 loss.item()=17846.091796875 n_predict=3 lr=1.0000 bs=131072 train_time:130346ms step_avg:407.33ms
step:321/1775 loss.item()=17048.5 n_predict=3 lr=1.0000 bs=131072 train_time:130748ms step_avg:407.31ms
step:322/1775 loss.item()=16943.73828125 n_predict=3 lr=1.0000 bs=131072 train_time:131152ms step_avg:407.31ms
step:323/1775 loss.item()=17784.64453125 n_predict=3 lr=1.0000 bs=131072 train_time:131553ms step_avg:407.29ms
step:324/1775 loss.item()=17567.986328125 n_predict=3 lr=1.0000 bs=131072 train_time:131958ms step_avg:407.28ms
step:325/1775 loss.item()=17231.359375 n_predict=3 lr=1.0000 bs=131072 train_time:132362ms step_avg:407.27ms
step:326/1775 loss.item()=17073.3671875 n_predict=3 lr=1.0000 bs=131072 train_time:132768ms step_avg:407.26ms
step:327/1775 loss.item()=16679.69140625 n_predict=3 lr=1.0000 bs=131072 train_time:133169ms step_avg:407.25ms
step:328/1775 loss.item()=17326.7890625 n_predict=3 lr=1.0000 bs=131072 train_time:133573ms step_avg:407.24ms
step:329/1775 loss.item()=17102.66796875 n_predict=3 lr=1.0000 bs=131072 train_time:133975ms step_avg:407.22ms
step:330/1775 loss.item()=17666.1484375 n_predict=3 lr=1.0000 bs=131072 train_time:134381ms step_avg:407.22ms
step:331/1775 loss.item()=17362.55859375 n_predict=3 lr=1.0000 bs=131072 train_time:134784ms step_avg:407.20ms
step:332/1775 loss.item()=17132.796875 n_predict=3 lr=1.0000 bs=131072 train_time:135190ms step_avg:407.20ms
step:333/1775 loss.item()=16765.689453125 n_predict=3 lr=1.0000 bs=131072 train_time:135594ms step_avg:407.19ms
step:334/1775 loss.item()=17258.73828125 n_predict=3 lr=1.0000 bs=131072 train_time:135999ms step_avg:407.18ms
step:335/1775 loss.item()=17176.55859375 n_predict=3 lr=1.0000 bs=131072 train_time:136403ms step_avg:407.17ms
step:336/1775 loss.item()=16709.7890625 n_predict=3 lr=1.0000 bs=131072 train_time:136806ms step_avg:407.16ms
step:337/1775 loss.item()=17401.77734375 n_predict=3 lr=1.0000 bs=131072 train_time:137212ms step_avg:407.16ms
step:338/1775 loss.item()=17188.07421875 n_predict=3 lr=1.0000 bs=131072 train_time:137616ms step_avg:407.15ms
step:339/1775 loss.item()=16984.5859375 n_predict=3 lr=1.0000 bs=131072 train_time:138018ms step_avg:407.13ms
step:340/1775 loss.item()=17201.73828125 n_predict=3 lr=1.0000 bs=131072 train_time:138425ms step_avg:407.13ms
step:341/1775 loss.item()=16484.21875 n_predict=3 lr=1.0000 bs=131072 train_time:138827ms step_avg:407.12ms
step:342/1775 loss.item()=17770.189453125 n_predict=3 lr=1.0000 bs=131072 train_time:139231ms step_avg:407.11ms
step:343/1775 loss.item()=17132.095703125 n_predict=3 lr=1.0000 bs=131072 train_time:139634ms step_avg:407.10ms
step:344/1775 loss.item()=17134.47265625 n_predict=3 lr=1.0000 bs=131072 train_time:140040ms step_avg:407.09ms
step:345/1775 loss.item()=17228.2265625 n_predict=3 lr=1.0000 bs=131072 train_time:140443ms step_avg:407.08ms
step:346/1775 loss.item()=17124.080078125 n_predict=3 lr=1.0000 bs=131072 train_time:140848ms step_avg:407.08ms
step:347/1775 loss.item()=15978.46484375 n_predict=3 lr=1.0000 bs=131072 train_time:141252ms step_avg:407.07ms
step:348/1775 loss.item()=17178.46484375 n_predict=3 lr=1.0000 bs=131072 train_time:141657ms step_avg:407.06ms
step:349/1775 loss.item()=17141.388671875 n_predict=3 lr=1.0000 bs=131072 train_time:142061ms step_avg:407.05ms
step:350/1775 loss.item()=17080.27734375 n_predict=3 lr=1.0000 bs=131072 train_time:142467ms step_avg:407.05ms
step:351/1775 loss.item()=17176.78125 n_predict=3 lr=1.0000 bs=131072 train_time:142869ms step_avg:407.03ms
step:352/1775 loss.item()=17057.73046875 n_predict=3 lr=1.0000 bs=131072 train_time:143274ms step_avg:407.03ms
step:353/1775 loss.item()=17021.078125 n_predict=3 lr=1.0000 bs=131072 train_time:143676ms step_avg:407.01ms
step:354/1775 loss.item()=17422.091796875 n_predict=3 lr=1.0000 bs=131072 train_time:144082ms step_avg:407.01ms
step:355/1775 loss.item()=17259.77734375 n_predict=3 lr=1.0000 bs=131072 train_time:144485ms step_avg:407.00ms
step:356/1775 loss.item()=16703.607421875 n_predict=3 lr=1.0000 bs=131072 train_time:144890ms step_avg:407.00ms
step:357/1775 loss.item()=17037.173828125 n_predict=3 lr=1.0000 bs=131072 train_time:145292ms step_avg:406.98ms
step:358/1775 loss.item()=16901.5625 n_predict=3 lr=1.0000 bs=131072 train_time:145697ms step_avg:406.98ms
step:359/1775 loss.item()=16441.203125 n_predict=3 lr=1.0000 bs=131072 train_time:146101ms step_avg:406.97ms
step:360/1775 loss.item()=16899.140625 n_predict=3 lr=1.0000 bs=131072 train_time:146507ms step_avg:406.96ms
step:361/1775 loss.item()=17307.16796875 n_predict=3 lr=1.0000 bs=131072 train_time:146908ms step_avg:406.95ms
step:362/1775 loss.item()=16758.61328125 n_predict=3 lr=1.0000 bs=131072 train_time:147313ms step_avg:406.94ms
step:363/1775 loss.item()=16700.640625 n_predict=3 lr=1.0000 bs=131072 train_time:147714ms step_avg:406.93ms
step:364/1775 loss.item()=16915.72265625 n_predict=3 lr=1.0000 bs=131072 train_time:148119ms step_avg:406.92ms
step:365/1775 loss.item()=17452.474609375 n_predict=3 lr=1.0000 bs=131072 train_time:148523ms step_avg:406.91ms
step:366/1775 loss.item()=16575.05859375 n_predict=3 lr=1.0000 bs=131072 train_time:148928ms step_avg:406.91ms
step:367/1775 loss.item()=16995.68359375 n_predict=3 lr=1.0000 bs=131072 train_time:149329ms step_avg:406.89ms
step:368/1775 loss.item()=16793.23046875 n_predict=3 lr=1.0000 bs=131072 train_time:149734ms step_avg:406.89ms
step:369/1775 loss.item()=16974.38671875 n_predict=3 lr=1.0000 bs=131072 train_time:150135ms step_avg:406.87ms
step:370/1775 loss.item()=17070.955078125 n_predict=3 lr=1.0000 bs=131072 train_time:150540ms step_avg:406.87ms
step:371/1775 loss.item()=16560.26953125 n_predict=3 lr=1.0000 bs=131072 train_time:150943ms step_avg:406.85ms
step:372/1775 loss.item()=17197.294921875 n_predict=3 lr=1.0000 bs=131072 train_time:151348ms step_avg:406.85ms
step:373/1775 loss.item()=17196.2421875 n_predict=3 lr=1.0000 bs=131072 train_time:151755ms step_avg:406.85ms
step:374/1775 loss.item()=16848.96875 n_predict=3 lr=1.0000 bs=131072 train_time:152159ms step_avg:406.84ms
step:375/1775 loss.item()=16878.650390625 n_predict=3 lr=1.0000 bs=131072 train_time:152560ms step_avg:406.83ms
step:376/1775 loss.item()=16691.43359375 n_predict=3 lr=1.0000 bs=131072 train_time:152968ms step_avg:406.83ms
step:377/1775 loss.item()=17148.08984375 n_predict=3 lr=1.0000 bs=131072 train_time:153370ms step_avg:406.82ms
step:378/1775 loss.item()=16860.90625 n_predict=3 lr=1.0000 bs=131072 train_time:153774ms step_avg:406.81ms
step:379/1775 loss.item()=16761.3984375 n_predict=3 lr=1.0000 bs=131072 train_time:154176ms step_avg:406.80ms
step:380/1775 loss.item()=16547.40234375 n_predict=3 lr=1.0000 bs=131072 train_time:154581ms step_avg:406.79ms
step:381/1775 loss.item()=16385.52734375 n_predict=3 lr=1.0000 bs=131072 train_time:154985ms step_avg:406.78ms
step:382/1775 loss.item()=16647.974609375 n_predict=3 lr=1.0000 bs=131072 train_time:155391ms step_avg:406.78ms
step:383/1775 loss.item()=16696.2734375 n_predict=3 lr=1.0000 bs=131072 train_time:155791ms step_avg:406.76ms
step:384/1775 loss.item()=16912.171875 n_predict=3 lr=1.0000 bs=131072 train_time:156196ms step_avg:406.76ms
step:385/1775 loss.item()=16509.1015625 n_predict=3 lr=1.0000 bs=131072 train_time:156596ms step_avg:406.74ms
step:386/1775 loss.item()=16752.294921875 n_predict=3 lr=1.0000 bs=131072 train_time:157001ms step_avg:406.74ms
step:387/1775 loss.item()=16489.57421875 n_predict=3 lr=1.0000 bs=131072 train_time:157404ms step_avg:406.73ms
step:388/1775 loss.item()=16604.853515625 n_predict=3 lr=1.0000 bs=131072 train_time:157810ms step_avg:406.73ms
step:389/1775 loss.item()=17159.078125 n_predict=3 lr=1.0000 bs=131072 train_time:158213ms step_avg:406.72ms
step:390/1775 loss.item()=17129.646484375 n_predict=3 lr=1.0000 bs=131072 train_time:158618ms step_avg:406.71ms
step:391/1775 loss.item()=16169.49609375 n_predict=3 lr=1.0000 bs=131072 train_time:159018ms step_avg:406.70ms
step:392/1775 loss.item()=16071.0322265625 n_predict=3 lr=1.0000 bs=131072 train_time:159427ms step_avg:406.70ms
step:393/1775 loss.item()=16302.177734375 n_predict=3 lr=1.0000 bs=131072 train_time:159829ms step_avg:406.69ms
step:394/1775 loss.item()=16890.87890625 n_predict=3 lr=1.0000 bs=131072 train_time:160233ms step_avg:406.68ms
step:395/1775 loss.item()=16161.80859375 n_predict=3 lr=1.0000 bs=131072 train_time:160636ms step_avg:406.67ms
step:396/1775 loss.item()=16314.6083984375 n_predict=3 lr=1.0000 bs=131072 train_time:161041ms step_avg:406.67ms
step:397/1775 loss.item()=16105.8212890625 n_predict=3 lr=1.0000 bs=131072 train_time:161444ms step_avg:406.66ms
step:398/1775 loss.item()=16644.60546875 n_predict=3 lr=1.0000 bs=131072 train_time:161850ms step_avg:406.66ms
step:399/1775 loss.item()=16404.1171875 n_predict=3 lr=1.0000 bs=131072 train_time:162251ms step_avg:406.64ms
step:400/1775 loss.item()=17051.2734375 n_predict=3 lr=1.0000 bs=131072 train_time:162656ms step_avg:406.64ms
step:401/1775 loss.item()=16707.220703125 n_predict=3 lr=1.0000 bs=131072 train_time:163056ms step_avg:406.62ms
step:402/1775 loss.item()=17017.076171875 n_predict=3 lr=1.0000 bs=131072 train_time:163461ms step_avg:406.62ms
step:403/1775 loss.item()=16488.20703125 n_predict=3 lr=1.0000 bs=131072 train_time:163864ms step_avg:406.61ms
step:404/1775 loss.item()=16292.73046875 n_predict=3 lr=1.0000 bs=131072 train_time:164270ms step_avg:406.61ms
step:405/1775 loss.item()=16612.001953125 n_predict=3 lr=1.0000 bs=131072 train_time:164673ms step_avg:406.60ms
step:406/1775 loss.item()=16140.275390625 n_predict=3 lr=1.0000 bs=131072 train_time:165078ms step_avg:406.60ms
step:407/1775 loss.item()=16587.04296875 n_predict=3 lr=1.0000 bs=131072 train_time:165478ms step_avg:406.58ms
step:408/1775 loss.item()=16474.359375 n_predict=3 lr=1.0000 bs=131072 train_time:165884ms step_avg:406.58ms
step:409/1775 loss.item()=16684.955078125 n_predict=3 lr=1.0000 bs=131072 train_time:166287ms step_avg:406.57ms
step:410/1775 loss.item()=16016.8681640625 n_predict=3 lr=1.0000 bs=131072 train_time:166692ms step_avg:406.57ms
step:411/1775 loss.item()=16645.408203125 n_predict=3 lr=1.0000 bs=131072 train_time:167096ms step_avg:406.56ms
step:412/1775 loss.item()=17213.92578125 n_predict=3 lr=1.0000 bs=131072 train_time:167500ms step_avg:406.55ms
step:413/1775 loss.item()=16169.8408203125 n_predict=3 lr=1.0000 bs=131072 train_time:167903ms step_avg:406.54ms
step:414/1775 loss.item()=16507.638671875 n_predict=3 lr=1.0000 bs=131072 train_time:168308ms step_avg:406.54ms
step:415/1775 loss.item()=15925.32421875 n_predict=3 lr=1.0000 bs=131072 train_time:168711ms step_avg:406.53ms
step:416/1775 loss.item()=16228.4443359375 n_predict=3 lr=1.0000 bs=131072 train_time:169117ms step_avg:406.53ms
step:417/1775 loss.item()=16550.740234375 n_predict=3 lr=1.0000 bs=131072 train_time:169518ms step_avg:406.52ms
step:418/1775 loss.item()=16189.318359375 n_predict=3 lr=1.0000 bs=131072 train_time:169922ms step_avg:406.51ms
step:419/1775 loss.item()=15870.044921875 n_predict=3 lr=1.0000 bs=131072 train_time:170325ms step_avg:406.50ms
step:420/1775 loss.item()=16863.53515625 n_predict=3 lr=1.0000 bs=131072 train_time:170732ms step_avg:406.51ms
step:421/1775 loss.item()=16426.984375 n_predict=3 lr=1.0000 bs=131072 train_time:171133ms step_avg:406.49ms
step:422/1775 loss.item()=15728.267578125 n_predict=3 lr=1.0000 bs=131072 train_time:171536ms step_avg:406.48ms
step:423/1775 loss.item()=16378.08203125 n_predict=3 lr=1.0000 bs=131072 train_time:171938ms step_avg:406.47ms
step:424/1775 loss.item()=15373.46484375 n_predict=3 lr=1.0000 bs=131072 train_time:172343ms step_avg:406.47ms
step:425/1775 loss.item()=15981.4775390625 n_predict=3 lr=1.0000 bs=131072 train_time:172746ms step_avg:406.46ms
step:426/1775 loss.item()=16027.6630859375 n_predict=3 lr=1.0000 bs=131072 train_time:173153ms step_avg:406.46ms
step:427/1775 loss.item()=15735.806640625 n_predict=3 lr=1.0000 bs=131072 train_time:173554ms step_avg:406.45ms
step:428/1775 loss.item()=16571.59765625 n_predict=3 lr=1.0000 bs=131072 train_time:173959ms step_avg:406.45ms
step:429/1775 loss.item()=16930.62890625 n_predict=3 lr=1.0000 bs=131072 train_time:174359ms step_avg:406.43ms
step:430/1775 loss.item()=16772.400390625 n_predict=3 lr=1.0000 bs=131072 train_time:174765ms step_avg:406.43ms
step:431/1775 loss.item()=15881.275390625 n_predict=3 lr=1.0000 bs=131072 train_time:175169ms step_avg:406.43ms
step:432/1775 loss.item()=16101.40234375 n_predict=3 lr=1.0000 bs=131072 train_time:175574ms step_avg:406.42ms
step:433/1775 loss.item()=16350.33203125 n_predict=3 lr=1.0000 bs=131072 train_time:175975ms step_avg:406.41ms
step:434/1775 loss.item()=15979.8046875 n_predict=3 lr=1.0000 bs=131072 train_time:176381ms step_avg:406.41ms
step:435/1775 loss.item()=16707.306640625 n_predict=3 lr=1.0000 bs=131072 train_time:176784ms step_avg:406.40ms
step:436/1775 loss.item()=16219.3291015625 n_predict=3 lr=1.0000 bs=131072 train_time:177189ms step_avg:406.40ms
step:437/1775 loss.item()=15994.7353515625 n_predict=3 lr=1.0000 bs=131072 train_time:177591ms step_avg:406.39ms
step:438/1775 loss.item()=16654.64453125 n_predict=3 lr=1.0000 bs=131072 train_time:177995ms step_avg:406.38ms
step:439/1775 loss.item()=15903.20703125 n_predict=3 lr=1.0000 bs=131072 train_time:178396ms step_avg:406.37ms
step:440/1775 loss.item()=15280.0166015625 n_predict=3 lr=1.0000 bs=131072 train_time:178802ms step_avg:406.37ms
step:441/1775 loss.item()=16420.05078125 n_predict=3 lr=1.0000 bs=131072 train_time:179206ms step_avg:406.36ms
step:442/1775 loss.item()=16105.33984375 n_predict=3 lr=1.0000 bs=131072 train_time:179613ms step_avg:406.36ms
step:443/1775 loss.item()=16223.26171875 n_predict=3 lr=1.0000 bs=131072 train_time:180014ms step_avg:406.35ms
step:444/1775 loss.item()=15705.060546875 n_predict=3 lr=1.0000 bs=131072 train_time:180419ms step_avg:406.35ms
step:445/1775 loss.item()=15939.240234375 n_predict=3 lr=1.0000 bs=131072 train_time:180820ms step_avg:406.34ms
step:446/1775 loss.item()=15354.6455078125 n_predict=3 lr=1.0000 bs=131072 train_time:181227ms step_avg:406.34ms
step:447/1775 loss.item()=15901.84375 n_predict=3 lr=1.0000 bs=131072 train_time:181630ms step_avg:406.33ms
step:448/1775 loss.item()=15748.173828125 n_predict=3 lr=1.0000 bs=131072 train_time:182035ms step_avg:406.33ms
step:449/1775 loss.item()=16419.08203125 n_predict=3 lr=1.0000 bs=131072 train_time:182434ms step_avg:406.31ms
step:450/1775 loss.item()=17832.7578125 n_predict=3 lr=1.0000 bs=131072 train_time:182840ms step_avg:406.31ms
step:451/1775 loss.item()=16147.462890625 n_predict=3 lr=1.0000 bs=131072 train_time:183241ms step_avg:406.30ms
step:452/1775 loss.item()=16689.830078125 n_predict=3 lr=1.0000 bs=131072 train_time:183647ms step_avg:406.30ms
step:453/1775 loss.item()=15972.912109375 n_predict=3 lr=1.0000 bs=131072 train_time:184049ms step_avg:406.29ms
step:454/1775 loss.item()=16029.31640625 n_predict=3 lr=1.0000 bs=131072 train_time:184453ms step_avg:406.28ms
step:455/1775 loss.item()=15886.12890625 n_predict=3 lr=1.0000 bs=131072 train_time:184856ms step_avg:406.28ms
step:456/1775 loss.item()=16809.630859375 n_predict=3 lr=1.0000 bs=131072 train_time:185260ms step_avg:406.27ms
step:457/1775 loss.item()=15949.9384765625 n_predict=3 lr=1.0000 bs=131072 train_time:185661ms step_avg:406.26ms
step:458/1775 loss.item()=15604.302734375 n_predict=3 lr=1.0000 bs=131072 train_time:186069ms step_avg:406.27ms
step:459/1775 loss.item()=16483.0234375 n_predict=3 lr=1.0000 bs=131072 train_time:186472ms step_avg:406.26ms
step:460/1775 loss.item()=16255.5966796875 n_predict=3 lr=1.0000 bs=131072 train_time:186877ms step_avg:406.25ms
step:461/1775 loss.item()=16027.47265625 n_predict=3 lr=1.0000 bs=131072 train_time:187279ms step_avg:406.24ms
step:462/1775 loss.item()=15989.8759765625 n_predict=3 lr=1.0000 bs=131072 train_time:187683ms step_avg:406.24ms
step:463/1775 loss.item()=16469.796875 n_predict=3 lr=1.0000 bs=131072 train_time:188086ms step_avg:406.23ms
step:464/1775 loss.item()=15246.4609375 n_predict=3 lr=1.0000 bs=131072 train_time:188492ms step_avg:406.23ms
step:465/1775 loss.item()=15840.017578125 n_predict=3 lr=1.0000 bs=131072 train_time:188893ms step_avg:406.22ms
step:466/1775 loss.item()=16195.65625 n_predict=3 lr=1.0000 bs=131072 train_time:189297ms step_avg:406.22ms
step:467/1775 loss.item()=16391.12890625 n_predict=3 lr=1.0000 bs=131072 train_time:189699ms step_avg:406.21ms
step:468/1775 loss.item()=15763.0654296875 n_predict=3 lr=1.0000 bs=131072 train_time:190103ms step_avg:406.20ms
step:469/1775 loss.item()=15675.2373046875 n_predict=3 lr=1.0000 bs=131072 train_time:190508ms step_avg:406.20ms
step:470/1775 loss.item()=15563.349609375 n_predict=3 lr=1.0000 bs=131072 train_time:190913ms step_avg:406.20ms
step:471/1775 loss.item()=15571.421875 n_predict=3 lr=1.0000 bs=131072 train_time:191314ms step_avg:406.19ms
step:472/1775 loss.item()=15742.12109375 n_predict=3 lr=1.0000 bs=131072 train_time:191721ms step_avg:406.19ms
step:473/1775 loss.item()=16284.0205078125 n_predict=3 lr=1.0000 bs=131072 train_time:192121ms step_avg:406.17ms
step:474/1775 loss.item()=15626.12109375 n_predict=3 lr=1.0000 bs=131072 train_time:192529ms step_avg:406.18ms
step:475/1775 loss.item()=16213.3212890625 n_predict=3 lr=1.0000 bs=131072 train_time:192930ms step_avg:406.17ms
step:476/1775 loss.item()=16112.68359375 n_predict=3 lr=1.0000 bs=131072 train_time:193334ms step_avg:406.16ms
step:477/1775 loss.item()=16216.7880859375 n_predict=3 lr=1.0000 bs=131072 train_time:193737ms step_avg:406.16ms
step:478/1775 loss.item()=15586.9375 n_predict=3 lr=1.0000 bs=131072 train_time:194142ms step_avg:406.16ms
step:479/1775 loss.item()=15738.271484375 n_predict=3 lr=1.0000 bs=131072 train_time:194544ms step_avg:406.15ms
step:480/1775 loss.item()=15534.65234375 n_predict=3 lr=1.0000 bs=131072 train_time:194950ms step_avg:406.15ms
step:481/1775 loss.item()=15560.251953125 n_predict=3 lr=1.0000 bs=131072 train_time:195351ms step_avg:406.14ms
step:482/1775 loss.item()=15486.6953125 n_predict=3 lr=1.0000 bs=131072 train_time:195756ms step_avg:406.13ms
step:483/1775 loss.item()=15468.6796875 n_predict=3 lr=1.0000 bs=131072 train_time:196157ms step_avg:406.12ms
step:484/1775 loss.item()=15959.13671875 n_predict=3 lr=1.0000 bs=131072 train_time:196560ms step_avg:406.12ms
step:485/1775 loss.item()=15522.6259765625 n_predict=3 lr=1.0000 bs=131072 train_time:196966ms step_avg:406.12ms
step:486/1775 loss.item()=15771.056640625 n_predict=3 lr=1.0000 bs=131072 train_time:197373ms step_avg:406.12ms
step:487/1775 loss.item()=15209.513671875 n_predict=3 lr=1.0000 bs=131072 train_time:197776ms step_avg:406.11ms
step:488/1775 loss.item()=15055.3369140625 n_predict=3 lr=1.0000 bs=131072 train_time:198181ms step_avg:406.11ms
step:489/1775 loss.item()=15327.5517578125 n_predict=3 lr=1.0000 bs=131072 train_time:198583ms step_avg:406.10ms
step:490/1775 loss.item()=15751.0927734375 n_predict=3 lr=1.0000 bs=131072 train_time:198989ms step_avg:406.10ms
step:491/1775 loss.item()=15764.8466796875 n_predict=3 lr=1.0000 bs=131072 train_time:199391ms step_avg:406.09ms
step:492/1775 loss.item()=15639.59375 n_predict=3 lr=1.0000 bs=131072 train_time:199796ms step_avg:406.09ms
step:493/1775 loss.item()=16259.951171875 n_predict=3 lr=1.0000 bs=131072 train_time:200195ms step_avg:406.07ms
step:494/1775 loss.item()=15488.78125 n_predict=3 lr=1.0000 bs=131072 train_time:200600ms step_avg:406.07ms
step:495/1775 loss.item()=15555.78515625 n_predict=3 lr=1.0000 bs=131072 train_time:201004ms step_avg:406.07ms
step:496/1775 loss.item()=15657.59765625 n_predict=3 lr=1.0000 bs=131072 train_time:201409ms step_avg:406.07ms
step:497/1775 loss.item()=15649.57421875 n_predict=3 lr=1.0000 bs=131072 train_time:201812ms step_avg:406.06ms
step:498/1775 loss.item()=15931.03125 n_predict=3 lr=1.0000 bs=131072 train_time:202216ms step_avg:406.06ms
step:499/1775 loss.item()=15254.533203125 n_predict=3 lr=1.0000 bs=131072 train_time:202618ms step_avg:406.05ms
step:500/1775 loss.item()=14809.3017578125 n_predict=3 lr=1.0000 bs=131072 train_time:203024ms step_avg:406.05ms
step:500/1775 lr=1.0000 bs=131072 n_predict=3 val_loss:4.2954 val_malbo_loss:4.3711 train_time:203055ms step_avg:406.11ms
step:501/1775 loss.item()=15603.59765625 n_predict=3 lr=1.0000 bs=131072 train_time:203429ms step_avg:406.05ms
step:502/1775 loss.item()=15970.2890625 n_predict=3 lr=1.0000 bs=131072 train_time:203835ms step_avg:406.05ms
step:503/1775 loss.item()=15739.2314453125 n_predict=3 lr=1.0000 bs=131072 train_time:204237ms step_avg:406.04ms
step:504/1775 loss.item()=15221.0302734375 n_predict=3 lr=1.0000 bs=131072 train_time:204639ms step_avg:406.03ms
step:505/1775 loss.item()=16062.94921875 n_predict=3 lr=1.0000 bs=131072 train_time:205041ms step_avg:406.02ms
step:506/1775 loss.item()=15668.3984375 n_predict=3 lr=1.0000 bs=131072 train_time:205447ms step_avg:406.02ms
step:507/1775 loss.item()=15202.396484375 n_predict=3 lr=1.0000 bs=131072 train_time:205847ms step_avg:406.01ms
step:508/1775 loss.item()=15851.533203125 n_predict=3 lr=1.0000 bs=131072 train_time:206256ms step_avg:406.02ms
step:509/1775 loss.item()=15321.38671875 n_predict=3 lr=1.0000 bs=131072 train_time:206657ms step_avg:406.01ms
step:510/1775 loss.item()=16180.04296875 n_predict=3 lr=1.0000 bs=131072 train_time:207062ms step_avg:406.00ms
step:511/1775 loss.item()=15891.73046875 n_predict=3 lr=1.0000 bs=131072 train_time:207463ms step_avg:405.99ms
step:512/1775 loss.item()=15298.560546875 n_predict=3 lr=1.0000 bs=131072 train_time:207868ms step_avg:405.99ms
step:513/1775 loss.item()=14743.29296875 n_predict=3 lr=1.0000 bs=131072 train_time:208271ms step_avg:405.99ms
step:514/1775 loss.item()=15499.169921875 n_predict=3 lr=1.0000 bs=131072 train_time:208677ms step_avg:405.99ms
step:515/1775 loss.item()=15227.234375 n_predict=3 lr=1.0000 bs=131072 train_time:209078ms step_avg:405.98ms
step:516/1775 loss.item()=16038.103515625 n_predict=3 lr=1.0000 bs=131072 train_time:209482ms step_avg:405.97ms
step:517/1775 loss.item()=15766.41015625 n_predict=3 lr=1.0000 bs=131072 train_time:209884ms step_avg:405.97ms
step:518/1775 loss.item()=15362.3525390625 n_predict=3 lr=1.0000 bs=131072 train_time:210290ms step_avg:405.96ms
step:519/1775 loss.item()=14932.9130859375 n_predict=3 lr=1.0000 bs=131072 train_time:210693ms step_avg:405.96ms
step:520/1775 loss.item()=15421.8583984375 n_predict=3 lr=1.0000 bs=131072 train_time:211098ms step_avg:405.96ms
step:521/1775 loss.item()=15068.0546875 n_predict=3 lr=1.0000 bs=131072 train_time:211500ms step_avg:405.95ms
step:522/1775 loss.item()=15602.1171875 n_predict=3 lr=1.0000 bs=131072 train_time:211905ms step_avg:405.95ms
step:523/1775 loss.item()=15743.4990234375 n_predict=3 lr=1.0000 bs=131072 train_time:212306ms step_avg:405.94ms
step:524/1775 loss.item()=15657.7919921875 n_predict=3 lr=1.0000 bs=131072 train_time:212712ms step_avg:405.94ms
step:525/1775 loss.item()=15770.376953125 n_predict=3 lr=1.0000 bs=131072 train_time:213115ms step_avg:405.93ms
step:526/1775 loss.item()=15692.763671875 n_predict=3 lr=1.0000 bs=131072 train_time:213520ms step_avg:405.93ms
step:527/1775 loss.item()=15131.61328125 n_predict=3 lr=1.0000 bs=131072 train_time:213921ms step_avg:405.92ms
step:528/1775 loss.item()=15492.09375 n_predict=3 lr=1.0000 bs=131072 train_time:214325ms step_avg:405.92ms
step:529/1775 loss.item()=15424.6796875 n_predict=3 lr=1.0000 bs=131072 train_time:214725ms step_avg:405.91ms
step:530/1775 loss.item()=15557.93359375 n_predict=3 lr=1.0000 bs=131072 train_time:215129ms step_avg:405.90ms
step:531/1775 loss.item()=15306.447265625 n_predict=3 lr=1.0000 bs=131072 train_time:215533ms step_avg:405.90ms
step:532/1775 loss.item()=15028.4404296875 n_predict=3 lr=1.0000 bs=131072 train_time:215939ms step_avg:405.90ms
step:533/1775 loss.item()=15413.2958984375 n_predict=3 lr=1.0000 bs=131072 train_time:216340ms step_avg:405.89ms
step:534/1775 loss.item()=15442.6640625 n_predict=3 lr=1.0000 bs=131072 train_time:216745ms step_avg:405.89ms
step:535/1775 loss.item()=15758.9375 n_predict=3 lr=1.0000 bs=131072 train_time:217146ms step_avg:405.88ms
step:536/1775 loss.item()=15297.41015625 n_predict=3 lr=1.0000 bs=131072 train_time:217552ms step_avg:405.88ms
step:537/1775 loss.item()=15310.71875 n_predict=3 lr=1.0000 bs=131072 train_time:217956ms step_avg:405.88ms
step:538/1775 loss.item()=14988.439453125 n_predict=3 lr=1.0000 bs=131072 train_time:218361ms step_avg:405.88ms
step:539/1775 loss.item()=15271.466796875 n_predict=3 lr=1.0000 bs=131072 train_time:218763ms step_avg:405.87ms
step:540/1775 loss.item()=15922.61328125 n_predict=3 lr=1.0000 bs=131072 train_time:219168ms step_avg:405.87ms
step:541/1775 loss.item()=15381.646484375 n_predict=3 lr=1.0000 bs=131072 train_time:219572ms step_avg:405.86ms
step:542/1775 loss.item()=15879.9794921875 n_predict=3 lr=1.0000 bs=131072 train_time:219977ms step_avg:405.86ms
step:543/1775 loss.item()=15308.708984375 n_predict=3 lr=1.0000 bs=131072 train_time:220378ms step_avg:405.85ms
step:544/1775 loss.item()=14749.1015625 n_predict=3 lr=1.0000 bs=131072 train_time:220782ms step_avg:405.85ms
step:545/1775 loss.item()=15797.02734375 n_predict=3 lr=1.0000 bs=131072 train_time:221184ms step_avg:405.84ms
step:546/1775 loss.item()=15716.890625 n_predict=3 lr=1.0000 bs=131072 train_time:221588ms step_avg:405.84ms
step:547/1775 loss.item()=16134.388671875 n_predict=3 lr=1.0000 bs=131072 train_time:221991ms step_avg:405.83ms
step:548/1775 loss.item()=14698.220703125 n_predict=3 lr=1.0000 bs=131072 train_time:222398ms step_avg:405.84ms
step:549/1775 loss.item()=14950.1953125 n_predict=3 lr=1.0000 bs=131072 train_time:222800ms step_avg:405.83ms
step:550/1775 loss.item()=15152.2314453125 n_predict=3 lr=1.0000 bs=131072 train_time:223205ms step_avg:405.83ms
step:551/1775 loss.item()=15256.404296875 n_predict=3 lr=1.0000 bs=131072 train_time:223606ms step_avg:405.82ms
step:552/1775 loss.item()=14586.443359375 n_predict=3 lr=1.0000 bs=131072 train_time:224013ms step_avg:405.82ms
step:553/1775 loss.item()=15040.359375 n_predict=3 lr=1.0000 bs=131072 train_time:224416ms step_avg:405.82ms
step:554/1775 loss.item()=15237.18359375 n_predict=3 lr=1.0000 bs=131072 train_time:224820ms step_avg:405.81ms
step:555/1775 loss.item()=15474.79296875 n_predict=3 lr=1.0000 bs=131072 train_time:225222ms step_avg:405.81ms
step:556/1775 loss.item()=15567.6796875 n_predict=3 lr=1.0000 bs=131072 train_time:225627ms step_avg:405.80ms
step:557/1775 loss.item()=15017.40625 n_predict=3 lr=1.0000 bs=131072 train_time:226028ms step_avg:405.79ms
step:558/1775 loss.item()=15314.765625 n_predict=3 lr=1.0000 bs=131072 train_time:226432ms step_avg:405.79ms
step:559/1775 loss.item()=15458.9736328125 n_predict=3 lr=1.0000 bs=131072 train_time:226834ms step_avg:405.79ms
step:560/1775 loss.item()=14885.0166015625 n_predict=3 lr=1.0000 bs=131072 train_time:227241ms step_avg:405.79ms
step:561/1775 loss.item()=15765.751953125 n_predict=3 lr=1.0000 bs=131072 train_time:227641ms step_avg:405.78ms
step:562/1775 loss.item()=14583.189453125 n_predict=3 lr=1.0000 bs=131072 train_time:228045ms step_avg:405.77ms
step:563/1775 loss.item()=14656.8515625 n_predict=3 lr=1.0000 bs=131072 train_time:228447ms step_avg:405.77ms
step:564/1775 loss.item()=15173.798828125 n_predict=3 lr=1.0000 bs=131072 train_time:228852ms step_avg:405.77ms
step:565/1775 loss.item()=15407.298828125 n_predict=3 lr=1.0000 bs=131072 train_time:229258ms step_avg:405.77ms
step:566/1775 loss.item()=14823.5107421875 n_predict=3 lr=1.0000 bs=131072 train_time:229661ms step_avg:405.76ms
step:567/1775 loss.item()=14541.12890625 n_predict=3 lr=1.0000 bs=131072 train_time:230062ms step_avg:405.75ms
step:568/1775 loss.item()=15237.099609375 n_predict=3 lr=1.0000 bs=131072 train_time:230468ms step_avg:405.75ms
step:569/1775 loss.item()=14672.03515625 n_predict=3 lr=1.0000 bs=131072 train_time:230869ms step_avg:405.75ms
step:570/1775 loss.item()=15304.619140625 n_predict=3 lr=1.0000 bs=131072 train_time:231276ms step_avg:405.75ms
step:571/1775 loss.item()=15363.1484375 n_predict=3 lr=1.0000 bs=131072 train_time:231677ms step_avg:405.74ms
step:572/1775 loss.item()=14418.83984375 n_predict=3 lr=1.0000 bs=131072 train_time:232081ms step_avg:405.74ms
step:573/1775 loss.item()=14981.951171875 n_predict=3 lr=1.0000 bs=131072 train_time:232483ms step_avg:405.73ms
step:574/1775 loss.item()=14939.876953125 n_predict=3 lr=1.0000 bs=131072 train_time:232886ms step_avg:405.73ms
step:575/1775 loss.item()=14598.8515625 n_predict=3 lr=1.0000 bs=131072 train_time:233289ms step_avg:405.72ms
step:576/1775 loss.item()=14763.16796875 n_predict=3 lr=1.0000 bs=131072 train_time:233695ms step_avg:405.72ms
step:577/1775 loss.item()=15147.81640625 n_predict=3 lr=1.0000 bs=131072 train_time:234098ms step_avg:405.72ms
step:578/1775 loss.item()=14801.009765625 n_predict=3 lr=1.0000 bs=131072 train_time:234503ms step_avg:405.71ms
step:579/1775 loss.item()=14400.525390625 n_predict=3 lr=1.0000 bs=131072 train_time:234904ms step_avg:405.71ms
step:580/1775 loss.item()=30504.140625 n_predict=2 lr=1.5200 bs=262144 train_time:261400ms step_avg:450.69ms
step:581/1775 loss.item()=31004.439453125 n_predict=2 lr=1.5200 bs=262144 train_time:262129ms step_avg:451.17ms
step:582/1775 loss.item()=29144.71875 n_predict=2 lr=1.5200 bs=262144 train_time:262864ms step_avg:451.66ms
step:583/1775 loss.item()=30947.234375 n_predict=2 lr=1.5200 bs=262144 train_time:263599ms step_avg:452.14ms
step:584/1775 loss.item()=30166.2734375 n_predict=2 lr=1.5200 bs=262144 train_time:264332ms step_avg:452.62ms
step:585/1775 loss.item()=30758.65625 n_predict=2 lr=1.5200 bs=262144 train_time:265059ms step_avg:453.09ms
step:586/1775 loss.item()=29969.91796875 n_predict=2 lr=1.5200 bs=262144 train_time:265792ms step_avg:453.57ms
step:587/1775 loss.item()=30950.15625 n_predict=2 lr=1.5200 bs=262144 train_time:266527ms step_avg:454.05ms
step:588/1775 loss.item()=30363.23046875 n_predict=2 lr=1.5200 bs=262144 train_time:267264ms step_avg:454.53ms
step:589/1775 loss.item()=30148.15234375 n_predict=2 lr=1.5200 bs=262144 train_time:267999ms step_avg:455.01ms
step:590/1775 loss.item()=29590.8359375 n_predict=2 lr=1.5200 bs=262144 train_time:268734ms step_avg:455.48ms
step:591/1775 loss.item()=29835.83203125 n_predict=2 lr=1.5200 bs=262144 train_time:269468ms step_avg:455.95ms
step:592/1775 loss.item()=30509.083984375 n_predict=2 lr=1.5200 bs=262144 train_time:270206ms step_avg:456.43ms
step:593/1775 loss.item()=30476.357421875 n_predict=2 lr=1.5200 bs=262144 train_time:270942ms step_avg:456.90ms
step:594/1775 loss.item()=29681.486328125 n_predict=2 lr=1.5200 bs=262144 train_time:271675ms step_avg:457.37ms
step:595/1775 loss.item()=30208.70703125 n_predict=2 lr=1.5200 bs=262144 train_time:272416ms step_avg:457.84ms
step:596/1775 loss.item()=29403.947265625 n_predict=2 lr=1.5200 bs=262144 train_time:273145ms step_avg:458.30ms
step:597/1775 loss.item()=28954.86328125 n_predict=2 lr=1.5200 bs=262144 train_time:273881ms step_avg:458.76ms
step:598/1775 loss.item()=29330.24609375 n_predict=2 lr=1.5200 bs=262144 train_time:274620ms step_avg:459.23ms
step:599/1775 loss.item()=29311.810546875 n_predict=2 lr=1.5200 bs=262144 train_time:275351ms step_avg:459.68ms
step:600/1775 loss.item()=28932.359375 n_predict=2 lr=1.5200 bs=262144 train_time:276090ms step_avg:460.15ms
step:601/1775 loss.item()=29553.501953125 n_predict=2 lr=1.5200 bs=262144 train_time:276826ms step_avg:460.61ms
step:602/1775 loss.item()=29900.720703125 n_predict=2 lr=1.5200 bs=262144 train_time:277578ms step_avg:461.09ms
step:603/1775 loss.item()=28461.81640625 n_predict=2 lr=1.5200 bs=262144 train_time:278315ms step_avg:461.55ms
step:604/1775 loss.item()=29177.802734375 n_predict=2 lr=1.5200 bs=262144 train_time:279052ms step_avg:462.01ms
step:605/1775 loss.item()=30229.7734375 n_predict=2 lr=1.5200 bs=262144 train_time:279788ms step_avg:462.46ms
step:606/1775 loss.item()=28787.798828125 n_predict=2 lr=1.5200 bs=262144 train_time:280529ms step_avg:462.92ms
step:607/1775 loss.item()=30046.279296875 n_predict=2 lr=1.5200 bs=262144 train_time:281263ms step_avg:463.37ms
step:608/1775 loss.item()=29637.15625 n_predict=2 lr=1.5200 bs=262144 train_time:282002ms step_avg:463.82ms
step:609/1775 loss.item()=28331.638671875 n_predict=2 lr=1.5200 bs=262144 train_time:282737ms step_avg:464.26ms
step:610/1775 loss.item()=29072.03515625 n_predict=2 lr=1.5200 bs=262144 train_time:283472ms step_avg:464.71ms
step:611/1775 loss.item()=29412.05078125 n_predict=2 lr=1.5200 bs=262144 train_time:284210ms step_avg:465.15ms
step:612/1775 loss.item()=28167.248046875 n_predict=2 lr=1.5200 bs=262144 train_time:284948ms step_avg:465.60ms
step:613/1775 loss.item()=29652.45703125 n_predict=2 lr=1.5200 bs=262144 train_time:285686ms step_avg:466.05ms
step:614/1775 loss.item()=29471.86328125 n_predict=2 lr=1.5200 bs=262144 train_time:286427ms step_avg:466.49ms
step:615/1775 loss.item()=28254.84765625 n_predict=2 lr=1.5200 bs=262144 train_time:287161ms step_avg:466.93ms
step:616/1775 loss.item()=29101.99609375 n_predict=2 lr=1.5200 bs=262144 train_time:287903ms step_avg:467.37ms
step:617/1775 loss.item()=28526.4609375 n_predict=2 lr=1.5200 bs=262144 train_time:288638ms step_avg:467.81ms
step:618/1775 loss.item()=27824.658203125 n_predict=2 lr=1.5200 bs=262144 train_time:289376ms step_avg:468.25ms
step:619/1775 loss.item()=29140.1640625 n_predict=2 lr=1.5200 bs=262144 train_time:290112ms step_avg:468.68ms
step:620/1775 loss.item()=28693.44140625 n_predict=2 lr=1.5200 bs=262144 train_time:290850ms step_avg:469.11ms
step:621/1775 loss.item()=28514.23046875 n_predict=2 lr=1.5200 bs=262144 train_time:291588ms step_avg:469.55ms
step:622/1775 loss.item()=28790.482421875 n_predict=2 lr=1.5200 bs=262144 train_time:292327ms step_avg:469.98ms
step:623/1775 loss.item()=28553.546875 n_predict=2 lr=1.5200 bs=262144 train_time:293066ms step_avg:470.41ms
step:624/1775 loss.item()=29132.7109375 n_predict=2 lr=1.5200 bs=262144 train_time:293810ms step_avg:470.85ms
step:625/1775 loss.item()=30221.240234375 n_predict=2 lr=1.5200 bs=262144 train_time:294542ms step_avg:471.27ms
step:626/1775 loss.item()=28368.21875 n_predict=2 lr=1.5200 bs=262144 train_time:295282ms step_avg:471.70ms
step:627/1775 loss.item()=27995.234375 n_predict=2 lr=1.5200 bs=262144 train_time:296024ms step_avg:472.13ms
step:628/1775 loss.item()=28729.25 n_predict=2 lr=1.5200 bs=262144 train_time:296765ms step_avg:472.56ms
step:629/1775 loss.item()=28823.8828125 n_predict=2 lr=1.5200 bs=262144 train_time:297502ms step_avg:472.98ms
step:630/1775 loss.item()=27853.751953125 n_predict=2 lr=1.5200 bs=262144 train_time:298244ms step_avg:473.40ms
step:631/1775 loss.item()=28228.2421875 n_predict=2 lr=1.5200 bs=262144 train_time:298984ms step_avg:473.83ms
step:632/1775 loss.item()=28422.603515625 n_predict=2 lr=1.5200 bs=262144 train_time:299724ms step_avg:474.25ms
step:633/1775 loss.item()=28171.6640625 n_predict=2 lr=1.5200 bs=262144 train_time:300461ms step_avg:474.66ms
step:634/1775 loss.item()=28261.4453125 n_predict=2 lr=1.5200 bs=262144 train_time:301206ms step_avg:475.09ms
step:635/1775 loss.item()=27819.248046875 n_predict=2 lr=1.5200 bs=262144 train_time:301941ms step_avg:475.50ms
step:636/1775 loss.item()=28478.7734375 n_predict=2 lr=1.5200 bs=262144 train_time:302683ms step_avg:475.92ms
step:637/1775 loss.item()=28112.099609375 n_predict=2 lr=1.5200 bs=262144 train_time:303421ms step_avg:476.33ms
step:638/1775 loss.item()=27857.63671875 n_predict=2 lr=1.5200 bs=262144 train_time:304163ms step_avg:476.74ms
step:639/1775 loss.item()=27862.841796875 n_predict=2 lr=1.5200 bs=262144 train_time:304902ms step_avg:477.15ms
step:640/1775 loss.item()=28460.53515625 n_predict=2 lr=1.5200 bs=262144 train_time:305645ms step_avg:477.57ms
step:641/1775 loss.item()=27950.2109375 n_predict=2 lr=1.5200 bs=262144 train_time:306380ms step_avg:477.97ms
step:642/1775 loss.item()=27984.95703125 n_predict=2 lr=1.5200 bs=262144 train_time:307121ms step_avg:478.38ms
step:643/1775 loss.item()=27819.65234375 n_predict=2 lr=1.5200 bs=262144 train_time:307860ms step_avg:478.79ms
step:644/1775 loss.item()=28250.89453125 n_predict=2 lr=1.5200 bs=262144 train_time:308597ms step_avg:479.19ms
step:645/1775 loss.item()=27441.666015625 n_predict=2 lr=1.5200 bs=262144 train_time:309332ms step_avg:479.58ms
step:646/1775 loss.item()=28656.263671875 n_predict=2 lr=1.5200 bs=262144 train_time:310074ms step_avg:479.99ms
step:647/1775 loss.item()=27994.25 n_predict=2 lr=1.5200 bs=262144 train_time:310809ms step_avg:480.39ms
step:648/1775 loss.item()=28071.33203125 n_predict=2 lr=1.5200 bs=262144 train_time:311549ms step_avg:480.79ms
step:649/1775 loss.item()=28848.95703125 n_predict=2 lr=1.5200 bs=262144 train_time:312287ms step_avg:481.18ms
step:650/1775 loss.item()=27428.640625 n_predict=2 lr=1.5200 bs=262144 train_time:313031ms step_avg:481.59ms
step:651/1775 loss.item()=27119.12109375 n_predict=2 lr=1.5200 bs=262144 train_time:313767ms step_avg:481.98ms
step:652/1775 loss.item()=27832.984375 n_predict=2 lr=1.5200 bs=262144 train_time:314509ms step_avg:482.38ms
step:653/1775 loss.item()=27986.62890625 n_predict=2 lr=1.5200 bs=262144 train_time:315245ms step_avg:482.76ms
step:654/1775 loss.item()=27750.4296875 n_predict=2 lr=1.5200 bs=262144 train_time:315988ms step_avg:483.16ms
step:655/1775 loss.item()=27529.5859375 n_predict=2 lr=1.5200 bs=262144 train_time:316728ms step_avg:483.55ms
step:656/1775 loss.item()=26933.447265625 n_predict=2 lr=1.5200 bs=262144 train_time:317466ms step_avg:483.94ms
step:657/1775 loss.item()=28000.47265625 n_predict=2 lr=1.5200 bs=262144 train_time:318205ms step_avg:484.33ms
step:658/1775 loss.item()=28588.69140625 n_predict=2 lr=1.5200 bs=262144 train_time:318947ms step_avg:484.72ms
step:659/1775 loss.item()=27518.240234375 n_predict=2 lr=1.5200 bs=262144 train_time:319684ms step_avg:485.11ms
step:660/1775 loss.item()=26059.7109375 n_predict=2 lr=1.5200 bs=262144 train_time:320425ms step_avg:485.49ms
step:661/1775 loss.item()=27529.41015625 n_predict=2 lr=1.5200 bs=262144 train_time:321163ms step_avg:485.87ms
step:662/1775 loss.item()=27646.18359375 n_predict=2 lr=1.5200 bs=262144 train_time:321905ms step_avg:486.26ms
step:663/1775 loss.item()=27745.83203125 n_predict=2 lr=1.5200 bs=262144 train_time:322645ms step_avg:486.64ms
step:664/1775 loss.item()=27996.15234375 n_predict=2 lr=1.5200 bs=262144 train_time:323385ms step_avg:487.03ms
step:665/1775 loss.item()=27337.2578125 n_predict=2 lr=1.5200 bs=262144 train_time:324123ms step_avg:487.40ms
step:666/1775 loss.item()=27760.962890625 n_predict=2 lr=1.5200 bs=262144 train_time:324866ms step_avg:487.79ms
step:667/1775 loss.item()=27440.85546875 n_predict=2 lr=1.5200 bs=262144 train_time:325604ms step_avg:488.16ms
step:668/1775 loss.item()=26938.44140625 n_predict=2 lr=1.5200 bs=262144 train_time:326348ms step_avg:488.54ms
step:669/1775 loss.item()=27703.490234375 n_predict=2 lr=1.5200 bs=262144 train_time:327087ms step_avg:488.92ms
step:670/1775 loss.item()=27615.16015625 n_predict=2 lr=1.5200 bs=262144 train_time:327830ms step_avg:489.30ms
step:671/1775 loss.item()=26824.203125 n_predict=2 lr=1.5200 bs=262144 train_time:328566ms step_avg:489.67ms
step:672/1775 loss.item()=27617.861328125 n_predict=2 lr=1.5200 bs=262144 train_time:329309ms step_avg:490.04ms
step:673/1775 loss.item()=27017.640625 n_predict=2 lr=1.5200 bs=262144 train_time:330047ms step_avg:490.41ms
step:674/1775 loss.item()=27450.1171875 n_predict=2 lr=1.5200 bs=262144 train_time:330786ms step_avg:490.78ms
step:675/1775 loss.item()=26836.12890625 n_predict=2 lr=1.5200 bs=262144 train_time:331526ms step_avg:491.15ms
step:676/1775 loss.item()=27540.232421875 n_predict=2 lr=1.5200 bs=262144 train_time:332266ms step_avg:491.52ms
step:677/1775 loss.item()=26356.98046875 n_predict=2 lr=1.5200 bs=262144 train_time:333006ms step_avg:491.88ms
step:678/1775 loss.item()=27720.875 n_predict=2 lr=1.5200 bs=262144 train_time:333750ms step_avg:492.26ms
step:679/1775 loss.item()=26709.53515625 n_predict=2 lr=1.5200 bs=262144 train_time:334487ms step_avg:492.62ms
step:680/1775 loss.item()=26428.91796875 n_predict=2 lr=1.5200 bs=262144 train_time:335230ms step_avg:492.99ms
step:681/1775 loss.item()=27835.251953125 n_predict=2 lr=1.5200 bs=262144 train_time:335968ms step_avg:493.34ms
step:682/1775 loss.item()=26581.4140625 n_predict=2 lr=1.5200 bs=262144 train_time:336711ms step_avg:493.71ms
step:683/1775 loss.item()=26606.041015625 n_predict=2 lr=1.5200 bs=262144 train_time:337450ms step_avg:494.07ms
step:684/1775 loss.item()=26644.39453125 n_predict=2 lr=1.5200 bs=262144 train_time:338191ms step_avg:494.43ms
step:685/1775 loss.item()=27195.685546875 n_predict=2 lr=1.5200 bs=262144 train_time:338932ms step_avg:494.79ms
step:686/1775 loss.item()=27210.064453125 n_predict=2 lr=1.5200 bs=262144 train_time:339671ms step_avg:495.15ms
step:687/1775 loss.item()=26952.6328125 n_predict=2 lr=1.5200 bs=262144 train_time:340410ms step_avg:495.50ms
step:688/1775 loss.item()=27338.01171875 n_predict=2 lr=1.5200 bs=262144 train_time:341151ms step_avg:495.86ms
step:689/1775 loss.item()=27585.548828125 n_predict=2 lr=1.5200 bs=262144 train_time:341891ms step_avg:496.21ms
step:690/1775 loss.item()=26704.107421875 n_predict=2 lr=1.5200 bs=262144 train_time:342631ms step_avg:496.57ms
step:691/1775 loss.item()=27556.541015625 n_predict=2 lr=1.5200 bs=262144 train_time:343370ms step_avg:496.92ms
step:692/1775 loss.item()=27456.208984375 n_predict=2 lr=1.5200 bs=262144 train_time:344109ms step_avg:497.27ms
step:693/1775 loss.item()=27555.44921875 n_predict=2 lr=1.5200 bs=262144 train_time:344848ms step_avg:497.62ms
step:694/1775 loss.item()=26824.03515625 n_predict=2 lr=1.5200 bs=262144 train_time:345584ms step_avg:497.96ms
step:695/1775 loss.item()=26660.482421875 n_predict=2 lr=1.5200 bs=262144 train_time:346322ms step_avg:498.30ms
step:696/1775 loss.item()=26872.408203125 n_predict=2 lr=1.5200 bs=262144 train_time:347065ms step_avg:498.66ms
step:697/1775 loss.item()=27072.49609375 n_predict=2 lr=1.5200 bs=262144 train_time:347802ms step_avg:499.00ms
step:698/1775 loss.item()=25817.08203125 n_predict=2 lr=1.5200 bs=262144 train_time:348543ms step_avg:499.35ms
step:699/1775 loss.item()=27593.208984375 n_predict=2 lr=1.5200 bs=262144 train_time:349281ms step_avg:499.69ms
step:700/1775 loss.item()=26014.0 n_predict=2 lr=1.5200 bs=262144 train_time:350024ms step_avg:500.03ms
step:701/1775 loss.item()=26271.4375 n_predict=2 lr=1.5200 bs=262144 train_time:350762ms step_avg:500.37ms
step:702/1775 loss.item()=25901.21875 n_predict=2 lr=1.5200 bs=262144 train_time:351506ms step_avg:500.72ms
step:703/1775 loss.item()=27609.802734375 n_predict=2 lr=1.5200 bs=262144 train_time:352244ms step_avg:501.06ms
step:704/1775 loss.item()=27186.53125 n_predict=2 lr=1.5200 bs=262144 train_time:352983ms step_avg:501.40ms
step:705/1775 loss.item()=26597.95703125 n_predict=2 lr=1.5200 bs=262144 train_time:353723ms step_avg:501.73ms
step:706/1775 loss.item()=27551.830078125 n_predict=2 lr=1.5200 bs=262144 train_time:354464ms step_avg:502.07ms
step:707/1775 loss.item()=25930.21484375 n_predict=2 lr=1.5200 bs=262144 train_time:355202ms step_avg:502.41ms
step:708/1775 loss.item()=26346.23046875 n_predict=2 lr=1.5200 bs=262144 train_time:355945ms step_avg:502.75ms
step:709/1775 loss.item()=26825.587890625 n_predict=2 lr=1.5200 bs=262144 train_time:356682ms step_avg:503.08ms
step:710/1775 loss.item()=26902.0 n_predict=2 lr=1.5200 bs=262144 train_time:357424ms step_avg:503.41ms
step:711/1775 loss.item()=25886.96484375 n_predict=2 lr=1.5200 bs=262144 train_time:358163ms step_avg:503.74ms
step:712/1775 loss.item()=26720.7421875 n_predict=2 lr=1.5200 bs=262144 train_time:358905ms step_avg:504.08ms
step:713/1775 loss.item()=26237.046875 n_predict=2 lr=1.5200 bs=262144 train_time:359644ms step_avg:504.41ms
step:714/1775 loss.item()=27428.734375 n_predict=2 lr=1.5200 bs=262144 train_time:360386ms step_avg:504.74ms
step:715/1775 loss.item()=26365.232421875 n_predict=2 lr=1.5200 bs=262144 train_time:361125ms step_avg:505.07ms
step:716/1775 loss.item()=25804.24609375 n_predict=2 lr=1.5200 bs=262144 train_time:361866ms step_avg:505.40ms
step:717/1775 loss.item()=26660.40234375 n_predict=2 lr=1.5200 bs=262144 train_time:362606ms step_avg:505.73ms
step:718/1775 loss.item()=25752.88671875 n_predict=2 lr=1.5200 bs=262144 train_time:363349ms step_avg:506.06ms
step:719/1775 loss.item()=25619.478515625 n_predict=2 lr=1.5200 bs=262144 train_time:364085ms step_avg:506.38ms
step:720/1775 loss.item()=26409.099609375 n_predict=2 lr=1.5200 bs=262144 train_time:364830ms step_avg:506.71ms
step:721/1775 loss.item()=25812.06640625 n_predict=2 lr=1.5200 bs=262144 train_time:365567ms step_avg:507.03ms
step:722/1775 loss.item()=25733.625 n_predict=2 lr=1.5200 bs=262144 train_time:366306ms step_avg:507.35ms
step:723/1775 loss.item()=26861.353515625 n_predict=2 lr=1.5200 bs=262144 train_time:367045ms step_avg:507.67ms
step:724/1775 loss.item()=26383.865234375 n_predict=2 lr=1.5200 bs=262144 train_time:367788ms step_avg:507.99ms
step:725/1775 loss.item()=25823.07421875 n_predict=2 lr=1.5200 bs=262144 train_time:368526ms step_avg:508.31ms
step:726/1775 loss.item()=26420.65625 n_predict=2 lr=1.5200 bs=262144 train_time:369265ms step_avg:508.63ms
step:727/1775 loss.item()=26070.978515625 n_predict=2 lr=1.5200 bs=262144 train_time:370004ms step_avg:508.95ms
step:728/1775 loss.item()=25579.283203125 n_predict=2 lr=1.5200 bs=262144 train_time:370747ms step_avg:509.27ms
step:729/1775 loss.item()=26019.216796875 n_predict=2 lr=1.5200 bs=262144 train_time:371482ms step_avg:509.58ms
step:730/1775 loss.item()=25763.87890625 n_predict=2 lr=1.5200 bs=262144 train_time:372223ms step_avg:509.89ms
step:731/1775 loss.item()=25199.798828125 n_predict=2 lr=1.5200 bs=262144 train_time:372960ms step_avg:510.20ms
step:732/1775 loss.item()=25572.216796875 n_predict=2 lr=1.5200 bs=262144 train_time:373703ms step_avg:510.52ms
step:733/1775 loss.item()=25960.884765625 n_predict=2 lr=1.5200 bs=262144 train_time:374441ms step_avg:510.83ms
step:734/1775 loss.item()=26468.3984375 n_predict=2 lr=1.5200 bs=262144 train_time:375177ms step_avg:511.14ms
step:735/1775 loss.item()=25602.642578125 n_predict=2 lr=1.5200 bs=262144 train_time:375915ms step_avg:511.45ms
step:736/1775 loss.item()=25931.748046875 n_predict=2 lr=1.5200 bs=262144 train_time:376658ms step_avg:511.76ms
step:737/1775 loss.item()=25306.56640625 n_predict=2 lr=1.5200 bs=262144 train_time:377392ms step_avg:512.07ms
step:738/1775 loss.item()=25888.26171875 n_predict=2 lr=1.5200 bs=262144 train_time:378133ms step_avg:512.38ms
step:739/1775 loss.item()=26045.048828125 n_predict=2 lr=1.5200 bs=262144 train_time:378868ms step_avg:512.68ms
step:740/1775 loss.item()=25254.005859375 n_predict=2 lr=1.5200 bs=262144 train_time:379609ms step_avg:512.99ms
step:741/1775 loss.item()=25545.05859375 n_predict=2 lr=1.5200 bs=262144 train_time:380346ms step_avg:513.29ms
step:742/1775 loss.item()=24344.556640625 n_predict=2 lr=1.5200 bs=262144 train_time:381090ms step_avg:513.60ms
step:743/1775 loss.item()=24771.61328125 n_predict=2 lr=1.5200 bs=262144 train_time:381830ms step_avg:513.90ms
step:744/1775 loss.item()=25655.640625 n_predict=2 lr=1.5200 bs=262144 train_time:382567ms step_avg:514.20ms
step:745/1775 loss.item()=25338.140625 n_predict=2 lr=1.5200 bs=262144 train_time:383307ms step_avg:514.51ms
step:746/1775 loss.item()=24749.0546875 n_predict=2 lr=1.5200 bs=262144 train_time:384050ms step_avg:514.81ms
step:747/1775 loss.item()=24946.470703125 n_predict=2 lr=1.5200 bs=262144 train_time:384785ms step_avg:515.11ms
step:748/1775 loss.item()=25035.71875 n_predict=2 lr=1.5200 bs=262144 train_time:385529ms step_avg:515.41ms
step:749/1775 loss.item()=25995.162109375 n_predict=2 lr=1.5200 bs=262144 train_time:386269ms step_avg:515.71ms
step:750/1775 loss.item()=25238.029296875 n_predict=2 lr=1.5200 bs=262144 train_time:387009ms step_avg:516.01ms
step:750/1775 lr=1.5200 bs=262144 n_predict=2 val_loss:3.9994 val_malbo_loss:4.0765 train_time:387070ms step_avg:516.09ms
step:751/1775 loss.item()=24670.310546875 n_predict=2 lr=1.5200 bs=262144 train_time:387745ms step_avg:516.31ms
step:752/1775 loss.item()=24917.263671875 n_predict=2 lr=1.5200 bs=262144 train_time:388480ms step_avg:516.60ms
step:753/1775 loss.item()=25346.984375 n_predict=2 lr=1.5200 bs=262144 train_time:389214ms step_avg:516.88ms
step:754/1775 loss.item()=24393.39453125 n_predict=2 lr=1.5200 bs=262144 train_time:389953ms step_avg:517.18ms
step:755/1775 loss.item()=25168.17578125 n_predict=2 lr=1.5200 bs=262144 train_time:390688ms step_avg:517.47ms
step:756/1775 loss.item()=25764.01171875 n_predict=2 lr=1.5200 bs=262144 train_time:391419ms step_avg:517.75ms
step:757/1775 loss.item()=24245.3125 n_predict=2 lr=1.5200 bs=262144 train_time:392155ms step_avg:518.04ms
step:758/1775 loss.item()=24884.1328125 n_predict=2 lr=1.5200 bs=262144 train_time:392891ms step_avg:518.33ms
step:759/1775 loss.item()=25054.724609375 n_predict=2 lr=1.5200 bs=262144 train_time:393625ms step_avg:518.61ms
step:760/1775 loss.item()=25337.5546875 n_predict=2 lr=1.5200 bs=262144 train_time:394363ms step_avg:518.90ms
step:761/1775 loss.item()=25232.68359375 n_predict=2 lr=1.5200 bs=262144 train_time:395100ms step_avg:519.19ms
step:762/1775 loss.item()=25207.537109375 n_predict=2 lr=1.5200 bs=262144 train_time:395840ms step_avg:519.47ms
step:763/1775 loss.item()=24749.603515625 n_predict=2 lr=1.5200 bs=262144 train_time:396576ms step_avg:519.76ms
step:764/1775 loss.item()=24739.79296875 n_predict=2 lr=1.5200 bs=262144 train_time:397312ms step_avg:520.04ms
step:765/1775 loss.item()=25497.921875 n_predict=2 lr=1.5200 bs=262144 train_time:398048ms step_avg:520.32ms
step:766/1775 loss.item()=24726.3515625 n_predict=2 lr=1.5200 bs=262144 train_time:398788ms step_avg:520.61ms
step:767/1775 loss.item()=24781.52734375 n_predict=2 lr=1.5200 bs=262144 train_time:399523ms step_avg:520.89ms
step:768/1775 loss.item()=24915.1484375 n_predict=2 lr=1.5200 bs=262144 train_time:400264ms step_avg:521.18ms
step:769/1775 loss.item()=24457.67578125 n_predict=2 lr=1.5200 bs=262144 train_time:401000ms step_avg:521.46ms
step:770/1775 loss.item()=24342.4921875 n_predict=2 lr=1.5200 bs=262144 train_time:401739ms step_avg:521.74ms
step:771/1775 loss.item()=24939.2265625 n_predict=2 lr=1.5200 bs=262144 train_time:402476ms step_avg:522.02ms
step:772/1775 loss.item()=24638.86328125 n_predict=2 lr=1.5200 bs=262144 train_time:403217ms step_avg:522.30ms
step:773/1775 loss.item()=24680.853515625 n_predict=2 lr=1.5200 bs=262144 train_time:403954ms step_avg:522.58ms
step:774/1775 loss.item()=25342.44140625 n_predict=2 lr=1.5200 bs=262144 train_time:404694ms step_avg:522.86ms
step:775/1775 loss.item()=23979.4375 n_predict=2 lr=1.5200 bs=262144 train_time:405429ms step_avg:523.13ms
step:776/1775 loss.item()=25226.6953125 n_predict=2 lr=1.5200 bs=262144 train_time:406168ms step_avg:523.41ms
step:777/1775 loss.item()=24241.484375 n_predict=2 lr=1.5200 bs=262144 train_time:406905ms step_avg:523.69ms
step:778/1775 loss.item()=25206.681640625 n_predict=2 lr=1.5200 bs=262144 train_time:407645ms step_avg:523.96ms
step:779/1775 loss.item()=24725.099609375 n_predict=2 lr=1.5200 bs=262144 train_time:408382ms step_avg:524.24ms
step:780/1775 loss.item()=24544.849609375 n_predict=2 lr=1.5200 bs=262144 train_time:409120ms step_avg:524.51ms
step:781/1775 loss.item()=24588.08984375 n_predict=2 lr=1.5200 bs=262144 train_time:409858ms step_avg:524.79ms
step:782/1775 loss.item()=24843.994140625 n_predict=2 lr=1.5200 bs=262144 train_time:410599ms step_avg:525.06ms
step:783/1775 loss.item()=24826.83203125 n_predict=2 lr=1.5200 bs=262144 train_time:411335ms step_avg:525.33ms
step:784/1775 loss.item()=24955.765625 n_predict=2 lr=1.5200 bs=262144 train_time:412073ms step_avg:525.60ms
step:785/1775 loss.item()=24007.79296875 n_predict=2 lr=1.5200 bs=262144 train_time:412808ms step_avg:525.87ms
step:786/1775 loss.item()=24846.525390625 n_predict=2 lr=1.5200 bs=262144 train_time:413547ms step_avg:526.14ms
step:787/1775 loss.item()=24094.66015625 n_predict=2 lr=1.5200 bs=262144 train_time:414287ms step_avg:526.41ms
step:788/1775 loss.item()=24968.46484375 n_predict=2 lr=1.5200 bs=262144 train_time:415023ms step_avg:526.68ms
step:789/1775 loss.item()=24395.029296875 n_predict=2 lr=1.5200 bs=262144 train_time:415762ms step_avg:526.95ms
step:790/1775 loss.item()=24622.19921875 n_predict=2 lr=1.5200 bs=262144 train_time:416502ms step_avg:527.22ms
step:791/1775 loss.item()=24593.9453125 n_predict=2 lr=1.5200 bs=262144 train_time:417237ms step_avg:527.48ms
step:792/1775 loss.item()=23771.51171875 n_predict=2 lr=1.5200 bs=262144 train_time:417978ms step_avg:527.75ms
step:793/1775 loss.item()=23758.95703125 n_predict=2 lr=1.5200 bs=262144 train_time:418716ms step_avg:528.02ms
step:794/1775 loss.item()=24555.51953125 n_predict=2 lr=1.5200 bs=262144 train_time:419458ms step_avg:528.29ms
step:795/1775 loss.item()=23765.53515625 n_predict=2 lr=1.5200 bs=262144 train_time:420195ms step_avg:528.55ms
step:796/1775 loss.item()=24046.7421875 n_predict=2 lr=1.5200 bs=262144 train_time:420934ms step_avg:528.81ms
step:797/1775 loss.item()=24380.921875 n_predict=2 lr=1.5200 bs=262144 train_time:421672ms step_avg:529.07ms
step:798/1775 loss.item()=23329.17578125 n_predict=2 lr=1.5200 bs=262144 train_time:422412ms step_avg:529.34ms
step:799/1775 loss.item()=23781.09375 n_predict=2 lr=1.5200 bs=262144 train_time:423148ms step_avg:529.60ms
step:800/1775 loss.item()=25090.97265625 n_predict=2 lr=1.5200 bs=262144 train_time:423887ms step_avg:529.86ms
step:801/1775 loss.item()=24460.21484375 n_predict=2 lr=1.5200 bs=262144 train_time:424622ms step_avg:530.12ms
step:802/1775 loss.item()=24573.5234375 n_predict=2 lr=1.5200 bs=262144 train_time:425363ms step_avg:530.38ms
step:803/1775 loss.item()=24325.0 n_predict=2 lr=1.5200 bs=262144 train_time:426103ms step_avg:530.64ms
step:804/1775 loss.item()=23408.49609375 n_predict=2 lr=1.5200 bs=262144 train_time:426841ms step_avg:530.90ms
step:805/1775 loss.item()=23881.97265625 n_predict=2 lr=1.5200 bs=262144 train_time:427581ms step_avg:531.16ms
step:806/1775 loss.item()=24017.01953125 n_predict=2 lr=1.5200 bs=262144 train_time:428320ms step_avg:531.41ms
step:807/1775 loss.item()=23127.306640625 n_predict=2 lr=1.5200 bs=262144 train_time:429058ms step_avg:531.67ms
step:808/1775 loss.item()=23853.37109375 n_predict=2 lr=1.5200 bs=262144 train_time:429799ms step_avg:531.93ms
step:809/1775 loss.item()=23578.07421875 n_predict=2 lr=1.5200 bs=262144 train_time:430537ms step_avg:532.18ms
step:810/1775 loss.item()=23485.0546875 n_predict=2 lr=1.5200 bs=262144 train_time:431277ms step_avg:532.44ms
step:811/1775 loss.item()=23625.369140625 n_predict=2 lr=1.5200 bs=262144 train_time:432017ms step_avg:532.70ms
step:812/1775 loss.item()=23739.10546875 n_predict=2 lr=1.5200 bs=262144 train_time:432754ms step_avg:532.95ms
step:813/1775 loss.item()=23469.822265625 n_predict=2 lr=1.5200 bs=262144 train_time:433492ms step_avg:533.20ms
step:814/1775 loss.item()=23448.681640625 n_predict=2 lr=1.5200 bs=262144 train_time:434230ms step_avg:533.45ms
step:815/1775 loss.item()=23406.2890625 n_predict=2 lr=1.5200 bs=262144 train_time:434966ms step_avg:533.70ms
step:816/1775 loss.item()=23646.31640625 n_predict=2 lr=1.5200 bs=262144 train_time:435707ms step_avg:533.95ms
step:817/1775 loss.item()=23497.98046875 n_predict=2 lr=1.5200 bs=262144 train_time:436444ms step_avg:534.20ms
step:818/1775 loss.item()=23588.98828125 n_predict=2 lr=1.5200 bs=262144 train_time:437186ms step_avg:534.46ms
step:819/1775 loss.item()=23372.41796875 n_predict=2 lr=1.5200 bs=262144 train_time:437923ms step_avg:534.70ms
step:820/1775 loss.item()=24199.287109375 n_predict=2 lr=1.5200 bs=262144 train_time:438663ms step_avg:534.95ms
step:821/1775 loss.item()=22879.44921875 n_predict=2 lr=1.5200 bs=262144 train_time:439399ms step_avg:535.20ms
step:822/1775 loss.item()=22808.123046875 n_predict=2 lr=1.5200 bs=262144 train_time:440142ms step_avg:535.45ms
step:823/1775 loss.item()=23585.71484375 n_predict=2 lr=1.5200 bs=262144 train_time:440882ms step_avg:535.70ms
step:824/1775 loss.item()=23726.3828125 n_predict=2 lr=1.5200 bs=262144 train_time:441622ms step_avg:535.95ms
step:825/1775 loss.item()=24272.61328125 n_predict=2 lr=1.5200 bs=262144 train_time:442362ms step_avg:536.20ms
step:826/1775 loss.item()=23864.4921875 n_predict=2 lr=1.5200 bs=262144 train_time:443103ms step_avg:536.44ms
step:827/1775 loss.item()=22650.236328125 n_predict=2 lr=1.5200 bs=262144 train_time:443839ms step_avg:536.69ms
step:828/1775 loss.item()=24245.025390625 n_predict=2 lr=1.5200 bs=262144 train_time:444583ms step_avg:536.94ms
step:829/1775 loss.item()=23049.173828125 n_predict=2 lr=1.5200 bs=262144 train_time:445322ms step_avg:537.18ms
step:830/1775 loss.item()=23184.5078125 n_predict=2 lr=1.5200 bs=262144 train_time:446065ms step_avg:537.43ms
step:831/1775 loss.item()=22603.4296875 n_predict=2 lr=1.5200 bs=262144 train_time:446805ms step_avg:537.67ms
step:832/1775 loss.item()=23002.1171875 n_predict=2 lr=1.5200 bs=262144 train_time:447543ms step_avg:537.91ms
step:833/1775 loss.item()=24208.009765625 n_predict=2 lr=1.5200 bs=262144 train_time:448282ms step_avg:538.15ms
step:834/1775 loss.item()=23299.109375 n_predict=2 lr=1.5200 bs=262144 train_time:449024ms step_avg:538.40ms
step:835/1775 loss.item()=23650.22265625 n_predict=2 lr=1.5200 bs=262144 train_time:449762ms step_avg:538.64ms
step:836/1775 loss.item()=23051.26171875 n_predict=2 lr=1.5200 bs=262144 train_time:450502ms step_avg:538.88ms
step:837/1775 loss.item()=22951.84765625 n_predict=2 lr=1.5200 bs=262144 train_time:451240ms step_avg:539.12ms
step:838/1775 loss.item()=23392.50390625 n_predict=2 lr=1.5200 bs=262144 train_time:451983ms step_avg:539.36ms
step:839/1775 loss.item()=22253.55078125 n_predict=2 lr=1.5200 bs=262144 train_time:452720ms step_avg:539.60ms
step:840/1775 loss.item()=22706.14453125 n_predict=2 lr=1.5200 bs=262144 train_time:453461ms step_avg:539.83ms
step:841/1775 loss.item()=23810.240234375 n_predict=2 lr=1.5200 bs=262144 train_time:454197ms step_avg:540.07ms
step:842/1775 loss.item()=23519.51171875 n_predict=2 lr=1.5200 bs=262144 train_time:454938ms step_avg:540.31ms
step:843/1775 loss.item()=24018.76171875 n_predict=2 lr=1.5200 bs=262144 train_time:455673ms step_avg:540.54ms
step:844/1775 loss.item()=23600.49609375 n_predict=2 lr=1.5200 bs=262144 train_time:456412ms step_avg:540.77ms
step:845/1775 loss.item()=23957.11328125 n_predict=2 lr=1.5200 bs=262144 train_time:457146ms step_avg:541.00ms
step:846/1775 loss.item()=23212.94921875 n_predict=2 lr=1.5200 bs=262144 train_time:457886ms step_avg:541.24ms
step:847/1775 loss.item()=23289.046875 n_predict=2 lr=1.5200 bs=262144 train_time:458620ms step_avg:541.46ms
step:848/1775 loss.item()=22182.45703125 n_predict=2 lr=1.5200 bs=262144 train_time:459362ms step_avg:541.70ms
step:849/1775 loss.item()=23445.736328125 n_predict=2 lr=1.5200 bs=262144 train_time:460100ms step_avg:541.93ms
step:850/1775 loss.item()=22911.0625 n_predict=2 lr=1.5200 bs=262144 train_time:460840ms step_avg:542.16ms
step:851/1775 loss.item()=22423.77734375 n_predict=2 lr=1.5200 bs=262144 train_time:461580ms step_avg:542.40ms
step:852/1775 loss.item()=23142.537109375 n_predict=2 lr=1.5200 bs=262144 train_time:462320ms step_avg:542.63ms
step:853/1775 loss.item()=22242.6640625 n_predict=2 lr=1.5200 bs=262144 train_time:463057ms step_avg:542.86ms
step:854/1775 loss.item()=23694.59765625 n_predict=2 lr=1.5200 bs=262144 train_time:463798ms step_avg:543.09ms
step:855/1775 loss.item()=23276.34375 n_predict=2 lr=1.5200 bs=262144 train_time:464537ms step_avg:543.32ms
step:856/1775 loss.item()=22736.310546875 n_predict=2 lr=1.5200 bs=262144 train_time:465278ms step_avg:543.55ms
step:857/1775 loss.item()=22995.6484375 n_predict=2 lr=1.5200 bs=262144 train_time:466016ms step_avg:543.78ms
step:858/1775 loss.item()=23267.849609375 n_predict=2 lr=1.5200 bs=262144 train_time:466755ms step_avg:544.00ms
step:859/1775 loss.item()=22202.912109375 n_predict=2 lr=1.5200 bs=262144 train_time:467493ms step_avg:544.23ms
step:860/1775 loss.item()=22557.935546875 n_predict=2 lr=1.5200 bs=262144 train_time:468231ms step_avg:544.45ms
step:861/1775 loss.item()=22922.5859375 n_predict=2 lr=1.5200 bs=262144 train_time:468967ms step_avg:544.68ms
step:862/1775 loss.item()=22876.48046875 n_predict=2 lr=1.5200 bs=262144 train_time:469710ms step_avg:544.91ms
step:863/1775 loss.item()=23318.41796875 n_predict=2 lr=1.5200 bs=262144 train_time:470446ms step_avg:545.13ms
step:864/1775 loss.item()=23532.25390625 n_predict=2 lr=1.5200 bs=262144 train_time:471187ms step_avg:545.36ms
step:865/1775 loss.item()=21650.7421875 n_predict=2 lr=1.5200 bs=262144 train_time:471923ms step_avg:545.58ms
step:866/1775 loss.item()=22287.759765625 n_predict=2 lr=1.5200 bs=262144 train_time:472664ms step_avg:545.80ms
step:867/1775 loss.item()=22581.8984375 n_predict=2 lr=1.5200 bs=262144 train_time:473400ms step_avg:546.02ms
step:868/1775 loss.item()=22206.2890625 n_predict=2 lr=1.5200 bs=262144 train_time:474141ms step_avg:546.25ms
step:869/1775 loss.item()=22578.75 n_predict=2 lr=1.5192 bs=262144 train_time:474879ms step_avg:546.47ms
step:870/1775 loss.item()=21450.9609375 n_predict=2 lr=1.5175 bs=262144 train_time:475620ms step_avg:546.69ms
step:871/1775 loss.item()=21542.630859375 n_predict=2 lr=1.5159 bs=262144 train_time:476358ms step_avg:546.91ms
step:872/1775 loss.item()=22197.7109375 n_predict=2 lr=1.5143 bs=262144 train_time:477099ms step_avg:547.13ms
step:873/1775 loss.item()=22372.271484375 n_predict=2 lr=1.5126 bs=262144 train_time:477835ms step_avg:547.35ms
step:874/1775 loss.item()=22522.75 n_predict=2 lr=1.5110 bs=262144 train_time:478575ms step_avg:547.57ms
step:875/1775 loss.item()=22899.13671875 n_predict=2 lr=1.5094 bs=262144 train_time:479313ms step_avg:547.79ms
step:876/1775 loss.item()=22197.884765625 n_predict=2 lr=1.5077 bs=262144 train_time:480051ms step_avg:548.00ms
step:877/1775 loss.item()=22058.68359375 n_predict=2 lr=1.5061 bs=262144 train_time:480791ms step_avg:548.22ms
step:878/1775 loss.item()=22396.3828125 n_predict=2 lr=1.5044 bs=262144 train_time:481528ms step_avg:548.44ms
step:879/1775 loss.item()=21087.0 n_predict=2 lr=1.5028 bs=262144 train_time:482267ms step_avg:548.65ms
step:880/1775 loss.item()=23141.84375 n_predict=2 lr=1.5012 bs=262144 train_time:483005ms step_avg:548.87ms
step:881/1775 loss.item()=21893.568359375 n_predict=2 lr=1.4995 bs=262144 train_time:483744ms step_avg:549.09ms
step:882/1775 loss.item()=21749.369140625 n_predict=2 lr=1.4979 bs=262144 train_time:484485ms step_avg:549.30ms
step:883/1775 loss.item()=21352.5703125 n_predict=2 lr=1.4963 bs=262144 train_time:485222ms step_avg:549.51ms
step:884/1775 loss.item()=22199.6640625 n_predict=2 lr=1.4946 bs=262144 train_time:485964ms step_avg:549.73ms
step:885/1775 loss.item()=22967.923828125 n_predict=2 lr=1.4930 bs=262144 train_time:486701ms step_avg:549.94ms
step:886/1775 loss.item()=21847.7890625 n_predict=2 lr=1.4914 bs=262144 train_time:487441ms step_avg:550.16ms
step:887/1775 loss.item()=22548.87890625 n_predict=2 lr=1.4897 bs=262144 train_time:488179ms step_avg:550.37ms
step:888/1775 loss.item()=21784.12890625 n_predict=2 lr=1.4881 bs=262144 train_time:488920ms step_avg:550.59ms
step:889/1775 loss.item()=21507.15234375 n_predict=2 lr=1.4864 bs=262144 train_time:489659ms step_avg:550.80ms
step:890/1775 loss.item()=23097.44140625 n_predict=2 lr=1.4848 bs=262144 train_time:490399ms step_avg:551.01ms
step:891/1775 loss.item()=21088.28515625 n_predict=2 lr=1.4832 bs=262144 train_time:491137ms step_avg:551.22ms
step:892/1775 loss.item()=22372.533203125 n_predict=2 lr=1.4815 bs=262144 train_time:491877ms step_avg:551.43ms
step:893/1775 loss.item()=21473.384765625 n_predict=2 lr=1.4799 bs=262144 train_time:492615ms step_avg:551.64ms
step:894/1775 loss.item()=21701.177734375 n_predict=2 lr=1.4783 bs=262144 train_time:493357ms step_avg:551.85ms
step:895/1775 loss.item()=21870.9296875 n_predict=2 lr=1.4766 bs=262144 train_time:494096ms step_avg:552.06ms
step:896/1775 loss.item()=21856.06640625 n_predict=2 lr=1.4750 bs=262144 train_time:494835ms step_avg:552.27ms
step:897/1775 loss.item()=22165.537109375 n_predict=2 lr=1.4733 bs=262144 train_time:495575ms step_avg:552.48ms
step:898/1775 loss.item()=22244.697265625 n_predict=2 lr=1.4717 bs=262144 train_time:496313ms step_avg:552.69ms
step:899/1775 loss.item()=21795.6953125 n_predict=2 lr=1.4701 bs=262144 train_time:497050ms step_avg:552.89ms
step:900/1775 loss.item()=22436.890625 n_predict=2 lr=1.4684 bs=262144 train_time:497791ms step_avg:553.10ms
step:901/1775 loss.item()=22139.875 n_predict=2 lr=1.4668 bs=262144 train_time:498527ms step_avg:553.30ms
step:902/1775 loss.item()=21246.048828125 n_predict=2 lr=1.4652 bs=262144 train_time:499270ms step_avg:553.51ms
step:903/1775 loss.item()=21890.275390625 n_predict=2 lr=1.4635 bs=262144 train_time:500006ms step_avg:553.72ms
step:904/1775 loss.item()=21924.90234375 n_predict=2 lr=1.4619 bs=262144 train_time:500744ms step_avg:553.92ms
step:905/1775 loss.item()=21663.015625 n_predict=2 lr=1.4603 bs=262144 train_time:501484ms step_avg:554.13ms
step:906/1775 loss.item()=21692.509765625 n_predict=2 lr=1.4586 bs=262144 train_time:502221ms step_avg:554.33ms
step:907/1775 loss.item()=21337.359375 n_predict=2 lr=1.4570 bs=262144 train_time:502961ms step_avg:554.53ms
step:908/1775 loss.item()=21392.16015625 n_predict=2 lr=1.4553 bs=262144 train_time:503702ms step_avg:554.74ms
step:909/1775 loss.item()=21974.23828125 n_predict=2 lr=1.4537 bs=262144 train_time:504439ms step_avg:554.94ms
step:910/1775 loss.item()=20619.48828125 n_predict=2 lr=1.4521 bs=262144 train_time:505181ms step_avg:555.14ms
step:911/1775 loss.item()=21047.90234375 n_predict=2 lr=1.4504 bs=262144 train_time:505919ms step_avg:555.34ms
step:912/1775 loss.item()=21409.375 n_predict=2 lr=1.4488 bs=262144 train_time:506661ms step_avg:555.55ms
step:913/1775 loss.item()=20831.666015625 n_predict=2 lr=1.4472 bs=262144 train_time:507398ms step_avg:555.75ms
step:914/1775 loss.item()=21632.4296875 n_predict=2 lr=1.4455 bs=262144 train_time:508137ms step_avg:555.95ms
step:915/1775 loss.item()=21393.00390625 n_predict=2 lr=1.4439 bs=262144 train_time:508875ms step_avg:556.15ms
step:916/1775 loss.item()=20721.1953125 n_predict=2 lr=1.4422 bs=262144 train_time:509612ms step_avg:556.34ms
step:917/1775 loss.item()=21563.0078125 n_predict=2 lr=1.4406 bs=262144 train_time:510349ms step_avg:556.54ms
step:918/1775 loss.item()=22183.82421875 n_predict=2 lr=1.4390 bs=262144 train_time:511090ms step_avg:556.74ms
step:919/1775 loss.item()=21063.125 n_predict=2 lr=1.4373 bs=262144 train_time:511826ms step_avg:556.94ms
step:920/1775 loss.item()=21416.296875 n_predict=2 lr=1.4357 bs=262144 train_time:512571ms step_avg:557.14ms
step:921/1775 loss.item()=21248.6171875 n_predict=2 lr=1.4341 bs=262144 train_time:513307ms step_avg:557.34ms
step:922/1775 loss.item()=20998.255859375 n_predict=2 lr=1.4324 bs=262144 train_time:514048ms step_avg:557.54ms
step:923/1775 loss.item()=21046.05859375 n_predict=2 lr=1.4308 bs=262144 train_time:514788ms step_avg:557.73ms
step:924/1775 loss.item()=20895.90625 n_predict=2 lr=1.4292 bs=262144 train_time:515529ms step_avg:557.93ms
step:925/1775 loss.item()=21149.765625 n_predict=2 lr=1.4275 bs=262144 train_time:516268ms step_avg:558.13ms
step:926/1775 loss.item()=20945.40234375 n_predict=2 lr=1.4259 bs=262144 train_time:517008ms step_avg:558.32ms
step:927/1775 loss.item()=20896.96875 n_predict=2 lr=1.4242 bs=262144 train_time:517747ms step_avg:558.52ms
step:928/1775 loss.item()=21107.27734375 n_predict=2 lr=1.4226 bs=262144 train_time:518485ms step_avg:558.71ms
step:929/1775 loss.item()=21183.234375 n_predict=2 lr=1.4210 bs=262144 train_time:519221ms step_avg:558.90ms
step:930/1775 loss.item()=20155.31640625 n_predict=2 lr=1.4193 bs=262144 train_time:519963ms step_avg:559.10ms
step:931/1775 loss.item()=21493.09375 n_predict=2 lr=1.4177 bs=262144 train_time:520700ms step_avg:559.29ms
step:932/1775 loss.item()=20400.75390625 n_predict=2 lr=1.4161 bs=262144 train_time:521439ms step_avg:559.48ms
step:933/1775 loss.item()=20527.48828125 n_predict=2 lr=1.4144 bs=262144 train_time:522177ms step_avg:559.68ms
step:934/1775 loss.item()=21009.06640625 n_predict=2 lr=1.4128 bs=262144 train_time:522917ms step_avg:559.87ms
step:935/1775 loss.item()=20698.666015625 n_predict=2 lr=1.4111 bs=262144 train_time:523655ms step_avg:560.06ms
step:936/1775 loss.item()=21424.4765625 n_predict=2 lr=1.4095 bs=262144 train_time:524395ms step_avg:560.25ms
step:937/1775 loss.item()=20441.583984375 n_predict=2 lr=1.4079 bs=262144 train_time:525134ms step_avg:560.44ms
step:938/1775 loss.item()=21057.6171875 n_predict=2 lr=1.4062 bs=262144 train_time:525872ms step_avg:560.63ms
step:939/1775 loss.item()=20907.876953125 n_predict=2 lr=1.4046 bs=262144 train_time:526609ms step_avg:560.82ms
step:940/1775 loss.item()=21464.48046875 n_predict=2 lr=1.4030 bs=262144 train_time:527348ms step_avg:561.01ms
step:941/1775 loss.item()=20264.404296875 n_predict=2 lr=1.4013 bs=262144 train_time:528087ms step_avg:561.20ms
step:942/1775 loss.item()=19909.58984375 n_predict=2 lr=1.3997 bs=262144 train_time:528824ms step_avg:561.38ms
step:943/1775 loss.item()=20688.765625 n_predict=2 lr=1.3981 bs=262144 train_time:529562ms step_avg:561.57ms
step:944/1775 loss.item()=19710.71484375 n_predict=2 lr=1.3964 bs=262144 train_time:530301ms step_avg:561.76ms
step:945/1775 loss.item()=20901.265625 n_predict=2 lr=1.3948 bs=262144 train_time:531039ms step_avg:561.95ms
step:946/1775 loss.item()=20348.75 n_predict=2 lr=1.3931 bs=262144 train_time:531781ms step_avg:562.14ms
step:947/1775 loss.item()=20442.28515625 n_predict=2 lr=1.3915 bs=262144 train_time:532518ms step_avg:562.32ms
step:948/1775 loss.item()=20234.1171875 n_predict=2 lr=1.3899 bs=262144 train_time:533261ms step_avg:562.51ms
step:949/1775 loss.item()=20257.51953125 n_predict=2 lr=1.3882 bs=262144 train_time:533999ms step_avg:562.70ms
step:950/1775 loss.item()=19927.82421875 n_predict=2 lr=1.3866 bs=262144 train_time:534740ms step_avg:562.88ms
step:951/1775 loss.item()=19997.3828125 n_predict=2 lr=1.3850 bs=262144 train_time:535479ms step_avg:563.07ms
step:952/1775 loss.item()=20225.3984375 n_predict=2 lr=1.3833 bs=262144 train_time:536219ms step_avg:563.26ms
step:953/1775 loss.item()=19689.859375 n_predict=2 lr=1.3817 bs=262144 train_time:536958ms step_avg:563.44ms
step:954/1775 loss.item()=19949.712890625 n_predict=2 lr=1.3800 bs=262144 train_time:537698ms step_avg:563.63ms
step:955/1775 loss.item()=19852.12890625 n_predict=2 lr=1.3784 bs=262144 train_time:538435ms step_avg:563.81ms
step:956/1775 loss.item()=20740.52734375 n_predict=2 lr=1.3768 bs=262144 train_time:539177ms step_avg:563.99ms
step:957/1775 loss.item()=20545.3359375 n_predict=2 lr=1.3751 bs=262144 train_time:539912ms step_avg:564.17ms
step:958/1775 loss.item()=19816.75390625 n_predict=2 lr=1.3735 bs=262144 train_time:540649ms step_avg:564.35ms
step:959/1775 loss.item()=20481.76953125 n_predict=2 lr=1.3719 bs=262144 train_time:541388ms step_avg:564.53ms
step:960/1775 loss.item()=21033.294921875 n_predict=2 lr=1.3702 bs=262144 train_time:542128ms step_avg:564.72ms
step:961/1775 loss.item()=19841.2890625 n_predict=2 lr=1.3686 bs=262144 train_time:542866ms step_avg:564.90ms
step:962/1775 loss.item()=19779.49609375 n_predict=2 lr=1.3670 bs=262144 train_time:543605ms step_avg:565.08ms
step:963/1775 loss.item()=20330.23828125 n_predict=2 lr=1.3653 bs=262144 train_time:544345ms step_avg:565.26ms
step:964/1775 loss.item()=19999.427734375 n_predict=2 lr=1.3637 bs=262144 train_time:545084ms step_avg:565.44ms
step:965/1775 loss.item()=20244.32421875 n_predict=2 lr=1.3620 bs=262144 train_time:545818ms step_avg:565.61ms
step:966/1775 loss.item()=19337.91015625 n_predict=2 lr=1.3604 bs=262144 train_time:546560ms step_avg:565.80ms
step:967/1775 loss.item()=19334.513671875 n_predict=2 lr=1.3588 bs=262144 train_time:547299ms step_avg:565.98ms
step:968/1775 loss.item()=19684.908203125 n_predict=2 lr=1.3571 bs=262144 train_time:548038ms step_avg:566.16ms
step:969/1775 loss.item()=19306.396484375 n_predict=2 lr=1.3555 bs=262144 train_time:548777ms step_avg:566.33ms
step:970/1775 loss.item()=19595.75 n_predict=2 lr=1.3539 bs=262144 train_time:549517ms step_avg:566.51ms
step:971/1775 loss.item()=19997.0234375 n_predict=2 lr=1.3522 bs=262144 train_time:550257ms step_avg:566.69ms
step:972/1775 loss.item()=19549.03125 n_predict=2 lr=1.3506 bs=262144 train_time:550995ms step_avg:566.87ms
step:973/1775 loss.item()=19702.7734375 n_predict=2 lr=1.3489 bs=262144 train_time:551730ms step_avg:567.04ms
step:974/1775 loss.item()=19710.1953125 n_predict=2 lr=1.3473 bs=262144 train_time:552469ms step_avg:567.22ms
step:975/1775 loss.item()=19356.17578125 n_predict=2 lr=1.3457 bs=262144 train_time:553208ms step_avg:567.39ms
step:976/1775 loss.item()=20151.01171875 n_predict=2 lr=1.3440 bs=262144 train_time:553948ms step_avg:567.57ms
step:977/1775 loss.item()=19749.04296875 n_predict=2 lr=1.3424 bs=262144 train_time:554687ms step_avg:567.75ms
step:978/1775 loss.item()=20006.365234375 n_predict=2 lr=1.3408 bs=262144 train_time:555422ms step_avg:567.92ms
step:979/1775 loss.item()=20064.22265625 n_predict=2 lr=1.3391 bs=262144 train_time:556159ms step_avg:568.09ms
step:980/1775 loss.item()=19205.6484375 n_predict=2 lr=1.3375 bs=262144 train_time:556900ms step_avg:568.27ms
step:981/1775 loss.item()=20704.5078125 n_predict=2 lr=1.3359 bs=262144 train_time:557638ms step_avg:568.44ms
step:982/1775 loss.item()=19352.96484375 n_predict=2 lr=1.3342 bs=262144 train_time:558377ms step_avg:568.61ms
step:983/1775 loss.item()=19716.44140625 n_predict=2 lr=1.3326 bs=262144 train_time:559112ms step_avg:568.78ms
step:984/1775 loss.item()=19475.1953125 n_predict=2 lr=1.3309 bs=262144 train_time:559854ms step_avg:568.96ms
step:985/1775 loss.item()=19565.37890625 n_predict=2 lr=1.3293 bs=262144 train_time:560589ms step_avg:569.13ms
step:986/1775 loss.item()=19349.4375 n_predict=2 lr=1.3277 bs=262144 train_time:561330ms step_avg:569.30ms
step:987/1775 loss.item()=19046.3984375 n_predict=2 lr=1.3260 bs=262144 train_time:562067ms step_avg:569.47ms
step:988/1775 loss.item()=19071.083984375 n_predict=2 lr=1.3244 bs=262144 train_time:562805ms step_avg:569.64ms
step:989/1775 loss.item()=19871.712890625 n_predict=2 lr=1.3228 bs=262144 train_time:563543ms step_avg:569.81ms
step:990/1775 loss.item()=18931.86328125 n_predict=2 lr=1.3211 bs=262144 train_time:564289ms step_avg:569.99ms
step:991/1775 loss.item()=18970.892578125 n_predict=2 lr=1.3195 bs=262144 train_time:565023ms step_avg:570.15ms
step:992/1775 loss.item()=19446.80078125 n_predict=2 lr=1.3178 bs=262144 train_time:565765ms step_avg:570.33ms
step:993/1775 loss.item()=19103.513671875 n_predict=2 lr=1.3162 bs=262144 train_time:566503ms step_avg:570.50ms
step:994/1775 loss.item()=18553.23828125 n_predict=2 lr=1.3146 bs=262144 train_time:567239ms step_avg:570.66ms
step:995/1775 loss.item()=19334.53515625 n_predict=2 lr=1.3129 bs=262144 train_time:567978ms step_avg:570.83ms
step:996/1775 loss.item()=19642.640625 n_predict=2 lr=1.3113 bs=262144 train_time:568718ms step_avg:571.00ms
step:997/1775 loss.item()=18917.67578125 n_predict=2 lr=1.3097 bs=262144 train_time:569449ms step_avg:571.16ms
step:998/1775 loss.item()=19063.43359375 n_predict=2 lr=1.3080 bs=262144 train_time:570189ms step_avg:571.33ms
step:999/1775 loss.item()=18825.515625 n_predict=2 lr=1.3064 bs=262144 train_time:570925ms step_avg:571.50ms
step:1000/1775 loss.item()=18479.0234375 n_predict=2 lr=1.3047 bs=262144 train_time:571667ms step_avg:571.67ms
step:1000/1775 lr=1.3031 bs=262144 n_predict=2 val_loss:3.7395 val_malbo_loss:3.8189 train_time:571728ms step_avg:571.73ms
step:1001/1775 loss.item()=19513.3203125 n_predict=2 lr=1.3031 bs=262144 train_time:572404ms step_avg:571.83ms
step:1002/1775 loss.item()=18413.70703125 n_predict=2 lr=1.3015 bs=262144 train_time:573143ms step_avg:572.00ms
step:1003/1775 loss.item()=18797.80859375 n_predict=2 lr=1.2998 bs=262144 train_time:573878ms step_avg:572.16ms
step:1004/1775 loss.item()=19165.439453125 n_predict=2 lr=1.2982 bs=262144 train_time:574619ms step_avg:572.33ms
step:1005/1775 loss.item()=19104.41796875 n_predict=2 lr=1.2966 bs=262144 train_time:575355ms step_avg:572.49ms
step:1006/1775 loss.item()=18666.26171875 n_predict=2 lr=1.2949 bs=262144 train_time:576096ms step_avg:572.66ms
step:1007/1775 loss.item()=19244.37109375 n_predict=2 lr=1.2933 bs=262144 train_time:576836ms step_avg:572.83ms
step:1008/1775 loss.item()=18573.765625 n_predict=2 lr=1.2917 bs=262144 train_time:577572ms step_avg:572.99ms
step:1009/1775 loss.item()=19553.890625 n_predict=2 lr=1.2900 bs=262144 train_time:578313ms step_avg:573.15ms
step:1010/1775 loss.item()=18939.693359375 n_predict=2 lr=1.2884 bs=262144 train_time:579054ms step_avg:573.32ms
step:1011/1775 loss.item()=19141.583984375 n_predict=2 lr=1.2867 bs=262144 train_time:579790ms step_avg:573.48ms
step:1012/1775 loss.item()=19593.53515625 n_predict=2 lr=1.2851 bs=262144 train_time:580532ms step_avg:573.65ms
step:1013/1775 loss.item()=18215.8125 n_predict=2 lr=1.2835 bs=262144 train_time:581268ms step_avg:573.81ms
step:1014/1775 loss.item()=18941.19140625 n_predict=2 lr=1.2818 bs=262144 train_time:582010ms step_avg:573.97ms
step:1015/1775 loss.item()=18698.09765625 n_predict=2 lr=1.2802 bs=262144 train_time:582748ms step_avg:574.14ms
step:1016/1775 loss.item()=18786.859375 n_predict=2 lr=1.2786 bs=262144 train_time:583489ms step_avg:574.30ms
step:1017/1775 loss.item()=17559.7734375 n_predict=2 lr=1.2769 bs=262144 train_time:584228ms step_avg:574.46ms
step:1018/1775 loss.item()=18677.44140625 n_predict=2 lr=1.2753 bs=262144 train_time:584969ms step_avg:574.63ms
step:1019/1775 loss.item()=18554.908203125 n_predict=2 lr=1.2736 bs=262144 train_time:585706ms step_avg:574.79ms
step:1020/1775 loss.item()=19050.62109375 n_predict=2 lr=1.2720 bs=262144 train_time:586445ms step_avg:574.95ms
step:1021/1775 loss.item()=18667.8671875 n_predict=2 lr=1.2704 bs=262144 train_time:587180ms step_avg:575.10ms
step:1022/1775 loss.item()=18795.109375 n_predict=2 lr=1.2687 bs=262144 train_time:587922ms step_avg:575.27ms
step:1023/1775 loss.item()=18875.67578125 n_predict=2 lr=1.2671 bs=262144 train_time:588658ms step_avg:575.42ms
step:1024/1775 loss.item()=18305.408203125 n_predict=2 lr=1.2655 bs=262144 train_time:589398ms step_avg:575.58ms
step:1025/1775 loss.item()=19073.458984375 n_predict=2 lr=1.2638 bs=262144 train_time:590137ms step_avg:575.74ms
step:1026/1775 loss.item()=18771.328125 n_predict=2 lr=1.2622 bs=262144 train_time:590877ms step_avg:575.90ms
step:1027/1775 loss.item()=18930.10546875 n_predict=2 lr=1.2606 bs=262144 train_time:591614ms step_avg:576.06ms
step:1028/1775 loss.item()=18332.04296875 n_predict=2 lr=1.2589 bs=262144 train_time:592351ms step_avg:576.22ms
step:1029/1775 loss.item()=18696.1328125 n_predict=2 lr=1.2573 bs=262144 train_time:593089ms step_avg:576.37ms
step:1030/1775 loss.item()=18298.607421875 n_predict=2 lr=1.2556 bs=262144 train_time:593830ms step_avg:576.53ms
step:1031/1775 loss.item()=18506.6953125 n_predict=2 lr=1.2540 bs=262144 train_time:594567ms step_avg:576.69ms
step:1032/1775 loss.item()=18315.4609375 n_predict=2 lr=1.2524 bs=262144 train_time:595307ms step_avg:576.85ms
step:1033/1775 loss.item()=18316.875 n_predict=2 lr=1.2507 bs=262144 train_time:596043ms step_avg:577.00ms
step:1034/1775 loss.item()=18337.5234375 n_predict=2 lr=1.2491 bs=262144 train_time:596783ms step_avg:577.16ms
step:1035/1775 loss.item()=17806.47265625 n_predict=2 lr=1.2475 bs=262144 train_time:597520ms step_avg:577.31ms
step:1036/1775 loss.item()=18887.35546875 n_predict=2 lr=1.2458 bs=262144 train_time:598263ms step_avg:577.47ms
step:1037/1775 loss.item()=18294.818359375 n_predict=2 lr=1.2442 bs=262144 train_time:598999ms step_avg:577.63ms
step:1038/1775 loss.item()=17675.162109375 n_predict=2 lr=1.2425 bs=262144 train_time:599741ms step_avg:577.79ms
step:1039/1775 loss.item()=17636.296875 n_predict=2 lr=1.2409 bs=262144 train_time:600478ms step_avg:577.94ms
step:1040/1775 loss.item()=18355.306640625 n_predict=2 lr=1.2393 bs=262144 train_time:601218ms step_avg:578.09ms
step:1041/1775 loss.item()=19556.970703125 n_predict=2 lr=1.2376 bs=262144 train_time:601952ms step_avg:578.24ms
step:1042/1775 loss.item()=17837.541015625 n_predict=2 lr=1.2360 bs=262144 train_time:602693ms step_avg:578.40ms
step:1043/1775 loss.item()=18271.63671875 n_predict=2 lr=1.2344 bs=262144 train_time:603433ms step_avg:578.56ms
step:1044/1775 loss.item()=17904.31640625 n_predict=2 lr=1.2327 bs=262144 train_time:604172ms step_avg:578.71ms
step:1045/1775 loss.item()=17515.763671875 n_predict=2 lr=1.2311 bs=262144 train_time:604910ms step_avg:578.86ms
step:1046/1775 loss.item()=17612.54296875 n_predict=2 lr=1.2295 bs=262144 train_time:605651ms step_avg:579.02ms
step:1047/1775 loss.item()=17386.6171875 n_predict=2 lr=1.2278 bs=262144 train_time:606390ms step_avg:579.17ms
step:1048/1775 loss.item()=18669.78125 n_predict=2 lr=1.2262 bs=262144 train_time:607131ms step_avg:579.32ms
step:1049/1775 loss.item()=17893.65234375 n_predict=2 lr=1.2245 bs=262144 train_time:607867ms step_avg:579.47ms
step:1050/1775 loss.item()=18157.197265625 n_predict=2 lr=1.2229 bs=262144 train_time:608608ms step_avg:579.63ms
step:1051/1775 loss.item()=18106.458984375 n_predict=2 lr=1.2213 bs=262144 train_time:609345ms step_avg:579.78ms
step:1052/1775 loss.item()=18450.75390625 n_predict=2 lr=1.2196 bs=262144 train_time:610084ms step_avg:579.93ms
step:1053/1775 loss.item()=17441.75 n_predict=2 lr=1.2180 bs=262144 train_time:610820ms step_avg:580.08ms
step:1054/1775 loss.item()=17572.24609375 n_predict=2 lr=1.2164 bs=262144 train_time:611559ms step_avg:580.23ms
step:1055/1775 loss.item()=17068.78515625 n_predict=2 lr=1.2147 bs=262144 train_time:612295ms step_avg:580.37ms
step:1056/1775 loss.item()=17404.59765625 n_predict=2 lr=1.2131 bs=262144 train_time:613032ms step_avg:580.52ms
step:1057/1775 loss.item()=17646.640625 n_predict=2 lr=1.2114 bs=262144 train_time:613770ms step_avg:580.67ms
step:1058/1775 loss.item()=17894.5625 n_predict=2 lr=1.2098 bs=262144 train_time:614512ms step_avg:580.82ms
step:1059/1775 loss.item()=17408.2265625 n_predict=2 lr=1.2082 bs=262144 train_time:615249ms step_avg:580.97ms
step:1060/1775 loss.item()=17356.521484375 n_predict=2 lr=1.2065 bs=262144 train_time:615991ms step_avg:581.12ms
step:1061/1775 loss.item()=17502.1015625 n_predict=2 lr=1.2049 bs=262144 train_time:616729ms step_avg:581.27ms
step:1062/1775 loss.item()=17724.205078125 n_predict=2 lr=1.2033 bs=262144 train_time:617469ms step_avg:581.42ms
step:1063/1775 loss.item()=17561.7109375 n_predict=2 lr=1.2016 bs=262144 train_time:618206ms step_avg:581.57ms
step:1064/1775 loss.item()=16634.197265625 n_predict=2 lr=1.2000 bs=262144 train_time:618945ms step_avg:581.72ms
step:1065/1775 loss.item()=17881.39453125 n_predict=2 lr=1.1984 bs=262144 train_time:619683ms step_avg:581.86ms
step:1066/1775 loss.item()=17206.53515625 n_predict=2 lr=1.1967 bs=262144 train_time:620423ms step_avg:582.01ms
step:1067/1775 loss.item()=17202.921875 n_predict=2 lr=1.1951 bs=262144 train_time:621161ms step_avg:582.16ms
step:1068/1775 loss.item()=17679.43359375 n_predict=2 lr=1.1934 bs=262144 train_time:621900ms step_avg:582.30ms
step:1069/1775 loss.item()=17836.65625 n_predict=2 lr=1.1918 bs=262144 train_time:622637ms step_avg:582.45ms
step:1070/1775 loss.item()=17140.818359375 n_predict=2 lr=1.1902 bs=262144 train_time:623379ms step_avg:582.60ms
step:1071/1775 loss.item()=17143.951171875 n_predict=2 lr=1.1885 bs=262144 train_time:624116ms step_avg:582.74ms
step:1072/1775 loss.item()=16715.89453125 n_predict=2 lr=1.1869 bs=262144 train_time:624852ms step_avg:582.88ms
step:1073/1775 loss.item()=16966.671875 n_predict=2 lr=1.1853 bs=262144 train_time:625589ms step_avg:583.03ms
step:1074/1775 loss.item()=17317.484375 n_predict=2 lr=1.1836 bs=262144 train_time:626329ms step_avg:583.17ms
step:1075/1775 loss.item()=16165.75390625 n_predict=2 lr=1.1820 bs=262144 train_time:627067ms step_avg:583.32ms
step:1076/1775 loss.item()=17663.384765625 n_predict=2 lr=1.1803 bs=262144 train_time:627807ms step_avg:583.46ms
step:1077/1775 loss.item()=17267.734375 n_predict=2 lr=1.1787 bs=262144 train_time:628544ms step_avg:583.61ms
step:1078/1775 loss.item()=17086.650390625 n_predict=2 lr=1.1771 bs=262144 train_time:629285ms step_avg:583.75ms
step:1079/1775 loss.item()=17998.22265625 n_predict=2 lr=1.1754 bs=262144 train_time:630021ms step_avg:583.89ms
step:1080/1775 loss.item()=17430.171875 n_predict=2 lr=1.1738 bs=262144 train_time:630761ms step_avg:584.04ms
step:1081/1775 loss.item()=16833.4140625 n_predict=2 lr=1.1722 bs=262144 train_time:631497ms step_avg:584.18ms
step:1082/1775 loss.item()=17266.93359375 n_predict=2 lr=1.1705 bs=262144 train_time:632237ms step_avg:584.32ms
step:1083/1775 loss.item()=17386.98828125 n_predict=2 lr=1.1689 bs=262144 train_time:632976ms step_avg:584.47ms
step:1084/1775 loss.item()=16505.58984375 n_predict=2 lr=1.1673 bs=262144 train_time:633719ms step_avg:584.61ms
step:1085/1775 loss.item()=16380.775390625 n_predict=2 lr=1.1656 bs=262144 train_time:634454ms step_avg:584.75ms
step:1086/1775 loss.item()=16162.357421875 n_predict=2 lr=1.1640 bs=262144 train_time:635196ms step_avg:584.89ms
step:1087/1775 loss.item()=16628.5234375 n_predict=2 lr=1.1623 bs=262144 train_time:635932ms step_avg:585.03ms
step:1088/1775 loss.item()=16998.8125 n_predict=2 lr=1.1607 bs=262144 train_time:636673ms step_avg:585.18ms
step:1089/1775 loss.item()=16665.212890625 n_predict=2 lr=1.1591 bs=262144 train_time:637410ms step_avg:585.32ms
step:1090/1775 loss.item()=16953.6875 n_predict=2 lr=1.1574 bs=262144 train_time:638152ms step_avg:585.46ms
step:1091/1775 loss.item()=16740.296875 n_predict=2 lr=1.1558 bs=262144 train_time:638889ms step_avg:585.60ms
step:1092/1775 loss.item()=16924.234375 n_predict=2 lr=1.1542 bs=262144 train_time:639630ms step_avg:585.74ms
step:1093/1775 loss.item()=17110.71875 n_predict=2 lr=1.1525 bs=262144 train_time:640368ms step_avg:585.88ms
step:1094/1775 loss.item()=17106.140625 n_predict=2 lr=1.1509 bs=262144 train_time:641109ms step_avg:586.02ms
step:1095/1775 loss.item()=15715.70703125 n_predict=2 lr=1.1492 bs=262144 train_time:641842ms step_avg:586.16ms
step:1096/1775 loss.item()=16298.1171875 n_predict=2 lr=1.1476 bs=262144 train_time:642583ms step_avg:586.30ms
step:1097/1775 loss.item()=15917.271484375 n_predict=2 lr=1.1460 bs=262144 train_time:643320ms step_avg:586.44ms
step:1098/1775 loss.item()=17435.16796875 n_predict=2 lr=1.1443 bs=262144 train_time:644060ms step_avg:586.58ms
step:1099/1775 loss.item()=16262.150390625 n_predict=2 lr=1.1427 bs=262144 train_time:644798ms step_avg:586.71ms
step:1100/1775 loss.item()=15918.2626953125 n_predict=2 lr=1.1411 bs=262144 train_time:645535ms step_avg:586.85ms
step:1101/1775 loss.item()=16572.525390625 n_predict=2 lr=1.1394 bs=262144 train_time:646268ms step_avg:586.98ms
step:1102/1775 loss.item()=16829.0546875 n_predict=2 lr=1.1378 bs=262144 train_time:647009ms step_avg:587.12ms
step:1103/1775 loss.item()=16571.755859375 n_predict=2 lr=1.1361 bs=262144 train_time:647743ms step_avg:587.26ms
step:1104/1775 loss.item()=16249.1826171875 n_predict=2 lr=1.1345 bs=262144 train_time:648481ms step_avg:587.39ms
step:1105/1775 loss.item()=16110.7021484375 n_predict=2 lr=1.1329 bs=262144 train_time:649220ms step_avg:587.53ms
step:1106/1775 loss.item()=16330.68359375 n_predict=2 lr=1.1312 bs=262144 train_time:649958ms step_avg:587.67ms
step:1107/1775 loss.item()=16776.216796875 n_predict=2 lr=1.1296 bs=262144 train_time:650697ms step_avg:587.80ms
step:1108/1775 loss.item()=15551.03515625 n_predict=2 lr=1.1280 bs=262144 train_time:651437ms step_avg:587.94ms
step:1109/1775 loss.item()=15831.3623046875 n_predict=2 lr=1.1263 bs=262144 train_time:652175ms step_avg:588.08ms
step:1110/1775 loss.item()=15951.943359375 n_predict=2 lr=1.1247 bs=262144 train_time:652915ms step_avg:588.21ms
step:1111/1775 loss.item()=15970.71875 n_predict=2 lr=1.1231 bs=262144 train_time:653653ms step_avg:588.35ms
step:1112/1775 loss.item()=15878.4814453125 n_predict=2 lr=1.1214 bs=262144 train_time:654393ms step_avg:588.48ms
step:1113/1775 loss.item()=16007.5654296875 n_predict=2 lr=1.1198 bs=262144 train_time:655130ms step_avg:588.62ms
step:1114/1775 loss.item()=15647.53125 n_predict=2 lr=1.1181 bs=262144 train_time:655872ms step_avg:588.75ms
step:1115/1775 loss.item()=16892.76171875 n_predict=2 lr=1.1165 bs=262144 train_time:656611ms step_avg:588.89ms
step:1116/1775 loss.item()=15953.845703125 n_predict=2 lr=1.1149 bs=262144 train_time:657351ms step_avg:589.02ms
step:1117/1775 loss.item()=16474.681640625 n_predict=2 lr=1.1132 bs=262144 train_time:658088ms step_avg:589.16ms
step:1118/1775 loss.item()=16093.546875 n_predict=2 lr=1.1116 bs=262144 train_time:658827ms step_avg:589.29ms
step:1119/1775 loss.item()=15692.0361328125 n_predict=2 lr=1.1100 bs=262144 train_time:659561ms step_avg:589.42ms
step:1120/1775 loss.item()=15707.52734375 n_predict=2 lr=1.1083 bs=262144 train_time:660301ms step_avg:589.55ms
step:1121/1775 loss.item()=15431.263671875 n_predict=2 lr=1.1067 bs=262144 train_time:661039ms step_avg:589.69ms
step:1122/1775 loss.item()=15523.314453125 n_predict=2 lr=1.1050 bs=262144 train_time:661778ms step_avg:589.82ms
step:1123/1775 loss.item()=15964.259765625 n_predict=2 lr=1.1034 bs=262144 train_time:662519ms step_avg:589.95ms
step:1124/1775 loss.item()=15612.7509765625 n_predict=2 lr=1.1018 bs=262144 train_time:663255ms step_avg:590.08ms
step:1125/1775 loss.item()=15688.5888671875 n_predict=2 lr=1.1001 bs=262144 train_time:663993ms step_avg:590.22ms
step:1126/1775 loss.item()=15407.34375 n_predict=2 lr=1.0985 bs=262144 train_time:664733ms step_avg:590.35ms
step:1127/1775 loss.item()=15956.2734375 n_predict=2 lr=1.0969 bs=262144 train_time:665470ms step_avg:590.48ms
step:1128/1775 loss.item()=15340.30078125 n_predict=2 lr=1.0952 bs=262144 train_time:666211ms step_avg:590.61ms
step:1129/1775 loss.item()=15692.30859375 n_predict=2 lr=1.0936 bs=262144 train_time:666948ms step_avg:590.74ms
step:1130/1775 loss.item()=15479.19140625 n_predict=2 lr=1.0920 bs=262144 train_time:667687ms step_avg:590.87ms
step:1131/1775 loss.item()=15799.705078125 n_predict=2 lr=1.0903 bs=262144 train_time:668425ms step_avg:591.00ms
step:1132/1775 loss.item()=15677.841796875 n_predict=2 lr=1.0887 bs=262144 train_time:669161ms step_avg:591.13ms
step:1133/1775 loss.item()=15581.400390625 n_predict=2 lr=1.0870 bs=262144 train_time:669901ms step_avg:591.26ms
step:1134/1775 loss.item()=15787.52734375 n_predict=2 lr=1.0854 bs=262144 train_time:670641ms step_avg:591.39ms
step:1135/1775 loss.item()=15738.990234375 n_predict=2 lr=1.0838 bs=262144 train_time:671376ms step_avg:591.52ms
step:1136/1775 loss.item()=14962.27734375 n_predict=2 lr=1.0821 bs=262144 train_time:672118ms step_avg:591.65ms
step:1137/1775 loss.item()=15211.513671875 n_predict=2 lr=1.0805 bs=262144 train_time:672854ms step_avg:591.78ms
step:1138/1775 loss.item()=15374.015625 n_predict=2 lr=1.0789 bs=262144 train_time:673592ms step_avg:591.91ms
step:1139/1775 loss.item()=15050.603515625 n_predict=2 lr=1.0772 bs=262144 train_time:674332ms step_avg:592.04ms
step:1140/1775 loss.item()=16134.263671875 n_predict=2 lr=1.0756 bs=262144 train_time:675071ms step_avg:592.17ms
step:1141/1775 loss.item()=15013.9111328125 n_predict=2 lr=1.0739 bs=262144 train_time:675808ms step_avg:592.29ms
step:1142/1775 loss.item()=15305.0595703125 n_predict=2 lr=1.0723 bs=262144 train_time:676548ms step_avg:592.42ms
step:1143/1775 loss.item()=14807.65234375 n_predict=2 lr=1.0707 bs=262144 train_time:677287ms step_avg:592.55ms
step:1144/1775 loss.item()=14374.439453125 n_predict=2 lr=1.0690 bs=262144 train_time:678028ms step_avg:592.68ms
step:1145/1775 loss.item()=15704.61328125 n_predict=2 lr=1.0674 bs=262144 train_time:678765ms step_avg:592.81ms
step:1146/1775 loss.item()=14621.97265625 n_predict=2 lr=1.0658 bs=262144 train_time:679505ms step_avg:592.94ms
step:1147/1775 loss.item()=15185.068359375 n_predict=2 lr=1.0641 bs=262144 train_time:680242ms step_avg:593.06ms
step:1148/1775 loss.item()=15434.306640625 n_predict=2 lr=1.0625 bs=262144 train_time:680981ms step_avg:593.19ms
step:1149/1775 loss.item()=14242.09765625 n_predict=2 lr=1.0609 bs=262144 train_time:681719ms step_avg:593.31ms
step:1150/1775 loss.item()=14355.5556640625 n_predict=2 lr=1.0592 bs=262144 train_time:682454ms step_avg:593.44ms
step:1151/1775 loss.item()=15158.994140625 n_predict=2 lr=1.0576 bs=262144 train_time:683193ms step_avg:593.57ms
step:1152/1775 loss.item()=15154.32421875 n_predict=2 lr=1.0559 bs=262144 train_time:683933ms step_avg:593.69ms
step:1153/1775 loss.item()=14579.966796875 n_predict=2 lr=1.0543 bs=262144 train_time:684671ms step_avg:593.82ms
step:1154/1775 loss.item()=15041.603515625 n_predict=2 lr=1.0527 bs=262144 train_time:685412ms step_avg:593.94ms
step:1155/1775 loss.item()=14847.259765625 n_predict=2 lr=1.0510 bs=262144 train_time:686148ms step_avg:594.07ms
step:1156/1775 loss.item()=15300.5947265625 n_predict=2 lr=1.0494 bs=262144 train_time:686890ms step_avg:594.20ms
step:1157/1775 loss.item()=14812.97265625 n_predict=2 lr=1.0478 bs=262144 train_time:687627ms step_avg:594.32ms
step:1158/1775 loss.item()=22187.48046875 n_predict=1 lr=1.1860 bs=393216 train_time:715751ms step_avg:618.09ms
step:1159/1775 loss.item()=21957.595703125 n_predict=1 lr=1.1842 bs=393216 train_time:742500ms step_avg:640.64ms
step:1160/1775 loss.item()=21424.98046875 n_predict=1 lr=1.1823 bs=393216 train_time:743567ms step_avg:641.01ms
step:1161/1775 loss.item()=22522.275390625 n_predict=1 lr=1.1804 bs=393216 train_time:744621ms step_avg:641.36ms
step:1162/1775 loss.item()=21856.453125 n_predict=1 lr=1.1785 bs=393216 train_time:745687ms step_avg:641.73ms
step:1163/1775 loss.item()=21898.51953125 n_predict=1 lr=1.1766 bs=393216 train_time:746745ms step_avg:642.09ms
step:1164/1775 loss.item()=22425.7578125 n_predict=1 lr=1.1748 bs=393216 train_time:747808ms step_avg:642.45ms
step:1165/1775 loss.item()=21971.84375 n_predict=1 lr=1.1729 bs=393216 train_time:748874ms step_avg:642.81ms
step:1166/1775 loss.item()=22393.3359375 n_predict=1 lr=1.1710 bs=393216 train_time:749939ms step_avg:643.17ms
step:1167/1775 loss.item()=21574.3203125 n_predict=1 lr=1.1691 bs=393216 train_time:751000ms step_avg:643.53ms
step:1168/1775 loss.item()=22431.650390625 n_predict=1 lr=1.1673 bs=393216 train_time:752069ms step_avg:643.89ms
step:1169/1775 loss.item()=22252.37890625 n_predict=1 lr=1.1654 bs=393216 train_time:753130ms step_avg:644.25ms
step:1170/1775 loss.item()=21667.2109375 n_predict=1 lr=1.1635 bs=393216 train_time:754196ms step_avg:644.61ms
step:1171/1775 loss.item()=21817.71484375 n_predict=1 lr=1.1616 bs=393216 train_time:755256ms step_avg:644.97ms
step:1172/1775 loss.item()=22230.94140625 n_predict=1 lr=1.1597 bs=393216 train_time:756321ms step_avg:645.33ms
step:1173/1775 loss.item()=22844.5546875 n_predict=1 lr=1.1579 bs=393216 train_time:757387ms step_avg:645.68ms
step:1174/1775 loss.item()=21323.39453125 n_predict=1 lr=1.1560 bs=393216 train_time:758451ms step_avg:646.04ms
step:1175/1775 loss.item()=21606.87890625 n_predict=1 lr=1.1541 bs=393216 train_time:759515ms step_avg:646.40ms
step:1176/1775 loss.item()=21888.046875 n_predict=1 lr=1.1522 bs=393216 train_time:760582ms step_avg:646.75ms
step:1177/1775 loss.item()=22311.2578125 n_predict=1 lr=1.1503 bs=393216 train_time:761648ms step_avg:647.11ms
step:1178/1775 loss.item()=21271.65625 n_predict=1 lr=1.1485 bs=393216 train_time:762718ms step_avg:647.47ms
step:1179/1775 loss.item()=22558.69921875 n_predict=1 lr=1.1466 bs=393216 train_time:763781ms step_avg:647.82ms
step:1180/1775 loss.item()=21948.265625 n_predict=1 lr=1.1447 bs=393216 train_time:764850ms step_avg:648.18ms
step:1181/1775 loss.item()=21516.71875 n_predict=1 lr=1.1428 bs=393216 train_time:765916ms step_avg:648.53ms
step:1182/1775 loss.item()=21653.52734375 n_predict=1 lr=1.1409 bs=393216 train_time:766985ms step_avg:648.89ms
step:1183/1775 loss.item()=22069.892578125 n_predict=1 lr=1.1391 bs=393216 train_time:768051ms step_avg:649.24ms
step:1184/1775 loss.item()=22366.67578125 n_predict=1 lr=1.1372 bs=393216 train_time:769119ms step_avg:649.59ms
step:1185/1775 loss.item()=21084.4140625 n_predict=1 lr=1.1353 bs=393216 train_time:770188ms step_avg:649.95ms
step:1186/1775 loss.item()=21202.12109375 n_predict=1 lr=1.1334 bs=393216 train_time:771259ms step_avg:650.30ms
step:1187/1775 loss.item()=22267.88671875 n_predict=1 lr=1.1316 bs=393216 train_time:772326ms step_avg:650.65ms
step:1188/1775 loss.item()=21225.361328125 n_predict=1 lr=1.1297 bs=393216 train_time:773391ms step_avg:651.00ms
step:1189/1775 loss.item()=21364.537109375 n_predict=1 lr=1.1278 bs=393216 train_time:774458ms step_avg:651.35ms
step:1190/1775 loss.item()=21576.369140625 n_predict=1 lr=1.1259 bs=393216 train_time:775530ms step_avg:651.71ms
step:1191/1775 loss.item()=21747.1171875 n_predict=1 lr=1.1240 bs=393216 train_time:776599ms step_avg:652.06ms
step:1192/1775 loss.item()=21685.146484375 n_predict=1 lr=1.1222 bs=393216 train_time:777671ms step_avg:652.41ms
step:1193/1775 loss.item()=21872.37109375 n_predict=1 lr=1.1203 bs=393216 train_time:778741ms step_avg:652.76ms
step:1194/1775 loss.item()=20769.654296875 n_predict=1 lr=1.1184 bs=393216 train_time:779811ms step_avg:653.11ms
step:1195/1775 loss.item()=22239.2578125 n_predict=1 lr=1.1165 bs=393216 train_time:780879ms step_avg:653.46ms
step:1196/1775 loss.item()=21649.796875 n_predict=1 lr=1.1146 bs=393216 train_time:781951ms step_avg:653.81ms
step:1197/1775 loss.item()=21625.2265625 n_predict=1 lr=1.1128 bs=393216 train_time:783018ms step_avg:654.15ms
step:1198/1775 loss.item()=23186.5 n_predict=1 lr=1.1109 bs=393216 train_time:784091ms step_avg:654.50ms
step:1199/1775 loss.item()=22000.40234375 n_predict=1 lr=1.1090 bs=393216 train_time:785159ms step_avg:654.84ms
step:1200/1775 loss.item()=22022.025390625 n_predict=1 lr=1.1071 bs=393216 train_time:786232ms step_avg:655.19ms
step:1201/1775 loss.item()=21739.296875 n_predict=1 lr=1.1052 bs=393216 train_time:787299ms step_avg:655.54ms
step:1202/1775 loss.item()=21361.541015625 n_predict=1 lr=1.1034 bs=393216 train_time:788371ms step_avg:655.88ms
step:1203/1775 loss.item()=21585.6171875 n_predict=1 lr=1.1015 bs=393216 train_time:789441ms step_avg:656.23ms
step:1204/1775 loss.item()=21789.2109375 n_predict=1 lr=1.0996 bs=393216 train_time:790515ms step_avg:656.57ms
step:1205/1775 loss.item()=21659.328125 n_predict=1 lr=1.0977 bs=393216 train_time:791584ms step_avg:656.92ms
step:1206/1775 loss.item()=21801.9375 n_predict=1 lr=1.0959 bs=393216 train_time:792654ms step_avg:657.26ms
step:1207/1775 loss.item()=21971.8984375 n_predict=1 lr=1.0940 bs=393216 train_time:793725ms step_avg:657.60ms
step:1208/1775 loss.item()=22586.3828125 n_predict=1 lr=1.0921 bs=393216 train_time:794795ms step_avg:657.94ms
step:1209/1775 loss.item()=22397.39453125 n_predict=1 lr=1.0902 bs=393216 train_time:795865ms step_avg:658.28ms
step:1210/1775 loss.item()=21339.37109375 n_predict=1 lr=1.0883 bs=393216 train_time:796939ms step_avg:658.63ms
step:1211/1775 loss.item()=22089.6171875 n_predict=1 lr=1.0865 bs=393216 train_time:798009ms step_avg:658.97ms
step:1212/1775 loss.item()=21209.6328125 n_predict=1 lr=1.0846 bs=393216 train_time:799090ms step_avg:659.31ms
step:1213/1775 loss.item()=20739.27734375 n_predict=1 lr=1.0827 bs=393216 train_time:800159ms step_avg:659.65ms
step:1214/1775 loss.item()=22028.072265625 n_predict=1 lr=1.0808 bs=393216 train_time:801241ms step_avg:660.00ms
step:1215/1775 loss.item()=21849.15625 n_predict=1 lr=1.0789 bs=393216 train_time:802314ms step_avg:660.34ms
step:1216/1775 loss.item()=21971.826171875 n_predict=1 lr=1.0771 bs=393216 train_time:803390ms step_avg:660.68ms
step:1217/1775 loss.item()=21025.0625 n_predict=1 lr=1.0752 bs=393216 train_time:804462ms step_avg:661.02ms
step:1218/1775 loss.item()=21895.92578125 n_predict=1 lr=1.0733 bs=393216 train_time:805535ms step_avg:661.36ms
step:1219/1775 loss.item()=21676.734375 n_predict=1 lr=1.0714 bs=393216 train_time:806607ms step_avg:661.70ms
step:1220/1775 loss.item()=21671.357421875 n_predict=1 lr=1.0695 bs=393216 train_time:807684ms step_avg:662.04ms
step:1221/1775 loss.item()=21130.189453125 n_predict=1 lr=1.0677 bs=393216 train_time:808756ms step_avg:662.37ms
step:1222/1775 loss.item()=22537.90625 n_predict=1 lr=1.0658 bs=393216 train_time:809833ms step_avg:662.71ms
step:1223/1775 loss.item()=22134.3359375 n_predict=1 lr=1.0639 bs=393216 train_time:810903ms step_avg:663.04ms
step:1224/1775 loss.item()=21864.32421875 n_predict=1 lr=1.0620 bs=393216 train_time:811980ms step_avg:663.38ms
step:1225/1775 loss.item()=21763.02734375 n_predict=1 lr=1.0601 bs=393216 train_time:813049ms step_avg:663.71ms
step:1226/1775 loss.item()=21946.72265625 n_predict=1 lr=1.0583 bs=393216 train_time:814124ms step_avg:664.05ms
step:1227/1775 loss.item()=22168.1796875 n_predict=1 lr=1.0564 bs=393216 train_time:815198ms step_avg:664.38ms
step:1228/1775 loss.item()=21314.85546875 n_predict=1 lr=1.0545 bs=393216 train_time:816274ms step_avg:664.72ms
step:1229/1775 loss.item()=21669.23046875 n_predict=1 lr=1.0526 bs=393216 train_time:817344ms step_avg:665.05ms
step:1230/1775 loss.item()=21889.09375 n_predict=1 lr=1.0508 bs=393216 train_time:818420ms step_avg:665.38ms
step:1231/1775 loss.item()=21769.400390625 n_predict=1 lr=1.0489 bs=393216 train_time:819495ms step_avg:665.71ms
step:1232/1775 loss.item()=21164.662109375 n_predict=1 lr=1.0470 bs=393216 train_time:820578ms step_avg:666.05ms
step:1233/1775 loss.item()=22035.28125 n_predict=1 lr=1.0451 bs=393216 train_time:821649ms step_avg:666.38ms
step:1234/1775 loss.item()=21895.6953125 n_predict=1 lr=1.0432 bs=393216 train_time:822725ms step_avg:666.71ms
step:1235/1775 loss.item()=21150.98828125 n_predict=1 lr=1.0414 bs=393216 train_time:823803ms step_avg:667.05ms
step:1236/1775 loss.item()=21570.40625 n_predict=1 lr=1.0395 bs=393216 train_time:824882ms step_avg:667.38ms
step:1237/1775 loss.item()=21494.07421875 n_predict=1 lr=1.0376 bs=393216 train_time:825958ms step_avg:667.71ms
step:1238/1775 loss.item()=22176.08984375 n_predict=1 lr=1.0357 bs=393216 train_time:827034ms step_avg:668.04ms
step:1239/1775 loss.item()=21507.126953125 n_predict=1 lr=1.0338 bs=393216 train_time:828108ms step_avg:668.37ms
step:1240/1775 loss.item()=22553.87109375 n_predict=1 lr=1.0320 bs=393216 train_time:829186ms step_avg:668.70ms
step:1241/1775 loss.item()=21962.65625 n_predict=1 lr=1.0301 bs=393216 train_time:830257ms step_avg:669.02ms
step:1242/1775 loss.item()=22326.806640625 n_predict=1 lr=1.0282 bs=393216 train_time:831334ms step_avg:669.35ms
step:1243/1775 loss.item()=21692.47265625 n_predict=1 lr=1.0263 bs=393216 train_time:832408ms step_avg:669.68ms
step:1244/1775 loss.item()=22101.796875 n_predict=1 lr=1.0244 bs=393216 train_time:833486ms step_avg:670.00ms
step:1245/1775 loss.item()=21783.21875 n_predict=1 lr=1.0226 bs=393216 train_time:834559ms step_avg:670.33ms
step:1246/1775 loss.item()=21523.27734375 n_predict=1 lr=1.0207 bs=393216 train_time:835637ms step_avg:670.66ms
step:1247/1775 loss.item()=21690.4609375 n_predict=1 lr=1.0188 bs=393216 train_time:836710ms step_avg:670.98ms
step:1248/1775 loss.item()=22162.400390625 n_predict=1 lr=1.0169 bs=393216 train_time:837786ms step_avg:671.30ms
step:1249/1775 loss.item()=21638.833984375 n_predict=1 lr=1.0151 bs=393216 train_time:838862ms step_avg:671.63ms
step:1250/1775 loss.item()=21007.791015625 n_predict=1 lr=1.0132 bs=393216 train_time:839939ms step_avg:671.95ms
step:1250/1775 lr=1.0113 bs=393216 n_predict=1 val_loss:3.5136 val_malbo_loss:3.5936 train_time:840025ms step_avg:672.02ms
step:1251/1775 loss.item()=21572.6875 n_predict=1 lr=1.0113 bs=393216 train_time:841002ms step_avg:672.26ms
step:1252/1775 loss.item()=21467.009765625 n_predict=1 lr=1.0094 bs=393216 train_time:842062ms step_avg:672.57ms
step:1253/1775 loss.item()=21117.091796875 n_predict=1 lr=1.0075 bs=393216 train_time:843121ms step_avg:672.88ms
step:1254/1775 loss.item()=21475.189453125 n_predict=1 lr=1.0057 bs=393216 train_time:844183ms step_avg:673.19ms
step:1255/1775 loss.item()=21506.171875 n_predict=1 lr=1.0038 bs=393216 train_time:845244ms step_avg:673.50ms
step:1256/1775 loss.item()=22200.595703125 n_predict=1 lr=1.0019 bs=393216 train_time:846309ms step_avg:673.81ms
step:1257/1775 loss.item()=21243.47265625 n_predict=1 lr=1.0000 bs=393216 train_time:847369ms step_avg:674.12ms
step:1258/1775 loss.item()=22585.84375 n_predict=1 lr=0.9981 bs=393216 train_time:848434ms step_avg:674.43ms
step:1259/1775 loss.item()=21194.701171875 n_predict=1 lr=0.9963 bs=393216 train_time:849497ms step_avg:674.74ms
step:1260/1775 loss.item()=20948.97265625 n_predict=1 lr=0.9944 bs=393216 train_time:850556ms step_avg:675.04ms
step:1261/1775 loss.item()=21697.529296875 n_predict=1 lr=0.9925 bs=393216 train_time:851615ms step_avg:675.35ms
step:1262/1775 loss.item()=21366.6015625 n_predict=1 lr=0.9906 bs=393216 train_time:852678ms step_avg:675.66ms
step:1263/1775 loss.item()=21806.30859375 n_predict=1 lr=0.9887 bs=393216 train_time:853741ms step_avg:675.96ms
step:1264/1775 loss.item()=21119.78515625 n_predict=1 lr=0.9869 bs=393216 train_time:854807ms step_avg:676.27ms
step:1265/1775 loss.item()=21340.20703125 n_predict=1 lr=0.9850 bs=393216 train_time:855871ms step_avg:676.58ms
step:1266/1775 loss.item()=21651.814453125 n_predict=1 lr=0.9831 bs=393216 train_time:856942ms step_avg:676.89ms
step:1267/1775 loss.item()=21937.935546875 n_predict=1 lr=0.9812 bs=393216 train_time:858006ms step_avg:677.19ms
step:1268/1775 loss.item()=21006.30078125 n_predict=1 lr=0.9794 bs=393216 train_time:859074ms step_avg:677.50ms
step:1269/1775 loss.item()=22060.07421875 n_predict=1 lr=0.9775 bs=393216 train_time:860139ms step_avg:677.81ms
step:1270/1775 loss.item()=21360.265625 n_predict=1 lr=0.9756 bs=393216 train_time:861211ms step_avg:678.12ms
step:1271/1775 loss.item()=21990.1640625 n_predict=1 lr=0.9737 bs=393216 train_time:862275ms step_avg:678.42ms
step:1272/1775 loss.item()=21133.0234375 n_predict=1 lr=0.9718 bs=393216 train_time:863345ms step_avg:678.73ms
step:1273/1775 loss.item()=21322.36328125 n_predict=1 lr=0.9700 bs=393216 train_time:864408ms step_avg:679.03ms
step:1274/1775 loss.item()=22115.080078125 n_predict=1 lr=0.9681 bs=393216 train_time:865475ms step_avg:679.34ms
step:1275/1775 loss.item()=21188.9609375 n_predict=1 lr=0.9662 bs=393216 train_time:866538ms step_avg:679.64ms
step:1276/1775 loss.item()=21759.09375 n_predict=1 lr=0.9643 bs=393216 train_time:867608ms step_avg:679.94ms
step:1277/1775 loss.item()=20799.970703125 n_predict=1 lr=0.9624 bs=393216 train_time:868672ms step_avg:680.24ms
step:1278/1775 loss.item()=21061.546875 n_predict=1 lr=0.9606 bs=393216 train_time:869742ms step_avg:680.55ms
step:1279/1775 loss.item()=21001.44921875 n_predict=1 lr=0.9587 bs=393216 train_time:870809ms step_avg:680.85ms
step:1280/1775 loss.item()=22197.44140625 n_predict=1 lr=0.9568 bs=393216 train_time:871880ms step_avg:681.16ms
step:1281/1775 loss.item()=20940.7265625 n_predict=1 lr=0.9549 bs=393216 train_time:872949ms step_avg:681.46ms
step:1282/1775 loss.item()=21402.4296875 n_predict=1 lr=0.9530 bs=393216 train_time:874017ms step_avg:681.76ms
step:1283/1775 loss.item()=21563.19140625 n_predict=1 lr=0.9512 bs=393216 train_time:875080ms step_avg:682.06ms
step:1284/1775 loss.item()=21242.115234375 n_predict=1 lr=0.9493 bs=393216 train_time:876153ms step_avg:682.36ms
step:1285/1775 loss.item()=21427.55859375 n_predict=1 lr=0.9474 bs=393216 train_time:877220ms step_avg:682.66ms
step:1286/1775 loss.item()=21503.72265625 n_predict=1 lr=0.9455 bs=393216 train_time:878293ms step_avg:682.97ms
step:1287/1775 loss.item()=21964.828125 n_predict=1 lr=0.9437 bs=393216 train_time:879361ms step_avg:683.26ms
step:1288/1775 loss.item()=21682.35546875 n_predict=1 lr=0.9418 bs=393216 train_time:880434ms step_avg:683.57ms
step:1289/1775 loss.item()=21324.4375 n_predict=1 lr=0.9399 bs=393216 train_time:881501ms step_avg:683.86ms
step:1290/1775 loss.item()=22263.91015625 n_predict=1 lr=0.9380 bs=393216 train_time:882572ms step_avg:684.16ms
step:1291/1775 loss.item()=21170.42578125 n_predict=1 lr=0.9361 bs=393216 train_time:883643ms step_avg:684.46ms
step:1292/1775 loss.item()=21787.61328125 n_predict=1 lr=0.9343 bs=393216 train_time:884715ms step_avg:684.76ms
step:1293/1775 loss.item()=21610.111328125 n_predict=1 lr=0.9324 bs=393216 train_time:885783ms step_avg:685.06ms
step:1294/1775 loss.item()=21227.662109375 n_predict=1 lr=0.9305 bs=393216 train_time:886852ms step_avg:685.36ms
step:1295/1775 loss.item()=21701.59765625 n_predict=1 lr=0.9286 bs=393216 train_time:887920ms step_avg:685.65ms
step:1296/1775 loss.item()=21631.375 n_predict=1 lr=0.9267 bs=393216 train_time:888992ms step_avg:685.95ms
step:1297/1775 loss.item()=21388.814453125 n_predict=1 lr=0.9249 bs=393216 train_time:890063ms step_avg:686.25ms
step:1298/1775 loss.item()=21096.583984375 n_predict=1 lr=0.9230 bs=393216 train_time:891137ms step_avg:686.55ms
step:1299/1775 loss.item()=21517.26953125 n_predict=1 lr=0.9211 bs=393216 train_time:892210ms step_avg:686.84ms
step:1300/1775 loss.item()=20923.3515625 n_predict=1 lr=0.9192 bs=393216 train_time:893282ms step_avg:687.14ms
step:1301/1775 loss.item()=21025.26171875 n_predict=1 lr=0.9173 bs=393216 train_time:894352ms step_avg:687.43ms
step:1302/1775 loss.item()=21226.68359375 n_predict=1 lr=0.9155 bs=393216 train_time:895424ms step_avg:687.73ms
step:1303/1775 loss.item()=22275.44140625 n_predict=1 lr=0.9136 bs=393216 train_time:896492ms step_avg:688.02ms
step:1304/1775 loss.item()=21574.15625 n_predict=1 lr=0.9117 bs=393216 train_time:897569ms step_avg:688.32ms
step:1305/1775 loss.item()=21507.896484375 n_predict=1 lr=0.9098 bs=393216 train_time:898639ms step_avg:688.61ms
step:1306/1775 loss.item()=21625.6328125 n_predict=1 lr=0.9080 bs=393216 train_time:899714ms step_avg:688.91ms
step:1307/1775 loss.item()=21490.3984375 n_predict=1 lr=0.9061 bs=393216 train_time:900779ms step_avg:689.20ms
step:1308/1775 loss.item()=21020.078125 n_predict=1 lr=0.9042 bs=393216 train_time:901854ms step_avg:689.49ms
step:1309/1775 loss.item()=21250.11328125 n_predict=1 lr=0.9023 bs=393216 train_time:902922ms step_avg:689.78ms
step:1310/1775 loss.item()=21248.728515625 n_predict=1 lr=0.9004 bs=393216 train_time:903999ms step_avg:690.08ms
step:1311/1775 loss.item()=21349.0703125 n_predict=1 lr=0.8986 bs=393216 train_time:905072ms step_avg:690.37ms
step:1312/1775 loss.item()=21620.630859375 n_predict=1 lr=0.8967 bs=393216 train_time:906146ms step_avg:690.66ms
step:1313/1775 loss.item()=21342.015625 n_predict=1 lr=0.8948 bs=393216 train_time:907218ms step_avg:690.95ms
step:1314/1775 loss.item()=21275.828125 n_predict=1 lr=0.8929 bs=393216 train_time:908290ms step_avg:691.24ms
step:1315/1775 loss.item()=20883.689453125 n_predict=1 lr=0.8910 bs=393216 train_time:909361ms step_avg:691.53ms
step:1316/1775 loss.item()=21974.017578125 n_predict=1 lr=0.8892 bs=393216 train_time:910437ms step_avg:691.82ms
step:1317/1775 loss.item()=21883.51171875 n_predict=1 lr=0.8873 bs=393216 train_time:911510ms step_avg:692.11ms
step:1318/1775 loss.item()=21532.46484375 n_predict=1 lr=0.8854 bs=393216 train_time:912580ms step_avg:692.40ms
step:1319/1775 loss.item()=21541.81640625 n_predict=1 lr=0.8835 bs=393216 train_time:913651ms step_avg:692.68ms
step:1320/1775 loss.item()=21084.345703125 n_predict=1 lr=0.8816 bs=393216 train_time:914726ms step_avg:692.97ms
step:1321/1775 loss.item()=22284.5390625 n_predict=1 lr=0.8798 bs=393216 train_time:915802ms step_avg:693.26ms
step:1322/1775 loss.item()=21796.4375 n_predict=1 lr=0.8779 bs=393216 train_time:916878ms step_avg:693.55ms
step:1323/1775 loss.item()=21566.162109375 n_predict=1 lr=0.8760 bs=393216 train_time:917952ms step_avg:693.84ms
step:1324/1775 loss.item()=21872.744140625 n_predict=1 lr=0.8741 bs=393216 train_time:919025ms step_avg:694.13ms
step:1325/1775 loss.item()=20749.76171875 n_predict=1 lr=0.8723 bs=393216 train_time:920098ms step_avg:694.41ms
step:1326/1775 loss.item()=21825.40234375 n_predict=1 lr=0.8704 bs=393216 train_time:921172ms step_avg:694.70ms
step:1327/1775 loss.item()=21279.35546875 n_predict=1 lr=0.8685 bs=393216 train_time:922244ms step_avg:694.98ms
step:1328/1775 loss.item()=21401.921875 n_predict=1 lr=0.8666 bs=393216 train_time:923322ms step_avg:695.27ms
step:1329/1775 loss.item()=21689.478515625 n_predict=1 lr=0.8647 bs=393216 train_time:924397ms step_avg:695.56ms
step:1330/1775 loss.item()=21143.533203125 n_predict=1 lr=0.8629 bs=393216 train_time:925472ms step_avg:695.84ms
step:1331/1775 loss.item()=21037.25390625 n_predict=1 lr=0.8610 bs=393216 train_time:926547ms step_avg:696.13ms
step:1332/1775 loss.item()=21642.83984375 n_predict=1 lr=0.8591 bs=393216 train_time:927620ms step_avg:696.41ms
step:1333/1775 loss.item()=21148.3671875 n_predict=1 lr=0.8572 bs=393216 train_time:928698ms step_avg:696.70ms
step:1334/1775 loss.item()=21228.138671875 n_predict=1 lr=0.8553 bs=393216 train_time:929772ms step_avg:696.98ms
step:1335/1775 loss.item()=21061.130859375 n_predict=1 lr=0.8535 bs=393216 train_time:930846ms step_avg:697.26ms
step:1336/1775 loss.item()=21324.83984375 n_predict=1 lr=0.8516 bs=393216 train_time:931920ms step_avg:697.55ms
step:1337/1775 loss.item()=21612.71875 n_predict=1 lr=0.8497 bs=393216 train_time:932996ms step_avg:697.83ms
step:1338/1775 loss.item()=21542.4609375 n_predict=1 lr=0.8478 bs=393216 train_time:934072ms step_avg:698.11ms
step:1339/1775 loss.item()=21569.541015625 n_predict=1 lr=0.8459 bs=393216 train_time:935147ms step_avg:698.39ms
step:1340/1775 loss.item()=21413.5546875 n_predict=1 lr=0.8441 bs=393216 train_time:936225ms step_avg:698.68ms
step:1341/1775 loss.item()=21430.525390625 n_predict=1 lr=0.8422 bs=393216 train_time:937299ms step_avg:698.96ms
step:1342/1775 loss.item()=20904.2421875 n_predict=1 lr=0.8403 bs=393216 train_time:938379ms step_avg:699.24ms
step:1343/1775 loss.item()=21858.4765625 n_predict=1 lr=0.8384 bs=393216 train_time:939452ms step_avg:699.52ms
step:1344/1775 loss.item()=21262.25390625 n_predict=1 lr=0.8366 bs=393216 train_time:940528ms step_avg:699.80ms
step:1345/1775 loss.item()=20675.15234375 n_predict=1 lr=0.8347 bs=393216 train_time:941600ms step_avg:700.07ms
step:1346/1775 loss.item()=21444.1640625 n_predict=1 lr=0.8328 bs=393216 train_time:942673ms step_avg:700.35ms
step:1347/1775 loss.item()=21564.89453125 n_predict=1 lr=0.8309 bs=393216 train_time:943751ms step_avg:700.63ms
step:1348/1775 loss.item()=22062.65234375 n_predict=1 lr=0.8290 bs=393216 train_time:944826ms step_avg:700.91ms
step:1349/1775 loss.item()=20837.63671875 n_predict=1 lr=0.8272 bs=393216 train_time:945898ms step_avg:701.18ms
step:1350/1775 loss.item()=21394.27734375 n_predict=1 lr=0.8253 bs=393216 train_time:946974ms step_avg:701.46ms
step:1351/1775 loss.item()=22456.24609375 n_predict=1 lr=0.8234 bs=393216 train_time:948047ms step_avg:701.74ms
step:1352/1775 loss.item()=21670.26953125 n_predict=1 lr=0.8215 bs=393216 train_time:949122ms step_avg:702.01ms
step:1353/1775 loss.item()=21533.90625 n_predict=1 lr=0.8196 bs=393216 train_time:950196ms step_avg:702.29ms
step:1354/1775 loss.item()=20756.671875 n_predict=1 lr=0.8178 bs=393216 train_time:951272ms step_avg:702.56ms
step:1355/1775 loss.item()=20841.48046875 n_predict=1 lr=0.8159 bs=393216 train_time:952346ms step_avg:702.84ms
step:1356/1775 loss.item()=21502.296875 n_predict=1 lr=0.8140 bs=393216 train_time:953424ms step_avg:703.12ms
step:1357/1775 loss.item()=21516.68359375 n_predict=1 lr=0.8121 bs=393216 train_time:954499ms step_avg:703.39ms
step:1358/1775 loss.item()=21362.0859375 n_predict=1 lr=0.8102 bs=393216 train_time:955575ms step_avg:703.66ms
step:1359/1775 loss.item()=21028.62109375 n_predict=1 lr=0.8084 bs=393216 train_time:956648ms step_avg:703.94ms
step:1360/1775 loss.item()=20859.71875 n_predict=1 lr=0.8065 bs=393216 train_time:957726ms step_avg:704.21ms
step:1361/1775 loss.item()=20991.078125 n_predict=1 lr=0.8046 bs=393216 train_time:958799ms step_avg:704.48ms
step:1362/1775 loss.item()=21126.98828125 n_predict=1 lr=0.8027 bs=393216 train_time:959875ms step_avg:704.75ms
step:1363/1775 loss.item()=21673.6484375 n_predict=1 lr=0.8009 bs=393216 train_time:960946ms step_avg:705.02ms
step:1364/1775 loss.item()=21528.494140625 n_predict=1 lr=0.7990 bs=393216 train_time:962022ms step_avg:705.29ms
step:1365/1775 loss.item()=20540.927734375 n_predict=1 lr=0.7971 bs=393216 train_time:963096ms step_avg:705.57ms
step:1366/1775 loss.item()=20765.85546875 n_predict=1 lr=0.7952 bs=393216 train_time:964173ms step_avg:705.84ms
step:1367/1775 loss.item()=21006.8515625 n_predict=1 lr=0.7933 bs=393216 train_time:965248ms step_avg:706.11ms
step:1368/1775 loss.item()=21320.01953125 n_predict=1 lr=0.7915 bs=393216 train_time:966324ms step_avg:706.38ms
step:1369/1775 loss.item()=21079.9921875 n_predict=1 lr=0.7896 bs=393216 train_time:967398ms step_avg:706.65ms
step:1370/1775 loss.item()=21725.875 n_predict=1 lr=0.7877 bs=393216 train_time:968474ms step_avg:706.92ms
step:1371/1775 loss.item()=21043.82421875 n_predict=1 lr=0.7858 bs=393216 train_time:969550ms step_avg:707.18ms
step:1372/1775 loss.item()=21020.396484375 n_predict=1 lr=0.7839 bs=393216 train_time:970628ms step_avg:707.45ms
step:1373/1775 loss.item()=21371.373046875 n_predict=1 lr=0.7821 bs=393216 train_time:971704ms step_avg:707.72ms
step:1374/1775 loss.item()=21888.0859375 n_predict=1 lr=0.7802 bs=393216 train_time:972780ms step_avg:707.99ms
step:1375/1775 loss.item()=20864.3203125 n_predict=1 lr=0.7783 bs=393216 train_time:973856ms step_avg:708.26ms
step:1376/1775 loss.item()=21730.1796875 n_predict=1 lr=0.7764 bs=393216 train_time:974936ms step_avg:708.53ms
step:1377/1775 loss.item()=21691.8125 n_predict=1 lr=0.7745 bs=393216 train_time:976011ms step_avg:708.80ms
step:1378/1775 loss.item()=21536.9140625 n_predict=1 lr=0.7727 bs=393216 train_time:977085ms step_avg:709.06ms
step:1379/1775 loss.item()=21539.5 n_predict=1 lr=0.7708 bs=393216 train_time:978158ms step_avg:709.32ms
step:1380/1775 loss.item()=20940.140625 n_predict=1 lr=0.7689 bs=393216 train_time:979236ms step_avg:709.59ms
step:1381/1775 loss.item()=21549.7890625 n_predict=1 lr=0.7670 bs=393216 train_time:980309ms step_avg:709.85ms
step:1382/1775 loss.item()=21495.140625 n_predict=1 lr=0.7652 bs=393216 train_time:981383ms step_avg:710.12ms
step:1383/1775 loss.item()=21830.9296875 n_predict=1 lr=0.7633 bs=393216 train_time:982458ms step_avg:710.38ms
step:1384/1775 loss.item()=20527.56640625 n_predict=1 lr=0.7614 bs=393216 train_time:983534ms step_avg:710.65ms
step:1385/1775 loss.item()=21117.748046875 n_predict=1 lr=0.7595 bs=393216 train_time:984606ms step_avg:710.91ms
step:1386/1775 loss.item()=20937.33984375 n_predict=1 lr=0.7576 bs=393216 train_time:985681ms step_avg:711.17ms
step:1387/1775 loss.item()=20850.51953125 n_predict=1 lr=0.7558 bs=393216 train_time:986755ms step_avg:711.43ms
step:1388/1775 loss.item()=21347.685546875 n_predict=1 lr=0.7539 bs=393216 train_time:987833ms step_avg:711.70ms
step:1389/1775 loss.item()=21610.44921875 n_predict=1 lr=0.7520 bs=393216 train_time:988906ms step_avg:711.96ms
step:1390/1775 loss.item()=21353.083984375 n_predict=1 lr=0.7501 bs=393216 train_time:989983ms step_avg:712.22ms
step:1391/1775 loss.item()=20861.96484375 n_predict=1 lr=0.7482 bs=393216 train_time:991058ms step_avg:712.48ms
step:1392/1775 loss.item()=22029.05078125 n_predict=1 lr=0.7464 bs=393216 train_time:992135ms step_avg:712.74ms
step:1393/1775 loss.item()=21194.818359375 n_predict=1 lr=0.7445 bs=393216 train_time:993209ms step_avg:713.00ms
step:1394/1775 loss.item()=20593.56640625 n_predict=1 lr=0.7426 bs=393216 train_time:994285ms step_avg:713.26ms
step:1395/1775 loss.item()=21614.87890625 n_predict=1 lr=0.7407 bs=393216 train_time:995359ms step_avg:713.52ms
step:1396/1775 loss.item()=21063.099609375 n_predict=1 lr=0.7388 bs=393216 train_time:996436ms step_avg:713.78ms
step:1397/1775 loss.item()=21433.328125 n_predict=1 lr=0.7370 bs=393216 train_time:997508ms step_avg:714.04ms
step:1398/1775 loss.item()=21197.87890625 n_predict=1 lr=0.7351 bs=393216 train_time:998585ms step_avg:714.30ms
step:1399/1775 loss.item()=21699.61328125 n_predict=1 lr=0.7332 bs=393216 train_time:999657ms step_avg:714.55ms
step:1400/1775 loss.item()=21278.1640625 n_predict=1 lr=0.7313 bs=393216 train_time:1000733ms step_avg:714.81ms
step:1401/1775 loss.item()=20922.51953125 n_predict=1 lr=0.7295 bs=393216 train_time:1001806ms step_avg:715.06ms
step:1402/1775 loss.item()=21433.51171875 n_predict=1 lr=0.7276 bs=393216 train_time:1002881ms step_avg:715.32ms
step:1403/1775 loss.item()=20957.6015625 n_predict=1 lr=0.7257 bs=393216 train_time:1003954ms step_avg:715.58ms
step:1404/1775 loss.item()=20819.25 n_predict=1 lr=0.7238 bs=393216 train_time:1005025ms step_avg:715.83ms
step:1405/1775 loss.item()=21213.556640625 n_predict=1 lr=0.7219 bs=393216 train_time:1006101ms step_avg:716.09ms
step:1406/1775 loss.item()=20818.96875 n_predict=1 lr=0.7201 bs=393216 train_time:1007178ms step_avg:716.34ms
step:1407/1775 loss.item()=21001.0078125 n_predict=1 lr=0.7182 bs=393216 train_time:1008252ms step_avg:716.60ms
step:1408/1775 loss.item()=20568.86328125 n_predict=1 lr=0.7163 bs=393216 train_time:1009327ms step_avg:716.85ms
step:1409/1775 loss.item()=21145.125 n_predict=1 lr=0.7144 bs=393216 train_time:1010403ms step_avg:717.11ms
step:1410/1775 loss.item()=20881.84765625 n_predict=1 lr=0.7125 bs=393216 train_time:1011481ms step_avg:717.36ms
step:1411/1775 loss.item()=20196.53125 n_predict=1 lr=0.7107 bs=393216 train_time:1012557ms step_avg:717.62ms
step:1412/1775 loss.item()=21147.318359375 n_predict=1 lr=0.7088 bs=393216 train_time:1013632ms step_avg:717.87ms
step:1413/1775 loss.item()=21358.033203125 n_predict=1 lr=0.7069 bs=393216 train_time:1014702ms step_avg:718.12ms
step:1414/1775 loss.item()=21117.60546875 n_predict=1 lr=0.7050 bs=393216 train_time:1015781ms step_avg:718.37ms
step:1415/1775 loss.item()=21036.818359375 n_predict=1 lr=0.7031 bs=393216 train_time:1016856ms step_avg:718.63ms
step:1416/1775 loss.item()=21971.87890625 n_predict=1 lr=0.7013 bs=393216 train_time:1017933ms step_avg:718.88ms
step:1417/1775 loss.item()=20854.568359375 n_predict=1 lr=0.6994 bs=393216 train_time:1019009ms step_avg:719.13ms
step:1418/1775 loss.item()=21676.8984375 n_predict=1 lr=0.6975 bs=393216 train_time:1020085ms step_avg:719.38ms
step:1419/1775 loss.item()=20109.4609375 n_predict=1 lr=0.6956 bs=393216 train_time:1021161ms step_avg:719.63ms
step:1420/1775 loss.item()=21187.8046875 n_predict=1 lr=0.6938 bs=393216 train_time:1022244ms step_avg:719.89ms
step:1421/1775 loss.item()=20350.46484375 n_predict=1 lr=0.6919 bs=393216 train_time:1023316ms step_avg:720.14ms
step:1422/1775 loss.item()=20846.328125 n_predict=1 lr=0.6900 bs=393216 train_time:1024392ms step_avg:720.39ms
step:1423/1775 loss.item()=21348.9375 n_predict=1 lr=0.6881 bs=393216 train_time:1025464ms step_avg:720.64ms
step:1424/1775 loss.item()=21298.34375 n_predict=1 lr=0.6862 bs=393216 train_time:1026557ms step_avg:720.90ms
step:1425/1775 loss.item()=21317.02734375 n_predict=1 lr=0.6844 bs=393216 train_time:1027634ms step_avg:721.15ms
step:1426/1775 loss.item()=21475.37109375 n_predict=1 lr=0.6825 bs=393216 train_time:1028710ms step_avg:721.40ms
step:1427/1775 loss.item()=21041.759765625 n_predict=1 lr=0.6806 bs=393216 train_time:1029785ms step_avg:721.64ms
step:1428/1775 loss.item()=21244.091796875 n_predict=1 lr=0.6787 bs=393216 train_time:1030863ms step_avg:721.89ms
step:1429/1775 loss.item()=21629.640625 n_predict=1 lr=0.6768 bs=393216 train_time:1031939ms step_avg:722.14ms
step:1430/1775 loss.item()=20715.564453125 n_predict=1 lr=0.6750 bs=393216 train_time:1033017ms step_avg:722.39ms
step:1431/1775 loss.item()=21125.31640625 n_predict=1 lr=0.6731 bs=393216 train_time:1034094ms step_avg:722.64ms
step:1432/1775 loss.item()=21563.25 n_predict=1 lr=0.6712 bs=393216 train_time:1035172ms step_avg:722.89ms
step:1433/1775 loss.item()=21629.87890625 n_predict=1 lr=0.6693 bs=393216 train_time:1036246ms step_avg:723.13ms
step:1434/1775 loss.item()=21745.4375 n_predict=1 lr=0.6674 bs=393216 train_time:1037323ms step_avg:723.38ms
step:1435/1775 loss.item()=20994.76171875 n_predict=1 lr=0.6656 bs=393216 train_time:1038396ms step_avg:723.62ms
step:1436/1775 loss.item()=21219.732421875 n_predict=1 lr=0.6637 bs=393216 train_time:1039475ms step_avg:723.87ms
step:1437/1775 loss.item()=21004.14453125 n_predict=1 lr=0.6618 bs=393216 train_time:1040549ms step_avg:724.11ms
step:1438/1775 loss.item()=21109.095703125 n_predict=1 lr=0.6599 bs=393216 train_time:1041623ms step_avg:724.36ms
step:1439/1775 loss.item()=20556.80078125 n_predict=1 lr=0.6581 bs=393216 train_time:1042697ms step_avg:724.60ms
step:1440/1775 loss.item()=21060.6953125 n_predict=1 lr=0.6562 bs=393216 train_time:1043775ms step_avg:724.84ms
step:1441/1775 loss.item()=20534.4453125 n_predict=1 lr=0.6543 bs=393216 train_time:1044846ms step_avg:725.08ms
step:1442/1775 loss.item()=21595.33203125 n_predict=1 lr=0.6524 bs=393216 train_time:1045925ms step_avg:725.33ms
step:1443/1775 loss.item()=21054.998046875 n_predict=1 lr=0.6505 bs=393216 train_time:1046996ms step_avg:725.57ms
step:1444/1775 loss.item()=20706.7734375 n_predict=1 lr=0.6487 bs=393216 train_time:1048077ms step_avg:725.82ms
step:1445/1775 loss.item()=21046.77734375 n_predict=1 lr=0.6468 bs=393216 train_time:1049149ms step_avg:726.06ms
step:1446/1775 loss.item()=20651.109375 n_predict=1 lr=0.6449 bs=393216 train_time:1050225ms step_avg:726.30ms
step:1447/1775 loss.item()=20644.41015625 n_predict=1 lr=0.6430 bs=393216 train_time:1051295ms step_avg:726.53ms
step:1448/1775 loss.item()=20848.380859375 n_predict=1 lr=0.6411 bs=393216 train_time:1052370ms step_avg:726.77ms
step:1449/1775 loss.item()=20661.1640625 n_predict=1 lr=0.6393 bs=393216 train_time:1053447ms step_avg:727.02ms
step:1450/1775 loss.item()=21060.986328125 n_predict=1 lr=0.6374 bs=393216 train_time:1054523ms step_avg:727.26ms
step:1451/1775 loss.item()=20663.63671875 n_predict=1 lr=0.6355 bs=393216 train_time:1055599ms step_avg:727.50ms
step:1452/1775 loss.item()=21523.619140625 n_predict=1 lr=0.6336 bs=393216 train_time:1056674ms step_avg:727.74ms
step:1453/1775 loss.item()=21433.78125 n_predict=1 lr=0.6317 bs=393216 train_time:1057751ms step_avg:727.98ms
step:1454/1775 loss.item()=20591.90625 n_predict=1 lr=0.6299 bs=393216 train_time:1058824ms step_avg:728.21ms
step:1455/1775 loss.item()=21739.09765625 n_predict=1 lr=0.6280 bs=393216 train_time:1059899ms step_avg:728.45ms
step:1456/1775 loss.item()=20675.00390625 n_predict=1 lr=0.6261 bs=393216 train_time:1060974ms step_avg:728.69ms
step:1457/1775 loss.item()=22455.81640625 n_predict=1 lr=0.6242 bs=393216 train_time:1062046ms step_avg:728.93ms
step:1458/1775 loss.item()=20865.62890625 n_predict=1 lr=0.6224 bs=393216 train_time:1063123ms step_avg:729.17ms
step:1459/1775 loss.item()=20506.880859375 n_predict=1 lr=0.6205 bs=393216 train_time:1064196ms step_avg:729.40ms
step:1460/1775 loss.item()=20653.455078125 n_predict=1 lr=0.6186 bs=393216 train_time:1065274ms step_avg:729.64ms
step:1461/1775 loss.item()=21085.833984375 n_predict=1 lr=0.6167 bs=393216 train_time:1066347ms step_avg:729.87ms
step:1462/1775 loss.item()=20519.80859375 n_predict=1 lr=0.6148 bs=393216 train_time:1067426ms step_avg:730.11ms
step:1463/1775 loss.item()=20743.78125 n_predict=1 lr=0.6130 bs=393216 train_time:1068500ms step_avg:730.35ms
step:1464/1775 loss.item()=20970.697265625 n_predict=1 lr=0.6111 bs=393216 train_time:1069581ms step_avg:730.59ms
step:1465/1775 loss.item()=20917.685546875 n_predict=1 lr=0.6092 bs=393216 train_time:1070657ms step_avg:730.82ms
step:1466/1775 loss.item()=20602.779296875 n_predict=1 lr=0.6073 bs=393216 train_time:1071735ms step_avg:731.06ms
step:1467/1775 loss.item()=20483.45703125 n_predict=1 lr=0.6054 bs=393216 train_time:1072813ms step_avg:731.30ms
step:1468/1775 loss.item()=21193.78125 n_predict=1 lr=0.6036 bs=393216 train_time:1073891ms step_avg:731.53ms
step:1469/1775 loss.item()=20775.9453125 n_predict=1 lr=0.6017 bs=393216 train_time:1074963ms step_avg:731.77ms
step:1470/1775 loss.item()=20763.33203125 n_predict=1 lr=0.5998 bs=393216 train_time:1076042ms step_avg:732.00ms
step:1471/1775 loss.item()=21029.78515625 n_predict=1 lr=0.5979 bs=393216 train_time:1077112ms step_avg:732.23ms
step:1472/1775 loss.item()=21148.171875 n_predict=1 lr=0.5960 bs=393216 train_time:1078188ms step_avg:732.46ms
step:1473/1775 loss.item()=20537.9140625 n_predict=1 lr=0.5942 bs=393216 train_time:1079265ms step_avg:732.70ms
step:1474/1775 loss.item()=21571.953125 n_predict=1 lr=0.5923 bs=393216 train_time:1080342ms step_avg:732.93ms
step:1475/1775 loss.item()=20626.728515625 n_predict=1 lr=0.5904 bs=393216 train_time:1081417ms step_avg:733.16ms
step:1476/1775 loss.item()=20299.353515625 n_predict=1 lr=0.5885 bs=393216 train_time:1082496ms step_avg:733.40ms
step:1477/1775 loss.item()=20179.59375 n_predict=1 lr=0.5867 bs=393216 train_time:1083569ms step_avg:733.63ms
step:1478/1775 loss.item()=21003.951171875 n_predict=1 lr=0.5848 bs=393216 train_time:1084646ms step_avg:733.86ms
step:1479/1775 loss.item()=21237.12890625 n_predict=1 lr=0.5829 bs=393216 train_time:1085720ms step_avg:734.09ms
step:1480/1775 loss.item()=21151.638671875 n_predict=1 lr=0.5810 bs=393216 train_time:1086796ms step_avg:734.32ms
step:1481/1775 loss.item()=21260.626953125 n_predict=1 lr=0.5791 bs=393216 train_time:1087872ms step_avg:734.55ms
step:1482/1775 loss.item()=21328.69140625 n_predict=1 lr=0.5773 bs=393216 train_time:1088951ms step_avg:734.78ms
step:1483/1775 loss.item()=20469.384765625 n_predict=1 lr=0.5754 bs=393216 train_time:1090022ms step_avg:735.01ms
step:1484/1775 loss.item()=21290.943359375 n_predict=1 lr=0.5735 bs=393216 train_time:1091102ms step_avg:735.24ms
step:1485/1775 loss.item()=20371.6484375 n_predict=1 lr=0.5716 bs=393216 train_time:1092178ms step_avg:735.47ms
step:1486/1775 loss.item()=20955.908203125 n_predict=1 lr=0.5697 bs=393216 train_time:1093256ms step_avg:735.70ms
step:1487/1775 loss.item()=21327.53515625 n_predict=1 lr=0.5679 bs=393216 train_time:1094328ms step_avg:735.93ms
step:1488/1775 loss.item()=20926.8671875 n_predict=1 lr=0.5660 bs=393216 train_time:1095403ms step_avg:736.16ms
step:1489/1775 loss.item()=21568.54296875 n_predict=1 lr=0.5641 bs=393216 train_time:1096480ms step_avg:736.39ms
step:1490/1775 loss.item()=20565.771484375 n_predict=1 lr=0.5622 bs=393216 train_time:1097561ms step_avg:736.62ms
step:1491/1775 loss.item()=22128.578125 n_predict=1 lr=0.5603 bs=393216 train_time:1098630ms step_avg:736.84ms
step:1492/1775 loss.item()=20415.423828125 n_predict=1 lr=0.5585 bs=393216 train_time:1099710ms step_avg:737.07ms
step:1493/1775 loss.item()=20192.595703125 n_predict=1 lr=0.5566 bs=393216 train_time:1100783ms step_avg:737.30ms
step:1494/1775 loss.item()=20580.859375 n_predict=1 lr=0.5547 bs=393216 train_time:1101863ms step_avg:737.53ms
step:1495/1775 loss.item()=20415.8828125 n_predict=1 lr=0.5528 bs=393216 train_time:1102940ms step_avg:737.75ms
step:1496/1775 loss.item()=20496.39453125 n_predict=1 lr=0.5510 bs=393216 train_time:1104019ms step_avg:737.98ms
step:1497/1775 loss.item()=20590.015625 n_predict=1 lr=0.5491 bs=393216 train_time:1105090ms step_avg:738.20ms
step:1498/1775 loss.item()=20631.9140625 n_predict=1 lr=0.5472 bs=393216 train_time:1106164ms step_avg:738.43ms
step:1499/1775 loss.item()=21006.330078125 n_predict=1 lr=0.5453 bs=393216 train_time:1107237ms step_avg:738.65ms
step:1500/1775 loss.item()=21607.15625 n_predict=1 lr=0.5434 bs=393216 train_time:1108315ms step_avg:738.88ms
step:1500/1775 lr=0.5416 bs=393216 n_predict=1 val_loss:3.3815 val_malbo_loss:3.4615 train_time:1108400ms step_avg:738.93ms
step:1501/1775 loss.item()=21483.931640625 n_predict=1 lr=0.5416 bs=393216 train_time:1109391ms step_avg:739.10ms
step:1502/1775 loss.item()=20464.810546875 n_predict=1 lr=0.5397 bs=393216 train_time:1110468ms step_avg:739.33ms
step:1503/1775 loss.item()=20766.66796875 n_predict=1 lr=0.5378 bs=393216 train_time:1111542ms step_avg:739.55ms
step:1504/1775 loss.item()=20808.48828125 n_predict=1 lr=0.5359 bs=393216 train_time:1112621ms step_avg:739.77ms
step:1505/1775 loss.item()=20598.279296875 n_predict=1 lr=0.5340 bs=393216 train_time:1113699ms step_avg:740.00ms
step:1506/1775 loss.item()=20485.4609375 n_predict=1 lr=0.5322 bs=393216 train_time:1114776ms step_avg:740.22ms
step:1507/1775 loss.item()=19911.8828125 n_predict=1 lr=0.5303 bs=393216 train_time:1115850ms step_avg:740.44ms
step:1508/1775 loss.item()=20591.37890625 n_predict=1 lr=0.5284 bs=393216 train_time:1116929ms step_avg:740.67ms
step:1509/1775 loss.item()=20664.09765625 n_predict=1 lr=0.5265 bs=393216 train_time:1118003ms step_avg:740.89ms
step:1510/1775 loss.item()=20401.671875 n_predict=1 lr=0.5246 bs=393216 train_time:1119083ms step_avg:741.11ms
step:1511/1775 loss.item()=20752.54296875 n_predict=1 lr=0.5228 bs=393216 train_time:1120157ms step_avg:741.34ms
step:1512/1775 loss.item()=20497.30078125 n_predict=1 lr=0.5209 bs=393216 train_time:1121233ms step_avg:741.56ms
step:1513/1775 loss.item()=21226.251953125 n_predict=1 lr=0.5190 bs=393216 train_time:1122308ms step_avg:741.78ms
step:1514/1775 loss.item()=20644.71875 n_predict=1 lr=0.5171 bs=393216 train_time:1123386ms step_avg:742.00ms
step:1515/1775 loss.item()=21735.291015625 n_predict=1 lr=0.5153 bs=393216 train_time:1124459ms step_avg:742.22ms
step:1516/1775 loss.item()=20798.0390625 n_predict=1 lr=0.5134 bs=393216 train_time:1125534ms step_avg:742.44ms
step:1517/1775 loss.item()=19832.9453125 n_predict=1 lr=0.5115 bs=393216 train_time:1126609ms step_avg:742.66ms
step:1518/1775 loss.item()=21482.77734375 n_predict=1 lr=0.5096 bs=393216 train_time:1127687ms step_avg:742.88ms
step:1519/1775 loss.item()=20519.4609375 n_predict=1 lr=0.5077 bs=393216 train_time:1128763ms step_avg:743.10ms
step:1520/1775 loss.item()=20677.30078125 n_predict=1 lr=0.5059 bs=393216 train_time:1129837ms step_avg:743.31ms
step:1521/1775 loss.item()=20912.638671875 n_predict=1 lr=0.5040 bs=393216 train_time:1130911ms step_avg:743.53ms
step:1522/1775 loss.item()=20537.357421875 n_predict=1 lr=0.5021 bs=393216 train_time:1131987ms step_avg:743.75ms
step:1523/1775 loss.item()=21712.59765625 n_predict=1 lr=0.5002 bs=393216 train_time:1133061ms step_avg:743.97ms
step:1524/1775 loss.item()=20155.244140625 n_predict=1 lr=0.4983 bs=393216 train_time:1134138ms step_avg:744.18ms
step:1525/1775 loss.item()=21124.9140625 n_predict=1 lr=0.4965 bs=393216 train_time:1135210ms step_avg:744.40ms
step:1526/1775 loss.item()=21018.55859375 n_predict=1 lr=0.4946 bs=393216 train_time:1136286ms step_avg:744.62ms
step:1527/1775 loss.item()=21425.69140625 n_predict=1 lr=0.4927 bs=393216 train_time:1137361ms step_avg:744.83ms
step:1528/1775 loss.item()=20555.236328125 n_predict=1 lr=0.4908 bs=393216 train_time:1138439ms step_avg:745.05ms
step:1529/1775 loss.item()=20610.28125 n_predict=1 lr=0.4889 bs=393216 train_time:1139511ms step_avg:745.27ms
step:1530/1775 loss.item()=21342.0390625 n_predict=1 lr=0.4871 bs=393216 train_time:1140592ms step_avg:745.48ms
step:1531/1775 loss.item()=20694.671875 n_predict=1 lr=0.4852 bs=393216 train_time:1141665ms step_avg:745.70ms
step:1532/1775 loss.item()=20511.76953125 n_predict=1 lr=0.4833 bs=393216 train_time:1142743ms step_avg:745.92ms
step:1533/1775 loss.item()=20698.521484375 n_predict=1 lr=0.4814 bs=393216 train_time:1143819ms step_avg:746.13ms
step:1534/1775 loss.item()=20220.52734375 n_predict=1 lr=0.4796 bs=393216 train_time:1144894ms step_avg:746.35ms
step:1535/1775 loss.item()=20616.7890625 n_predict=1 lr=0.4777 bs=393216 train_time:1145969ms step_avg:746.56ms
step:1536/1775 loss.item()=20525.7265625 n_predict=1 lr=0.4758 bs=393216 train_time:1147048ms step_avg:746.78ms
step:1537/1775 loss.item()=20426.87109375 n_predict=1 lr=0.4739 bs=393216 train_time:1148124ms step_avg:746.99ms
step:1538/1775 loss.item()=21641.3046875 n_predict=1 lr=0.4720 bs=393216 train_time:1149202ms step_avg:747.21ms
step:1539/1775 loss.item()=21007.5234375 n_predict=1 lr=0.4702 bs=393216 train_time:1150274ms step_avg:747.42ms
step:1540/1775 loss.item()=20470.97265625 n_predict=1 lr=0.4683 bs=393216 train_time:1151354ms step_avg:747.63ms
step:1541/1775 loss.item()=20210.75 n_predict=1 lr=0.4664 bs=393216 train_time:1152430ms step_avg:747.85ms
step:1542/1775 loss.item()=20754.38671875 n_predict=1 lr=0.4645 bs=393216 train_time:1153506ms step_avg:748.06ms
step:1543/1775 loss.item()=20777.755859375 n_predict=1 lr=0.4626 bs=393216 train_time:1154583ms step_avg:748.27ms
step:1544/1775 loss.item()=20518.869140625 n_predict=1 lr=0.4608 bs=393216 train_time:1155660ms step_avg:748.48ms
step:1545/1775 loss.item()=20680.4375 n_predict=1 lr=0.4589 bs=393216 train_time:1156735ms step_avg:748.70ms
step:1546/1775 loss.item()=20511.1171875 n_predict=1 lr=0.4570 bs=393216 train_time:1157813ms step_avg:748.91ms
step:1547/1775 loss.item()=21354.236328125 n_predict=1 lr=0.4551 bs=393216 train_time:1158889ms step_avg:749.12ms
step:1548/1775 loss.item()=20489.19921875 n_predict=1 lr=0.4532 bs=393216 train_time:1159968ms step_avg:749.33ms
step:1549/1775 loss.item()=20434.02734375 n_predict=1 lr=0.4514 bs=393216 train_time:1161043ms step_avg:749.54ms
step:1550/1775 loss.item()=21236.78125 n_predict=1 lr=0.4495 bs=393216 train_time:1162120ms step_avg:749.75ms
step:1551/1775 loss.item()=20763.13671875 n_predict=1 lr=0.4476 bs=393216 train_time:1163194ms step_avg:749.96ms
step:1552/1775 loss.item()=21384.193359375 n_predict=1 lr=0.4457 bs=393216 train_time:1164273ms step_avg:750.18ms
step:1553/1775 loss.item()=21207.189453125 n_predict=1 lr=0.4439 bs=393216 train_time:1165349ms step_avg:750.39ms
step:1554/1775 loss.item()=20825.10546875 n_predict=1 lr=0.4420 bs=393216 train_time:1166429ms step_avg:750.60ms
step:1555/1775 loss.item()=20108.556640625 n_predict=1 lr=0.4401 bs=393216 train_time:1167504ms step_avg:750.81ms
step:1556/1775 loss.item()=20255.8515625 n_predict=1 lr=0.4382 bs=393216 train_time:1168581ms step_avg:751.02ms
step:1557/1775 loss.item()=20655.060546875 n_predict=1 lr=0.4363 bs=393216 train_time:1169658ms step_avg:751.23ms
step:1558/1775 loss.item()=21304.234375 n_predict=1 lr=0.4345 bs=393216 train_time:1170735ms step_avg:751.43ms
step:1559/1775 loss.item()=20822.1875 n_predict=1 lr=0.4326 bs=393216 train_time:1171810ms step_avg:751.64ms
step:1560/1775 loss.item()=20639.591796875 n_predict=1 lr=0.4307 bs=393216 train_time:1172885ms step_avg:751.85ms
step:1561/1775 loss.item()=20553.0078125 n_predict=1 lr=0.4288 bs=393216 train_time:1173959ms step_avg:752.06ms
step:1562/1775 loss.item()=20844.341796875 n_predict=1 lr=0.4269 bs=393216 train_time:1175036ms step_avg:752.26ms
step:1563/1775 loss.item()=20665.48828125 n_predict=1 lr=0.4251 bs=393216 train_time:1176110ms step_avg:752.47ms
step:1564/1775 loss.item()=20245.01171875 n_predict=1 lr=0.4232 bs=393216 train_time:1177187ms step_avg:752.68ms
step:1565/1775 loss.item()=20986.5703125 n_predict=1 lr=0.4213 bs=393216 train_time:1178265ms step_avg:752.88ms
step:1566/1775 loss.item()=20261.21875 n_predict=1 lr=0.4194 bs=393216 train_time:1179339ms step_avg:753.09ms
step:1567/1775 loss.item()=21173.3046875 n_predict=1 lr=0.4175 bs=393216 train_time:1180417ms step_avg:753.30ms
step:1568/1775 loss.item()=20203.54296875 n_predict=1 lr=0.4157 bs=393216 train_time:1181494ms step_avg:753.50ms
step:1569/1775 loss.item()=21616.78515625 n_predict=1 lr=0.4138 bs=393216 train_time:1182570ms step_avg:753.71ms
step:1570/1775 loss.item()=20439.17578125 n_predict=1 lr=0.4119 bs=393216 train_time:1183649ms step_avg:753.92ms
step:1571/1775 loss.item()=20502.55859375 n_predict=1 lr=0.4100 bs=393216 train_time:1184725ms step_avg:754.12ms
step:1572/1775 loss.item()=20724.662109375 n_predict=1 lr=0.4081 bs=393216 train_time:1185803ms step_avg:754.33ms
step:1573/1775 loss.item()=21024.94921875 n_predict=1 lr=0.4063 bs=393216 train_time:1186874ms step_avg:754.53ms
step:1574/1775 loss.item()=21855.826171875 n_predict=1 lr=0.4044 bs=393216 train_time:1187953ms step_avg:754.74ms
step:1575/1775 loss.item()=20337.326171875 n_predict=1 lr=0.4025 bs=393216 train_time:1189030ms step_avg:754.94ms
step:1576/1775 loss.item()=21043.7734375 n_predict=1 lr=0.4006 bs=393216 train_time:1190109ms step_avg:755.15ms
step:1577/1775 loss.item()=21036.3671875 n_predict=1 lr=0.3988 bs=393216 train_time:1191182ms step_avg:755.35ms
step:1578/1775 loss.item()=19853.5234375 n_predict=1 lr=0.3969 bs=393216 train_time:1192260ms step_avg:755.55ms
step:1579/1775 loss.item()=21161.37890625 n_predict=1 lr=0.3950 bs=393216 train_time:1193335ms step_avg:755.75ms
step:1580/1775 loss.item()=20576.240234375 n_predict=1 lr=0.3931 bs=393216 train_time:1194413ms step_avg:755.96ms
step:1581/1775 loss.item()=21337.40234375 n_predict=1 lr=0.3912 bs=393216 train_time:1195486ms step_avg:756.16ms
step:1582/1775 loss.item()=20817.265625 n_predict=1 lr=0.3894 bs=393216 train_time:1196566ms step_avg:756.36ms
step:1583/1775 loss.item()=19970.498046875 n_predict=1 lr=0.3875 bs=393216 train_time:1197636ms step_avg:756.56ms
step:1584/1775 loss.item()=20272.40625 n_predict=1 lr=0.3856 bs=393216 train_time:1198713ms step_avg:756.76ms
step:1585/1775 loss.item()=20726.8515625 n_predict=1 lr=0.3837 bs=393216 train_time:1199787ms step_avg:756.96ms
step:1586/1775 loss.item()=20424.61328125 n_predict=1 lr=0.3818 bs=393216 train_time:1200866ms step_avg:757.17ms
step:1587/1775 loss.item()=20921.541015625 n_predict=1 lr=0.3800 bs=393216 train_time:1201939ms step_avg:757.37ms
step:1588/1775 loss.item()=21327.73046875 n_predict=1 lr=0.3781 bs=393216 train_time:1203016ms step_avg:757.57ms
step:1589/1775 loss.item()=21519.56640625 n_predict=1 lr=0.3762 bs=393216 train_time:1204091ms step_avg:757.77ms
step:1590/1775 loss.item()=20321.50390625 n_predict=1 lr=0.3743 bs=393216 train_time:1205169ms step_avg:757.97ms
step:1591/1775 loss.item()=20298.423828125 n_predict=1 lr=0.3724 bs=393216 train_time:1206240ms step_avg:758.16ms
step:1592/1775 loss.item()=20364.25390625 n_predict=1 lr=0.3706 bs=393216 train_time:1207320ms step_avg:758.37ms
step:1593/1775 loss.item()=20312.61328125 n_predict=1 lr=0.3687 bs=393216 train_time:1208391ms step_avg:758.56ms
step:1594/1775 loss.item()=20692.849609375 n_predict=1 lr=0.3668 bs=393216 train_time:1209466ms step_avg:758.76ms
step:1595/1775 loss.item()=20585.421875 n_predict=1 lr=0.3649 bs=393216 train_time:1210540ms step_avg:758.96ms
step:1596/1775 loss.item()=20303.01171875 n_predict=1 lr=0.3631 bs=393216 train_time:1211622ms step_avg:759.16ms
step:1597/1775 loss.item()=21176.515625 n_predict=1 lr=0.3612 bs=393216 train_time:1212692ms step_avg:759.36ms
step:1598/1775 loss.item()=20366.52734375 n_predict=1 lr=0.3593 bs=393216 train_time:1213771ms step_avg:759.56ms
step:1599/1775 loss.item()=21550.28515625 n_predict=1 lr=0.3574 bs=393216 train_time:1214844ms step_avg:759.75ms
step:1600/1775 loss.item()=21399.0234375 n_predict=1 lr=0.3555 bs=393216 train_time:1215922ms step_avg:759.95ms
step:1601/1775 loss.item()=21249.54296875 n_predict=1 lr=0.3537 bs=393216 train_time:1216995ms step_avg:760.15ms
step:1602/1775 loss.item()=21248.255859375 n_predict=1 lr=0.3518 bs=393216 train_time:1218075ms step_avg:760.35ms
step:1603/1775 loss.item()=21612.9765625 n_predict=1 lr=0.3499 bs=393216 train_time:1219149ms step_avg:760.54ms
step:1604/1775 loss.item()=20764.63671875 n_predict=1 lr=0.3480 bs=393216 train_time:1220226ms step_avg:760.74ms
step:1605/1775 loss.item()=20288.701171875 n_predict=1 lr=0.3461 bs=393216 train_time:1221298ms step_avg:760.93ms
step:1606/1775 loss.item()=21223.23828125 n_predict=1 lr=0.3443 bs=393216 train_time:1222374ms step_avg:761.13ms
step:1607/1775 loss.item()=21381.35546875 n_predict=1 lr=0.3424 bs=393216 train_time:1223450ms step_avg:761.33ms
step:1608/1775 loss.item()=20423.380859375 n_predict=1 lr=0.3405 bs=393216 train_time:1224527ms step_avg:761.52ms
step:1609/1775 loss.item()=20990.224609375 n_predict=1 lr=0.3386 bs=393216 train_time:1225603ms step_avg:761.72ms
step:1610/1775 loss.item()=21003.962890625 n_predict=1 lr=0.3367 bs=393216 train_time:1226681ms step_avg:761.91ms
step:1611/1775 loss.item()=20478.43359375 n_predict=1 lr=0.3349 bs=393216 train_time:1227756ms step_avg:762.11ms
step:1612/1775 loss.item()=20557.1640625 n_predict=1 lr=0.3330 bs=393216 train_time:1228834ms step_avg:762.30ms
step:1613/1775 loss.item()=19775.48828125 n_predict=1 lr=0.3311 bs=393216 train_time:1229910ms step_avg:762.50ms
step:1614/1775 loss.item()=19974.9765625 n_predict=1 lr=0.3292 bs=393216 train_time:1230985ms step_avg:762.69ms
step:1615/1775 loss.item()=21107.96875 n_predict=1 lr=0.3274 bs=393216 train_time:1232061ms step_avg:762.89ms
step:1616/1775 loss.item()=19524.216796875 n_predict=1 lr=0.3255 bs=393216 train_time:1233137ms step_avg:763.08ms
step:1617/1775 loss.item()=21245.1640625 n_predict=1 lr=0.3236 bs=393216 train_time:1234211ms step_avg:763.27ms
step:1618/1775 loss.item()=20867.3203125 n_predict=1 lr=0.3217 bs=393216 train_time:1235289ms step_avg:763.47ms
step:1619/1775 loss.item()=20906.359375 n_predict=1 lr=0.3198 bs=393216 train_time:1236364ms step_avg:763.66ms
step:1620/1775 loss.item()=20550.146484375 n_predict=1 lr=0.3180 bs=393216 train_time:1237439ms step_avg:763.85ms
step:1621/1775 loss.item()=20267.822265625 n_predict=1 lr=0.3161 bs=393216 train_time:1238512ms step_avg:764.04ms
step:1622/1775 loss.item()=20414.875 n_predict=1 lr=0.3142 bs=393216 train_time:1239594ms step_avg:764.24ms
step:1623/1775 loss.item()=21210.3359375 n_predict=1 lr=0.3123 bs=393216 train_time:1240670ms step_avg:764.43ms
step:1624/1775 loss.item()=21268.658203125 n_predict=1 lr=0.3104 bs=393216 train_time:1241752ms step_avg:764.63ms
step:1625/1775 loss.item()=20472.775390625 n_predict=1 lr=0.3086 bs=393216 train_time:1242828ms step_avg:764.82ms
step:1626/1775 loss.item()=20864.55078125 n_predict=1 lr=0.3067 bs=393216 train_time:1243904ms step_avg:765.01ms
step:1627/1775 loss.item()=21001.27734375 n_predict=1 lr=0.3048 bs=393216 train_time:1244977ms step_avg:765.20ms
step:1628/1775 loss.item()=20855.013671875 n_predict=1 lr=0.3029 bs=393216 train_time:1246054ms step_avg:765.39ms
step:1629/1775 loss.item()=20401.24609375 n_predict=1 lr=0.3010 bs=393216 train_time:1247129ms step_avg:765.58ms
step:1630/1775 loss.item()=20298.30078125 n_predict=1 lr=0.2992 bs=393216 train_time:1248207ms step_avg:765.77ms
step:1631/1775 loss.item()=20727.6171875 n_predict=1 lr=0.2973 bs=393216 train_time:1249283ms step_avg:765.96ms
step:1632/1775 loss.item()=20601.66796875 n_predict=1 lr=0.2954 bs=393216 train_time:1250362ms step_avg:766.15ms
step:1633/1775 loss.item()=20360.421875 n_predict=1 lr=0.2935 bs=393216 train_time:1251434ms step_avg:766.34ms
step:1634/1775 loss.item()=21772.51171875 n_predict=1 lr=0.2917 bs=393216 train_time:1252510ms step_avg:766.53ms
step:1635/1775 loss.item()=21045.83203125 n_predict=1 lr=0.2898 bs=393216 train_time:1253583ms step_avg:766.72ms
step:1636/1775 loss.item()=21040.48828125 n_predict=1 lr=0.2879 bs=393216 train_time:1254661ms step_avg:766.91ms
step:1637/1775 loss.item()=21714.69921875 n_predict=1 lr=0.2860 bs=393216 train_time:1255734ms step_avg:767.09ms
step:1638/1775 loss.item()=19717.8671875 n_predict=1 lr=0.2841 bs=393216 train_time:1256814ms step_avg:767.29ms
step:1639/1775 loss.item()=20805.0625 n_predict=1 lr=0.2823 bs=393216 train_time:1257890ms step_avg:767.47ms
step:1640/1775 loss.item()=20143.26953125 n_predict=1 lr=0.2804 bs=393216 train_time:1258969ms step_avg:767.66ms
step:1641/1775 loss.item()=19032.087890625 n_predict=1 lr=0.2785 bs=393216 train_time:1260043ms step_avg:767.85ms
step:1642/1775 loss.item()=20017.08203125 n_predict=1 lr=0.2766 bs=393216 train_time:1261121ms step_avg:768.04ms
step:1643/1775 loss.item()=21166.208984375 n_predict=1 lr=0.2747 bs=393216 train_time:1262193ms step_avg:768.22ms
step:1644/1775 loss.item()=20737.7578125 n_predict=1 lr=0.2729 bs=393216 train_time:1263268ms step_avg:768.41ms
step:1645/1775 loss.item()=20433.47265625 n_predict=1 lr=0.2710 bs=393216 train_time:1264345ms step_avg:768.60ms
step:1646/1775 loss.item()=20385.9609375 n_predict=1 lr=0.2691 bs=393216 train_time:1265421ms step_avg:768.79ms
step:1647/1775 loss.item()=21100.30078125 n_predict=1 lr=0.2672 bs=393216 train_time:1266493ms step_avg:768.97ms
step:1648/1775 loss.item()=20027.88671875 n_predict=1 lr=0.2653 bs=393216 train_time:1267569ms step_avg:769.16ms
step:1649/1775 loss.item()=20959.5390625 n_predict=1 lr=0.2635 bs=393216 train_time:1268644ms step_avg:769.34ms
step:1650/1775 loss.item()=21054.88671875 n_predict=1 lr=0.2616 bs=393216 train_time:1269723ms step_avg:769.53ms
step:1651/1775 loss.item()=20280.5625 n_predict=1 lr=0.2597 bs=393216 train_time:1270795ms step_avg:769.71ms
step:1652/1775 loss.item()=20875.994140625 n_predict=1 lr=0.2578 bs=393216 train_time:1271873ms step_avg:769.90ms
step:1653/1775 loss.item()=20917.9140625 n_predict=1 lr=0.2560 bs=393216 train_time:1272944ms step_avg:770.08ms
step:1654/1775 loss.item()=20982.48046875 n_predict=1 lr=0.2541 bs=393216 train_time:1274022ms step_avg:770.27ms
step:1655/1775 loss.item()=20687.486328125 n_predict=1 lr=0.2522 bs=393216 train_time:1275096ms step_avg:770.45ms
step:1656/1775 loss.item()=20037.705078125 n_predict=1 lr=0.2503 bs=393216 train_time:1276174ms step_avg:770.64ms
step:1657/1775 loss.item()=20840.791015625 n_predict=1 lr=0.2484 bs=393216 train_time:1277250ms step_avg:770.82ms
step:1658/1775 loss.item()=20302.94140625 n_predict=1 lr=0.2466 bs=393216 train_time:1278326ms step_avg:771.00ms
step:1659/1775 loss.item()=20689.1171875 n_predict=1 lr=0.2447 bs=393216 train_time:1279398ms step_avg:771.19ms
step:1660/1775 loss.item()=20208.1953125 n_predict=1 lr=0.2428 bs=393216 train_time:1280475ms step_avg:771.37ms
step:1661/1775 loss.item()=20470.109375 n_predict=1 lr=0.2409 bs=393216 train_time:1281551ms step_avg:771.55ms
step:1662/1775 loss.item()=20219.50390625 n_predict=1 lr=0.2390 bs=393216 train_time:1282631ms step_avg:771.74ms
step:1663/1775 loss.item()=19900.7421875 n_predict=1 lr=0.2372 bs=393216 train_time:1283704ms step_avg:771.92ms
step:1664/1775 loss.item()=20550.001953125 n_predict=1 lr=0.2353 bs=393216 train_time:1284782ms step_avg:772.10ms
step:1665/1775 loss.item()=20649.640625 n_predict=1 lr=0.2334 bs=393216 train_time:1285860ms step_avg:772.29ms
step:1666/1775 loss.item()=20090.859375 n_predict=1 lr=0.2315 bs=393216 train_time:1286934ms step_avg:772.47ms
step:1667/1775 loss.item()=21346.8828125 n_predict=1 lr=0.2296 bs=393216 train_time:1288011ms step_avg:772.65ms
step:1668/1775 loss.item()=20381.55078125 n_predict=1 lr=0.2278 bs=393216 train_time:1289086ms step_avg:772.83ms
step:1669/1775 loss.item()=20669.13671875 n_predict=1 lr=0.2259 bs=393216 train_time:1290159ms step_avg:773.01ms
step:1670/1775 loss.item()=20826.86328125 n_predict=1 lr=0.2240 bs=393216 train_time:1291239ms step_avg:773.20ms
step:1671/1775 loss.item()=20210.1640625 n_predict=1 lr=0.2221 bs=393216 train_time:1292312ms step_avg:773.38ms
step:1672/1775 loss.item()=20530.09765625 n_predict=1 lr=0.2203 bs=393216 train_time:1293388ms step_avg:773.56ms
step:1673/1775 loss.item()=21123.212890625 n_predict=1 lr=0.2184 bs=393216 train_time:1294465ms step_avg:773.74ms
step:1674/1775 loss.item()=19994.35546875 n_predict=1 lr=0.2165 bs=393216 train_time:1295541ms step_avg:773.92ms
step:1675/1775 loss.item()=20767.41015625 n_predict=1 lr=0.2146 bs=393216 train_time:1296618ms step_avg:774.10ms
step:1676/1775 loss.item()=20448.37109375 n_predict=1 lr=0.2127 bs=393216 train_time:1297695ms step_avg:774.28ms
step:1677/1775 loss.item()=20447.451171875 n_predict=1 lr=0.2109 bs=393216 train_time:1298768ms step_avg:774.46ms
step:1678/1775 loss.item()=19694.181640625 n_predict=1 lr=0.2090 bs=393216 train_time:1299846ms step_avg:774.64ms
step:1679/1775 loss.item()=20817.185546875 n_predict=1 lr=0.2071 bs=393216 train_time:1300919ms step_avg:774.82ms
step:1680/1775 loss.item()=20298.7578125 n_predict=1 lr=0.2052 bs=393216 train_time:1301994ms step_avg:775.00ms
step:1681/1775 loss.item()=20079.041015625 n_predict=1 lr=0.2033 bs=393216 train_time:1303067ms step_avg:775.17ms
step:1682/1775 loss.item()=20100.984375 n_predict=1 lr=0.2015 bs=393216 train_time:1304143ms step_avg:775.35ms
step:1683/1775 loss.item()=20858.8203125 n_predict=1 lr=0.1996 bs=393216 train_time:1305221ms step_avg:775.53ms
step:1684/1775 loss.item()=20381.8046875 n_predict=1 lr=0.1977 bs=393216 train_time:1306296ms step_avg:775.71ms
step:1685/1775 loss.item()=20012.85546875 n_predict=1 lr=0.1958 bs=393216 train_time:1307371ms step_avg:775.89ms
step:1686/1775 loss.item()=20000.2890625 n_predict=1 lr=0.1939 bs=393216 train_time:1308447ms step_avg:776.07ms
step:1687/1775 loss.item()=20606.775390625 n_predict=1 lr=0.1921 bs=393216 train_time:1309522ms step_avg:776.24ms
step:1688/1775 loss.item()=20401.78515625 n_predict=1 lr=0.1902 bs=393216 train_time:1310598ms step_avg:776.42ms
step:1689/1775 loss.item()=19726.203125 n_predict=1 lr=0.1883 bs=393216 train_time:1311672ms step_avg:776.60ms
step:1690/1775 loss.item()=20985.875 n_predict=1 lr=0.1864 bs=393216 train_time:1312746ms step_avg:776.77ms
step:1691/1775 loss.item()=19992.146484375 n_predict=1 lr=0.1846 bs=393216 train_time:1313822ms step_avg:776.95ms
step:1692/1775 loss.item()=20280.974609375 n_predict=1 lr=0.1827 bs=393216 train_time:1314900ms step_avg:777.13ms
step:1693/1775 loss.item()=20724.2109375 n_predict=1 lr=0.1808 bs=393216 train_time:1315974ms step_avg:777.30ms
step:1694/1775 loss.item()=19518.56640625 n_predict=1 lr=0.1789 bs=393216 train_time:1317057ms step_avg:777.48ms
step:1695/1775 loss.item()=20119.17578125 n_predict=1 lr=0.1770 bs=393216 train_time:1318131ms step_avg:777.66ms
step:1696/1775 loss.item()=20408.72265625 n_predict=1 lr=0.1752 bs=393216 train_time:1319207ms step_avg:777.83ms
step:1697/1775 loss.item()=20099.4765625 n_predict=1 lr=0.1733 bs=393216 train_time:1320279ms step_avg:778.01ms
step:1698/1775 loss.item()=20767.193359375 n_predict=1 lr=0.1714 bs=393216 train_time:1321359ms step_avg:778.19ms
step:1699/1775 loss.item()=20456.748046875 n_predict=1 lr=0.1695 bs=393216 train_time:1322433ms step_avg:778.36ms
step:1700/1775 loss.item()=20511.43359375 n_predict=1 lr=0.1676 bs=393216 train_time:1323516ms step_avg:778.54ms
step:1701/1775 loss.item()=20310.640625 n_predict=1 lr=0.1658 bs=393216 train_time:1324590ms step_avg:778.71ms
step:1702/1775 loss.item()=20950.7578125 n_predict=1 lr=0.1639 bs=393216 train_time:1325667ms step_avg:778.89ms
step:1703/1775 loss.item()=20622.419921875 n_predict=1 lr=0.1620 bs=393216 train_time:1326737ms step_avg:779.06ms
step:1704/1775 loss.item()=20042.396484375 n_predict=1 lr=0.1601 bs=393216 train_time:1327816ms step_avg:779.23ms
step:1705/1775 loss.item()=20427.18359375 n_predict=1 lr=0.1582 bs=393216 train_time:1328891ms step_avg:779.41ms
step:1706/1775 loss.item()=20043.974609375 n_predict=1 lr=0.1564 bs=393216 train_time:1329968ms step_avg:779.58ms
step:1707/1775 loss.item()=19999.837890625 n_predict=1 lr=0.1545 bs=393216 train_time:1331040ms step_avg:779.75ms
step:1708/1775 loss.item()=20448.033203125 n_predict=1 lr=0.1526 bs=393216 train_time:1332115ms step_avg:779.93ms
step:1709/1775 loss.item()=20239.451171875 n_predict=1 lr=0.1507 bs=393216 train_time:1333191ms step_avg:780.10ms
step:1710/1775 loss.item()=20130.80859375 n_predict=1 lr=0.1489 bs=393216 train_time:1334265ms step_avg:780.27ms
step:1711/1775 loss.item()=20998.97265625 n_predict=1 lr=0.1470 bs=393216 train_time:1335341ms step_avg:780.44ms
step:1712/1775 loss.item()=19673.23046875 n_predict=1 lr=0.1451 bs=393216 train_time:1336422ms step_avg:780.62ms
step:1713/1775 loss.item()=20537.81640625 n_predict=1 lr=0.1432 bs=393216 train_time:1337494ms step_avg:780.79ms
step:1714/1775 loss.item()=20925.611328125 n_predict=1 lr=0.1413 bs=393216 train_time:1338569ms step_avg:780.96ms
step:1715/1775 loss.item()=20275.76953125 n_predict=1 lr=0.1395 bs=393216 train_time:1339641ms step_avg:781.13ms
step:1716/1775 loss.item()=20943.91796875 n_predict=1 lr=0.1376 bs=393216 train_time:1340714ms step_avg:781.30ms
step:1717/1775 loss.item()=19765.0625 n_predict=1 lr=0.1357 bs=393216 train_time:1341792ms step_avg:781.47ms
step:1718/1775 loss.item()=20558.1484375 n_predict=1 lr=0.1338 bs=393216 train_time:1342873ms step_avg:781.65ms
step:1719/1775 loss.item()=19918.51953125 n_predict=1 lr=0.1319 bs=393216 train_time:1343947ms step_avg:781.82ms
step:1720/1775 loss.item()=20657.31640625 n_predict=1 lr=0.1301 bs=393216 train_time:1345023ms step_avg:781.99ms
step:1721/1775 loss.item()=20255.455078125 n_predict=1 lr=0.1282 bs=393216 train_time:1346097ms step_avg:782.16ms
step:1722/1775 loss.item()=19660.623046875 n_predict=1 lr=0.1263 bs=393216 train_time:1347180ms step_avg:782.33ms
step:1723/1775 loss.item()=20402.884765625 n_predict=1 lr=0.1244 bs=393216 train_time:1348252ms step_avg:782.50ms
step:1724/1775 loss.item()=20077.798828125 n_predict=1 lr=0.1225 bs=393216 train_time:1349328ms step_avg:782.67ms
step:1725/1775 loss.item()=20304.23828125 n_predict=1 lr=0.1207 bs=393216 train_time:1350403ms step_avg:782.84ms
step:1726/1775 loss.item()=20236.162109375 n_predict=1 lr=0.1188 bs=393216 train_time:1351483ms step_avg:783.01ms
step:1727/1775 loss.item()=20521.47265625 n_predict=1 lr=0.1169 bs=393216 train_time:1352559ms step_avg:783.18ms
step:1728/1775 loss.item()=21212.63671875 n_predict=1 lr=0.1150 bs=393216 train_time:1353637ms step_avg:783.35ms
step:1729/1775 loss.item()=20340.19921875 n_predict=1 lr=0.1132 bs=393216 train_time:1354712ms step_avg:783.52ms
step:1730/1775 loss.item()=20496.75390625 n_predict=1 lr=0.1113 bs=393216 train_time:1355790ms step_avg:783.69ms
step:1731/1775 loss.item()=21315.740234375 n_predict=1 lr=0.1094 bs=393216 train_time:1356863ms step_avg:783.86ms
step:1732/1775 loss.item()=19957.99609375 n_predict=1 lr=0.1075 bs=393216 train_time:1357944ms step_avg:784.03ms
step:1733/1775 loss.item()=20063.546875 n_predict=1 lr=0.1056 bs=393216 train_time:1359018ms step_avg:784.20ms
step:1734/1775 loss.item()=20461.24609375 n_predict=1 lr=0.1038 bs=393216 train_time:1360098ms step_avg:784.37ms
step:1735/1775 loss.item()=19998.267578125 n_predict=1 lr=0.1019 bs=393216 train_time:1361172ms step_avg:784.54ms
step:1736/1775 loss.item()=20514.4375 n_predict=1 lr=0.1000 bs=393216 train_time:1492206ms step_avg:859.57ms
step:1737/1775 loss.item()=20907.79296875 n_predict=1 lr=0.1000 bs=393216 train_time:1493262ms step_avg:859.68ms
step:1738/1775 loss.item()=20357.935546875 n_predict=1 lr=0.1000 bs=393216 train_time:1494325ms step_avg:859.80ms
step:1739/1775 loss.item()=20767.619140625 n_predict=1 lr=0.1000 bs=393216 train_time:1495383ms step_avg:859.91ms
step:1740/1775 loss.item()=20784.76953125 n_predict=1 lr=0.1000 bs=393216 train_time:1496448ms step_avg:860.03ms
step:1741/1775 loss.item()=19927.3671875 n_predict=1 lr=0.1000 bs=393216 train_time:1497511ms step_avg:860.14ms
step:1742/1775 loss.item()=20670.00390625 n_predict=1 lr=0.1000 bs=393216 train_time:1498575ms step_avg:860.26ms
step:1743/1775 loss.item()=20232.46875 n_predict=1 lr=0.1000 bs=393216 train_time:1499632ms step_avg:860.37ms
step:1744/1775 loss.item()=20068.20703125 n_predict=1 lr=0.1000 bs=393216 train_time:1500697ms step_avg:860.49ms
step:1745/1775 loss.item()=20411.2109375 n_predict=1 lr=0.1000 bs=393216 train_time:1501761ms step_avg:860.61ms
step:1746/1775 loss.item()=20164.9375 n_predict=1 lr=0.1000 bs=393216 train_time:1502827ms step_avg:860.73ms
step:1747/1775 loss.item()=20132.017578125 n_predict=1 lr=0.1000 bs=393216 train_time:1503889ms step_avg:860.84ms
step:1748/1775 loss.item()=19789.9921875 n_predict=1 lr=0.1000 bs=393216 train_time:1504955ms step_avg:860.96ms
step:1749/1775 loss.item()=21275.13671875 n_predict=1 lr=0.1000 bs=393216 train_time:1506020ms step_avg:861.08ms
step:1750/1775 loss.item()=19668.69140625 n_predict=1 lr=0.1000 bs=393216 train_time:1507086ms step_avg:861.19ms
step:1750/1775 lr=0.1000 bs=393216 n_predict=1 val_loss:3.2904 val_malbo_loss:3.3702 train_time:1507172ms step_avg:861.24ms
step:1751/1775 loss.item()=19968.80859375 n_predict=1 lr=0.1000 bs=393216 train_time:1508150ms step_avg:861.31ms
step:1752/1775 loss.item()=20297.154296875 n_predict=1 lr=0.1000 bs=393216 train_time:1509214ms step_avg:861.42ms
step:1753/1775 loss.item()=20430.390625 n_predict=1 lr=0.1000 bs=393216 train_time:1510274ms step_avg:861.54ms
step:1754/1775 loss.item()=21077.75 n_predict=1 lr=0.1000 bs=393216 train_time:1511341ms step_avg:861.65ms
step:1755/1775 loss.item()=20583.7734375 n_predict=1 lr=0.1000 bs=393216 train_time:1512406ms step_avg:861.77ms
step:1756/1775 loss.item()=20363.701171875 n_predict=1 lr=0.1000 bs=393216 train_time:1513474ms step_avg:861.89ms
step:1757/1775 loss.item()=20496.98046875 n_predict=1 lr=0.1000 bs=393216 train_time:1514535ms step_avg:862.00ms
step:1758/1775 loss.item()=20953.984375 n_predict=1 lr=0.1000 bs=393216 train_time:1515602ms step_avg:862.12ms
step:1759/1775 loss.item()=20058.76953125 n_predict=1 lr=0.1000 bs=393216 train_time:1516665ms step_avg:862.23ms
step:1760/1775 loss.item()=20248.8515625 n_predict=1 lr=0.1000 bs=393216 train_time:1517734ms step_avg:862.35ms
step:1761/1775 loss.item()=21007.37109375 n_predict=1 lr=0.1000 bs=393216 train_time:1518796ms step_avg:862.46ms
step:1762/1775 loss.item()=21386.48046875 n_predict=1 lr=0.1000 bs=393216 train_time:1519865ms step_avg:862.58ms
step:1763/1775 loss.item()=20752.935546875 n_predict=1 lr=0.1000 bs=393216 train_time:1520934ms step_avg:862.70ms
step:1764/1775 loss.item()=20645.634765625 n_predict=1 lr=0.1000 bs=393216 train_time:1522004ms step_avg:862.81ms
step:1765/1775 loss.item()=20896.9609375 n_predict=1 lr=0.1000 bs=393216 train_time:1523073ms step_avg:862.93ms
step:1766/1775 loss.item()=20297.1015625 n_predict=1 lr=0.1000 bs=393216 train_time:1524145ms step_avg:863.05ms
step:1767/1775 loss.item()=20542.8671875 n_predict=1 lr=0.1000 bs=393216 train_time:1525210ms step_avg:863.16ms
step:1768/1775 loss.item()=20640.13671875 n_predict=1 lr=0.1000 bs=393216 train_time:1526281ms step_avg:863.28ms
step:1769/1775 loss.item()=20344.3359375 n_predict=1 lr=0.1000 bs=393216 train_time:1527348ms step_avg:863.40ms
step:1770/1775 loss.item()=20362.63671875 n_predict=1 lr=0.1000 bs=393216 train_time:1528419ms step_avg:863.51ms
step:1771/1775 loss.item()=19844.46484375 n_predict=1 lr=0.1000 bs=393216 train_time:1529483ms step_avg:863.63ms
step:1772/1775 loss.item()=18655.595703125 n_predict=1 lr=0.1000 bs=393216 train_time:1530554ms step_avg:863.74ms
step:1773/1775 loss.item()=20466.517578125 n_predict=1 lr=0.1000 bs=393216 train_time:1531619ms step_avg:863.86ms
step:1774/1775 loss.item()=20549.4296875 n_predict=1 lr=0.1000 bs=393216 train_time:1532692ms step_avg:863.98ms
step:1775/1775 loss.item()=20720.69140625 n_predict=1 lr=0.1000 bs=393216 train_time:1533760ms step_avg:864.09ms
step:1775/1775 lr=0.1000 bs=393216 n_predict=1 val_loss:3.2841 val_malbo_loss:3.3639 train_time:1533847ms step_avg:864.14ms
peak memory allocated: 30169 MiB reserved: 30558 MiB
