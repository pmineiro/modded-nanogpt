import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

from malbo import compute_malbo_parameters

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int, use_malbo: bool):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

        self.use_malbo = use_malbo

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0

            loss = (cross_entropy * mtp_weights).sum()
            if self.use_malbo:
                T, K = logits_flat.shape
                with torch.no_grad():
                    vhat, kappa, gamma = compute_malbo_parameters(cross_entropy.T, K)
                    weights_transposed = kappa * gamma

                malbo_loss = T * (cross_entropy * weights_transposed.T * mtp_weights).sum()
            else:
                malbo_loss = loss
        elif self.training:
            if self.use_malbo:
                logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
                T, K = logits_flat.shape
                cross_entropy = F.cross_entropy(logits_flat, target_seq, reduction="none")
                loss = cross_entropy.sum()

                with torch.no_grad():
                    vhat, kappa, gamma = compute_malbo_parameters(cross_entropy.unsqueeze(0), K)
                    weights = (kappa * gamma).squeeze(0)

                malbo_loss = T * (weights * cross_entropy).sum()
            else:
                loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
                malbo_loss = loss
        else:
            if self.use_malbo:
                K = logits_for_loss.size(-1)
                cross_entropy = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="none")
                loss = cross_entropy.mean()

                with torch.no_grad():
                    vhat, kappa, gamma = compute_malbo_parameters(cross_entropy.unsqueeze(0), K)
                    weights = (kappa * gamma).squeeze(0)

                malbo_loss = (weights * cross_entropy).sum()
            else:
                loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
                malbo_loss = loss

        return loss, malbo_loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size),
    use_malbo=os.environ.get('use_malbo', 'True') == 'True',
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=False)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args())[1] / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
    break
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss, val_malbo_loss = 0, 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                this_val_loss, this_val_malbo_loss = model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
                val_loss += this_val_loss
                val_malbo_loss += this_val_malbo_loss
        val_loss /= val_steps
        val_malbo_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        dist.reduce(val_malbo_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} val_malbo_loss:{val_malbo_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args())[1] / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.19 (main, Oct 21 2025, 16:43:05) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Thu Jan 15 02:44:13 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:D6:00.0 Off |                    0 |
| N/A   28C    P0             76W /  310W |    1103MiB /  81559MiB |      7%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            7178      C   .../envs/speedrun/bin/python3.10       1094MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8294 val_malbo_loss:10.8294 train_time:0ms step_avg:0.03ms
step:1/1775 train_time:1166ms step_avg:1166.32ms
step:2/1775 train_time:2829ms step_avg:1414.26ms
step:3/1775 train_time:3313ms step_avg:1104.24ms
step:4/1775 train_time:3808ms step_avg:952.06ms
step:5/1775 train_time:4293ms step_avg:858.56ms
step:6/1775 train_time:4799ms step_avg:799.85ms
step:7/1775 train_time:5283ms step_avg:754.78ms
step:8/1775 train_time:5781ms step_avg:722.64ms
step:9/1775 train_time:6266ms step_avg:696.20ms
step:10/1775 train_time:6741ms step_avg:674.09ms
step:11/1775 train_time:7219ms step_avg:656.27ms
step:12/1775 train_time:7716ms step_avg:642.99ms
step:13/1775 train_time:8211ms step_avg:631.61ms
step:14/1775 train_time:8703ms step_avg:621.61ms
step:15/1775 train_time:9192ms step_avg:612.82ms
step:16/1775 train_time:9682ms step_avg:605.11ms
step:17/1775 train_time:10169ms step_avg:598.20ms
step:18/1775 train_time:10668ms step_avg:592.68ms
step:19/1775 train_time:11159ms step_avg:587.33ms
step:20/1775 train_time:11654ms step_avg:582.69ms
step:21/1775 train_time:12168ms step_avg:579.42ms
step:22/1775 train_time:12799ms step_avg:581.76ms
step:23/1775 train_time:13276ms step_avg:577.23ms
step:24/1775 train_time:13748ms step_avg:572.84ms
step:25/1775 train_time:14233ms step_avg:569.33ms
step:26/1775 train_time:14724ms step_avg:566.31ms
step:27/1775 train_time:15203ms step_avg:563.07ms
step:28/1775 train_time:15683ms step_avg:560.11ms
step:29/1775 train_time:16194ms step_avg:558.41ms
step:30/1775 train_time:16703ms step_avg:556.76ms
step:31/1775 train_time:17182ms step_avg:554.26ms
step:32/1775 train_time:17663ms step_avg:551.98ms
step:33/1775 train_time:18148ms step_avg:549.93ms
step:34/1775 train_time:18624ms step_avg:547.77ms
step:35/1775 train_time:19103ms step_avg:545.80ms
step:36/1775 train_time:19593ms step_avg:544.25ms
step:37/1775 train_time:20074ms step_avg:542.55ms
step:38/1775 train_time:20566ms step_avg:541.20ms
step:39/1775 train_time:21045ms step_avg:539.60ms
step:40/1775 train_time:21520ms step_avg:538.00ms
step:41/1775 train_time:21990ms step_avg:536.35ms
step:42/1775 train_time:22464ms step_avg:534.87ms
step:43/1775 train_time:22942ms step_avg:533.53ms
step:44/1775 train_time:23428ms step_avg:532.47ms
step:45/1775 train_time:23919ms step_avg:531.53ms
step:46/1775 train_time:24395ms step_avg:530.32ms
step:47/1775 train_time:24868ms step_avg:529.10ms
step:48/1775 train_time:25361ms step_avg:528.34ms
step:49/1775 train_time:25835ms step_avg:527.24ms
step:50/1775 train_time:26316ms step_avg:526.33ms
step:51/1775 train_time:26791ms step_avg:525.32ms
step:52/1775 train_time:27286ms step_avg:524.73ms
step:53/1775 train_time:27761ms step_avg:523.78ms
step:54/1775 train_time:28251ms step_avg:523.17ms
step:55/1775 train_time:28730ms step_avg:522.37ms
step:56/1775 train_time:29219ms step_avg:521.76ms
step:57/1775 train_time:29696ms step_avg:520.98ms
step:58/1775 train_time:30181ms step_avg:520.36ms
step:59/1775 train_time:30668ms step_avg:519.80ms
step:60/1775 train_time:31159ms step_avg:519.31ms
step:61/1775 train_time:31635ms step_avg:518.60ms
step:62/1775 train_time:32118ms step_avg:518.03ms
step:63/1775 train_time:32591ms step_avg:517.32ms
step:64/1775 train_time:33081ms step_avg:516.88ms
step:65/1775 train_time:33560ms step_avg:516.30ms
step:66/1775 train_time:34050ms step_avg:515.91ms
step:67/1775 train_time:34531ms step_avg:515.39ms
step:68/1775 train_time:35019ms step_avg:514.98ms
step:69/1775 train_time:35501ms step_avg:514.51ms
step:70/1775 train_time:35999ms step_avg:514.27ms
step:71/1775 train_time:36477ms step_avg:513.77ms
step:72/1775 train_time:36953ms step_avg:513.24ms
step:73/1775 train_time:37437ms step_avg:512.84ms
step:74/1775 train_time:37920ms step_avg:512.43ms
step:75/1775 train_time:38394ms step_avg:511.92ms
step:76/1775 train_time:38878ms step_avg:511.56ms
step:77/1775 train_time:39355ms step_avg:511.11ms
step:78/1775 train_time:39847ms step_avg:510.86ms
step:79/1775 train_time:40319ms step_avg:510.37ms
step:80/1775 train_time:40810ms step_avg:510.12ms
step:81/1775 train_time:41287ms step_avg:509.71ms
step:82/1775 train_time:41764ms step_avg:509.31ms
step:83/1775 train_time:42247ms step_avg:509.01ms
step:84/1775 train_time:42734ms step_avg:508.74ms
step:85/1775 train_time:43209ms step_avg:508.34ms
step:86/1775 train_time:43673ms step_avg:507.83ms
step:87/1775 train_time:44148ms step_avg:507.45ms
step:88/1775 train_time:44633ms step_avg:507.19ms
step:89/1775 train_time:45128ms step_avg:507.05ms
step:90/1775 train_time:45750ms step_avg:508.33ms
step:91/1775 train_time:46224ms step_avg:507.96ms
step:92/1775 train_time:46708ms step_avg:507.70ms
step:93/1775 train_time:47182ms step_avg:507.33ms
step:94/1775 train_time:47676ms step_avg:507.19ms
step:95/1775 train_time:48153ms step_avg:506.87ms
step:96/1775 train_time:48645ms step_avg:506.71ms
step:97/1775 train_time:49116ms step_avg:506.35ms
step:98/1775 train_time:49635ms step_avg:506.48ms
step:99/1775 train_time:50132ms step_avg:506.39ms
step:100/1775 train_time:50614ms step_avg:506.14ms
step:101/1775 train_time:51093ms step_avg:505.88ms
step:102/1775 train_time:51584ms step_avg:505.73ms
step:103/1775 train_time:52069ms step_avg:505.52ms
step:104/1775 train_time:52545ms step_avg:505.24ms
step:105/1775 train_time:53038ms step_avg:505.13ms
step:106/1775 train_time:53546ms step_avg:505.15ms
step:107/1775 train_time:54054ms step_avg:505.18ms
step:108/1775 train_time:54556ms step_avg:505.15ms
step:109/1775 train_time:55058ms step_avg:505.12ms
step:110/1775 train_time:55576ms step_avg:505.24ms
step:111/1775 train_time:56074ms step_avg:505.17ms
step:112/1775 train_time:56580ms step_avg:505.18ms
step:113/1775 train_time:57085ms step_avg:505.18ms
step:114/1775 train_time:57570ms step_avg:505.00ms
step:115/1775 train_time:58068ms step_avg:504.94ms
step:116/1775 train_time:58572ms step_avg:504.93ms
step:117/1775 train_time:59045ms step_avg:504.66ms
step:118/1775 train_time:59539ms step_avg:504.56ms
step:119/1775 train_time:60014ms step_avg:504.32ms
step:120/1775 train_time:60516ms step_avg:504.30ms
step:121/1775 train_time:60995ms step_avg:504.09ms
step:122/1775 train_time:61485ms step_avg:503.97ms
step:123/1775 train_time:61974ms step_avg:503.85ms
step:124/1775 train_time:62457ms step_avg:503.69ms
step:125/1775 train_time:62955ms step_avg:503.64ms
step:126/1775 train_time:63440ms step_avg:503.50ms
step:127/1775 train_time:63920ms step_avg:503.31ms
step:128/1775 train_time:64396ms step_avg:503.09ms
step:129/1775 train_time:64874ms step_avg:502.90ms
step:130/1775 train_time:65348ms step_avg:502.68ms
step:131/1775 train_time:65824ms step_avg:502.47ms
step:132/1775 train_time:66307ms step_avg:502.33ms
step:133/1775 train_time:66781ms step_avg:502.11ms
step:134/1775 train_time:67267ms step_avg:501.99ms
step:135/1775 train_time:67735ms step_avg:501.74ms
step:136/1775 train_time:68214ms step_avg:501.57ms
step:137/1775 train_time:68685ms step_avg:501.35ms
step:138/1775 train_time:69165ms step_avg:501.20ms
step:139/1775 train_time:69637ms step_avg:500.99ms
step:140/1775 train_time:70100ms step_avg:500.72ms
step:141/1775 train_time:70540ms step_avg:500.28ms
step:142/1775 train_time:71018ms step_avg:500.13ms
step:143/1775 train_time:71488ms step_avg:499.92ms
step:144/1775 train_time:71970ms step_avg:499.79ms
step:145/1775 train_time:72453ms step_avg:499.67ms
step:146/1775 train_time:72925ms step_avg:499.49ms
step:147/1775 train_time:73400ms step_avg:499.32ms
step:148/1775 train_time:73889ms step_avg:499.25ms
step:149/1775 train_time:74364ms step_avg:499.09ms
step:150/1775 train_time:74843ms step_avg:498.95ms
step:151/1775 train_time:75314ms step_avg:498.77ms
step:152/1775 train_time:75803ms step_avg:498.71ms
step:153/1775 train_time:76280ms step_avg:498.56ms
step:154/1775 train_time:76755ms step_avg:498.41ms
step:155/1775 train_time:77223ms step_avg:498.21ms
step:156/1775 train_time:77704ms step_avg:498.11ms
step:157/1775 train_time:78306ms step_avg:498.76ms
step:158/1775 train_time:78856ms step_avg:499.09ms
step:159/1775 train_time:79325ms step_avg:498.90ms
step:160/1775 train_time:79809ms step_avg:498.81ms
step:161/1775 train_time:80278ms step_avg:498.62ms
step:162/1775 train_time:80763ms step_avg:498.54ms
step:163/1775 train_time:81231ms step_avg:498.35ms
step:164/1775 train_time:81726ms step_avg:498.33ms
step:165/1775 train_time:82202ms step_avg:498.19ms
step:166/1775 train_time:82681ms step_avg:498.08ms
step:167/1775 train_time:83161ms step_avg:497.97ms
step:168/1775 train_time:83654ms step_avg:497.94ms
step:169/1775 train_time:84133ms step_avg:497.83ms
step:170/1775 train_time:84604ms step_avg:497.67ms
step:171/1775 train_time:85079ms step_avg:497.54ms
step:172/1775 train_time:85563ms step_avg:497.46ms
step:173/1775 train_time:86047ms step_avg:497.38ms
step:174/1775 train_time:86534ms step_avg:497.32ms
step:175/1775 train_time:87008ms step_avg:497.19ms
step:176/1775 train_time:87485ms step_avg:497.07ms
step:177/1775 train_time:87956ms step_avg:496.93ms
step:178/1775 train_time:88430ms step_avg:496.80ms
step:179/1775 train_time:88899ms step_avg:496.64ms
step:180/1775 train_time:89385ms step_avg:496.58ms
step:181/1775 train_time:89855ms step_avg:496.44ms
step:182/1775 train_time:90331ms step_avg:496.32ms
step:183/1775 train_time:90813ms step_avg:496.24ms
step:184/1775 train_time:91313ms step_avg:496.26ms
step:185/1775 train_time:91779ms step_avg:496.10ms
step:186/1775 train_time:92268ms step_avg:496.07ms
step:187/1775 train_time:92739ms step_avg:495.93ms
step:188/1775 train_time:93218ms step_avg:495.84ms
step:189/1775 train_time:93685ms step_avg:495.69ms
step:190/1775 train_time:94161ms step_avg:495.58ms
step:191/1775 train_time:94632ms step_avg:495.46ms
step:192/1775 train_time:95108ms step_avg:495.35ms
step:193/1775 train_time:95583ms step_avg:495.25ms
step:194/1775 train_time:96074ms step_avg:495.23ms
step:195/1775 train_time:96548ms step_avg:495.12ms
step:196/1775 train_time:97051ms step_avg:495.16ms
step:197/1775 train_time:97543ms step_avg:495.14ms
step:198/1775 train_time:98035ms step_avg:495.13ms
step:199/1775 train_time:98510ms step_avg:495.02ms
step:200/1775 train_time:98976ms step_avg:494.88ms
step:201/1775 train_time:99445ms step_avg:494.75ms
step:202/1775 train_time:99920ms step_avg:494.66ms
step:203/1775 train_time:100390ms step_avg:494.53ms
step:204/1775 train_time:100884ms step_avg:494.53ms
step:205/1775 train_time:101369ms step_avg:494.48ms
step:206/1775 train_time:101842ms step_avg:494.38ms
step:207/1775 train_time:102328ms step_avg:494.34ms
step:208/1775 train_time:102814ms step_avg:494.30ms
step:209/1775 train_time:103285ms step_avg:494.19ms
step:210/1775 train_time:103773ms step_avg:494.16ms
step:211/1775 train_time:104246ms step_avg:494.05ms
step:212/1775 train_time:104738ms step_avg:494.04ms
step:213/1775 train_time:105218ms step_avg:493.98ms
step:214/1775 train_time:105701ms step_avg:493.93ms
step:215/1775 train_time:106182ms step_avg:493.87ms
step:216/1775 train_time:106659ms step_avg:493.79ms
step:217/1775 train_time:107152ms step_avg:493.79ms
step:218/1775 train_time:107629ms step_avg:493.71ms
step:219/1775 train_time:108103ms step_avg:493.62ms
step:220/1775 train_time:108582ms step_avg:493.56ms
step:221/1775 train_time:109051ms step_avg:493.44ms
step:222/1775 train_time:109525ms step_avg:493.36ms
step:223/1775 train_time:110015ms step_avg:493.34ms
step:224/1775 train_time:110493ms step_avg:493.27ms
step:225/1775 train_time:110990ms step_avg:493.29ms
step:226/1775 train_time:111630ms step_avg:493.94ms
step:227/1775 train_time:112149ms step_avg:494.05ms
step:228/1775 train_time:112628ms step_avg:493.98ms
step:229/1775 train_time:113110ms step_avg:493.93ms
step:230/1775 train_time:113597ms step_avg:493.90ms
step:231/1775 train_time:114074ms step_avg:493.83ms
step:232/1775 train_time:114541ms step_avg:493.71ms
step:233/1775 train_time:115007ms step_avg:493.59ms
step:234/1775 train_time:115478ms step_avg:493.50ms
step:235/1775 train_time:115966ms step_avg:493.47ms
step:236/1775 train_time:116455ms step_avg:493.45ms
step:237/1775 train_time:116953ms step_avg:493.47ms
step:238/1775 train_time:117438ms step_avg:493.44ms
step:239/1775 train_time:117907ms step_avg:493.34ms
step:240/1775 train_time:118380ms step_avg:493.25ms
step:241/1775 train_time:118848ms step_avg:493.14ms
step:242/1775 train_time:119324ms step_avg:493.07ms
step:243/1775 train_time:119799ms step_avg:493.00ms
step:244/1775 train_time:120273ms step_avg:492.92ms
step:245/1775 train_time:120762ms step_avg:492.91ms
step:246/1775 train_time:121234ms step_avg:492.82ms
step:247/1775 train_time:121706ms step_avg:492.74ms
step:248/1775 train_time:122181ms step_avg:492.67ms
step:249/1775 train_time:122661ms step_avg:492.61ms
step:250/1775 train_time:123147ms step_avg:492.59ms
step:250/1775 val_loss:4.5974 val_malbo_loss:4.5659 train_time:123148ms step_avg:492.59ms
step:251/1775 train_time:123607ms step_avg:492.46ms
step:252/1775 train_time:124082ms step_avg:492.39ms
step:253/1775 train_time:124550ms step_avg:492.29ms
step:254/1775 train_time:125016ms step_avg:492.19ms
step:255/1775 train_time:125502ms step_avg:492.16ms
step:256/1775 train_time:125976ms step_avg:492.09ms
step:257/1775 train_time:126453ms step_avg:492.04ms
step:258/1775 train_time:126931ms step_avg:491.98ms
step:259/1775 train_time:127405ms step_avg:491.91ms
step:260/1775 train_time:127875ms step_avg:491.83ms
step:261/1775 train_time:128349ms step_avg:491.76ms
step:262/1775 train_time:128850ms step_avg:491.79ms
step:263/1775 train_time:129364ms step_avg:491.88ms
step:264/1775 train_time:129888ms step_avg:492.00ms
step:265/1775 train_time:130408ms step_avg:492.11ms
step:266/1775 train_time:130940ms step_avg:492.26ms
step:267/1775 train_time:131464ms step_avg:492.38ms
step:268/1775 train_time:131991ms step_avg:492.50ms
step:269/1775 train_time:132512ms step_avg:492.61ms
step:270/1775 train_time:133037ms step_avg:492.73ms
step:271/1775 train_time:133602ms step_avg:493.00ms
step:272/1775 train_time:134259ms step_avg:493.60ms
step:273/1775 train_time:134789ms step_avg:493.73ms
step:274/1775 train_time:135317ms step_avg:493.86ms
step:275/1775 train_time:135794ms step_avg:493.79ms
step:276/1775 train_time:136269ms step_avg:493.73ms
step:277/1775 train_time:136744ms step_avg:493.66ms
step:278/1775 train_time:137233ms step_avg:493.64ms
step:279/1775 train_time:137712ms step_avg:493.59ms
step:280/1775 train_time:138204ms step_avg:493.59ms
step:281/1775 train_time:138701ms step_avg:493.60ms
step:282/1775 train_time:139180ms step_avg:493.54ms
step:283/1775 train_time:139673ms step_avg:493.54ms
step:284/1775 train_time:140172ms step_avg:493.56ms
step:285/1775 train_time:140712ms step_avg:493.72ms
step:286/1775 train_time:141231ms step_avg:493.82ms
step:287/1775 train_time:141720ms step_avg:493.80ms
step:288/1775 train_time:142199ms step_avg:493.75ms
step:289/1775 train_time:142674ms step_avg:493.68ms
step:290/1775 train_time:143152ms step_avg:493.63ms
step:291/1775 train_time:143633ms step_avg:493.58ms
step:292/1775 train_time:144101ms step_avg:493.50ms
step:293/1775 train_time:144574ms step_avg:493.43ms
step:294/1775 train_time:145034ms step_avg:493.31ms
step:295/1775 train_time:145519ms step_avg:493.29ms
step:296/1775 train_time:145998ms step_avg:493.24ms
step:297/1775 train_time:146479ms step_avg:493.20ms
step:298/1775 train_time:146969ms step_avg:493.18ms
step:299/1775 train_time:147438ms step_avg:493.10ms
step:300/1775 train_time:147921ms step_avg:493.07ms
step:301/1775 train_time:148394ms step_avg:493.00ms
step:302/1775 train_time:148867ms step_avg:492.94ms
step:303/1775 train_time:149341ms step_avg:492.88ms
step:304/1775 train_time:149817ms step_avg:492.82ms
step:305/1775 train_time:150288ms step_avg:492.75ms
step:306/1775 train_time:150781ms step_avg:492.75ms
step:307/1775 train_time:151252ms step_avg:492.68ms
step:308/1775 train_time:151726ms step_avg:492.62ms
step:309/1775 train_time:152207ms step_avg:492.58ms
step:310/1775 train_time:152677ms step_avg:492.51ms
step:311/1775 train_time:153153ms step_avg:492.45ms
step:312/1775 train_time:153642ms step_avg:492.44ms
step:313/1775 train_time:154119ms step_avg:492.39ms
step:314/1775 train_time:154598ms step_avg:492.35ms
step:315/1775 train_time:155074ms step_avg:492.30ms
step:316/1775 train_time:155555ms step_avg:492.26ms
step:317/1775 train_time:156034ms step_avg:492.22ms
step:318/1775 train_time:156524ms step_avg:492.21ms
step:319/1775 train_time:156993ms step_avg:492.14ms
step:320/1775 train_time:157480ms step_avg:492.12ms
step:321/1775 train_time:157966ms step_avg:492.10ms
step:322/1775 train_time:158455ms step_avg:492.10ms
step:323/1775 train_time:158970ms step_avg:492.17ms
step:324/1775 train_time:159470ms step_avg:492.19ms
step:325/1775 train_time:159957ms step_avg:492.18ms
step:326/1775 train_time:160465ms step_avg:492.22ms
step:327/1775 train_time:160980ms step_avg:492.29ms
step:328/1775 train_time:161493ms step_avg:492.36ms
step:329/1775 train_time:162004ms step_avg:492.41ms
step:330/1775 train_time:162515ms step_avg:492.47ms
step:331/1775 train_time:163022ms step_avg:492.51ms
step:332/1775 train_time:163533ms step_avg:492.57ms
step:333/1775 train_time:164036ms step_avg:492.60ms
step:334/1775 train_time:164541ms step_avg:492.64ms
step:335/1775 train_time:165044ms step_avg:492.67ms
step:336/1775 train_time:165557ms step_avg:492.73ms
step:337/1775 train_time:166037ms step_avg:492.69ms
step:338/1775 train_time:166540ms step_avg:492.72ms
step:339/1775 train_time:167028ms step_avg:492.71ms
step:340/1775 train_time:167548ms step_avg:492.79ms
step:341/1775 train_time:168210ms step_avg:493.28ms
step:342/1775 train_time:168829ms step_avg:493.65ms
step:343/1775 train_time:169369ms step_avg:493.79ms
step:344/1775 train_time:169897ms step_avg:493.89ms
step:345/1775 train_time:170399ms step_avg:493.91ms
step:346/1775 train_time:170909ms step_avg:493.96ms
step:347/1775 train_time:171413ms step_avg:493.98ms
step:348/1775 train_time:171916ms step_avg:494.01ms
step:349/1775 train_time:172421ms step_avg:494.04ms
step:350/1775 train_time:172923ms step_avg:494.07ms
step:351/1775 train_time:173410ms step_avg:494.05ms
step:352/1775 train_time:173892ms step_avg:494.01ms
step:353/1775 train_time:174353ms step_avg:493.92ms
step:354/1775 train_time:174839ms step_avg:493.90ms
step:355/1775 train_time:175311ms step_avg:493.83ms
step:356/1775 train_time:175802ms step_avg:493.82ms
step:357/1775 train_time:176293ms step_avg:493.82ms
step:358/1775 train_time:176769ms step_avg:493.77ms
step:359/1775 train_time:177243ms step_avg:493.71ms
step:360/1775 train_time:177736ms step_avg:493.71ms
step:361/1775 train_time:178206ms step_avg:493.65ms
step:362/1775 train_time:178691ms step_avg:493.62ms
step:363/1775 train_time:179163ms step_avg:493.56ms
step:364/1775 train_time:179674ms step_avg:493.61ms
step:365/1775 train_time:180203ms step_avg:493.71ms
step:366/1775 train_time:180736ms step_avg:493.81ms
step:367/1775 train_time:181242ms step_avg:493.85ms
step:368/1775 train_time:181722ms step_avg:493.81ms
step:369/1775 train_time:182192ms step_avg:493.75ms
step:370/1775 train_time:182675ms step_avg:493.72ms
step:371/1775 train_time:183148ms step_avg:493.66ms
step:372/1775 train_time:183629ms step_avg:493.63ms
step:373/1775 train_time:184111ms step_avg:493.60ms
step:374/1775 train_time:184587ms step_avg:493.55ms
step:375/1775 train_time:185077ms step_avg:493.54ms
step:376/1775 train_time:185579ms step_avg:493.56ms
step:377/1775 train_time:186055ms step_avg:493.51ms
step:378/1775 train_time:186551ms step_avg:493.52ms
step:379/1775 train_time:187021ms step_avg:493.46ms
step:380/1775 train_time:187495ms step_avg:493.41ms
step:381/1775 train_time:187976ms step_avg:493.38ms
step:382/1775 train_time:188445ms step_avg:493.31ms
step:383/1775 train_time:188924ms step_avg:493.27ms
step:384/1775 train_time:189400ms step_avg:493.23ms
step:385/1775 train_time:189885ms step_avg:493.21ms
step:386/1775 train_time:190361ms step_avg:493.16ms
step:387/1775 train_time:190851ms step_avg:493.16ms
step:388/1775 train_time:191328ms step_avg:493.11ms
step:389/1775 train_time:191809ms step_avg:493.08ms
step:390/1775 train_time:192293ms step_avg:493.06ms
step:391/1775 train_time:192778ms step_avg:493.04ms
step:392/1775 train_time:193264ms step_avg:493.02ms
step:393/1775 train_time:193742ms step_avg:492.98ms
step:394/1775 train_time:194227ms step_avg:492.96ms
step:395/1775 train_time:194696ms step_avg:492.90ms
step:396/1775 train_time:195179ms step_avg:492.88ms
step:397/1775 train_time:195660ms step_avg:492.85ms
step:398/1775 train_time:196129ms step_avg:492.79ms
step:399/1775 train_time:196619ms step_avg:492.78ms
step:400/1775 train_time:197110ms step_avg:492.77ms
step:401/1775 train_time:197583ms step_avg:492.73ms
step:402/1775 train_time:198060ms step_avg:492.69ms
step:403/1775 train_time:198543ms step_avg:492.66ms
step:404/1775 train_time:199023ms step_avg:492.63ms
step:405/1775 train_time:199502ms step_avg:492.60ms
step:406/1775 train_time:199978ms step_avg:492.56ms
step:407/1775 train_time:200463ms step_avg:492.54ms
step:408/1775 train_time:200948ms step_avg:492.52ms
step:409/1775 train_time:201419ms step_avg:492.47ms
step:410/1775 train_time:201917ms step_avg:492.48ms
step:411/1775 train_time:202617ms step_avg:492.99ms
step:412/1775 train_time:203127ms step_avg:493.03ms
step:413/1775 train_time:203597ms step_avg:492.97ms
step:414/1775 train_time:204075ms step_avg:492.93ms
step:415/1775 train_time:204568ms step_avg:492.93ms
step:416/1775 train_time:205064ms step_avg:492.94ms
step:417/1775 train_time:205534ms step_avg:492.89ms
step:418/1775 train_time:206022ms step_avg:492.88ms
step:419/1775 train_time:206511ms step_avg:492.87ms
step:420/1775 train_time:206984ms step_avg:492.82ms
step:421/1775 train_time:207453ms step_avg:492.76ms
step:422/1775 train_time:207935ms step_avg:492.74ms
step:423/1775 train_time:208411ms step_avg:492.70ms
step:424/1775 train_time:208944ms step_avg:492.79ms
step:425/1775 train_time:209468ms step_avg:492.87ms
step:426/1775 train_time:209992ms step_avg:492.94ms
step:427/1775 train_time:210473ms step_avg:492.91ms
step:428/1775 train_time:210957ms step_avg:492.89ms
step:429/1775 train_time:211433ms step_avg:492.85ms
step:430/1775 train_time:211928ms step_avg:492.86ms
step:431/1775 train_time:212411ms step_avg:492.83ms
step:432/1775 train_time:212933ms step_avg:492.90ms
step:433/1775 train_time:213460ms step_avg:492.98ms
step:434/1775 train_time:213992ms step_avg:493.07ms
step:435/1775 train_time:214484ms step_avg:493.07ms
step:436/1775 train_time:214964ms step_avg:493.04ms
step:437/1775 train_time:215448ms step_avg:493.02ms
step:438/1775 train_time:215941ms step_avg:493.01ms
step:439/1775 train_time:216410ms step_avg:492.96ms
step:440/1775 train_time:216896ms step_avg:492.94ms
step:441/1775 train_time:217368ms step_avg:492.90ms
step:442/1775 train_time:217852ms step_avg:492.88ms
step:443/1775 train_time:218330ms step_avg:492.84ms
step:444/1775 train_time:218815ms step_avg:492.83ms
step:445/1775 train_time:219295ms step_avg:492.80ms
step:446/1775 train_time:219793ms step_avg:492.81ms
step:447/1775 train_time:220322ms step_avg:492.89ms
step:448/1775 train_time:220861ms step_avg:492.99ms
step:449/1775 train_time:221387ms step_avg:493.07ms
step:450/1775 train_time:221888ms step_avg:493.08ms
step:451/1775 train_time:222376ms step_avg:493.07ms
step:452/1775 train_time:222887ms step_avg:493.11ms
step:453/1775 train_time:223370ms step_avg:493.09ms
step:454/1775 train_time:223874ms step_avg:493.11ms
step:455/1775 train_time:224348ms step_avg:493.07ms
step:456/1775 train_time:224833ms step_avg:493.05ms
step:457/1775 train_time:225325ms step_avg:493.05ms
step:458/1775 train_time:225828ms step_avg:493.07ms
step:459/1775 train_time:226336ms step_avg:493.11ms
step:460/1775 train_time:226822ms step_avg:493.09ms
step:461/1775 train_time:227305ms step_avg:493.07ms
step:462/1775 train_time:227807ms step_avg:493.09ms
step:463/1775 train_time:228280ms step_avg:493.05ms
step:464/1775 train_time:228782ms step_avg:493.06ms
step:465/1775 train_time:229274ms step_avg:493.06ms
step:466/1775 train_time:229761ms step_avg:493.05ms
step:467/1775 train_time:230257ms step_avg:493.06ms
step:468/1775 train_time:230735ms step_avg:493.02ms
step:469/1775 train_time:231231ms step_avg:493.03ms
step:470/1775 train_time:231729ms step_avg:493.04ms
step:471/1775 train_time:232232ms step_avg:493.06ms
step:472/1775 train_time:232739ms step_avg:493.09ms
step:473/1775 train_time:233240ms step_avg:493.11ms
step:474/1775 train_time:233748ms step_avg:493.14ms
step:475/1775 train_time:234243ms step_avg:493.14ms
step:476/1775 train_time:234735ms step_avg:493.14ms
step:477/1775 train_time:235245ms step_avg:493.18ms
step:478/1775 train_time:235975ms step_avg:493.67ms
step:479/1775 train_time:236473ms step_avg:493.68ms
step:480/1775 train_time:236960ms step_avg:493.67ms
step:481/1775 train_time:237435ms step_avg:493.63ms
step:482/1775 train_time:237921ms step_avg:493.61ms
step:483/1775 train_time:238384ms step_avg:493.55ms
step:484/1775 train_time:238867ms step_avg:493.53ms
step:485/1775 train_time:239344ms step_avg:493.49ms
step:486/1775 train_time:239827ms step_avg:493.47ms
step:487/1775 train_time:240304ms step_avg:493.44ms
step:488/1775 train_time:240777ms step_avg:493.40ms
step:489/1775 train_time:241246ms step_avg:493.35ms
step:490/1775 train_time:241735ms step_avg:493.34ms
step:491/1775 train_time:242205ms step_avg:493.29ms
step:492/1775 train_time:242683ms step_avg:493.26ms
step:493/1775 train_time:243174ms step_avg:493.25ms
step:494/1775 train_time:243647ms step_avg:493.21ms
step:495/1775 train_time:244120ms step_avg:493.17ms
step:496/1775 train_time:244608ms step_avg:493.16ms
step:497/1775 train_time:245078ms step_avg:493.11ms
step:498/1775 train_time:245549ms step_avg:493.07ms
step:499/1775 train_time:246022ms step_avg:493.03ms
step:500/1775 train_time:246498ms step_avg:493.00ms
step:500/1775 val_loss:4.2798 val_malbo_loss:4.2499 train_time:246501ms step_avg:493.00ms
step:501/1775 train_time:246973ms step_avg:492.96ms
step:502/1775 train_time:247456ms step_avg:492.94ms
step:503/1775 train_time:247936ms step_avg:492.91ms
step:504/1775 train_time:248423ms step_avg:492.90ms
step:505/1775 train_time:248900ms step_avg:492.87ms
step:506/1775 train_time:249376ms step_avg:492.84ms
step:507/1775 train_time:249841ms step_avg:492.78ms
step:508/1775 train_time:250313ms step_avg:492.74ms
step:509/1775 train_time:250781ms step_avg:492.69ms
step:510/1775 train_time:251259ms step_avg:492.66ms
step:511/1775 train_time:251730ms step_avg:492.62ms
step:512/1775 train_time:252208ms step_avg:492.59ms
step:513/1775 train_time:252683ms step_avg:492.56ms
step:514/1775 train_time:253153ms step_avg:492.51ms
step:515/1775 train_time:253625ms step_avg:492.48ms
step:516/1775 train_time:254102ms step_avg:492.45ms
step:517/1775 train_time:254573ms step_avg:492.40ms
step:518/1775 train_time:255068ms step_avg:492.41ms
step:519/1775 train_time:255531ms step_avg:492.35ms
step:520/1775 train_time:255993ms step_avg:492.29ms
step:521/1775 train_time:256455ms step_avg:492.24ms
step:522/1775 train_time:256927ms step_avg:492.20ms
step:523/1775 train_time:257431ms step_avg:492.22ms
step:524/1775 train_time:258163ms step_avg:492.68ms
step:525/1775 train_time:258638ms step_avg:492.64ms
step:526/1775 train_time:259121ms step_avg:492.63ms
step:527/1775 train_time:259605ms step_avg:492.61ms
step:528/1775 train_time:260092ms step_avg:492.60ms
step:529/1775 train_time:260562ms step_avg:492.56ms
step:530/1775 train_time:261039ms step_avg:492.53ms
step:531/1775 train_time:261499ms step_avg:492.47ms
step:532/1775 train_time:261980ms step_avg:492.44ms
step:533/1775 train_time:262455ms step_avg:492.41ms
step:534/1775 train_time:262935ms step_avg:492.39ms
step:535/1775 train_time:263411ms step_avg:492.36ms
step:536/1775 train_time:263890ms step_avg:492.33ms
step:537/1775 train_time:264365ms step_avg:492.30ms
step:538/1775 train_time:264853ms step_avg:492.29ms
step:539/1775 train_time:265318ms step_avg:492.24ms
step:540/1775 train_time:265800ms step_avg:492.22ms
step:541/1775 train_time:266270ms step_avg:492.18ms
step:542/1775 train_time:266728ms step_avg:492.12ms
step:543/1775 train_time:267203ms step_avg:492.09ms
step:544/1775 train_time:267681ms step_avg:492.06ms
step:545/1775 train_time:268170ms step_avg:492.05ms
step:546/1775 train_time:268651ms step_avg:492.03ms
step:547/1775 train_time:269137ms step_avg:492.02ms
step:548/1775 train_time:269613ms step_avg:491.99ms
step:549/1775 train_time:270094ms step_avg:491.97ms
step:550/1775 train_time:270561ms step_avg:491.93ms
step:551/1775 train_time:271031ms step_avg:491.89ms
step:552/1775 train_time:271509ms step_avg:491.86ms
step:553/1775 train_time:271983ms step_avg:491.83ms
step:554/1775 train_time:272478ms step_avg:491.84ms
step:555/1775 train_time:272955ms step_avg:491.81ms
step:556/1775 train_time:273434ms step_avg:491.79ms
step:557/1775 train_time:273914ms step_avg:491.77ms
step:558/1775 train_time:274387ms step_avg:491.73ms
step:559/1775 train_time:274865ms step_avg:491.71ms
step:560/1775 train_time:275340ms step_avg:491.68ms
step:561/1775 train_time:275833ms step_avg:491.68ms
step:562/1775 train_time:276309ms step_avg:491.65ms
step:563/1775 train_time:276779ms step_avg:491.61ms
step:564/1775 train_time:277264ms step_avg:491.60ms
step:565/1775 train_time:277774ms step_avg:491.64ms
step:566/1775 train_time:278292ms step_avg:491.68ms
step:567/1775 train_time:278815ms step_avg:491.74ms
step:568/1775 train_time:279304ms step_avg:491.73ms
step:569/1775 train_time:279782ms step_avg:491.71ms
step:570/1775 train_time:280259ms step_avg:491.68ms
step:571/1775 train_time:280741ms step_avg:491.67ms
step:572/1775 train_time:281217ms step_avg:491.64ms
step:573/1775 train_time:281691ms step_avg:491.61ms
step:574/1775 train_time:282170ms step_avg:491.59ms
step:575/1775 train_time:282648ms step_avg:491.56ms
step:576/1775 train_time:283135ms step_avg:491.55ms
step:577/1775 train_time:283624ms step_avg:491.55ms
step:578/1775 train_time:284098ms step_avg:491.52ms
step:579/1775 train_time:284577ms step_avg:491.50ms
step:580/1775 train_time:310677ms step_avg:535.65ms
step:581/1775 train_time:311451ms step_avg:536.06ms
step:582/1775 train_time:312232ms step_avg:536.48ms
step:583/1775 train_time:313007ms step_avg:536.89ms
step:584/1775 train_time:313785ms step_avg:537.30ms
step:585/1775 train_time:314562ms step_avg:537.71ms
step:586/1775 train_time:315345ms step_avg:538.13ms
step:587/1775 train_time:316123ms step_avg:538.54ms
step:588/1775 train_time:316902ms step_avg:538.95ms
step:589/1775 train_time:317680ms step_avg:539.35ms
step:590/1775 train_time:318465ms step_avg:539.77ms
step:591/1775 train_time:319242ms step_avg:540.17ms
step:592/1775 train_time:320021ms step_avg:540.58ms
step:593/1775 train_time:320798ms step_avg:540.97ms
step:594/1775 train_time:321589ms step_avg:541.40ms
step:595/1775 train_time:322370ms step_avg:541.80ms
step:596/1775 train_time:323151ms step_avg:542.20ms
step:597/1775 train_time:323941ms step_avg:542.61ms
step:598/1775 train_time:324781ms step_avg:543.11ms
step:599/1775 train_time:325542ms step_avg:543.48ms
step:600/1775 train_time:326332ms step_avg:543.89ms
step:601/1775 train_time:327105ms step_avg:544.27ms
step:602/1775 train_time:327978ms step_avg:544.81ms
step:603/1775 train_time:328761ms step_avg:545.21ms
step:604/1775 train_time:329544ms step_avg:545.60ms
step:605/1775 train_time:330323ms step_avg:545.99ms
step:606/1775 train_time:331107ms step_avg:546.38ms
step:607/1775 train_time:331894ms step_avg:546.78ms
step:608/1775 train_time:332674ms step_avg:547.16ms
step:609/1775 train_time:333453ms step_avg:547.54ms
step:610/1775 train_time:334240ms step_avg:547.93ms
step:611/1775 train_time:335022ms step_avg:548.32ms
step:612/1775 train_time:335804ms step_avg:548.70ms
step:613/1775 train_time:336589ms step_avg:549.08ms
step:614/1775 train_time:337378ms step_avg:549.48ms
step:615/1775 train_time:338157ms step_avg:549.85ms
step:616/1775 train_time:338942ms step_avg:550.23ms
step:617/1775 train_time:339722ms step_avg:550.60ms
step:618/1775 train_time:340505ms step_avg:550.98ms
step:619/1775 train_time:341291ms step_avg:551.36ms
step:620/1775 train_time:342073ms step_avg:551.73ms
step:621/1775 train_time:342865ms step_avg:552.12ms
step:622/1775 train_time:343647ms step_avg:552.49ms
step:623/1775 train_time:344429ms step_avg:552.85ms
step:624/1775 train_time:345214ms step_avg:553.23ms
step:625/1775 train_time:345996ms step_avg:553.59ms
step:626/1775 train_time:346789ms step_avg:553.98ms
step:627/1775 train_time:347566ms step_avg:554.33ms
step:628/1775 train_time:348357ms step_avg:554.71ms
step:629/1775 train_time:349138ms step_avg:555.07ms
step:630/1775 train_time:349926ms step_avg:555.44ms
step:631/1775 train_time:350699ms step_avg:555.78ms
step:632/1775 train_time:351488ms step_avg:556.15ms
step:633/1775 train_time:352272ms step_avg:556.51ms
step:634/1775 train_time:353061ms step_avg:556.88ms
step:635/1775 train_time:353839ms step_avg:557.23ms
step:636/1775 train_time:354627ms step_avg:557.59ms
step:637/1775 train_time:355402ms step_avg:557.93ms
step:638/1775 train_time:356193ms step_avg:558.30ms
step:639/1775 train_time:356980ms step_avg:558.65ms
step:640/1775 train_time:357825ms step_avg:559.10ms
step:641/1775 train_time:358642ms step_avg:559.50ms
step:642/1775 train_time:359432ms step_avg:559.86ms
step:643/1775 train_time:360212ms step_avg:560.20ms
step:644/1775 train_time:361002ms step_avg:560.56ms
step:645/1775 train_time:361785ms step_avg:560.91ms
step:646/1775 train_time:362571ms step_avg:561.26ms
step:647/1775 train_time:363352ms step_avg:561.59ms
step:648/1775 train_time:364129ms step_avg:561.93ms
step:649/1775 train_time:364921ms step_avg:562.28ms
step:650/1775 train_time:365704ms step_avg:562.62ms
step:651/1775 train_time:366489ms step_avg:562.96ms
step:652/1775 train_time:367279ms step_avg:563.31ms
step:653/1775 train_time:368067ms step_avg:563.66ms
step:654/1775 train_time:368855ms step_avg:564.00ms
step:655/1775 train_time:369635ms step_avg:564.33ms
step:656/1775 train_time:370421ms step_avg:564.67ms
step:657/1775 train_time:371201ms step_avg:564.99ms
step:658/1775 train_time:371993ms step_avg:565.34ms
step:659/1775 train_time:372773ms step_avg:565.67ms
step:660/1775 train_time:373562ms step_avg:566.00ms
step:661/1775 train_time:374343ms step_avg:566.33ms
step:662/1775 train_time:375127ms step_avg:566.66ms
step:663/1775 train_time:375906ms step_avg:566.98ms
step:664/1775 train_time:376696ms step_avg:567.31ms
step:665/1775 train_time:377482ms step_avg:567.64ms
step:666/1775 train_time:378273ms step_avg:567.98ms
step:667/1775 train_time:379057ms step_avg:568.30ms
step:668/1775 train_time:379843ms step_avg:568.63ms
step:669/1775 train_time:380624ms step_avg:568.94ms
step:670/1775 train_time:381409ms step_avg:569.27ms
step:671/1775 train_time:382190ms step_avg:569.58ms
step:672/1775 train_time:382983ms step_avg:569.92ms
step:673/1775 train_time:383763ms step_avg:570.23ms
step:674/1775 train_time:384549ms step_avg:570.55ms
step:675/1775 train_time:385331ms step_avg:570.86ms
step:676/1775 train_time:386131ms step_avg:571.20ms
step:677/1775 train_time:386916ms step_avg:571.52ms
step:678/1775 train_time:387704ms step_avg:571.84ms
step:679/1775 train_time:388489ms step_avg:572.15ms
step:680/1775 train_time:389280ms step_avg:572.47ms
step:681/1775 train_time:390067ms step_avg:572.78ms
step:682/1775 train_time:390854ms step_avg:573.10ms
step:683/1775 train_time:391681ms step_avg:573.47ms
step:684/1775 train_time:392458ms step_avg:573.77ms
step:685/1775 train_time:393234ms step_avg:574.06ms
step:686/1775 train_time:394030ms step_avg:574.39ms
step:687/1775 train_time:394807ms step_avg:574.68ms
step:688/1775 train_time:395594ms step_avg:574.99ms
step:689/1775 train_time:396375ms step_avg:575.29ms
step:690/1775 train_time:397167ms step_avg:575.60ms
step:691/1775 train_time:397949ms step_avg:575.90ms
step:692/1775 train_time:398734ms step_avg:576.21ms
step:693/1775 train_time:399514ms step_avg:576.50ms
step:694/1775 train_time:400304ms step_avg:576.81ms
step:695/1775 train_time:401094ms step_avg:577.11ms
step:696/1775 train_time:401875ms step_avg:577.41ms
step:697/1775 train_time:402656ms step_avg:577.70ms
step:698/1775 train_time:403442ms step_avg:578.00ms
step:699/1775 train_time:404226ms step_avg:578.29ms
step:700/1775 train_time:405008ms step_avg:578.58ms
step:701/1775 train_time:405794ms step_avg:578.88ms
step:702/1775 train_time:406586ms step_avg:579.18ms
step:703/1775 train_time:407365ms step_avg:579.47ms
step:704/1775 train_time:408150ms step_avg:579.76ms
step:705/1775 train_time:408937ms step_avg:580.05ms
step:706/1775 train_time:409729ms step_avg:580.35ms
step:707/1775 train_time:410498ms step_avg:580.62ms
step:708/1775 train_time:411290ms step_avg:580.92ms
step:709/1775 train_time:412074ms step_avg:581.20ms
step:710/1775 train_time:412865ms step_avg:581.50ms
step:711/1775 train_time:413644ms step_avg:581.78ms
step:712/1775 train_time:414429ms step_avg:582.06ms
step:713/1775 train_time:415209ms step_avg:582.34ms
step:714/1775 train_time:415999ms step_avg:582.63ms
step:715/1775 train_time:416785ms step_avg:582.92ms
step:716/1775 train_time:417575ms step_avg:583.21ms
step:717/1775 train_time:418355ms step_avg:583.48ms
step:718/1775 train_time:419141ms step_avg:583.76ms
step:719/1775 train_time:419925ms step_avg:584.04ms
step:720/1775 train_time:420707ms step_avg:584.32ms
step:721/1775 train_time:421492ms step_avg:584.59ms
step:722/1775 train_time:422282ms step_avg:584.88ms
step:723/1775 train_time:423065ms step_avg:585.15ms
step:724/1775 train_time:423846ms step_avg:585.42ms
step:725/1775 train_time:424662ms step_avg:585.74ms
step:726/1775 train_time:425450ms step_avg:586.02ms
step:727/1775 train_time:426234ms step_avg:586.29ms
step:728/1775 train_time:427023ms step_avg:586.57ms
step:729/1775 train_time:427799ms step_avg:586.83ms
step:730/1775 train_time:428597ms step_avg:587.12ms
step:731/1775 train_time:429381ms step_avg:587.39ms
step:732/1775 train_time:430171ms step_avg:587.67ms
step:733/1775 train_time:430955ms step_avg:587.93ms
step:734/1775 train_time:431741ms step_avg:588.20ms
step:735/1775 train_time:432523ms step_avg:588.47ms
step:736/1775 train_time:433307ms step_avg:588.73ms
step:737/1775 train_time:434091ms step_avg:589.00ms
step:738/1775 train_time:434884ms step_avg:589.27ms
step:739/1775 train_time:435667ms step_avg:589.54ms
step:740/1775 train_time:436451ms step_avg:589.80ms
step:741/1775 train_time:437230ms step_avg:590.05ms
step:742/1775 train_time:438020ms step_avg:590.32ms
step:743/1775 train_time:438805ms step_avg:590.58ms
step:744/1775 train_time:439594ms step_avg:590.85ms
step:745/1775 train_time:440371ms step_avg:591.10ms
step:746/1775 train_time:441163ms step_avg:591.37ms
step:747/1775 train_time:441943ms step_avg:591.62ms
step:748/1775 train_time:442725ms step_avg:591.88ms
step:749/1775 train_time:443505ms step_avg:592.13ms
step:750/1775 train_time:444303ms step_avg:592.40ms
step:750/1775 val_loss:4.0009 val_malbo_loss:3.9713 train_time:444330ms step_avg:592.44ms
step:751/1775 train_time:445076ms step_avg:592.64ms
step:752/1775 train_time:445858ms step_avg:592.90ms
step:753/1775 train_time:446642ms step_avg:593.15ms
step:754/1775 train_time:447427ms step_avg:593.40ms
step:755/1775 train_time:448201ms step_avg:593.64ms
step:756/1775 train_time:448990ms step_avg:593.90ms
step:757/1775 train_time:449763ms step_avg:594.14ms
step:758/1775 train_time:450553ms step_avg:594.40ms
step:759/1775 train_time:451329ms step_avg:594.64ms
step:760/1775 train_time:452111ms step_avg:594.88ms
step:761/1775 train_time:452887ms step_avg:595.12ms
step:762/1775 train_time:453667ms step_avg:595.36ms
step:763/1775 train_time:454447ms step_avg:595.61ms
step:764/1775 train_time:455229ms step_avg:595.85ms
step:765/1775 train_time:456009ms step_avg:596.09ms
step:766/1775 train_time:456799ms step_avg:596.34ms
step:767/1775 train_time:457576ms step_avg:596.58ms
step:768/1775 train_time:458361ms step_avg:596.82ms
step:769/1775 train_time:459142ms step_avg:597.06ms
step:770/1775 train_time:459928ms step_avg:597.31ms
step:771/1775 train_time:460706ms step_avg:597.54ms
step:772/1775 train_time:461491ms step_avg:597.79ms
step:773/1775 train_time:462267ms step_avg:598.02ms
step:774/1775 train_time:463072ms step_avg:598.28ms
step:775/1775 train_time:463869ms step_avg:598.54ms
step:776/1775 train_time:464663ms step_avg:598.79ms
step:777/1775 train_time:465448ms step_avg:599.03ms
step:778/1775 train_time:466233ms step_avg:599.27ms
step:779/1775 train_time:467008ms step_avg:599.50ms
step:780/1775 train_time:467792ms step_avg:599.73ms
step:781/1775 train_time:468567ms step_avg:599.96ms
step:782/1775 train_time:469358ms step_avg:600.20ms
step:783/1775 train_time:470138ms step_avg:600.43ms
step:784/1775 train_time:470924ms step_avg:600.67ms
step:785/1775 train_time:471702ms step_avg:600.89ms
step:786/1775 train_time:472492ms step_avg:601.14ms
step:787/1775 train_time:473266ms step_avg:601.35ms
step:788/1775 train_time:474060ms step_avg:601.60ms
step:789/1775 train_time:474844ms step_avg:601.83ms
step:790/1775 train_time:475628ms step_avg:602.06ms
step:791/1775 train_time:476403ms step_avg:602.28ms
step:792/1775 train_time:477191ms step_avg:602.51ms
step:793/1775 train_time:477966ms step_avg:602.73ms
step:794/1775 train_time:478761ms step_avg:602.97ms
step:795/1775 train_time:479541ms step_avg:603.20ms
step:796/1775 train_time:480329ms step_avg:603.43ms
step:797/1775 train_time:481105ms step_avg:603.64ms
step:798/1775 train_time:481894ms step_avg:603.88ms
step:799/1775 train_time:482665ms step_avg:604.09ms
step:800/1775 train_time:483454ms step_avg:604.32ms
step:801/1775 train_time:484238ms step_avg:604.54ms
step:802/1775 train_time:485025ms step_avg:604.77ms
step:803/1775 train_time:485804ms step_avg:604.99ms
step:804/1775 train_time:486593ms step_avg:605.22ms
step:805/1775 train_time:487369ms step_avg:605.43ms
step:806/1775 train_time:488156ms step_avg:605.65ms
step:807/1775 train_time:488944ms step_avg:605.88ms
step:808/1775 train_time:489739ms step_avg:606.11ms
step:809/1775 train_time:490515ms step_avg:606.32ms
step:810/1775 train_time:491300ms step_avg:606.54ms
step:811/1775 train_time:492073ms step_avg:606.75ms
step:812/1775 train_time:492866ms step_avg:606.98ms
step:813/1775 train_time:493652ms step_avg:607.20ms
step:814/1775 train_time:494436ms step_avg:607.41ms
step:815/1775 train_time:495211ms step_avg:607.62ms
step:816/1775 train_time:496000ms step_avg:607.84ms
step:817/1775 train_time:496837ms step_avg:608.12ms
step:818/1775 train_time:497606ms step_avg:608.32ms
step:819/1775 train_time:498386ms step_avg:608.53ms
step:820/1775 train_time:499168ms step_avg:608.74ms
step:821/1775 train_time:499956ms step_avg:608.96ms
step:822/1775 train_time:500756ms step_avg:609.19ms
step:823/1775 train_time:501532ms step_avg:609.40ms
step:824/1775 train_time:502313ms step_avg:609.60ms
step:825/1775 train_time:503094ms step_avg:609.81ms
step:826/1775 train_time:503880ms step_avg:610.02ms
step:827/1775 train_time:504663ms step_avg:610.23ms
step:828/1775 train_time:505452ms step_avg:610.45ms
step:829/1775 train_time:506230ms step_avg:610.65ms
step:830/1775 train_time:507013ms step_avg:610.86ms
step:831/1775 train_time:507793ms step_avg:611.06ms
step:832/1775 train_time:508580ms step_avg:611.27ms
step:833/1775 train_time:509364ms step_avg:611.48ms
step:834/1775 train_time:510155ms step_avg:611.70ms
step:835/1775 train_time:510931ms step_avg:611.89ms
step:836/1775 train_time:511715ms step_avg:612.10ms
step:837/1775 train_time:512497ms step_avg:612.30ms
step:838/1775 train_time:513290ms step_avg:612.52ms
step:839/1775 train_time:514064ms step_avg:612.71ms
step:840/1775 train_time:514861ms step_avg:612.93ms
step:841/1775 train_time:515641ms step_avg:613.13ms
step:842/1775 train_time:516428ms step_avg:613.34ms
step:843/1775 train_time:517203ms step_avg:613.53ms
step:844/1775 train_time:517995ms step_avg:613.74ms
step:845/1775 train_time:518769ms step_avg:613.93ms
step:846/1775 train_time:519563ms step_avg:614.14ms
step:847/1775 train_time:520354ms step_avg:614.35ms
step:848/1775 train_time:521138ms step_avg:614.55ms
step:849/1775 train_time:521918ms step_avg:614.74ms
step:850/1775 train_time:522707ms step_avg:614.95ms
step:851/1775 train_time:523486ms step_avg:615.14ms
step:852/1775 train_time:524268ms step_avg:615.34ms
step:853/1775 train_time:525053ms step_avg:615.54ms
step:854/1775 train_time:525839ms step_avg:615.74ms
step:855/1775 train_time:526625ms step_avg:615.94ms
step:856/1775 train_time:527408ms step_avg:616.13ms
step:857/1775 train_time:528186ms step_avg:616.32ms
step:858/1775 train_time:528970ms step_avg:616.51ms
step:859/1775 train_time:529775ms step_avg:616.73ms
step:860/1775 train_time:530636ms step_avg:617.02ms
step:861/1775 train_time:531391ms step_avg:617.18ms
step:862/1775 train_time:532170ms step_avg:617.37ms
step:863/1775 train_time:532957ms step_avg:617.56ms
step:864/1775 train_time:533750ms step_avg:617.77ms
step:865/1775 train_time:534530ms step_avg:617.95ms
step:866/1775 train_time:535307ms step_avg:618.14ms
step:867/1775 train_time:536090ms step_avg:618.33ms
step:868/1775 train_time:536873ms step_avg:618.52ms
step:869/1775 train_time:537659ms step_avg:618.71ms
step:870/1775 train_time:538449ms step_avg:618.91ms
step:871/1775 train_time:539227ms step_avg:619.09ms
step:872/1775 train_time:540013ms step_avg:619.28ms
step:873/1775 train_time:540797ms step_avg:619.47ms
step:874/1775 train_time:541581ms step_avg:619.66ms
step:875/1775 train_time:542361ms step_avg:619.84ms
step:876/1775 train_time:543151ms step_avg:620.04ms
step:877/1775 train_time:543929ms step_avg:620.22ms
step:878/1775 train_time:544713ms step_avg:620.40ms
step:879/1775 train_time:545487ms step_avg:620.58ms
step:880/1775 train_time:546272ms step_avg:620.76ms
step:881/1775 train_time:547054ms step_avg:620.95ms
step:882/1775 train_time:547847ms step_avg:621.14ms
step:883/1775 train_time:548628ms step_avg:621.32ms
step:884/1775 train_time:549410ms step_avg:621.50ms
step:885/1775 train_time:550193ms step_avg:621.69ms
step:886/1775 train_time:550976ms step_avg:621.87ms
step:887/1775 train_time:551760ms step_avg:622.05ms
step:888/1775 train_time:552554ms step_avg:622.25ms
step:889/1775 train_time:553333ms step_avg:622.42ms
step:890/1775 train_time:554112ms step_avg:622.60ms
step:891/1775 train_time:554891ms step_avg:622.77ms
step:892/1775 train_time:555678ms step_avg:622.96ms
step:893/1775 train_time:556454ms step_avg:623.13ms
step:894/1775 train_time:557248ms step_avg:623.32ms
step:895/1775 train_time:558030ms step_avg:623.50ms
step:896/1775 train_time:558810ms step_avg:623.67ms
step:897/1775 train_time:559590ms step_avg:623.85ms
step:898/1775 train_time:560372ms step_avg:624.02ms
step:899/1775 train_time:561156ms step_avg:624.20ms
step:900/1775 train_time:561950ms step_avg:624.39ms
step:901/1775 train_time:562729ms step_avg:624.56ms
step:902/1775 train_time:563538ms step_avg:624.77ms
step:903/1775 train_time:564334ms step_avg:624.95ms
step:904/1775 train_time:565119ms step_avg:625.13ms
step:905/1775 train_time:565897ms step_avg:625.30ms
step:906/1775 train_time:566689ms step_avg:625.48ms
step:907/1775 train_time:567463ms step_avg:625.65ms
step:908/1775 train_time:568257ms step_avg:625.83ms
step:909/1775 train_time:569040ms step_avg:626.01ms
step:910/1775 train_time:569835ms step_avg:626.19ms
step:911/1775 train_time:570612ms step_avg:626.36ms
step:912/1775 train_time:571393ms step_avg:626.53ms
step:913/1775 train_time:572164ms step_avg:626.69ms
step:914/1775 train_time:572962ms step_avg:626.87ms
step:915/1775 train_time:573743ms step_avg:627.04ms
step:916/1775 train_time:574529ms step_avg:627.21ms
step:917/1775 train_time:575303ms step_avg:627.38ms
step:918/1775 train_time:576094ms step_avg:627.55ms
step:919/1775 train_time:576930ms step_avg:627.78ms
step:920/1775 train_time:577717ms step_avg:627.95ms
step:921/1775 train_time:578488ms step_avg:628.11ms
step:922/1775 train_time:579271ms step_avg:628.28ms
step:923/1775 train_time:580060ms step_avg:628.45ms
step:924/1775 train_time:580853ms step_avg:628.63ms
step:925/1775 train_time:581632ms step_avg:628.79ms
step:926/1775 train_time:582410ms step_avg:628.95ms
step:927/1775 train_time:583193ms step_avg:629.12ms
step:928/1775 train_time:583979ms step_avg:629.29ms
step:929/1775 train_time:584760ms step_avg:629.45ms
step:930/1775 train_time:585550ms step_avg:629.62ms
step:931/1775 train_time:586330ms step_avg:629.79ms
step:932/1775 train_time:587114ms step_avg:629.95ms
step:933/1775 train_time:587891ms step_avg:630.11ms
step:934/1775 train_time:588676ms step_avg:630.27ms
step:935/1775 train_time:589457ms step_avg:630.43ms
step:936/1775 train_time:590247ms step_avg:630.61ms
step:937/1775 train_time:591029ms step_avg:630.77ms
step:938/1775 train_time:591812ms step_avg:630.93ms
step:939/1775 train_time:592593ms step_avg:631.09ms
step:940/1775 train_time:593380ms step_avg:631.26ms
step:941/1775 train_time:594166ms step_avg:631.42ms
step:942/1775 train_time:594957ms step_avg:631.59ms
step:943/1775 train_time:595733ms step_avg:631.74ms
step:944/1775 train_time:596521ms step_avg:631.91ms
step:945/1775 train_time:597344ms step_avg:632.11ms
step:946/1775 train_time:598113ms step_avg:632.25ms
step:947/1775 train_time:598894ms step_avg:632.41ms
step:948/1775 train_time:599680ms step_avg:632.57ms
step:949/1775 train_time:600460ms step_avg:632.73ms
step:950/1775 train_time:601252ms step_avg:632.90ms
step:951/1775 train_time:602031ms step_avg:633.05ms
step:952/1775 train_time:602814ms step_avg:633.21ms
step:953/1775 train_time:603595ms step_avg:633.36ms
step:954/1775 train_time:604380ms step_avg:633.52ms
step:955/1775 train_time:605160ms step_avg:633.68ms
step:956/1775 train_time:605962ms step_avg:633.85ms
step:957/1775 train_time:606745ms step_avg:634.01ms
step:958/1775 train_time:607534ms step_avg:634.17ms
step:959/1775 train_time:608309ms step_avg:634.32ms
step:960/1775 train_time:609095ms step_avg:634.47ms
step:961/1775 train_time:609877ms step_avg:634.63ms
step:962/1775 train_time:610670ms step_avg:634.79ms
step:963/1775 train_time:611454ms step_avg:634.95ms
step:964/1775 train_time:612240ms step_avg:635.10ms
step:965/1775 train_time:613021ms step_avg:635.25ms
step:966/1775 train_time:613806ms step_avg:635.41ms
step:967/1775 train_time:614586ms step_avg:635.56ms
step:968/1775 train_time:615369ms step_avg:635.71ms
step:969/1775 train_time:616151ms step_avg:635.86ms
step:970/1775 train_time:616939ms step_avg:636.02ms
step:971/1775 train_time:617722ms step_avg:636.17ms
step:972/1775 train_time:618505ms step_avg:636.32ms
step:973/1775 train_time:619288ms step_avg:636.47ms
step:974/1775 train_time:620071ms step_avg:636.62ms
step:975/1775 train_time:620859ms step_avg:636.78ms
step:976/1775 train_time:621647ms step_avg:636.93ms
step:977/1775 train_time:622419ms step_avg:637.07ms
step:978/1775 train_time:623209ms step_avg:637.23ms
step:979/1775 train_time:623988ms step_avg:637.37ms
step:980/1775 train_time:624771ms step_avg:637.52ms
step:981/1775 train_time:625555ms step_avg:637.67ms
step:982/1775 train_time:626339ms step_avg:637.82ms
step:983/1775 train_time:627121ms step_avg:637.97ms
step:984/1775 train_time:627908ms step_avg:638.12ms
step:985/1775 train_time:628687ms step_avg:638.26ms
step:986/1775 train_time:629471ms step_avg:638.41ms
step:987/1775 train_time:630301ms step_avg:638.60ms
step:988/1775 train_time:631080ms step_avg:638.74ms
step:989/1775 train_time:631856ms step_avg:638.88ms
step:990/1775 train_time:632650ms step_avg:639.04ms
step:991/1775 train_time:633433ms step_avg:639.19ms
step:992/1775 train_time:634216ms step_avg:639.33ms
step:993/1775 train_time:634993ms step_avg:639.47ms
step:994/1775 train_time:635783ms step_avg:639.62ms
step:995/1775 train_time:636574ms step_avg:639.77ms
step:996/1775 train_time:637362ms step_avg:639.92ms
step:997/1775 train_time:638144ms step_avg:640.06ms
step:998/1775 train_time:638930ms step_avg:640.21ms
step:999/1775 train_time:639708ms step_avg:640.35ms
step:1000/1775 train_time:640493ms step_avg:640.49ms
step:1000/1775 val_loss:3.7277 val_malbo_loss:3.6998 train_time:640526ms step_avg:640.53ms
step:1001/1775 train_time:641291ms step_avg:640.65ms
step:1002/1775 train_time:642063ms step_avg:640.78ms
step:1003/1775 train_time:642844ms step_avg:640.92ms
step:1004/1775 train_time:643633ms step_avg:641.07ms
step:1005/1775 train_time:644407ms step_avg:641.20ms
step:1006/1775 train_time:645199ms step_avg:641.35ms
step:1007/1775 train_time:645982ms step_avg:641.49ms
step:1008/1775 train_time:646769ms step_avg:641.64ms
step:1009/1775 train_time:647550ms step_avg:641.77ms
step:1010/1775 train_time:648335ms step_avg:641.92ms
step:1011/1775 train_time:649117ms step_avg:642.05ms
step:1012/1775 train_time:649906ms step_avg:642.20ms
step:1013/1775 train_time:650690ms step_avg:642.34ms
step:1014/1775 train_time:651476ms step_avg:642.48ms
step:1015/1775 train_time:652302ms step_avg:642.66ms
step:1016/1775 train_time:653083ms step_avg:642.80ms
step:1017/1775 train_time:653861ms step_avg:642.93ms
step:1018/1775 train_time:654651ms step_avg:643.08ms
step:1019/1775 train_time:655426ms step_avg:643.20ms
step:1020/1775 train_time:656211ms step_avg:643.34ms
step:1021/1775 train_time:656994ms step_avg:643.48ms
step:1022/1775 train_time:657781ms step_avg:643.62ms
step:1023/1775 train_time:658563ms step_avg:643.76ms
step:1024/1775 train_time:659347ms step_avg:643.89ms
step:1025/1775 train_time:660123ms step_avg:644.02ms
step:1026/1775 train_time:660909ms step_avg:644.16ms
step:1027/1775 train_time:661696ms step_avg:644.30ms
step:1028/1775 train_time:662482ms step_avg:644.44ms
step:1029/1775 train_time:663262ms step_avg:644.57ms
step:1030/1775 train_time:664046ms step_avg:644.71ms
step:1031/1775 train_time:664821ms step_avg:644.83ms
step:1032/1775 train_time:665609ms step_avg:644.97ms
step:1033/1775 train_time:666392ms step_avg:645.10ms
step:1034/1775 train_time:667184ms step_avg:645.25ms
step:1035/1775 train_time:667967ms step_avg:645.38ms
step:1036/1775 train_time:668749ms step_avg:645.51ms
step:1037/1775 train_time:669527ms step_avg:645.64ms
step:1038/1775 train_time:670313ms step_avg:645.77ms
step:1039/1775 train_time:671099ms step_avg:645.91ms
step:1040/1775 train_time:671887ms step_avg:646.04ms
step:1041/1775 train_time:672668ms step_avg:646.17ms
step:1042/1775 train_time:673451ms step_avg:646.31ms
step:1043/1775 train_time:674231ms step_avg:646.43ms
step:1044/1775 train_time:675014ms step_avg:646.56ms
step:1045/1775 train_time:675798ms step_avg:646.70ms
step:1046/1775 train_time:676588ms step_avg:646.83ms
step:1047/1775 train_time:677366ms step_avg:646.96ms
step:1048/1775 train_time:678156ms step_avg:647.10ms
step:1049/1775 train_time:678932ms step_avg:647.22ms
step:1050/1775 train_time:679707ms step_avg:647.34ms
step:1051/1775 train_time:680493ms step_avg:647.47ms
step:1052/1775 train_time:681280ms step_avg:647.60ms
step:1053/1775 train_time:682059ms step_avg:647.73ms
step:1054/1775 train_time:682844ms step_avg:647.86ms
step:1055/1775 train_time:683626ms step_avg:647.99ms
step:1056/1775 train_time:684407ms step_avg:648.11ms
step:1057/1775 train_time:685200ms step_avg:648.25ms
step:1058/1775 train_time:686024ms step_avg:648.42ms
step:1059/1775 train_time:686799ms step_avg:648.54ms
step:1060/1775 train_time:687590ms step_avg:648.67ms
step:1061/1775 train_time:688367ms step_avg:648.79ms
step:1062/1775 train_time:689149ms step_avg:648.92ms
step:1063/1775 train_time:689935ms step_avg:649.04ms
step:1064/1775 train_time:690720ms step_avg:649.17ms
step:1065/1775 train_time:691491ms step_avg:649.29ms
step:1066/1775 train_time:692280ms step_avg:649.42ms
step:1067/1775 train_time:693060ms step_avg:649.54ms
step:1068/1775 train_time:693846ms step_avg:649.67ms
step:1069/1775 train_time:694626ms step_avg:649.79ms
step:1070/1775 train_time:695409ms step_avg:649.91ms
step:1071/1775 train_time:696194ms step_avg:650.04ms
step:1072/1775 train_time:696979ms step_avg:650.17ms
step:1073/1775 train_time:697760ms step_avg:650.29ms
step:1074/1775 train_time:698543ms step_avg:650.41ms
step:1075/1775 train_time:699327ms step_avg:650.54ms
step:1076/1775 train_time:700109ms step_avg:650.66ms
step:1077/1775 train_time:700891ms step_avg:650.78ms
step:1078/1775 train_time:701681ms step_avg:650.91ms
step:1079/1775 train_time:702463ms step_avg:651.03ms
step:1080/1775 train_time:703251ms step_avg:651.16ms
step:1081/1775 train_time:704027ms step_avg:651.27ms
step:1082/1775 train_time:704807ms step_avg:651.39ms
step:1083/1775 train_time:705591ms step_avg:651.52ms
step:1084/1775 train_time:706378ms step_avg:651.64ms
step:1085/1775 train_time:707158ms step_avg:651.76ms
step:1086/1775 train_time:707943ms step_avg:651.88ms
step:1087/1775 train_time:708726ms step_avg:652.00ms
step:1088/1775 train_time:709510ms step_avg:652.12ms
step:1089/1775 train_time:710289ms step_avg:652.24ms
step:1090/1775 train_time:711074ms step_avg:652.36ms
step:1091/1775 train_time:711854ms step_avg:652.48ms
step:1092/1775 train_time:712639ms step_avg:652.60ms
step:1093/1775 train_time:713421ms step_avg:652.72ms
step:1094/1775 train_time:714205ms step_avg:652.84ms
step:1095/1775 train_time:714988ms step_avg:652.96ms
step:1096/1775 train_time:715774ms step_avg:653.08ms
step:1097/1775 train_time:716551ms step_avg:653.19ms
step:1098/1775 train_time:717336ms step_avg:653.31ms
step:1099/1775 train_time:718119ms step_avg:653.43ms
step:1100/1775 train_time:718937ms step_avg:653.58ms
step:1101/1775 train_time:719769ms step_avg:653.74ms
step:1102/1775 train_time:720556ms step_avg:653.86ms
step:1103/1775 train_time:721341ms step_avg:653.98ms
step:1104/1775 train_time:722119ms step_avg:654.09ms
step:1105/1775 train_time:722899ms step_avg:654.21ms
step:1106/1775 train_time:723685ms step_avg:654.33ms
step:1107/1775 train_time:724478ms step_avg:654.45ms
step:1108/1775 train_time:725271ms step_avg:654.58ms
step:1109/1775 train_time:726047ms step_avg:654.69ms
step:1110/1775 train_time:726832ms step_avg:654.80ms
step:1111/1775 train_time:727606ms step_avg:654.91ms
step:1112/1775 train_time:728399ms step_avg:655.04ms
step:1113/1775 train_time:729178ms step_avg:655.15ms
step:1114/1775 train_time:729976ms step_avg:655.28ms
step:1115/1775 train_time:730730ms step_avg:655.36ms
step:1116/1775 train_time:731531ms step_avg:655.49ms
step:1117/1775 train_time:732303ms step_avg:655.60ms
step:1118/1775 train_time:733097ms step_avg:655.72ms
step:1119/1775 train_time:733876ms step_avg:655.83ms
step:1120/1775 train_time:734665ms step_avg:655.95ms
step:1121/1775 train_time:735443ms step_avg:656.06ms
step:1122/1775 train_time:736229ms step_avg:656.18ms
step:1123/1775 train_time:737004ms step_avg:656.28ms
step:1124/1775 train_time:737798ms step_avg:656.40ms
step:1125/1775 train_time:738576ms step_avg:656.51ms
step:1126/1775 train_time:739364ms step_avg:656.63ms
step:1127/1775 train_time:740146ms step_avg:656.74ms
step:1128/1775 train_time:740930ms step_avg:656.85ms
step:1129/1775 train_time:741702ms step_avg:656.95ms
step:1130/1775 train_time:742496ms step_avg:657.08ms
step:1131/1775 train_time:743275ms step_avg:657.18ms
step:1132/1775 train_time:744063ms step_avg:657.30ms
step:1133/1775 train_time:744841ms step_avg:657.41ms
step:1134/1775 train_time:745627ms step_avg:657.52ms
step:1135/1775 train_time:746401ms step_avg:657.62ms
step:1136/1775 train_time:747196ms step_avg:657.74ms
step:1137/1775 train_time:747975ms step_avg:657.85ms
step:1138/1775 train_time:748760ms step_avg:657.96ms
step:1139/1775 train_time:749540ms step_avg:658.07ms
step:1140/1775 train_time:750324ms step_avg:658.18ms
step:1141/1775 train_time:751098ms step_avg:658.28ms
step:1142/1775 train_time:751892ms step_avg:658.40ms
step:1143/1775 train_time:752724ms step_avg:658.55ms
step:1144/1775 train_time:753502ms step_avg:658.66ms
step:1145/1775 train_time:754291ms step_avg:658.77ms
step:1146/1775 train_time:755075ms step_avg:658.88ms
step:1147/1775 train_time:755852ms step_avg:658.98ms
step:1148/1775 train_time:756635ms step_avg:659.09ms
step:1149/1775 train_time:757414ms step_avg:659.19ms
step:1150/1775 train_time:758209ms step_avg:659.31ms
step:1151/1775 train_time:758989ms step_avg:659.42ms
step:1152/1775 train_time:759770ms step_avg:659.52ms
step:1153/1775 train_time:760545ms step_avg:659.62ms
step:1154/1775 train_time:761330ms step_avg:659.73ms
step:1155/1775 train_time:762106ms step_avg:659.83ms
step:1156/1775 train_time:762899ms step_avg:659.95ms
step:1157/1775 train_time:763678ms step_avg:660.05ms
step:1158/1775 train_time:790406ms step_avg:682.56ms
step:1159/1775 train_time:932354ms step_avg:804.45ms
step:1160/1775 train_time:933281ms step_avg:804.55ms
step:1161/1775 train_time:934201ms step_avg:804.65ms
step:1162/1775 train_time:935141ms step_avg:804.77ms
step:1163/1775 train_time:936059ms step_avg:804.87ms
step:1164/1775 train_time:936992ms step_avg:804.98ms
step:1165/1775 train_time:937916ms step_avg:805.08ms
step:1166/1775 train_time:938848ms step_avg:805.19ms
step:1167/1775 train_time:939773ms step_avg:805.29ms
step:1168/1775 train_time:940703ms step_avg:805.40ms
step:1169/1775 train_time:941627ms step_avg:805.50ms
step:1170/1775 train_time:942561ms step_avg:805.61ms
step:1171/1775 train_time:943486ms step_avg:805.71ms
step:1172/1775 train_time:944418ms step_avg:805.82ms
step:1173/1775 train_time:945343ms step_avg:805.92ms
step:1174/1775 train_time:946275ms step_avg:806.03ms
step:1175/1775 train_time:947201ms step_avg:806.13ms
step:1176/1775 train_time:948133ms step_avg:806.24ms
step:1177/1775 train_time:949057ms step_avg:806.34ms
step:1178/1775 train_time:949991ms step_avg:806.44ms
step:1179/1775 train_time:950913ms step_avg:806.54ms
step:1180/1775 train_time:951843ms step_avg:806.65ms
step:1181/1775 train_time:952809ms step_avg:806.78ms
step:1182/1775 train_time:953776ms step_avg:806.92ms
step:1183/1775 train_time:954699ms step_avg:807.02ms
step:1184/1775 train_time:955630ms step_avg:807.12ms
step:1185/1775 train_time:956552ms step_avg:807.22ms
step:1186/1775 train_time:957482ms step_avg:807.32ms
step:1187/1775 train_time:958410ms step_avg:807.42ms
step:1188/1775 train_time:959342ms step_avg:807.53ms
step:1189/1775 train_time:960264ms step_avg:807.62ms
step:1190/1775 train_time:961197ms step_avg:807.73ms
step:1191/1775 train_time:962122ms step_avg:807.83ms
step:1192/1775 train_time:963051ms step_avg:807.93ms
step:1193/1775 train_time:963975ms step_avg:808.03ms
step:1194/1775 train_time:964909ms step_avg:808.13ms
step:1195/1775 train_time:965832ms step_avg:808.23ms
step:1196/1775 train_time:966760ms step_avg:808.33ms
step:1197/1775 train_time:967684ms step_avg:808.42ms
step:1198/1775 train_time:968615ms step_avg:808.53ms
step:1199/1775 train_time:969542ms step_avg:808.63ms
step:1200/1775 train_time:970471ms step_avg:808.73ms
step:1201/1775 train_time:971391ms step_avg:808.82ms
step:1202/1775 train_time:972324ms step_avg:808.92ms
step:1203/1775 train_time:973248ms step_avg:809.02ms
step:1204/1775 train_time:974179ms step_avg:809.12ms
step:1205/1775 train_time:975101ms step_avg:809.21ms
step:1206/1775 train_time:976034ms step_avg:809.32ms
step:1207/1775 train_time:976956ms step_avg:809.41ms
step:1208/1775 train_time:977888ms step_avg:809.51ms
step:1209/1775 train_time:978813ms step_avg:809.61ms
step:1210/1775 train_time:979745ms step_avg:809.71ms
step:1211/1775 train_time:980665ms step_avg:809.80ms
step:1212/1775 train_time:981828ms step_avg:810.09ms
step:1213/1775 train_time:982751ms step_avg:810.18ms
step:1214/1775 train_time:983683ms step_avg:810.28ms
step:1215/1775 train_time:984607ms step_avg:810.38ms
step:1216/1775 train_time:985540ms step_avg:810.48ms
step:1217/1775 train_time:986496ms step_avg:810.60ms
step:1218/1775 train_time:987421ms step_avg:810.69ms
step:1219/1775 train_time:988345ms step_avg:810.78ms
step:1220/1775 train_time:989312ms step_avg:810.91ms
step:1221/1775 train_time:990236ms step_avg:811.00ms
step:1222/1775 train_time:991167ms step_avg:811.10ms
step:1223/1775 train_time:992092ms step_avg:811.20ms
step:1224/1775 train_time:993025ms step_avg:811.29ms
step:1225/1775 train_time:993947ms step_avg:811.39ms
step:1226/1775 train_time:994879ms step_avg:811.48ms
step:1227/1775 train_time:995804ms step_avg:811.58ms
step:1228/1775 train_time:996735ms step_avg:811.67ms
step:1229/1775 train_time:997659ms step_avg:811.76ms
step:1230/1775 train_time:998598ms step_avg:811.87ms
step:1231/1775 train_time:999523ms step_avg:811.96ms
step:1232/1775 train_time:1000459ms step_avg:812.06ms
step:1233/1775 train_time:1001377ms step_avg:812.15ms
step:1234/1775 train_time:1002309ms step_avg:812.24ms
step:1235/1775 train_time:1003231ms step_avg:812.33ms
step:1236/1775 train_time:1004166ms step_avg:812.43ms
step:1237/1775 train_time:1005092ms step_avg:812.52ms
step:1238/1775 train_time:1006024ms step_avg:812.62ms
step:1239/1775 train_time:1006947ms step_avg:812.71ms
step:1240/1775 train_time:1007880ms step_avg:812.81ms
step:1241/1775 train_time:1008805ms step_avg:812.90ms
step:1242/1775 train_time:1009742ms step_avg:813.00ms
step:1243/1775 train_time:1010667ms step_avg:813.09ms
step:1244/1775 train_time:1011596ms step_avg:813.18ms
step:1245/1775 train_time:1012523ms step_avg:813.27ms
step:1246/1775 train_time:1013454ms step_avg:813.37ms
step:1247/1775 train_time:1014378ms step_avg:813.45ms
step:1248/1775 train_time:1015309ms step_avg:813.55ms
step:1249/1775 train_time:1016235ms step_avg:813.64ms
step:1250/1775 train_time:1017169ms step_avg:813.74ms
step:1250/1775 val_loss:nan val_malbo_loss:nan train_time:1017231ms step_avg:813.78ms
step:1251/1775 train_time:1018101ms step_avg:813.83ms
step:1252/1775 train_time:1019032ms step_avg:813.92ms
step:1253/1775 train_time:1019952ms step_avg:814.01ms
step:1254/1775 train_time:1020884ms step_avg:814.10ms
step:1255/1775 train_time:1021810ms step_avg:814.19ms
step:1256/1775 train_time:1022740ms step_avg:814.28ms
step:1257/1775 train_time:1023664ms step_avg:814.37ms
step:1258/1775 train_time:1024598ms step_avg:814.47ms
step:1259/1775 train_time:1025520ms step_avg:814.55ms
step:1260/1775 train_time:1026453ms step_avg:814.65ms
step:1261/1775 train_time:1027377ms step_avg:814.73ms
step:1262/1775 train_time:1028308ms step_avg:814.82ms
step:1263/1775 train_time:1029234ms step_avg:814.91ms
step:1264/1775 train_time:1030164ms step_avg:815.00ms
step:1265/1775 train_time:1031087ms step_avg:815.09ms
step:1266/1775 train_time:1032024ms step_avg:815.18ms
step:1267/1775 train_time:1032946ms step_avg:815.27ms
step:1268/1775 train_time:1033895ms step_avg:815.37ms
step:1269/1775 train_time:1034820ms step_avg:815.46ms
step:1270/1775 train_time:1035751ms step_avg:815.55ms
step:1271/1775 train_time:1036673ms step_avg:815.64ms
step:1272/1775 train_time:1037621ms step_avg:815.74ms
step:1273/1775 train_time:1038534ms step_avg:815.82ms
step:1274/1775 train_time:1039467ms step_avg:815.91ms
step:1275/1775 train_time:1040391ms step_avg:815.99ms
step:1276/1775 train_time:1041323ms step_avg:816.08ms
step:1277/1775 train_time:1042242ms step_avg:816.16ms
step:1278/1775 train_time:1043172ms step_avg:816.25ms
step:1279/1775 train_time:1044097ms step_avg:816.34ms
step:1280/1775 train_time:1045027ms step_avg:816.43ms
step:1281/1775 train_time:1045950ms step_avg:816.51ms
step:1282/1775 train_time:1046881ms step_avg:816.60ms
step:1283/1775 train_time:1047801ms step_avg:816.68ms
step:1284/1775 train_time:1048736ms step_avg:816.77ms
step:1285/1775 train_time:1049658ms step_avg:816.85ms
step:1286/1775 train_time:1050589ms step_avg:816.94ms
step:1287/1775 train_time:1051514ms step_avg:817.03ms
step:1288/1775 train_time:1052447ms step_avg:817.12ms
step:1289/1775 train_time:1053369ms step_avg:817.20ms
step:1290/1775 train_time:1054302ms step_avg:817.29ms
step:1291/1775 train_time:1055226ms step_avg:817.37ms
step:1292/1775 train_time:1056157ms step_avg:817.46ms
step:1293/1775 train_time:1057082ms step_avg:817.54ms
step:1294/1775 train_time:1058012ms step_avg:817.63ms
step:1295/1775 train_time:1058936ms step_avg:817.71ms
step:1296/1775 train_time:1059865ms step_avg:817.80ms
step:1297/1775 train_time:1060792ms step_avg:817.88ms
step:1298/1775 train_time:1061722ms step_avg:817.97ms
step:1299/1775 train_time:1062647ms step_avg:818.05ms
step:1300/1775 train_time:1063579ms step_avg:818.14ms
step:1301/1775 train_time:1064516ms step_avg:818.23ms
step:1302/1775 train_time:1065445ms step_avg:818.31ms
step:1303/1775 train_time:1066361ms step_avg:818.39ms
step:1304/1775 train_time:1067299ms step_avg:818.48ms
step:1305/1775 train_time:1068377ms step_avg:818.68ms
step:1306/1775 train_time:1069338ms step_avg:818.79ms
step:1307/1775 train_time:1070273ms step_avg:818.88ms
step:1308/1775 train_time:1071209ms step_avg:818.97ms
step:1309/1775 train_time:1072131ms step_avg:819.05ms
step:1310/1775 train_time:1073063ms step_avg:819.13ms
step:1311/1775 train_time:1073985ms step_avg:819.21ms
step:1312/1775 train_time:1074918ms step_avg:819.30ms
step:1313/1775 train_time:1075843ms step_avg:819.38ms
step:1314/1775 train_time:1076775ms step_avg:819.46ms
step:1315/1775 train_time:1077703ms step_avg:819.55ms
step:1316/1775 train_time:1078638ms step_avg:819.63ms
step:1317/1775 train_time:1079562ms step_avg:819.71ms
step:1318/1775 train_time:1080496ms step_avg:819.80ms
step:1319/1775 train_time:1081421ms step_avg:819.88ms
step:1320/1775 train_time:1082357ms step_avg:819.97ms
step:1321/1775 train_time:1083277ms step_avg:820.04ms
step:1322/1775 train_time:1084207ms step_avg:820.13ms
step:1323/1775 train_time:1085130ms step_avg:820.20ms
step:1324/1775 train_time:1086065ms step_avg:820.29ms
step:1325/1775 train_time:1086988ms step_avg:820.37ms
step:1326/1775 train_time:1087930ms step_avg:820.46ms
step:1327/1775 train_time:1088849ms step_avg:820.53ms
step:1328/1775 train_time:1089782ms step_avg:820.62ms
step:1329/1775 train_time:1090707ms step_avg:820.70ms
step:1330/1775 train_time:1091760ms step_avg:820.87ms
step:1331/1775 train_time:1092661ms step_avg:820.93ms
step:1332/1775 train_time:1093592ms step_avg:821.02ms
step:1333/1775 train_time:1094519ms step_avg:821.09ms
step:1334/1775 train_time:1095449ms step_avg:821.18ms
step:1335/1775 train_time:1096372ms step_avg:821.25ms
step:1336/1775 train_time:1097306ms step_avg:821.34ms
step:1337/1775 train_time:1098230ms step_avg:821.41ms
step:1338/1775 train_time:1099163ms step_avg:821.50ms
step:1339/1775 train_time:1100088ms step_avg:821.57ms
step:1340/1775 train_time:1101027ms step_avg:821.66ms
step:1341/1775 train_time:1101978ms step_avg:821.76ms
step:1342/1775 train_time:1102910ms step_avg:821.84ms
step:1343/1775 train_time:1103835ms step_avg:821.92ms
step:1344/1775 train_time:1104765ms step_avg:822.00ms
step:1345/1775 train_time:1105688ms step_avg:822.07ms
step:1346/1775 train_time:1106622ms step_avg:822.16ms
step:1347/1775 train_time:1107546ms step_avg:822.23ms
step:1348/1775 train_time:1108478ms step_avg:822.31ms
step:1349/1775 train_time:1109404ms step_avg:822.39ms
step:1350/1775 train_time:1110338ms step_avg:822.47ms
step:1351/1775 train_time:1111258ms step_avg:822.54ms
step:1352/1775 train_time:1112191ms step_avg:822.63ms
step:1353/1775 train_time:1113118ms step_avg:822.70ms
step:1354/1775 train_time:1114050ms step_avg:822.78ms
step:1355/1775 train_time:1114975ms step_avg:822.86ms
step:1356/1775 train_time:1115908ms step_avg:822.94ms
step:1357/1775 train_time:1116834ms step_avg:823.02ms
step:1358/1775 train_time:1117773ms step_avg:823.10ms
step:1359/1775 train_time:1118696ms step_avg:823.18ms
step:1360/1775 train_time:1119634ms step_avg:823.26ms
step:1361/1775 train_time:1120557ms step_avg:823.33ms
step:1362/1775 train_time:1121491ms step_avg:823.41ms
step:1363/1775 train_time:1122418ms step_avg:823.49ms
step:1364/1775 train_time:1123349ms step_avg:823.57ms
step:1365/1775 train_time:1124274ms step_avg:823.64ms
step:1366/1775 train_time:1125205ms step_avg:823.72ms
step:1367/1775 train_time:1126129ms step_avg:823.80ms
step:1368/1775 train_time:1127064ms step_avg:823.88ms
step:1369/1775 train_time:1127989ms step_avg:823.95ms
step:1370/1775 train_time:1128932ms step_avg:824.04ms
step:1371/1775 train_time:1129853ms step_avg:824.11ms
step:1372/1775 train_time:1130787ms step_avg:824.19ms
step:1373/1775 train_time:1131707ms step_avg:824.26ms
step:1374/1775 train_time:1132639ms step_avg:824.34ms
step:1375/1775 train_time:1133565ms step_avg:824.41ms
step:1376/1775 train_time:1134496ms step_avg:824.49ms
step:1377/1775 train_time:1135485ms step_avg:824.61ms
step:1378/1775 train_time:1136425ms step_avg:824.69ms
step:1379/1775 train_time:1137350ms step_avg:824.76ms
step:1380/1775 train_time:1138286ms step_avg:824.85ms
step:1381/1775 train_time:1139212ms step_avg:824.92ms
step:1382/1775 train_time:1140146ms step_avg:825.00ms
step:1383/1775 train_time:1141070ms step_avg:825.07ms
step:1384/1775 train_time:1142005ms step_avg:825.15ms
step:1385/1775 train_time:1142930ms step_avg:825.22ms
step:1386/1775 train_time:1143863ms step_avg:825.30ms
step:1387/1775 train_time:1144787ms step_avg:825.37ms
step:1388/1775 train_time:1145726ms step_avg:825.45ms
step:1389/1775 train_time:1146649ms step_avg:825.52ms
step:1390/1775 train_time:1147582ms step_avg:825.60ms
step:1391/1775 train_time:1148508ms step_avg:825.67ms
step:1392/1775 train_time:1149443ms step_avg:825.75ms
step:1393/1775 train_time:1150369ms step_avg:825.82ms
step:1394/1775 train_time:1151302ms step_avg:825.90ms
step:1395/1775 train_time:1152229ms step_avg:825.97ms
step:1396/1775 train_time:1153163ms step_avg:826.05ms
step:1397/1775 train_time:1154088ms step_avg:826.12ms
step:1398/1775 train_time:1155024ms step_avg:826.20ms
step:1399/1775 train_time:1155948ms step_avg:826.27ms
step:1400/1775 train_time:1156880ms step_avg:826.34ms
step:1401/1775 train_time:1157812ms step_avg:826.42ms
step:1402/1775 train_time:1158747ms step_avg:826.50ms
step:1403/1775 train_time:1159672ms step_avg:826.57ms
step:1404/1775 train_time:1160606ms step_avg:826.64ms
step:1405/1775 train_time:1161527ms step_avg:826.71ms
step:1406/1775 train_time:1162469ms step_avg:826.79ms
step:1407/1775 train_time:1163391ms step_avg:826.86ms
step:1408/1775 train_time:1164329ms step_avg:826.94ms
step:1409/1775 train_time:1165250ms step_avg:827.00ms
step:1410/1775 train_time:1166187ms step_avg:827.08ms
step:1411/1775 train_time:1167106ms step_avg:827.15ms
step:1412/1775 train_time:1168049ms step_avg:827.23ms
step:1413/1775 train_time:1169086ms step_avg:827.38ms
step:1414/1775 train_time:1170009ms step_avg:827.45ms
step:1415/1775 train_time:1170933ms step_avg:827.51ms
step:1416/1775 train_time:1171865ms step_avg:827.59ms
step:1417/1775 train_time:1172791ms step_avg:827.66ms
step:1418/1775 train_time:1173728ms step_avg:827.74ms
step:1419/1775 train_time:1174651ms step_avg:827.80ms
step:1420/1775 train_time:1175587ms step_avg:827.88ms
step:1421/1775 train_time:1176513ms step_avg:827.95ms
step:1422/1775 train_time:1177447ms step_avg:828.02ms
step:1423/1775 train_time:1178373ms step_avg:828.09ms
step:1424/1775 train_time:1179598ms step_avg:828.37ms
step:1425/1775 train_time:1180522ms step_avg:828.44ms
step:1426/1775 train_time:1181454ms step_avg:828.51ms
step:1427/1775 train_time:1182380ms step_avg:828.58ms
step:1428/1775 train_time:1183316ms step_avg:828.65ms
step:1429/1775 train_time:1184243ms step_avg:828.72ms
step:1430/1775 train_time:1185176ms step_avg:828.79ms
step:1431/1775 train_time:1186102ms step_avg:828.86ms
step:1432/1775 train_time:1187037ms step_avg:828.94ms
step:1433/1775 train_time:1187962ms step_avg:829.00ms
step:1434/1775 train_time:1188900ms step_avg:829.08ms
step:1435/1775 train_time:1189820ms step_avg:829.14ms
step:1436/1775 train_time:1190756ms step_avg:829.22ms
step:1437/1775 train_time:1191682ms step_avg:829.28ms
step:1438/1775 train_time:1192621ms step_avg:829.36ms
step:1439/1775 train_time:1193544ms step_avg:829.43ms
step:1440/1775 train_time:1194477ms step_avg:829.50ms
step:1441/1775 train_time:1195402ms step_avg:829.56ms
step:1442/1775 train_time:1196334ms step_avg:829.64ms
step:1443/1775 train_time:1197264ms step_avg:829.71ms
step:1444/1775 train_time:1198201ms step_avg:829.78ms
step:1445/1775 train_time:1199129ms step_avg:829.85ms
step:1446/1775 train_time:1200065ms step_avg:829.92ms
step:1447/1775 train_time:1200988ms step_avg:829.98ms
step:1448/1775 train_time:1201999ms step_avg:830.11ms
step:1449/1775 train_time:1202922ms step_avg:830.17ms
step:1450/1775 train_time:1203855ms step_avg:830.25ms
step:1451/1775 train_time:1204786ms step_avg:830.31ms
step:1452/1775 train_time:1205718ms step_avg:830.38ms
step:1453/1775 train_time:1206645ms step_avg:830.45ms
step:1454/1775 train_time:1207579ms step_avg:830.52ms
step:1455/1775 train_time:1208505ms step_avg:830.59ms
step:1456/1775 train_time:1209439ms step_avg:830.66ms
step:1457/1775 train_time:1210366ms step_avg:830.72ms
step:1458/1775 train_time:1211302ms step_avg:830.80ms
step:1459/1775 train_time:1212230ms step_avg:830.86ms
step:1460/1775 train_time:1213161ms step_avg:830.93ms
step:1461/1775 train_time:1214083ms step_avg:830.99ms
step:1462/1775 train_time:1215019ms step_avg:831.07ms
step:1463/1775 train_time:1215947ms step_avg:831.13ms
step:1464/1775 train_time:1216880ms step_avg:831.20ms
step:1465/1775 train_time:1217806ms step_avg:831.27ms
step:1466/1775 train_time:1218742ms step_avg:831.34ms
step:1467/1775 train_time:1219667ms step_avg:831.40ms
step:1468/1775 train_time:1220602ms step_avg:831.47ms
step:1469/1775 train_time:1221527ms step_avg:831.54ms
step:1470/1775 train_time:1222463ms step_avg:831.61ms
step:1471/1775 train_time:1223388ms step_avg:831.67ms
step:1472/1775 train_time:1224323ms step_avg:831.74ms
step:1473/1775 train_time:1225248ms step_avg:831.80ms
step:1474/1775 train_time:1226179ms step_avg:831.87ms
step:1475/1775 train_time:1227103ms step_avg:831.93ms
step:1476/1775 train_time:1228039ms step_avg:832.00ms
step:1477/1775 train_time:1228968ms step_avg:832.07ms
step:1478/1775 train_time:1229901ms step_avg:832.14ms
step:1479/1775 train_time:1230828ms step_avg:832.20ms
step:1480/1775 train_time:1231762ms step_avg:832.27ms
step:1481/1775 train_time:1232688ms step_avg:832.33ms
step:1482/1775 train_time:1233622ms step_avg:832.40ms
step:1483/1775 train_time:1234553ms step_avg:832.47ms
step:1484/1775 train_time:1235577ms step_avg:832.60ms
step:1485/1775 train_time:1236506ms step_avg:832.66ms
step:1486/1775 train_time:1237440ms step_avg:832.73ms
step:1487/1775 train_time:1238367ms step_avg:832.80ms
step:1488/1775 train_time:1239304ms step_avg:832.87ms
step:1489/1775 train_time:1240228ms step_avg:832.93ms
step:1490/1775 train_time:1241164ms step_avg:833.00ms
step:1491/1775 train_time:1242086ms step_avg:833.06ms
step:1492/1775 train_time:1243028ms step_avg:833.13ms
step:1493/1775 train_time:1243949ms step_avg:833.19ms
step:1494/1775 train_time:1244884ms step_avg:833.26ms
step:1495/1775 train_time:1245809ms step_avg:833.32ms
step:1496/1775 train_time:1246743ms step_avg:833.38ms
step:1497/1775 train_time:1247669ms step_avg:833.45ms
step:1498/1775 train_time:1248605ms step_avg:833.51ms
step:1499/1775 train_time:1249536ms step_avg:833.58ms
step:1500/1775 train_time:1250471ms step_avg:833.65ms
step:1500/1775 val_loss:nan val_malbo_loss:nan train_time:1250527ms step_avg:833.68ms
step:1501/1775 train_time:1251397ms step_avg:833.71ms
step:1502/1775 train_time:1252332ms step_avg:833.78ms
step:1503/1775 train_time:1253258ms step_avg:833.84ms
step:1504/1775 train_time:1254192ms step_avg:833.90ms
step:1505/1775 train_time:1255120ms step_avg:833.97ms
step:1506/1775 train_time:1256054ms step_avg:834.03ms
step:1507/1775 train_time:1256982ms step_avg:834.10ms
step:1508/1775 train_time:1257914ms step_avg:834.16ms
step:1509/1775 train_time:1258839ms step_avg:834.22ms
step:1510/1775 train_time:1259775ms step_avg:834.29ms
step:1511/1775 train_time:1260700ms step_avg:834.35ms
step:1512/1775 train_time:1261709ms step_avg:834.46ms
step:1513/1775 train_time:1262630ms step_avg:834.52ms
step:1514/1775 train_time:1263565ms step_avg:834.59ms
step:1515/1775 train_time:1264492ms step_avg:834.65ms
step:1516/1775 train_time:1265431ms step_avg:834.72ms
step:1517/1775 train_time:1266353ms step_avg:834.77ms
step:1518/1775 train_time:1267291ms step_avg:834.84ms
step:1519/1775 train_time:1268216ms step_avg:834.90ms
step:1520/1775 train_time:1269154ms step_avg:834.97ms
step:1521/1775 train_time:1270082ms step_avg:835.03ms
step:1522/1775 train_time:1271017ms step_avg:835.10ms
step:1523/1775 train_time:1271945ms step_avg:835.16ms
step:1524/1775 train_time:1272882ms step_avg:835.22ms
step:1525/1775 train_time:1273801ms step_avg:835.28ms
step:1526/1775 train_time:1274724ms step_avg:835.34ms
step:1527/1775 train_time:1275661ms step_avg:835.40ms
step:1528/1775 train_time:1276594ms step_avg:835.47ms
step:1529/1775 train_time:1277523ms step_avg:835.53ms
step:1530/1775 train_time:1278456ms step_avg:835.59ms
step:1531/1775 train_time:1279383ms step_avg:835.65ms
step:1532/1775 train_time:1280322ms step_avg:835.72ms
step:1533/1775 train_time:1281250ms step_avg:835.78ms
step:1534/1775 train_time:1282183ms step_avg:835.84ms
step:1535/1775 train_time:1283112ms step_avg:835.90ms
step:1536/1775 train_time:1284050ms step_avg:835.97ms
step:1537/1775 train_time:1284972ms step_avg:836.03ms
step:1538/1775 train_time:1285904ms step_avg:836.09ms
step:1539/1775 train_time:1286830ms step_avg:836.15ms
step:1540/1775 train_time:1287764ms step_avg:836.21ms
step:1541/1775 train_time:1288692ms step_avg:836.27ms
step:1542/1775 train_time:1289625ms step_avg:836.33ms
step:1543/1775 train_time:1290552ms step_avg:836.39ms
step:1544/1775 train_time:1291487ms step_avg:836.46ms
step:1545/1775 train_time:1292415ms step_avg:836.51ms
step:1546/1775 train_time:1293348ms step_avg:836.58ms
step:1547/1775 train_time:1294306ms step_avg:836.66ms
step:1548/1775 train_time:1295257ms step_avg:836.73ms
step:1549/1775 train_time:1296182ms step_avg:836.79ms
step:1550/1775 train_time:1297117ms step_avg:836.85ms
step:1551/1775 train_time:1298044ms step_avg:836.91ms
step:1552/1775 train_time:1298981ms step_avg:836.97ms
step:1553/1775 train_time:1299906ms step_avg:837.03ms
step:1554/1775 train_time:1300839ms step_avg:837.09ms
step:1555/1775 train_time:1301768ms step_avg:837.15ms
step:1556/1775 train_time:1302702ms step_avg:837.21ms
step:1557/1775 train_time:1303627ms step_avg:837.27ms
step:1558/1775 train_time:1304563ms step_avg:837.33ms
step:1559/1775 train_time:1305488ms step_avg:837.39ms
step:1560/1775 train_time:1306426ms step_avg:837.45ms
step:1561/1775 train_time:1307350ms step_avg:837.51ms
step:1562/1775 train_time:1308284ms step_avg:837.57ms
step:1563/1775 train_time:1309213ms step_avg:837.63ms
step:1564/1775 train_time:1310150ms step_avg:837.69ms
step:1565/1775 train_time:1311078ms step_avg:837.75ms
step:1566/1775 train_time:1312016ms step_avg:837.81ms
step:1567/1775 train_time:1312940ms step_avg:837.87ms
step:1568/1775 train_time:1313872ms step_avg:837.93ms
step:1569/1775 train_time:1314800ms step_avg:837.99ms
step:1570/1775 train_time:1315734ms step_avg:838.05ms
step:1571/1775 train_time:1316663ms step_avg:838.10ms
step:1572/1775 train_time:1317595ms step_avg:838.17ms
step:1573/1775 train_time:1318521ms step_avg:838.22ms
step:1574/1775 train_time:1319458ms step_avg:838.28ms
step:1575/1775 train_time:1320390ms step_avg:838.34ms
step:1576/1775 train_time:1321324ms step_avg:838.40ms
step:1577/1775 train_time:1322249ms step_avg:838.46ms
step:1578/1775 train_time:1323180ms step_avg:838.52ms
step:1579/1775 train_time:1324109ms step_avg:838.57ms
step:1580/1775 train_time:1325052ms step_avg:838.64ms
step:1581/1775 train_time:1325969ms step_avg:838.69ms
step:1582/1775 train_time:1326904ms step_avg:838.75ms
step:1583/1775 train_time:1327910ms step_avg:838.86ms
step:1584/1775 train_time:1328832ms step_avg:838.91ms
step:1585/1775 train_time:1329760ms step_avg:838.97ms
step:1586/1775 train_time:1330695ms step_avg:839.03ms
step:1587/1775 train_time:1331621ms step_avg:839.08ms
step:1588/1775 train_time:1332555ms step_avg:839.14ms
step:1589/1775 train_time:1333482ms step_avg:839.20ms
step:1590/1775 train_time:1334418ms step_avg:839.26ms
step:1591/1775 train_time:1335343ms step_avg:839.31ms
step:1592/1775 train_time:1336280ms step_avg:839.37ms
step:1593/1775 train_time:1337206ms step_avg:839.43ms
step:1594/1775 train_time:1338142ms step_avg:839.49ms
step:1595/1775 train_time:1339070ms step_avg:839.54ms
step:1596/1775 train_time:1340002ms step_avg:839.60ms
step:1597/1775 train_time:1340927ms step_avg:839.65ms
step:1598/1775 train_time:1341863ms step_avg:839.71ms
step:1599/1775 train_time:1342791ms step_avg:839.77ms
step:1600/1775 train_time:1343725ms step_avg:839.83ms
step:1601/1775 train_time:1344655ms step_avg:839.88ms
step:1602/1775 train_time:1345587ms step_avg:839.94ms
step:1603/1775 train_time:1346511ms step_avg:839.99ms
step:1604/1775 train_time:1347448ms step_avg:840.05ms
step:1605/1775 train_time:1348372ms step_avg:840.11ms
step:1606/1775 train_time:1349314ms step_avg:840.17ms
step:1607/1775 train_time:1350239ms step_avg:840.22ms
step:1608/1775 train_time:1351175ms step_avg:840.28ms
step:1609/1775 train_time:1352103ms step_avg:840.34ms
step:1610/1775 train_time:1353041ms step_avg:840.40ms
step:1611/1775 train_time:1353961ms step_avg:840.45ms
step:1612/1775 train_time:1354893ms step_avg:840.50ms
step:1613/1775 train_time:1355820ms step_avg:840.56ms
step:1614/1775 train_time:1356756ms step_avg:840.62ms
step:1615/1775 train_time:1357684ms step_avg:840.67ms
step:1616/1775 train_time:1358617ms step_avg:840.73ms
step:1617/1775 train_time:1359543ms step_avg:840.78ms
step:1618/1775 train_time:1360486ms step_avg:840.84ms
step:1619/1775 train_time:1361498ms step_avg:840.95ms
step:1620/1775 train_time:1362439ms step_avg:841.01ms
step:1621/1775 train_time:1363365ms step_avg:841.06ms
step:1622/1775 train_time:1364299ms step_avg:841.12ms
step:1623/1775 train_time:1365227ms step_avg:841.17ms
step:1624/1775 train_time:1366158ms step_avg:841.23ms
step:1625/1775 train_time:1367088ms step_avg:841.28ms
step:1626/1775 train_time:1368021ms step_avg:841.34ms
step:1627/1775 train_time:1368946ms step_avg:841.39ms
step:1628/1775 train_time:1369881ms step_avg:841.45ms
step:1629/1775 train_time:1370809ms step_avg:841.50ms
step:1630/1775 train_time:1371746ms step_avg:841.56ms
step:1631/1775 train_time:1372673ms step_avg:841.61ms
step:1632/1775 train_time:1373609ms step_avg:841.67ms
step:1633/1775 train_time:1374535ms step_avg:841.72ms
step:1634/1775 train_time:1375469ms step_avg:841.78ms
step:1635/1775 train_time:1376574ms step_avg:841.94ms
step:1636/1775 train_time:1377503ms step_avg:841.99ms
step:1637/1775 train_time:1378431ms step_avg:842.05ms
step:1638/1775 train_time:1379364ms step_avg:842.10ms
step:1639/1775 train_time:1380292ms step_avg:842.15ms
step:1640/1775 train_time:1381232ms step_avg:842.21ms
step:1641/1775 train_time:1382156ms step_avg:842.26ms
step:1642/1775 train_time:1383092ms step_avg:842.32ms
step:1643/1775 train_time:1384014ms step_avg:842.37ms
step:1644/1775 train_time:1384950ms step_avg:842.43ms
step:1645/1775 train_time:1385875ms step_avg:842.48ms
step:1646/1775 train_time:1386812ms step_avg:842.53ms
step:1647/1775 train_time:1387737ms step_avg:842.58ms
step:1648/1775 train_time:1388672ms step_avg:842.64ms
step:1649/1775 train_time:1389601ms step_avg:842.69ms
step:1650/1775 train_time:1390536ms step_avg:842.75ms
step:1651/1775 train_time:1391463ms step_avg:842.80ms
step:1652/1775 train_time:1392399ms step_avg:842.86ms
step:1653/1775 train_time:1393321ms step_avg:842.90ms
step:1654/1775 train_time:1394260ms step_avg:842.96ms
step:1655/1775 train_time:1395253ms step_avg:843.05ms
step:1656/1775 train_time:1396162ms step_avg:843.09ms
step:1657/1775 train_time:1397089ms step_avg:843.14ms
step:1658/1775 train_time:1398026ms step_avg:843.20ms
step:1659/1775 train_time:1398953ms step_avg:843.25ms
step:1660/1775 train_time:1399887ms step_avg:843.31ms
step:1661/1775 train_time:1400817ms step_avg:843.36ms
step:1662/1775 train_time:1401750ms step_avg:843.41ms
step:1663/1775 train_time:1402673ms step_avg:843.46ms
step:1664/1775 train_time:1403612ms step_avg:843.52ms
step:1665/1775 train_time:1404539ms step_avg:843.57ms
step:1666/1775 train_time:1405476ms step_avg:843.62ms
step:1667/1775 train_time:1406401ms step_avg:843.67ms
step:1668/1775 train_time:1407336ms step_avg:843.73ms
step:1669/1775 train_time:1408260ms step_avg:843.77ms
step:1670/1775 train_time:1409195ms step_avg:843.83ms
step:1671/1775 train_time:1410123ms step_avg:843.88ms
step:1672/1775 train_time:1411055ms step_avg:843.93ms
step:1673/1775 train_time:1411981ms step_avg:843.98ms
step:1674/1775 train_time:1412915ms step_avg:844.04ms
step:1675/1775 train_time:1413843ms step_avg:844.09ms
step:1676/1775 train_time:1414775ms step_avg:844.14ms
step:1677/1775 train_time:1415701ms step_avg:844.19ms
step:1678/1775 train_time:1416635ms step_avg:844.24ms
step:1679/1775 train_time:1417558ms step_avg:844.29ms
step:1680/1775 train_time:1418495ms step_avg:844.34ms
step:1681/1775 train_time:1419422ms step_avg:844.39ms
step:1682/1775 train_time:1420359ms step_avg:844.45ms
step:1683/1775 train_time:1421286ms step_avg:844.50ms
step:1684/1775 train_time:1422219ms step_avg:844.55ms
step:1685/1775 train_time:1423145ms step_avg:844.60ms
step:1686/1775 train_time:1424079ms step_avg:844.65ms
step:1687/1775 train_time:1425006ms step_avg:844.70ms
step:1688/1775 train_time:1425938ms step_avg:844.75ms
step:1689/1775 train_time:1426865ms step_avg:844.80ms
step:1690/1775 train_time:1427810ms step_avg:844.86ms
step:1691/1775 train_time:1428769ms step_avg:844.93ms
step:1692/1775 train_time:1429704ms step_avg:844.98ms
step:1693/1775 train_time:1430630ms step_avg:845.03ms
step:1694/1775 train_time:1431561ms step_avg:845.08ms
step:1695/1775 train_time:1432488ms step_avg:845.13ms
step:1696/1775 train_time:1433421ms step_avg:845.18ms
step:1697/1775 train_time:1434348ms step_avg:845.23ms
step:1698/1775 train_time:1435280ms step_avg:845.28ms
step:1699/1775 train_time:1436210ms step_avg:845.33ms
step:1700/1775 train_time:1437143ms step_avg:845.38ms
step:1701/1775 train_time:1438070ms step_avg:845.43ms
step:1702/1775 train_time:1439004ms step_avg:845.48ms
step:1703/1775 train_time:1439931ms step_avg:845.53ms
step:1704/1775 train_time:1440871ms step_avg:845.58ms
step:1705/1775 train_time:1441792ms step_avg:845.63ms
step:1706/1775 train_time:1442731ms step_avg:845.68ms
step:1707/1775 train_time:1443656ms step_avg:845.73ms
step:1708/1775 train_time:1444592ms step_avg:845.78ms
step:1709/1775 train_time:1445518ms step_avg:845.83ms
step:1710/1775 train_time:1446451ms step_avg:845.88ms
step:1711/1775 train_time:1447377ms step_avg:845.92ms
step:1712/1775 train_time:1448315ms step_avg:845.98ms
step:1713/1775 train_time:1449238ms step_avg:846.02ms
step:1714/1775 train_time:1450174ms step_avg:846.08ms
step:1715/1775 train_time:1451099ms step_avg:846.12ms
step:1716/1775 train_time:1452035ms step_avg:846.17ms
step:1717/1775 train_time:1452961ms step_avg:846.22ms
step:1718/1775 train_time:1453895ms step_avg:846.27ms
step:1719/1775 train_time:1454824ms step_avg:846.32ms
step:1720/1775 train_time:1455757ms step_avg:846.37ms
step:1721/1775 train_time:1456686ms step_avg:846.42ms
step:1722/1775 train_time:1457621ms step_avg:846.47ms
step:1723/1775 train_time:1458547ms step_avg:846.52ms
step:1724/1775 train_time:1459479ms step_avg:846.57ms
step:1725/1775 train_time:1460405ms step_avg:846.61ms
step:1726/1775 train_time:1461387ms step_avg:846.69ms
step:1727/1775 train_time:1462317ms step_avg:846.74ms
step:1728/1775 train_time:1463249ms step_avg:846.79ms
step:1729/1775 train_time:1464176ms step_avg:846.83ms
step:1730/1775 train_time:1465111ms step_avg:846.89ms
step:1731/1775 train_time:1466039ms step_avg:846.93ms
step:1732/1775 train_time:1466974ms step_avg:846.98ms
step:1733/1775 train_time:1467899ms step_avg:847.03ms
step:1734/1775 train_time:1468831ms step_avg:847.08ms
step:1735/1775 train_time:1469766ms step_avg:847.13ms
step:1736/1775 train_time:1596315ms step_avg:919.54ms
step:1737/1775 train_time:1597238ms step_avg:919.54ms
step:1738/1775 train_time:1598169ms step_avg:919.54ms
step:1739/1775 train_time:1599093ms step_avg:919.55ms
step:1740/1775 train_time:1600033ms step_avg:919.56ms
step:1741/1775 train_time:1600958ms step_avg:919.56ms
step:1742/1775 train_time:1601895ms step_avg:919.57ms
step:1743/1775 train_time:1602823ms step_avg:919.58ms
step:1744/1775 train_time:1603762ms step_avg:919.59ms
step:1745/1775 train_time:1604679ms step_avg:919.59ms
step:1746/1775 train_time:1605615ms step_avg:919.60ms
step:1747/1775 train_time:1606541ms step_avg:919.60ms
step:1748/1775 train_time:1607477ms step_avg:919.61ms
step:1749/1775 train_time:1608405ms step_avg:919.61ms
step:1750/1775 train_time:1609338ms step_avg:919.62ms
step:1750/1775 val_loss:nan val_malbo_loss:nan train_time:1609397ms step_avg:919.66ms
step:1751/1775 train_time:1610267ms step_avg:919.63ms
step:1752/1775 train_time:1611199ms step_avg:919.63ms
step:1753/1775 train_time:1612127ms step_avg:919.64ms
step:1754/1775 train_time:1613059ms step_avg:919.65ms
step:1755/1775 train_time:1613986ms step_avg:919.65ms
step:1756/1775 train_time:1614922ms step_avg:919.66ms
step:1757/1775 train_time:1615847ms step_avg:919.66ms
step:1758/1775 train_time:1616781ms step_avg:919.67ms
step:1759/1775 train_time:1617707ms step_avg:919.67ms
step:1760/1775 train_time:1618640ms step_avg:919.68ms
step:1761/1775 train_time:1619566ms step_avg:919.69ms
step:1762/1775 train_time:1620502ms step_avg:919.69ms
step:1763/1775 train_time:1621428ms step_avg:919.70ms
step:1764/1775 train_time:1622363ms step_avg:919.71ms
step:1765/1775 train_time:1623290ms step_avg:919.71ms
step:1766/1775 train_time:1624227ms step_avg:919.72ms
step:1767/1775 train_time:1625152ms step_avg:919.72ms
step:1768/1775 train_time:1626085ms step_avg:919.73ms
step:1769/1775 train_time:1627012ms step_avg:919.74ms
step:1770/1775 train_time:1627946ms step_avg:919.74ms
step:1771/1775 train_time:1628872ms step_avg:919.75ms
step:1772/1775 train_time:1629813ms step_avg:919.76ms
step:1773/1775 train_time:1630730ms step_avg:919.76ms
step:1774/1775 train_time:1631670ms step_avg:919.77ms
step:1775/1775 train_time:1632594ms step_avg:919.77ms
