import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

from malbo import compute_malbo_parameters

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from triton.tools.tensor_descriptor import TensorDescriptor
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

@triton.jit
def linear_relu_square_kernel(a_desc, b_desc, c_desc, aux_desc,
                                 M, N, K,
                                 BLOCK_SIZE_M: tl.constexpr,
                                 BLOCK_SIZE_N: tl.constexpr,
                                 BLOCK_SIZE_K: tl.constexpr,
                                 GROUP_SIZE_M: tl.constexpr,
                                 NUM_SMS: tl.constexpr,
                                 FORWARD: tl.constexpr,
                                 ):
    dtype = tl.bfloat16
    start_pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)
    num_tiles = num_pid_m * num_pid_n

    tile_id_c = start_pid - NUM_SMS
    num_pid_in_group = GROUP_SIZE_M * num_pid_n

    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True):
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am = pid_m * BLOCK_SIZE_M
        offs_bn = pid_n * BLOCK_SIZE_N

        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for ki in range(k_tiles):
            offs_k = ki * BLOCK_SIZE_K
            a = a_desc.load([offs_am, offs_k])
            b = b_desc.load([offs_bn, offs_k])
            accumulator = tl.dot(a, b.T, accumulator)

        tile_id_c += NUM_SMS
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am_c = pid_m * BLOCK_SIZE_M
        offs_bn_c = pid_n * BLOCK_SIZE_N

        acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))
        acc = tl.permute(acc, (0, 2, 1))
        acc0, acc1 = tl.split(acc)

        c0 = acc0.to(dtype)
        if not FORWARD:
            c0_pre = aux_desc.load([offs_am_c, offs_bn_c])
            c0 = 2 * c0 * tl.where(c0_pre > 0, c0_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c], c0)

        if FORWARD:
            c0_post = tl.maximum(c0, 0)
            c0_post = c0_post * c0_post
            aux_desc.store([offs_am_c, offs_bn_c], c0_post)

        c1 = acc1.to(dtype)
        if not FORWARD:
            c1_pre = aux_desc.load([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2])
            c1 = 2 * c1 * tl.where(c1_pre > 0, c1_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1)

        if FORWARD:
            c1_post = tl.maximum(c1, 0)
            c1_post = c1_post * c1_post
            aux_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1_post)


def linear_relu_square(a, b, aux=None):
    M, K = a.shape
    N, K = b.shape
    dtype = a.dtype

    c = torch.empty((M, N), device=a.device, dtype=dtype)

    FORWARD = False
    if aux is None:
        FORWARD = True
        aux = torch.empty((M, N), device=a.device, dtype=dtype)

    NUM_SMS = torch.cuda.get_device_properties("cuda").multi_processor_count

    BLOCK_SIZE_M = 128
    BLOCK_SIZE_N = 256
    BLOCK_SIZE_K = 64
    num_stages = 4 if FORWARD else 3
    num_warps = 8

    a_desc = TensorDescriptor.from_tensor(a, [BLOCK_SIZE_M, BLOCK_SIZE_K])
    b_desc = TensorDescriptor.from_tensor(b, [BLOCK_SIZE_N, BLOCK_SIZE_K])
    c_desc = TensorDescriptor.from_tensor(c, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])
    aux_desc = TensorDescriptor.from_tensor(aux, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])

    def grid(META):
        return (min(
            NUM_SMS,
            triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),
        ), )

    linear_relu_square_kernel[grid](
        a_desc, b_desc, c_desc, aux_desc,
        M, N, K,
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=1,
        NUM_SMS=NUM_SMS,
        FORWARD=FORWARD,
        num_stages=num_stages,
        num_warps=num_warps
    )

    if FORWARD:
        return c, aux
    else:
        return c

class FusedLinearReLUSquareFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, W1, W2):
        pre, post = linear_relu_square(x.view((-1, x.shape[-1])), W1)
        x3 = post @ W2
        ctx.save_for_backward(x, W1, W2, pre, post)
        return x3.view(x.shape)

    @staticmethod
    def backward(ctx, grad_output):
        x, W1, W2, pre, post = ctx.saved_tensors
        dW2 = post.T @ grad_output
        dpre = linear_relu_square(grad_output.view((-1, grad_output.shape[-1])), W2, aux=pre)
        dW1 = dpre.T @ x
        dx = dpre @ W1
        return dx.view(x.shape), dW1, dW2

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        # relu(x)^2:
        # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977

        # This call computes relu(x @ W1.T)^2 @ W2.T
        return FusedLinearReLUSquareFunction.apply(x, self.c_fc, self.c_proj)


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# Fused Softcapped Cross Entropy

@triton.jit
def fused_softcapped_entropy_fwd_kernel(
    logits_ptr, losses_ptr, lse_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)
    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    
    max_val = -float('inf')
    sum_exp = 0.0
    
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=-float('inf')).to(tl.float32)
        z = A * tl.sigmoid((val + B) / C)
        z = tl.where(mask, z, -float('inf'))
        curr_max = tl.max(z, axis=0)
        new_max = tl.maximum(max_val, curr_max)
        sum_exp = sum_exp * tl.exp(max_val - new_max) + tl.sum(tl.exp(z - new_max), axis=0)
        max_val = new_max
    
    lse = max_val + tl.log(sum_exp)
    tl.store(lse_ptr + row_idx, lse)
    
    total_loss = 0.0
    for k in range(n_predict):
        target_idx = row_idx + k
        if target_idx < n_rows:
            weight = tl.load(mtp_weights_ptr + k)
            if weight > 0:
                target = tl.load(targets_ptr + target_idx).to(tl.int32)
                if target >= 0 and target < n_cols:
                    val_target = tl.load(logits_row_ptr + target).to(tl.float32)
                    z_target = A * tl.sigmoid((val_target + B) / C)
                    total_loss += weight * (lse - z_target)
    
    tl.store(losses_ptr + row_idx, total_loss)

@triton.jit
def fused_softcapped_entropy_bwd_kernel(
    grad_input_ptr, grad_output_ptr, lse_ptr, logits_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v, stride_grad_n, stride_grad_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)

    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    grad_row_ptr = grad_input_ptr + row_idx * stride_grad_n
    
    lse = tl.load(lse_ptr + row_idx)
    grad_loss = tl.load(grad_output_ptr + row_idx)
    
    S_w = 0.0
    for k in range(n_predict):
        if row_idx + k < n_rows:
            S_w += tl.load(mtp_weights_ptr + k)
            
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=0.0).to(tl.float32)
        u = (val + B) / C
        sigmoid_u = tl.sigmoid(u)
        z = A * sigmoid_u
        p = tl.exp(z - lse)
        
        term1 = S_w * p
        term2 = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
        for k in range(n_predict):
            if row_idx + k < n_rows:
                target = tl.load(targets_ptr + row_idx + k).to(tl.int32)
                weight = tl.load(mtp_weights_ptr + k)
                term2 += tl.where(cols == target, weight, 0.0)
        
        grad_z = grad_loss * (term1 - term2)
        dz_dx = (1.0 / C) * z * (1.0 - sigmoid_u)
        grad_x = grad_z * dz_dx
        tl.store(grad_row_ptr + cols, grad_x.to(tl.bfloat16), mask=mask)

class FusedSoftcappedCrossEntropy(torch.autograd.Function):
    @staticmethod
    def forward(ctx, logits, targets, mtp_weights, A=23.0, B=5.0, C=7.5):
        n_rows, n_cols = logits.shape
        if mtp_weights is None:
             mtp_weights = torch.tensor([1.0], device=logits.device, dtype=torch.float32)
        n_predict = mtp_weights.shape[0]

        losses = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        lse = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        
        logits = logits.contiguous()
        targets = targets.contiguous()
        mtp_weights = mtp_weights.contiguous()

        grid = (n_rows,)
        fused_softcapped_entropy_fwd_kernel[grid](
            logits, losses, lse, targets, mtp_weights,
            logits.stride(0), logits.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        
        ctx.save_for_backward(logits, targets, mtp_weights, lse)
        ctx.params = (A, B, C)
        return losses

    @staticmethod
    def backward(ctx, grad_output):
        logits, targets, mtp_weights, lse = ctx.saved_tensors
        A, B, C = ctx.params
        n_rows, n_cols = logits.shape
        n_predict = mtp_weights.shape[0]
        
        grad_input = torch.empty((n_rows, n_cols), dtype=torch.bfloat16, device=logits.device)
        grad_output = grad_output.contiguous()
        
        grid = (n_rows,)
        fused_softcapped_entropy_bwd_kernel[grid](
            grad_input, grad_output, lse, logits, targets, mtp_weights,
            logits.stride(0), logits.stride(1), grad_input.stride(0), grad_input.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        return grad_input, None, None, None, None, None

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int, use_malbo: bool):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

        self.use_malbo = use_malbo

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        if self.training:
            losses = FusedSoftcappedCrossEntropy.apply(logits.view(-1, logits.size(-1)), target_seq, mtp_weights)
            loss = losses.sum()
            if self.use_malbo:
                T, K = logits.view(-1, logits.size(-1)).shape
                with torch.no_grad():
                    vhat, kappa, gamma = compute_malbo_parameters((-losses).float().exp().unsqueeze(0), K)
                    weights = (kappa * gamma).squeeze(0)
                malbo_loss = T * (weights * losses).sum()
            else:
                malbo_loss = loss
        else:
            logits = 23 * torch.sigmoid((logits + 5) / 7.5)
            logits_for_loss = logits.float()
            losses = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="none")
            loss = losses.mean()
            if self.use_malbo:
                T, K = logits.view(-1, logits.size(-1)).shape
                with torch.no_grad():
                    vhat, kappa, gamma = compute_malbo_parameters((-losses).float().exp().unsqueeze(0), K)
                    weights = (kappa * gamma).squeeze(0)
                malbo_loss = (weights * losses).sum()
            else:
                malbo_loss = loss
        return loss, malbo_loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 1 # scales with world size
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size),
    use_malbo=os.environ.get('use_malbo', 'True') == 'True',
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args())[1] / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
    break
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss, val_malbo_loss = 0, 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                this_val_loss, this_val_malbo_loss = model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
                val_loss += this_val_loss
                val_malbo_loss += this_val_malbo_loss
        val_loss /= val_steps
        val_malbo_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        dist.reduce(val_malbo_loss, 0, op=dist.ReduceOp.AVG)
        n_predict = training_manager.mtp_weights_schedule[step].size(0)
        lr = get_lr(step)
        bs = get_bs(step)
        print0(f"step:{step}/{train_steps} {lr=:.4f} {bs=} {n_predict=} val_loss:{val_loss:.4f} val_malbo_loss:{val_malbo_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args())[1] / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    n_predict = training_manager.mtp_weights_schedule[step].size(0)
    lr = get_lr(step)
    bs = get_bs(step)
    print0(f"step:{step+1}/{train_steps} {n_predict=} {lr=:.4f} {bs=} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.19 (main, Oct 21 2025, 16:43:05) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Tue Jan 20 05:01:44 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:00:0F.0 Off |                    0 |
| N/A   27C    P0             71W /  310W |    1103MiB /  81559MiB |      3%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           57789      C   .../envs/speedrun/bin/python3.10       1094MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 lr=1.0000 bs=131072 n_predict=3 val_loss:10.8263 val_malbo_loss:10.8168 train_time:0ms step_avg:0.03ms
step:1/1775 n_predict=3 lr=1.0000 bs=131072 train_time:650ms step_avg:649.73ms
step:2/1775 n_predict=3 lr=1.0000 bs=131072 train_time:1765ms step_avg:882.47ms
step:3/1775 n_predict=3 lr=1.0000 bs=131072 train_time:2125ms step_avg:708.24ms
step:4/1775 n_predict=3 lr=1.0000 bs=131072 train_time:2520ms step_avg:630.02ms
step:5/1775 n_predict=3 lr=1.0000 bs=131072 train_time:2924ms step_avg:584.77ms
step:6/1775 n_predict=3 lr=1.0000 bs=131072 train_time:3320ms step_avg:553.35ms
step:7/1775 n_predict=3 lr=1.0000 bs=131072 train_time:3718ms step_avg:531.14ms
step:8/1775 n_predict=3 lr=1.0000 bs=131072 train_time:4122ms step_avg:515.21ms
step:9/1775 n_predict=3 lr=1.0000 bs=131072 train_time:4520ms step_avg:502.20ms
step:10/1775 n_predict=3 lr=1.0000 bs=131072 train_time:4922ms step_avg:492.24ms
step:11/1775 n_predict=3 lr=1.0000 bs=131072 train_time:5323ms step_avg:483.87ms
step:12/1775 n_predict=3 lr=1.0000 bs=131072 train_time:5722ms step_avg:476.87ms
step:13/1775 n_predict=3 lr=1.0000 bs=131072 train_time:6126ms step_avg:471.20ms
step:14/1775 n_predict=3 lr=1.0000 bs=131072 train_time:6526ms step_avg:466.12ms
step:15/1775 n_predict=3 lr=1.0000 bs=131072 train_time:6925ms step_avg:461.67ms
step:16/1775 n_predict=3 lr=1.0000 bs=131072 train_time:7329ms step_avg:458.07ms
step:17/1775 n_predict=3 lr=1.0000 bs=131072 train_time:7729ms step_avg:454.64ms
step:18/1775 n_predict=3 lr=1.0000 bs=131072 train_time:8133ms step_avg:451.81ms
step:19/1775 n_predict=3 lr=1.0000 bs=131072 train_time:8533ms step_avg:449.09ms
step:20/1775 n_predict=3 lr=1.0000 bs=131072 train_time:8935ms step_avg:446.75ms
step:21/1775 n_predict=3 lr=1.0000 bs=131072 train_time:9337ms step_avg:444.60ms
step:22/1775 n_predict=3 lr=1.0000 bs=131072 train_time:9736ms step_avg:442.53ms
step:23/1775 n_predict=3 lr=1.0000 bs=131072 train_time:10136ms step_avg:440.68ms
step:24/1775 n_predict=3 lr=1.0000 bs=131072 train_time:10537ms step_avg:439.06ms
step:25/1775 n_predict=3 lr=1.0000 bs=131072 train_time:10936ms step_avg:437.46ms
step:26/1775 n_predict=3 lr=1.0000 bs=131072 train_time:11337ms step_avg:436.04ms
step:27/1775 n_predict=3 lr=1.0000 bs=131072 train_time:11737ms step_avg:434.71ms
step:28/1775 n_predict=3 lr=1.0000 bs=131072 train_time:12136ms step_avg:433.44ms
step:29/1775 n_predict=3 lr=1.0000 bs=131072 train_time:12536ms step_avg:432.26ms
step:30/1775 n_predict=3 lr=1.0000 bs=131072 train_time:12937ms step_avg:431.23ms
step:31/1775 n_predict=3 lr=1.0000 bs=131072 train_time:13338ms step_avg:430.26ms
step:32/1775 n_predict=3 lr=1.0000 bs=131072 train_time:13737ms step_avg:429.29ms
step:33/1775 n_predict=3 lr=1.0000 bs=131072 train_time:14137ms step_avg:428.40ms
step:34/1775 n_predict=3 lr=1.0000 bs=131072 train_time:14538ms step_avg:427.58ms
step:35/1775 n_predict=3 lr=1.0000 bs=131072 train_time:14937ms step_avg:426.76ms
step:36/1775 n_predict=3 lr=1.0000 bs=131072 train_time:15339ms step_avg:426.07ms
step:37/1775 n_predict=3 lr=1.0000 bs=131072 train_time:15736ms step_avg:425.29ms
step:38/1775 n_predict=3 lr=1.0000 bs=131072 train_time:16139ms step_avg:424.70ms
step:39/1775 n_predict=3 lr=1.0000 bs=131072 train_time:16537ms step_avg:424.03ms
step:40/1775 n_predict=3 lr=1.0000 bs=131072 train_time:16937ms step_avg:423.42ms
step:41/1775 n_predict=3 lr=1.0000 bs=131072 train_time:17339ms step_avg:422.90ms
step:42/1775 n_predict=3 lr=1.0000 bs=131072 train_time:17739ms step_avg:422.36ms
step:43/1775 n_predict=3 lr=1.0000 bs=131072 train_time:18138ms step_avg:421.82ms
step:44/1775 n_predict=3 lr=1.0000 bs=131072 train_time:18541ms step_avg:421.38ms
step:45/1775 n_predict=3 lr=1.0000 bs=131072 train_time:18940ms step_avg:420.88ms
step:46/1775 n_predict=3 lr=1.0000 bs=131072 train_time:19342ms step_avg:420.48ms
step:47/1775 n_predict=3 lr=1.0000 bs=131072 train_time:19741ms step_avg:420.01ms
step:48/1775 n_predict=3 lr=1.0000 bs=131072 train_time:20142ms step_avg:419.63ms
step:49/1775 n_predict=3 lr=1.0000 bs=131072 train_time:20543ms step_avg:419.25ms
step:50/1775 n_predict=3 lr=1.0000 bs=131072 train_time:20945ms step_avg:418.90ms
step:51/1775 n_predict=3 lr=1.0000 bs=131072 train_time:21346ms step_avg:418.55ms
step:52/1775 n_predict=3 lr=1.0000 bs=131072 train_time:21750ms step_avg:418.27ms
step:53/1775 n_predict=3 lr=1.0000 bs=131072 train_time:22151ms step_avg:417.95ms
step:54/1775 n_predict=3 lr=1.0000 bs=131072 train_time:22554ms step_avg:417.68ms
step:55/1775 n_predict=3 lr=1.0000 bs=131072 train_time:22955ms step_avg:417.37ms
step:56/1775 n_predict=3 lr=1.0000 bs=131072 train_time:23359ms step_avg:417.13ms
step:57/1775 n_predict=3 lr=1.0000 bs=131072 train_time:23763ms step_avg:416.89ms
step:58/1775 n_predict=3 lr=1.0000 bs=131072 train_time:24166ms step_avg:416.66ms
step:59/1775 n_predict=3 lr=1.0000 bs=131072 train_time:24567ms step_avg:416.39ms
step:60/1775 n_predict=3 lr=1.0000 bs=131072 train_time:24970ms step_avg:416.16ms
step:61/1775 n_predict=3 lr=1.0000 bs=131072 train_time:25372ms step_avg:415.93ms
step:62/1775 n_predict=3 lr=1.0000 bs=131072 train_time:25774ms step_avg:415.71ms
step:63/1775 n_predict=3 lr=1.0000 bs=131072 train_time:26176ms step_avg:415.49ms
step:64/1775 n_predict=3 lr=1.0000 bs=131072 train_time:26583ms step_avg:415.35ms
step:65/1775 n_predict=3 lr=1.0000 bs=131072 train_time:26984ms step_avg:415.14ms
step:66/1775 n_predict=3 lr=1.0000 bs=131072 train_time:27386ms step_avg:414.95ms
step:67/1775 n_predict=3 lr=1.0000 bs=131072 train_time:27787ms step_avg:414.73ms
step:68/1775 n_predict=3 lr=1.0000 bs=131072 train_time:28187ms step_avg:414.52ms
step:69/1775 n_predict=3 lr=1.0000 bs=131072 train_time:28586ms step_avg:414.28ms
step:70/1775 n_predict=3 lr=1.0000 bs=131072 train_time:28988ms step_avg:414.11ms
step:71/1775 n_predict=3 lr=1.0000 bs=131072 train_time:29387ms step_avg:413.91ms
step:72/1775 n_predict=3 lr=1.0000 bs=131072 train_time:29788ms step_avg:413.72ms
step:73/1775 n_predict=3 lr=1.0000 bs=131072 train_time:30187ms step_avg:413.52ms
step:74/1775 n_predict=3 lr=1.0000 bs=131072 train_time:30587ms step_avg:413.34ms
step:75/1775 n_predict=3 lr=1.0000 bs=131072 train_time:30986ms step_avg:413.15ms
step:76/1775 n_predict=3 lr=1.0000 bs=131072 train_time:31388ms step_avg:413.01ms
step:77/1775 n_predict=3 lr=1.0000 bs=131072 train_time:31787ms step_avg:412.82ms
step:78/1775 n_predict=3 lr=1.0000 bs=131072 train_time:32187ms step_avg:412.66ms
step:79/1775 n_predict=3 lr=1.0000 bs=131072 train_time:32587ms step_avg:412.49ms
step:80/1775 n_predict=3 lr=1.0000 bs=131072 train_time:32988ms step_avg:412.35ms
step:81/1775 n_predict=3 lr=1.0000 bs=131072 train_time:33387ms step_avg:412.19ms
step:82/1775 n_predict=3 lr=1.0000 bs=131072 train_time:33788ms step_avg:412.05ms
step:83/1775 n_predict=3 lr=1.0000 bs=131072 train_time:34186ms step_avg:411.89ms
step:84/1775 n_predict=3 lr=1.0000 bs=131072 train_time:34588ms step_avg:411.77ms
step:85/1775 n_predict=3 lr=1.0000 bs=131072 train_time:34987ms step_avg:411.61ms
step:86/1775 n_predict=3 lr=1.0000 bs=131072 train_time:35388ms step_avg:411.49ms
step:87/1775 n_predict=3 lr=1.0000 bs=131072 train_time:35786ms step_avg:411.34ms
step:88/1775 n_predict=3 lr=1.0000 bs=131072 train_time:36188ms step_avg:411.23ms
step:89/1775 n_predict=3 lr=1.0000 bs=131072 train_time:36587ms step_avg:411.09ms
step:90/1775 n_predict=3 lr=1.0000 bs=131072 train_time:36987ms step_avg:410.97ms
step:91/1775 n_predict=3 lr=1.0000 bs=131072 train_time:37387ms step_avg:410.85ms
step:92/1775 n_predict=3 lr=1.0000 bs=131072 train_time:37788ms step_avg:410.74ms
step:93/1775 n_predict=3 lr=1.0000 bs=131072 train_time:38187ms step_avg:410.61ms
step:94/1775 n_predict=3 lr=1.0000 bs=131072 train_time:38593ms step_avg:410.56ms
step:95/1775 n_predict=3 lr=1.0000 bs=131072 train_time:38989ms step_avg:410.41ms
step:96/1775 n_predict=3 lr=1.0000 bs=131072 train_time:39391ms step_avg:410.33ms
step:97/1775 n_predict=3 lr=1.0000 bs=131072 train_time:39790ms step_avg:410.21ms
step:98/1775 n_predict=3 lr=1.0000 bs=131072 train_time:40191ms step_avg:410.12ms
step:99/1775 n_predict=3 lr=1.0000 bs=131072 train_time:40587ms step_avg:409.97ms
step:100/1775 n_predict=3 lr=1.0000 bs=131072 train_time:40987ms step_avg:409.87ms
step:101/1775 n_predict=3 lr=1.0000 bs=131072 train_time:41386ms step_avg:409.76ms
step:102/1775 n_predict=3 lr=1.0000 bs=131072 train_time:41788ms step_avg:409.69ms
step:103/1775 n_predict=3 lr=1.0000 bs=131072 train_time:42186ms step_avg:409.57ms
step:104/1775 n_predict=3 lr=1.0000 bs=131072 train_time:42590ms step_avg:409.51ms
step:105/1775 n_predict=3 lr=1.0000 bs=131072 train_time:42988ms step_avg:409.40ms
step:106/1775 n_predict=3 lr=1.0000 bs=131072 train_time:43388ms step_avg:409.32ms
step:107/1775 n_predict=3 lr=1.0000 bs=131072 train_time:43789ms step_avg:409.24ms
step:108/1775 n_predict=3 lr=1.0000 bs=131072 train_time:44188ms step_avg:409.15ms
step:109/1775 n_predict=3 lr=1.0000 bs=131072 train_time:44588ms step_avg:409.06ms
step:110/1775 n_predict=3 lr=1.0000 bs=131072 train_time:44988ms step_avg:408.98ms
step:111/1775 n_predict=3 lr=1.0000 bs=131072 train_time:45387ms step_avg:408.89ms
step:112/1775 n_predict=3 lr=1.0000 bs=131072 train_time:45788ms step_avg:408.82ms
step:113/1775 n_predict=3 lr=1.0000 bs=131072 train_time:46186ms step_avg:408.73ms
step:114/1775 n_predict=3 lr=1.0000 bs=131072 train_time:46588ms step_avg:408.67ms
step:115/1775 n_predict=3 lr=1.0000 bs=131072 train_time:46987ms step_avg:408.58ms
step:116/1775 n_predict=3 lr=1.0000 bs=131072 train_time:47388ms step_avg:408.52ms
step:117/1775 n_predict=3 lr=1.0000 bs=131072 train_time:47787ms step_avg:408.44ms
step:118/1775 n_predict=3 lr=1.0000 bs=131072 train_time:48190ms step_avg:408.39ms
step:119/1775 n_predict=3 lr=1.0000 bs=131072 train_time:48588ms step_avg:408.30ms
step:120/1775 n_predict=3 lr=1.0000 bs=131072 train_time:48988ms step_avg:408.23ms
step:121/1775 n_predict=3 lr=1.0000 bs=131072 train_time:49388ms step_avg:408.16ms
step:122/1775 n_predict=3 lr=1.0000 bs=131072 train_time:49789ms step_avg:408.11ms
step:123/1775 n_predict=3 lr=1.0000 bs=131072 train_time:50188ms step_avg:408.03ms
step:124/1775 n_predict=3 lr=1.0000 bs=131072 train_time:50590ms step_avg:407.98ms
step:125/1775 n_predict=3 lr=1.0000 bs=131072 train_time:50988ms step_avg:407.90ms
step:126/1775 n_predict=3 lr=1.0000 bs=131072 train_time:51389ms step_avg:407.85ms
step:127/1775 n_predict=3 lr=1.0000 bs=131072 train_time:51787ms step_avg:407.77ms
step:128/1775 n_predict=3 lr=1.0000 bs=131072 train_time:52188ms step_avg:407.72ms
step:129/1775 n_predict=3 lr=1.0000 bs=131072 train_time:52587ms step_avg:407.65ms
step:130/1775 n_predict=3 lr=1.0000 bs=131072 train_time:52989ms step_avg:407.61ms
step:131/1775 n_predict=3 lr=1.0000 bs=131072 train_time:53387ms step_avg:407.54ms
step:132/1775 n_predict=3 lr=1.0000 bs=131072 train_time:53788ms step_avg:407.49ms
step:133/1775 n_predict=3 lr=1.0000 bs=131072 train_time:54187ms step_avg:407.42ms
step:134/1775 n_predict=3 lr=1.0000 bs=131072 train_time:54589ms step_avg:407.38ms
step:135/1775 n_predict=3 lr=1.0000 bs=131072 train_time:54988ms step_avg:407.32ms
step:136/1775 n_predict=3 lr=1.0000 bs=131072 train_time:55389ms step_avg:407.27ms
step:137/1775 n_predict=3 lr=1.0000 bs=131072 train_time:55787ms step_avg:407.21ms
step:138/1775 n_predict=3 lr=1.0000 bs=131072 train_time:56189ms step_avg:407.17ms
step:139/1775 n_predict=3 lr=1.0000 bs=131072 train_time:56587ms step_avg:407.10ms
step:140/1775 n_predict=3 lr=1.0000 bs=131072 train_time:56988ms step_avg:407.06ms
step:141/1775 n_predict=3 lr=1.0000 bs=131072 train_time:57388ms step_avg:407.01ms
step:142/1775 n_predict=3 lr=1.0000 bs=131072 train_time:57789ms step_avg:406.96ms
step:143/1775 n_predict=3 lr=1.0000 bs=131072 train_time:58188ms step_avg:406.91ms
step:144/1775 n_predict=3 lr=1.0000 bs=131072 train_time:58589ms step_avg:406.87ms
step:145/1775 n_predict=3 lr=1.0000 bs=131072 train_time:58987ms step_avg:406.81ms
step:146/1775 n_predict=3 lr=1.0000 bs=131072 train_time:59390ms step_avg:406.78ms
step:147/1775 n_predict=3 lr=1.0000 bs=131072 train_time:59788ms step_avg:406.72ms
step:148/1775 n_predict=3 lr=1.0000 bs=131072 train_time:60192ms step_avg:406.70ms
step:149/1775 n_predict=3 lr=1.0000 bs=131072 train_time:60592ms step_avg:406.66ms
step:150/1775 n_predict=3 lr=1.0000 bs=131072 train_time:60992ms step_avg:406.61ms
step:151/1775 n_predict=3 lr=1.0000 bs=131072 train_time:61391ms step_avg:406.57ms
step:152/1775 n_predict=3 lr=1.0000 bs=131072 train_time:61793ms step_avg:406.53ms
step:153/1775 n_predict=3 lr=1.0000 bs=131072 train_time:62192ms step_avg:406.48ms
step:154/1775 n_predict=3 lr=1.0000 bs=131072 train_time:62590ms step_avg:406.43ms
step:155/1775 n_predict=3 lr=1.0000 bs=131072 train_time:62989ms step_avg:406.38ms
step:156/1775 n_predict=3 lr=1.0000 bs=131072 train_time:63389ms step_avg:406.34ms
step:157/1775 n_predict=3 lr=1.0000 bs=131072 train_time:63787ms step_avg:406.29ms
step:158/1775 n_predict=3 lr=1.0000 bs=131072 train_time:64190ms step_avg:406.27ms
step:159/1775 n_predict=3 lr=1.0000 bs=131072 train_time:64593ms step_avg:406.24ms
step:160/1775 n_predict=3 lr=1.0000 bs=131072 train_time:64995ms step_avg:406.22ms
step:161/1775 n_predict=3 lr=1.0000 bs=131072 train_time:65392ms step_avg:406.16ms
step:162/1775 n_predict=3 lr=1.0000 bs=131072 train_time:65791ms step_avg:406.12ms
step:163/1775 n_predict=3 lr=1.0000 bs=131072 train_time:66189ms step_avg:406.07ms
step:164/1775 n_predict=3 lr=1.0000 bs=131072 train_time:66590ms step_avg:406.04ms
step:165/1775 n_predict=3 lr=1.0000 bs=131072 train_time:66989ms step_avg:405.99ms
step:166/1775 n_predict=3 lr=1.0000 bs=131072 train_time:67391ms step_avg:405.97ms
step:167/1775 n_predict=3 lr=1.0000 bs=131072 train_time:67787ms step_avg:405.91ms
step:168/1775 n_predict=3 lr=1.0000 bs=131072 train_time:68191ms step_avg:405.90ms
step:169/1775 n_predict=3 lr=1.0000 bs=131072 train_time:68590ms step_avg:405.86ms
step:170/1775 n_predict=3 lr=1.0000 bs=131072 train_time:68990ms step_avg:405.82ms
step:171/1775 n_predict=3 lr=1.0000 bs=131072 train_time:69389ms step_avg:405.78ms
step:172/1775 n_predict=3 lr=1.0000 bs=131072 train_time:69790ms step_avg:405.75ms
step:173/1775 n_predict=3 lr=1.0000 bs=131072 train_time:70189ms step_avg:405.71ms
step:174/1775 n_predict=3 lr=1.0000 bs=131072 train_time:70589ms step_avg:405.69ms
step:175/1775 n_predict=3 lr=1.0000 bs=131072 train_time:70989ms step_avg:405.65ms
step:176/1775 n_predict=3 lr=1.0000 bs=131072 train_time:71390ms step_avg:405.62ms
step:177/1775 n_predict=3 lr=1.0000 bs=131072 train_time:71788ms step_avg:405.58ms
step:178/1775 n_predict=3 lr=1.0000 bs=131072 train_time:72190ms step_avg:405.56ms
step:179/1775 n_predict=3 lr=1.0000 bs=131072 train_time:72589ms step_avg:405.52ms
step:180/1775 n_predict=3 lr=1.0000 bs=131072 train_time:72989ms step_avg:405.50ms
step:181/1775 n_predict=3 lr=1.0000 bs=131072 train_time:73388ms step_avg:405.46ms
step:182/1775 n_predict=3 lr=1.0000 bs=131072 train_time:73790ms step_avg:405.44ms
step:183/1775 n_predict=3 lr=1.0000 bs=131072 train_time:74190ms step_avg:405.41ms
step:184/1775 n_predict=3 lr=1.0000 bs=131072 train_time:74589ms step_avg:405.38ms
step:185/1775 n_predict=3 lr=1.0000 bs=131072 train_time:74988ms step_avg:405.34ms
step:186/1775 n_predict=3 lr=1.0000 bs=131072 train_time:75390ms step_avg:405.32ms
step:187/1775 n_predict=3 lr=1.0000 bs=131072 train_time:75788ms step_avg:405.28ms
step:188/1775 n_predict=3 lr=1.0000 bs=131072 train_time:76190ms step_avg:405.27ms
step:189/1775 n_predict=3 lr=1.0000 bs=131072 train_time:76589ms step_avg:405.23ms
step:190/1775 n_predict=3 lr=1.0000 bs=131072 train_time:76991ms step_avg:405.21ms
step:191/1775 n_predict=3 lr=1.0000 bs=131072 train_time:77389ms step_avg:405.18ms
step:192/1775 n_predict=3 lr=1.0000 bs=131072 train_time:77790ms step_avg:405.16ms
step:193/1775 n_predict=3 lr=1.0000 bs=131072 train_time:78190ms step_avg:405.13ms
step:194/1775 n_predict=3 lr=1.0000 bs=131072 train_time:78592ms step_avg:405.11ms
step:195/1775 n_predict=3 lr=1.0000 bs=131072 train_time:78988ms step_avg:405.07ms
step:196/1775 n_predict=3 lr=1.0000 bs=131072 train_time:79390ms step_avg:405.05ms
step:197/1775 n_predict=3 lr=1.0000 bs=131072 train_time:79791ms step_avg:405.03ms
step:198/1775 n_predict=3 lr=1.0000 bs=131072 train_time:80191ms step_avg:405.00ms
step:199/1775 n_predict=3 lr=1.0000 bs=131072 train_time:80590ms step_avg:404.98ms
step:200/1775 n_predict=3 lr=1.0000 bs=131072 train_time:80990ms step_avg:404.95ms
step:201/1775 n_predict=3 lr=1.0000 bs=131072 train_time:81389ms step_avg:404.92ms
step:202/1775 n_predict=3 lr=1.0000 bs=131072 train_time:81791ms step_avg:404.91ms
step:203/1775 n_predict=3 lr=1.0000 bs=131072 train_time:82191ms step_avg:404.88ms
step:204/1775 n_predict=3 lr=1.0000 bs=131072 train_time:82592ms step_avg:404.86ms
step:205/1775 n_predict=3 lr=1.0000 bs=131072 train_time:82992ms step_avg:404.84ms
step:206/1775 n_predict=3 lr=1.0000 bs=131072 train_time:83393ms step_avg:404.82ms
step:207/1775 n_predict=3 lr=1.0000 bs=131072 train_time:83793ms step_avg:404.80ms
step:208/1775 n_predict=3 lr=1.0000 bs=131072 train_time:84195ms step_avg:404.78ms
step:209/1775 n_predict=3 lr=1.0000 bs=131072 train_time:84599ms step_avg:404.78ms
step:210/1775 n_predict=3 lr=1.0000 bs=131072 train_time:85007ms step_avg:404.80ms
step:211/1775 n_predict=3 lr=1.0000 bs=131072 train_time:85408ms step_avg:404.78ms
step:212/1775 n_predict=3 lr=1.0000 bs=131072 train_time:85811ms step_avg:404.77ms
step:213/1775 n_predict=3 lr=1.0000 bs=131072 train_time:86213ms step_avg:404.75ms
step:214/1775 n_predict=3 lr=1.0000 bs=131072 train_time:86617ms step_avg:404.75ms
step:215/1775 n_predict=3 lr=1.0000 bs=131072 train_time:87022ms step_avg:404.76ms
step:216/1775 n_predict=3 lr=1.0000 bs=131072 train_time:87426ms step_avg:404.75ms
step:217/1775 n_predict=3 lr=1.0000 bs=131072 train_time:87833ms step_avg:404.76ms
step:218/1775 n_predict=3 lr=1.0000 bs=131072 train_time:88237ms step_avg:404.76ms
step:219/1775 n_predict=3 lr=1.0000 bs=131072 train_time:88637ms step_avg:404.74ms
step:220/1775 n_predict=3 lr=1.0000 bs=131072 train_time:89040ms step_avg:404.73ms
step:221/1775 n_predict=3 lr=1.0000 bs=131072 train_time:89442ms step_avg:404.71ms
step:222/1775 n_predict=3 lr=1.0000 bs=131072 train_time:89843ms step_avg:404.70ms
step:223/1775 n_predict=3 lr=1.0000 bs=131072 train_time:90244ms step_avg:404.68ms
step:224/1775 n_predict=3 lr=1.0000 bs=131072 train_time:90651ms step_avg:404.69ms
step:225/1775 n_predict=3 lr=1.0000 bs=131072 train_time:91052ms step_avg:404.68ms
step:226/1775 n_predict=3 lr=1.0000 bs=131072 train_time:91457ms step_avg:404.68ms
step:227/1775 n_predict=3 lr=1.0000 bs=131072 train_time:91864ms step_avg:404.69ms
step:228/1775 n_predict=3 lr=1.0000 bs=131072 train_time:92267ms step_avg:404.68ms
step:229/1775 n_predict=3 lr=1.0000 bs=131072 train_time:92672ms step_avg:404.68ms
step:230/1775 n_predict=3 lr=1.0000 bs=131072 train_time:93076ms step_avg:404.68ms
step:231/1775 n_predict=3 lr=1.0000 bs=131072 train_time:93480ms step_avg:404.67ms
step:232/1775 n_predict=3 lr=1.0000 bs=131072 train_time:93887ms step_avg:404.69ms
step:233/1775 n_predict=3 lr=1.0000 bs=131072 train_time:94287ms step_avg:404.66ms
step:234/1775 n_predict=3 lr=1.0000 bs=131072 train_time:94690ms step_avg:404.66ms
step:235/1775 n_predict=3 lr=1.0000 bs=131072 train_time:95089ms step_avg:404.64ms
step:236/1775 n_predict=3 lr=1.0000 bs=131072 train_time:95490ms step_avg:404.62ms
step:237/1775 n_predict=3 lr=1.0000 bs=131072 train_time:95889ms step_avg:404.60ms
step:238/1775 n_predict=3 lr=1.0000 bs=131072 train_time:96290ms step_avg:404.58ms
step:239/1775 n_predict=3 lr=1.0000 bs=131072 train_time:96688ms step_avg:404.55ms
step:240/1775 n_predict=3 lr=1.0000 bs=131072 train_time:97089ms step_avg:404.54ms
step:241/1775 n_predict=3 lr=1.0000 bs=131072 train_time:97489ms step_avg:404.52ms
step:242/1775 n_predict=3 lr=1.0000 bs=131072 train_time:97891ms step_avg:404.51ms
step:243/1775 n_predict=3 lr=1.0000 bs=131072 train_time:98290ms step_avg:404.48ms
step:244/1775 n_predict=3 lr=1.0000 bs=131072 train_time:98691ms step_avg:404.47ms
step:245/1775 n_predict=3 lr=1.0000 bs=131072 train_time:99090ms step_avg:404.45ms
step:246/1775 n_predict=3 lr=1.0000 bs=131072 train_time:99491ms step_avg:404.44ms
step:247/1775 n_predict=3 lr=1.0000 bs=131072 train_time:99890ms step_avg:404.41ms
step:248/1775 n_predict=3 lr=1.0000 bs=131072 train_time:100294ms step_avg:404.41ms
step:249/1775 n_predict=3 lr=1.0000 bs=131072 train_time:100690ms step_avg:404.38ms
step:250/1775 n_predict=3 lr=1.0000 bs=131072 train_time:101093ms step_avg:404.37ms
step:250/1775 lr=1.0000 bs=131072 n_predict=3 val_loss:4.6065 val_malbo_loss:4.6788 train_time:101127ms step_avg:404.51ms
step:251/1775 n_predict=3 lr=1.0000 bs=131072 train_time:101497ms step_avg:404.37ms
step:252/1775 n_predict=3 lr=1.0000 bs=131072 train_time:101903ms step_avg:404.38ms
step:253/1775 n_predict=3 lr=1.0000 bs=131072 train_time:102303ms step_avg:404.36ms
step:254/1775 n_predict=3 lr=1.0000 bs=131072 train_time:102706ms step_avg:404.36ms
step:255/1775 n_predict=3 lr=1.0000 bs=131072 train_time:103108ms step_avg:404.34ms
step:256/1775 n_predict=3 lr=1.0000 bs=131072 train_time:103511ms step_avg:404.34ms
step:257/1775 n_predict=3 lr=1.0000 bs=131072 train_time:103916ms step_avg:404.34ms
step:258/1775 n_predict=3 lr=1.0000 bs=131072 train_time:104322ms step_avg:404.35ms
step:259/1775 n_predict=3 lr=1.0000 bs=131072 train_time:104724ms step_avg:404.34ms
step:260/1775 n_predict=3 lr=1.0000 bs=131072 train_time:105132ms step_avg:404.35ms
step:261/1775 n_predict=3 lr=1.0000 bs=131072 train_time:105532ms step_avg:404.34ms
step:262/1775 n_predict=3 lr=1.0000 bs=131072 train_time:105938ms step_avg:404.34ms
step:263/1775 n_predict=3 lr=1.0000 bs=131072 train_time:106341ms step_avg:404.34ms
step:264/1775 n_predict=3 lr=1.0000 bs=131072 train_time:106749ms step_avg:404.35ms
step:265/1775 n_predict=3 lr=1.0000 bs=131072 train_time:107153ms step_avg:404.35ms
step:266/1775 n_predict=3 lr=1.0000 bs=131072 train_time:107554ms step_avg:404.34ms
step:267/1775 n_predict=3 lr=1.0000 bs=131072 train_time:107953ms step_avg:404.32ms
step:268/1775 n_predict=3 lr=1.0000 bs=131072 train_time:108357ms step_avg:404.32ms
step:269/1775 n_predict=3 lr=1.0000 bs=131072 train_time:108756ms step_avg:404.30ms
step:270/1775 n_predict=3 lr=1.0000 bs=131072 train_time:109157ms step_avg:404.29ms
step:271/1775 n_predict=3 lr=1.0000 bs=131072 train_time:109557ms step_avg:404.27ms
step:272/1775 n_predict=3 lr=1.0000 bs=131072 train_time:109961ms step_avg:404.27ms
step:273/1775 n_predict=3 lr=1.0000 bs=131072 train_time:110361ms step_avg:404.25ms
step:274/1775 n_predict=3 lr=1.0000 bs=131072 train_time:110761ms step_avg:404.24ms
step:275/1775 n_predict=3 lr=1.0000 bs=131072 train_time:111166ms step_avg:404.24ms
step:276/1775 n_predict=3 lr=1.0000 bs=131072 train_time:111573ms step_avg:404.25ms
step:277/1775 n_predict=3 lr=1.0000 bs=131072 train_time:111975ms step_avg:404.24ms
step:278/1775 n_predict=3 lr=1.0000 bs=131072 train_time:112381ms step_avg:404.25ms
step:279/1775 n_predict=3 lr=1.0000 bs=131072 train_time:112786ms step_avg:404.25ms
step:280/1775 n_predict=3 lr=1.0000 bs=131072 train_time:113192ms step_avg:404.26ms
step:281/1775 n_predict=3 lr=1.0000 bs=131072 train_time:113595ms step_avg:404.25ms
step:282/1775 n_predict=3 lr=1.0000 bs=131072 train_time:114003ms step_avg:404.27ms
step:283/1775 n_predict=3 lr=1.0000 bs=131072 train_time:114403ms step_avg:404.25ms
step:284/1775 n_predict=3 lr=1.0000 bs=131072 train_time:114806ms step_avg:404.25ms
step:285/1775 n_predict=3 lr=1.0000 bs=131072 train_time:115208ms step_avg:404.24ms
step:286/1775 n_predict=3 lr=1.0000 bs=131072 train_time:115611ms step_avg:404.23ms
step:287/1775 n_predict=3 lr=1.0000 bs=131072 train_time:116013ms step_avg:404.23ms
step:288/1775 n_predict=3 lr=1.0000 bs=131072 train_time:116420ms step_avg:404.24ms
step:289/1775 n_predict=3 lr=1.0000 bs=131072 train_time:116821ms step_avg:404.23ms
step:290/1775 n_predict=3 lr=1.0000 bs=131072 train_time:117228ms step_avg:404.23ms
step:291/1775 n_predict=3 lr=1.0000 bs=131072 train_time:117633ms step_avg:404.24ms
step:292/1775 n_predict=3 lr=1.0000 bs=131072 train_time:118038ms step_avg:404.24ms
step:293/1775 n_predict=3 lr=1.0000 bs=131072 train_time:118441ms step_avg:404.23ms
step:294/1775 n_predict=3 lr=1.0000 bs=131072 train_time:118849ms step_avg:404.25ms
step:295/1775 n_predict=3 lr=1.0000 bs=131072 train_time:119252ms step_avg:404.24ms
step:296/1775 n_predict=3 lr=1.0000 bs=131072 train_time:119656ms step_avg:404.24ms
step:297/1775 n_predict=3 lr=1.0000 bs=131072 train_time:120056ms step_avg:404.23ms
step:298/1775 n_predict=3 lr=1.0000 bs=131072 train_time:120458ms step_avg:404.22ms
step:299/1775 n_predict=3 lr=1.0000 bs=131072 train_time:120856ms step_avg:404.20ms
step:300/1775 n_predict=3 lr=1.0000 bs=131072 train_time:121258ms step_avg:404.19ms
step:301/1775 n_predict=3 lr=1.0000 bs=131072 train_time:121658ms step_avg:404.18ms
step:302/1775 n_predict=3 lr=1.0000 bs=131072 train_time:122059ms step_avg:404.17ms
step:303/1775 n_predict=3 lr=1.0000 bs=131072 train_time:122462ms step_avg:404.16ms
step:304/1775 n_predict=3 lr=1.0000 bs=131072 train_time:122870ms step_avg:404.18ms
step:305/1775 n_predict=3 lr=1.0000 bs=131072 train_time:123272ms step_avg:404.17ms
step:306/1775 n_predict=3 lr=1.0000 bs=131072 train_time:123675ms step_avg:404.17ms
step:307/1775 n_predict=3 lr=1.0000 bs=131072 train_time:124079ms step_avg:404.16ms
step:308/1775 n_predict=3 lr=1.0000 bs=131072 train_time:124482ms step_avg:404.16ms
step:309/1775 n_predict=3 lr=1.0000 bs=131072 train_time:124888ms step_avg:404.17ms
step:310/1775 n_predict=3 lr=1.0000 bs=131072 train_time:125294ms step_avg:404.17ms
step:311/1775 n_predict=3 lr=1.0000 bs=131072 train_time:125699ms step_avg:404.18ms
step:312/1775 n_predict=3 lr=1.0000 bs=131072 train_time:126103ms step_avg:404.18ms
step:313/1775 n_predict=3 lr=1.0000 bs=131072 train_time:126505ms step_avg:404.17ms
step:314/1775 n_predict=3 lr=1.0000 bs=131072 train_time:126906ms step_avg:404.16ms
step:315/1775 n_predict=3 lr=1.0000 bs=131072 train_time:127308ms step_avg:404.15ms
step:316/1775 n_predict=3 lr=1.0000 bs=131072 train_time:127713ms step_avg:404.15ms
step:317/1775 n_predict=3 lr=1.0000 bs=131072 train_time:128117ms step_avg:404.16ms
step:318/1775 n_predict=3 lr=1.0000 bs=131072 train_time:128522ms step_avg:404.16ms
step:319/1775 n_predict=3 lr=1.0000 bs=131072 train_time:128924ms step_avg:404.15ms
step:320/1775 n_predict=3 lr=1.0000 bs=131072 train_time:129332ms step_avg:404.16ms
step:321/1775 n_predict=3 lr=1.0000 bs=131072 train_time:129734ms step_avg:404.16ms
step:322/1775 n_predict=3 lr=1.0000 bs=131072 train_time:130139ms step_avg:404.16ms
step:323/1775 n_predict=3 lr=1.0000 bs=131072 train_time:130546ms step_avg:404.17ms
step:324/1775 n_predict=3 lr=1.0000 bs=131072 train_time:130953ms step_avg:404.18ms
step:325/1775 n_predict=3 lr=1.0000 bs=131072 train_time:131354ms step_avg:404.16ms
step:326/1775 n_predict=3 lr=1.0000 bs=131072 train_time:131756ms step_avg:404.16ms
step:327/1775 n_predict=3 lr=1.0000 bs=131072 train_time:132156ms step_avg:404.15ms
step:328/1775 n_predict=3 lr=1.0000 bs=131072 train_time:132557ms step_avg:404.14ms
step:329/1775 n_predict=3 lr=1.0000 bs=131072 train_time:132958ms step_avg:404.13ms
step:330/1775 n_predict=3 lr=1.0000 bs=131072 train_time:133358ms step_avg:404.12ms
step:331/1775 n_predict=3 lr=1.0000 bs=131072 train_time:133761ms step_avg:404.11ms
step:332/1775 n_predict=3 lr=1.0000 bs=131072 train_time:134170ms step_avg:404.13ms
step:333/1775 n_predict=3 lr=1.0000 bs=131072 train_time:134573ms step_avg:404.12ms
step:334/1775 n_predict=3 lr=1.0000 bs=131072 train_time:134974ms step_avg:404.11ms
step:335/1775 n_predict=3 lr=1.0000 bs=131072 train_time:135381ms step_avg:404.12ms
step:336/1775 n_predict=3 lr=1.0000 bs=131072 train_time:135784ms step_avg:404.12ms
step:337/1775 n_predict=3 lr=1.0000 bs=131072 train_time:136188ms step_avg:404.12ms
step:338/1775 n_predict=3 lr=1.0000 bs=131072 train_time:136594ms step_avg:404.13ms
step:339/1775 n_predict=3 lr=1.0000 bs=131072 train_time:137001ms step_avg:404.13ms
step:340/1775 n_predict=3 lr=1.0000 bs=131072 train_time:137404ms step_avg:404.13ms
step:341/1775 n_predict=3 lr=1.0000 bs=131072 train_time:137806ms step_avg:404.12ms
step:342/1775 n_predict=3 lr=1.0000 bs=131072 train_time:138209ms step_avg:404.12ms
step:343/1775 n_predict=3 lr=1.0000 bs=131072 train_time:138610ms step_avg:404.11ms
step:344/1775 n_predict=3 lr=1.0000 bs=131072 train_time:139014ms step_avg:404.11ms
step:345/1775 n_predict=3 lr=1.0000 bs=131072 train_time:139418ms step_avg:404.11ms
step:346/1775 n_predict=3 lr=1.0000 bs=131072 train_time:139823ms step_avg:404.11ms
step:347/1775 n_predict=3 lr=1.0000 bs=131072 train_time:140228ms step_avg:404.12ms
step:348/1775 n_predict=3 lr=1.0000 bs=131072 train_time:140636ms step_avg:404.13ms
step:349/1775 n_predict=3 lr=1.0000 bs=131072 train_time:141038ms step_avg:404.12ms
step:350/1775 n_predict=3 lr=1.0000 bs=131072 train_time:141442ms step_avg:404.12ms
step:351/1775 n_predict=3 lr=1.0000 bs=131072 train_time:141852ms step_avg:404.14ms
step:352/1775 n_predict=3 lr=1.0000 bs=131072 train_time:142256ms step_avg:404.14ms
step:353/1775 n_predict=3 lr=1.0000 bs=131072 train_time:142655ms step_avg:404.12ms
step:354/1775 n_predict=3 lr=1.0000 bs=131072 train_time:143061ms step_avg:404.13ms
step:355/1775 n_predict=3 lr=1.0000 bs=131072 train_time:143457ms step_avg:404.10ms
step:356/1775 n_predict=3 lr=1.0000 bs=131072 train_time:143858ms step_avg:404.09ms
step:357/1775 n_predict=3 lr=1.0000 bs=131072 train_time:144256ms step_avg:404.08ms
step:358/1775 n_predict=3 lr=1.0000 bs=131072 train_time:144660ms step_avg:404.08ms
step:359/1775 n_predict=3 lr=1.0000 bs=131072 train_time:145057ms step_avg:404.06ms
step:360/1775 n_predict=3 lr=1.0000 bs=131072 train_time:145459ms step_avg:404.05ms
step:361/1775 n_predict=3 lr=1.0000 bs=131072 train_time:145860ms step_avg:404.04ms
step:362/1775 n_predict=3 lr=1.0000 bs=131072 train_time:146263ms step_avg:404.04ms
step:363/1775 n_predict=3 lr=1.0000 bs=131072 train_time:146662ms step_avg:404.03ms
step:364/1775 n_predict=3 lr=1.0000 bs=131072 train_time:147070ms step_avg:404.04ms
step:365/1775 n_predict=3 lr=1.0000 bs=131072 train_time:147474ms step_avg:404.04ms
step:366/1775 n_predict=3 lr=1.0000 bs=131072 train_time:147875ms step_avg:404.03ms
step:367/1775 n_predict=3 lr=1.0000 bs=131072 train_time:148278ms step_avg:404.03ms
step:368/1775 n_predict=3 lr=1.0000 bs=131072 train_time:148684ms step_avg:404.03ms
step:369/1775 n_predict=3 lr=1.0000 bs=131072 train_time:149091ms step_avg:404.04ms
step:370/1775 n_predict=3 lr=1.0000 bs=131072 train_time:149494ms step_avg:404.04ms
step:371/1775 n_predict=3 lr=1.0000 bs=131072 train_time:149901ms step_avg:404.05ms
step:372/1775 n_predict=3 lr=1.0000 bs=131072 train_time:150305ms step_avg:404.04ms
step:373/1775 n_predict=3 lr=1.0000 bs=131072 train_time:150706ms step_avg:404.04ms
step:374/1775 n_predict=3 lr=1.0000 bs=131072 train_time:151109ms step_avg:404.04ms
step:375/1775 n_predict=3 lr=1.0000 bs=131072 train_time:151513ms step_avg:404.03ms
step:376/1775 n_predict=3 lr=1.0000 bs=131072 train_time:151920ms step_avg:404.04ms
step:377/1775 n_predict=3 lr=1.0000 bs=131072 train_time:152321ms step_avg:404.04ms
step:378/1775 n_predict=3 lr=1.0000 bs=131072 train_time:152728ms step_avg:404.04ms
step:379/1775 n_predict=3 lr=1.0000 bs=131072 train_time:153132ms step_avg:404.04ms
step:380/1775 n_predict=3 lr=1.0000 bs=131072 train_time:153537ms step_avg:404.05ms
step:381/1775 n_predict=3 lr=1.0000 bs=131072 train_time:153940ms step_avg:404.04ms
step:382/1775 n_predict=3 lr=1.0000 bs=131072 train_time:154344ms step_avg:404.04ms
step:383/1775 n_predict=3 lr=1.0000 bs=131072 train_time:154751ms step_avg:404.05ms
step:384/1775 n_predict=3 lr=1.0000 bs=131072 train_time:155154ms step_avg:404.05ms
step:385/1775 n_predict=3 lr=1.0000 bs=131072 train_time:155556ms step_avg:404.04ms
step:386/1775 n_predict=3 lr=1.0000 bs=131072 train_time:155956ms step_avg:404.03ms
step:387/1775 n_predict=3 lr=1.0000 bs=131072 train_time:156356ms step_avg:404.02ms
step:388/1775 n_predict=3 lr=1.0000 bs=131072 train_time:156759ms step_avg:404.02ms
step:389/1775 n_predict=3 lr=1.0000 bs=131072 train_time:157156ms step_avg:404.00ms
step:390/1775 n_predict=3 lr=1.0000 bs=131072 train_time:157557ms step_avg:403.99ms
step:391/1775 n_predict=3 lr=1.0000 bs=131072 train_time:157957ms step_avg:403.98ms
step:392/1775 n_predict=3 lr=1.0000 bs=131072 train_time:158359ms step_avg:403.98ms
step:393/1775 n_predict=3 lr=1.0000 bs=131072 train_time:158758ms step_avg:403.96ms
step:394/1775 n_predict=3 lr=1.0000 bs=131072 train_time:159159ms step_avg:403.96ms
step:395/1775 n_predict=3 lr=1.0000 bs=131072 train_time:159560ms step_avg:403.95ms
step:396/1775 n_predict=3 lr=1.0000 bs=131072 train_time:159962ms step_avg:403.94ms
step:397/1775 n_predict=3 lr=1.0000 bs=131072 train_time:160365ms step_avg:403.94ms
step:398/1775 n_predict=3 lr=1.0000 bs=131072 train_time:160772ms step_avg:403.95ms
step:399/1775 n_predict=3 lr=1.0000 bs=131072 train_time:161172ms step_avg:403.94ms
step:400/1775 n_predict=3 lr=1.0000 bs=131072 train_time:161576ms step_avg:403.94ms
step:401/1775 n_predict=3 lr=1.0000 bs=131072 train_time:161981ms step_avg:403.94ms
step:402/1775 n_predict=3 lr=1.0000 bs=131072 train_time:162388ms step_avg:403.95ms
step:403/1775 n_predict=3 lr=1.0000 bs=131072 train_time:162792ms step_avg:403.95ms
step:404/1775 n_predict=3 lr=1.0000 bs=131072 train_time:163196ms step_avg:403.95ms
step:405/1775 n_predict=3 lr=1.0000 bs=131072 train_time:163601ms step_avg:403.95ms
step:406/1775 n_predict=3 lr=1.0000 bs=131072 train_time:164006ms step_avg:403.96ms
step:407/1775 n_predict=3 lr=1.0000 bs=131072 train_time:164407ms step_avg:403.95ms
step:408/1775 n_predict=3 lr=1.0000 bs=131072 train_time:164812ms step_avg:403.95ms
step:409/1775 n_predict=3 lr=1.0000 bs=131072 train_time:165217ms step_avg:403.95ms
step:410/1775 n_predict=3 lr=1.0000 bs=131072 train_time:165622ms step_avg:403.96ms
step:411/1775 n_predict=3 lr=1.0000 bs=131072 train_time:166026ms step_avg:403.96ms
step:412/1775 n_predict=3 lr=1.0000 bs=131072 train_time:166433ms step_avg:403.96ms
step:413/1775 n_predict=3 lr=1.0000 bs=131072 train_time:166836ms step_avg:403.96ms
step:414/1775 n_predict=3 lr=1.0000 bs=131072 train_time:167242ms step_avg:403.97ms
step:415/1775 n_predict=3 lr=1.0000 bs=131072 train_time:167648ms step_avg:403.97ms
step:416/1775 n_predict=3 lr=1.0000 bs=131072 train_time:168054ms step_avg:403.98ms
step:417/1775 n_predict=3 lr=1.0000 bs=131072 train_time:168454ms step_avg:403.97ms
step:418/1775 n_predict=3 lr=1.0000 bs=131072 train_time:168857ms step_avg:403.96ms
step:419/1775 n_predict=3 lr=1.0000 bs=131072 train_time:169255ms step_avg:403.95ms
step:420/1775 n_predict=3 lr=1.0000 bs=131072 train_time:169657ms step_avg:403.95ms
step:421/1775 n_predict=3 lr=1.0000 bs=131072 train_time:170057ms step_avg:403.93ms
step:422/1775 n_predict=3 lr=1.0000 bs=131072 train_time:170457ms step_avg:403.93ms
step:423/1775 n_predict=3 lr=1.0000 bs=131072 train_time:170857ms step_avg:403.92ms
step:424/1775 n_predict=3 lr=1.0000 bs=131072 train_time:171259ms step_avg:403.91ms
step:425/1775 n_predict=3 lr=1.0000 bs=131072 train_time:171659ms step_avg:403.90ms
step:426/1775 n_predict=3 lr=1.0000 bs=131072 train_time:172066ms step_avg:403.91ms
step:427/1775 n_predict=3 lr=1.0000 bs=131072 train_time:172470ms step_avg:403.91ms
step:428/1775 n_predict=3 lr=1.0000 bs=131072 train_time:172875ms step_avg:403.91ms
step:429/1775 n_predict=3 lr=1.0000 bs=131072 train_time:173277ms step_avg:403.91ms
step:430/1775 n_predict=3 lr=1.0000 bs=131072 train_time:173682ms step_avg:403.91ms
step:431/1775 n_predict=3 lr=1.0000 bs=131072 train_time:174087ms step_avg:403.92ms
step:432/1775 n_predict=3 lr=1.0000 bs=131072 train_time:174492ms step_avg:403.92ms
step:433/1775 n_predict=3 lr=1.0000 bs=131072 train_time:174899ms step_avg:403.92ms
step:434/1775 n_predict=3 lr=1.0000 bs=131072 train_time:175304ms step_avg:403.93ms
step:435/1775 n_predict=3 lr=1.0000 bs=131072 train_time:175705ms step_avg:403.92ms
step:436/1775 n_predict=3 lr=1.0000 bs=131072 train_time:176109ms step_avg:403.92ms
step:437/1775 n_predict=3 lr=1.0000 bs=131072 train_time:176515ms step_avg:403.92ms
step:438/1775 n_predict=3 lr=1.0000 bs=131072 train_time:176921ms step_avg:403.93ms
step:439/1775 n_predict=3 lr=1.0000 bs=131072 train_time:177322ms step_avg:403.92ms
step:440/1775 n_predict=3 lr=1.0000 bs=131072 train_time:177730ms step_avg:403.93ms
step:441/1775 n_predict=3 lr=1.0000 bs=131072 train_time:178133ms step_avg:403.93ms
step:442/1775 n_predict=3 lr=1.0000 bs=131072 train_time:178540ms step_avg:403.94ms
step:443/1775 n_predict=3 lr=1.0000 bs=131072 train_time:178942ms step_avg:403.93ms
step:444/1775 n_predict=3 lr=1.0000 bs=131072 train_time:179351ms step_avg:403.94ms
step:445/1775 n_predict=3 lr=1.0000 bs=131072 train_time:179752ms step_avg:403.94ms
step:446/1775 n_predict=3 lr=1.0000 bs=131072 train_time:180156ms step_avg:403.94ms
step:447/1775 n_predict=3 lr=1.0000 bs=131072 train_time:180556ms step_avg:403.93ms
step:448/1775 n_predict=3 lr=1.0000 bs=131072 train_time:180957ms step_avg:403.92ms
step:449/1775 n_predict=3 lr=1.0000 bs=131072 train_time:181357ms step_avg:403.91ms
step:450/1775 n_predict=3 lr=1.0000 bs=131072 train_time:181758ms step_avg:403.91ms
step:451/1775 n_predict=3 lr=1.0000 bs=131072 train_time:182158ms step_avg:403.90ms
step:452/1775 n_predict=3 lr=1.0000 bs=131072 train_time:182558ms step_avg:403.89ms
step:453/1775 n_predict=3 lr=1.0000 bs=131072 train_time:182959ms step_avg:403.88ms
step:454/1775 n_predict=3 lr=1.0000 bs=131072 train_time:183361ms step_avg:403.88ms
step:455/1775 n_predict=3 lr=1.0000 bs=131072 train_time:183767ms step_avg:403.88ms
step:456/1775 n_predict=3 lr=1.0000 bs=131072 train_time:184174ms step_avg:403.89ms
step:457/1775 n_predict=3 lr=1.0000 bs=131072 train_time:184576ms step_avg:403.89ms
step:458/1775 n_predict=3 lr=1.0000 bs=131072 train_time:184982ms step_avg:403.89ms
step:459/1775 n_predict=3 lr=1.0000 bs=131072 train_time:185387ms step_avg:403.89ms
step:460/1775 n_predict=3 lr=1.0000 bs=131072 train_time:185793ms step_avg:403.90ms
step:461/1775 n_predict=3 lr=1.0000 bs=131072 train_time:186198ms step_avg:403.90ms
step:462/1775 n_predict=3 lr=1.0000 bs=131072 train_time:186605ms step_avg:403.91ms
step:463/1775 n_predict=3 lr=1.0000 bs=131072 train_time:187006ms step_avg:403.90ms
step:464/1775 n_predict=3 lr=1.0000 bs=131072 train_time:187408ms step_avg:403.90ms
step:465/1775 n_predict=3 lr=1.0000 bs=131072 train_time:187811ms step_avg:403.89ms
step:466/1775 n_predict=3 lr=1.0000 bs=131072 train_time:188223ms step_avg:403.91ms
step:467/1775 n_predict=3 lr=1.0000 bs=131072 train_time:188620ms step_avg:403.90ms
step:468/1775 n_predict=3 lr=1.0000 bs=131072 train_time:189026ms step_avg:403.90ms
step:469/1775 n_predict=3 lr=1.0000 bs=131072 train_time:189430ms step_avg:403.90ms
step:470/1775 n_predict=3 lr=1.0000 bs=131072 train_time:189835ms step_avg:403.90ms
step:471/1775 n_predict=3 lr=1.0000 bs=131072 train_time:190238ms step_avg:403.90ms
step:472/1775 n_predict=3 lr=1.0000 bs=131072 train_time:190642ms step_avg:403.90ms
step:473/1775 n_predict=3 lr=1.0000 bs=131072 train_time:191049ms step_avg:403.91ms
step:474/1775 n_predict=3 lr=1.0000 bs=131072 train_time:191454ms step_avg:403.91ms
step:475/1775 n_predict=3 lr=1.0000 bs=131072 train_time:191855ms step_avg:403.91ms
step:476/1775 n_predict=3 lr=1.0000 bs=131072 train_time:192259ms step_avg:403.91ms
step:477/1775 n_predict=3 lr=1.0000 bs=131072 train_time:192658ms step_avg:403.89ms
step:478/1775 n_predict=3 lr=1.0000 bs=131072 train_time:193060ms step_avg:403.89ms
step:479/1775 n_predict=3 lr=1.0000 bs=131072 train_time:193461ms step_avg:403.89ms
step:480/1775 n_predict=3 lr=1.0000 bs=131072 train_time:193865ms step_avg:403.89ms
step:481/1775 n_predict=3 lr=1.0000 bs=131072 train_time:194271ms step_avg:403.89ms
step:482/1775 n_predict=3 lr=1.0000 bs=131072 train_time:194675ms step_avg:403.89ms
step:483/1775 n_predict=3 lr=1.0000 bs=131072 train_time:195078ms step_avg:403.89ms
step:484/1775 n_predict=3 lr=1.0000 bs=131072 train_time:195484ms step_avg:403.89ms
step:485/1775 n_predict=3 lr=1.0000 bs=131072 train_time:195888ms step_avg:403.89ms
step:486/1775 n_predict=3 lr=1.0000 bs=131072 train_time:196297ms step_avg:403.90ms
step:487/1775 n_predict=3 lr=1.0000 bs=131072 train_time:196701ms step_avg:403.90ms
step:488/1775 n_predict=3 lr=1.0000 bs=131072 train_time:197105ms step_avg:403.90ms
step:489/1775 n_predict=3 lr=1.0000 bs=131072 train_time:197508ms step_avg:403.90ms
step:490/1775 n_predict=3 lr=1.0000 bs=131072 train_time:197910ms step_avg:403.90ms
step:491/1775 n_predict=3 lr=1.0000 bs=131072 train_time:198313ms step_avg:403.90ms
step:492/1775 n_predict=3 lr=1.0000 bs=131072 train_time:198719ms step_avg:403.90ms
step:493/1775 n_predict=3 lr=1.0000 bs=131072 train_time:199121ms step_avg:403.90ms
step:494/1775 n_predict=3 lr=1.0000 bs=131072 train_time:199526ms step_avg:403.90ms
step:495/1775 n_predict=3 lr=1.0000 bs=131072 train_time:199934ms step_avg:403.91ms
step:496/1775 n_predict=3 lr=1.0000 bs=131072 train_time:200336ms step_avg:403.90ms
step:497/1775 n_predict=3 lr=1.0000 bs=131072 train_time:200740ms step_avg:403.90ms
step:498/1775 n_predict=3 lr=1.0000 bs=131072 train_time:201144ms step_avg:403.90ms
step:499/1775 n_predict=3 lr=1.0000 bs=131072 train_time:201553ms step_avg:403.91ms
step:500/1775 n_predict=3 lr=1.0000 bs=131072 train_time:201955ms step_avg:403.91ms
step:500/1775 lr=1.0000 bs=131072 n_predict=3 val_loss:4.2821 val_malbo_loss:4.3546 train_time:201991ms step_avg:403.98ms
step:501/1775 n_predict=3 lr=1.0000 bs=131072 train_time:202359ms step_avg:403.91ms
step:502/1775 n_predict=3 lr=1.0000 bs=131072 train_time:202764ms step_avg:403.91ms
step:503/1775 n_predict=3 lr=1.0000 bs=131072 train_time:203167ms step_avg:403.91ms
step:504/1775 n_predict=3 lr=1.0000 bs=131072 train_time:203574ms step_avg:403.92ms
step:505/1775 n_predict=3 lr=1.0000 bs=131072 train_time:203977ms step_avg:403.91ms
step:506/1775 n_predict=3 lr=1.0000 bs=131072 train_time:204381ms step_avg:403.92ms
step:507/1775 n_predict=3 lr=1.0000 bs=131072 train_time:204783ms step_avg:403.91ms
step:508/1775 n_predict=3 lr=1.0000 bs=131072 train_time:205189ms step_avg:403.91ms
step:509/1775 n_predict=3 lr=1.0000 bs=131072 train_time:205594ms step_avg:403.92ms
step:510/1775 n_predict=3 lr=1.0000 bs=131072 train_time:206000ms step_avg:403.92ms
step:511/1775 n_predict=3 lr=1.0000 bs=131072 train_time:206401ms step_avg:403.92ms
step:512/1775 n_predict=3 lr=1.0000 bs=131072 train_time:206801ms step_avg:403.91ms
step:513/1775 n_predict=3 lr=1.0000 bs=131072 train_time:207202ms step_avg:403.90ms
step:514/1775 n_predict=3 lr=1.0000 bs=131072 train_time:207604ms step_avg:403.90ms
step:515/1775 n_predict=3 lr=1.0000 bs=131072 train_time:208003ms step_avg:403.89ms
step:516/1775 n_predict=3 lr=1.0000 bs=131072 train_time:208405ms step_avg:403.89ms
step:517/1775 n_predict=3 lr=1.0000 bs=131072 train_time:208806ms step_avg:403.88ms
step:518/1775 n_predict=3 lr=1.0000 bs=131072 train_time:209210ms step_avg:403.88ms
step:519/1775 n_predict=3 lr=1.0000 bs=131072 train_time:209612ms step_avg:403.88ms
step:520/1775 n_predict=3 lr=1.0000 bs=131072 train_time:210020ms step_avg:403.89ms
step:521/1775 n_predict=3 lr=1.0000 bs=131072 train_time:210422ms step_avg:403.88ms
step:522/1775 n_predict=3 lr=1.0000 bs=131072 train_time:210828ms step_avg:403.89ms
step:523/1775 n_predict=3 lr=1.0000 bs=131072 train_time:211234ms step_avg:403.89ms
step:524/1775 n_predict=3 lr=1.0000 bs=131072 train_time:211639ms step_avg:403.89ms
step:525/1775 n_predict=3 lr=1.0000 bs=131072 train_time:212046ms step_avg:403.90ms
step:526/1775 n_predict=3 lr=1.0000 bs=131072 train_time:212450ms step_avg:403.90ms
step:527/1775 n_predict=3 lr=1.0000 bs=131072 train_time:212851ms step_avg:403.89ms
step:528/1775 n_predict=3 lr=1.0000 bs=131072 train_time:213255ms step_avg:403.89ms
step:529/1775 n_predict=3 lr=1.0000 bs=131072 train_time:213657ms step_avg:403.89ms
step:530/1775 n_predict=3 lr=1.0000 bs=131072 train_time:214062ms step_avg:403.89ms
step:531/1775 n_predict=3 lr=1.0000 bs=131072 train_time:214466ms step_avg:403.89ms
step:532/1775 n_predict=3 lr=1.0000 bs=131072 train_time:214870ms step_avg:403.89ms
step:533/1775 n_predict=3 lr=1.0000 bs=131072 train_time:215274ms step_avg:403.89ms
step:534/1775 n_predict=3 lr=1.0000 bs=131072 train_time:215681ms step_avg:403.90ms
step:535/1775 n_predict=3 lr=1.0000 bs=131072 train_time:216084ms step_avg:403.90ms
step:536/1775 n_predict=3 lr=1.0000 bs=131072 train_time:216488ms step_avg:403.90ms
step:537/1775 n_predict=3 lr=1.0000 bs=131072 train_time:216895ms step_avg:403.90ms
step:538/1775 n_predict=3 lr=1.0000 bs=131072 train_time:217300ms step_avg:403.90ms
step:539/1775 n_predict=3 lr=1.0000 bs=131072 train_time:217701ms step_avg:403.90ms
step:540/1775 n_predict=3 lr=1.0000 bs=131072 train_time:218103ms step_avg:403.89ms
step:541/1775 n_predict=3 lr=1.0000 bs=131072 train_time:218503ms step_avg:403.89ms
step:542/1775 n_predict=3 lr=1.0000 bs=131072 train_time:218904ms step_avg:403.88ms
step:543/1775 n_predict=3 lr=1.0000 bs=131072 train_time:219303ms step_avg:403.87ms
step:544/1775 n_predict=3 lr=1.0000 bs=131072 train_time:219705ms step_avg:403.87ms
step:545/1775 n_predict=3 lr=1.0000 bs=131072 train_time:220104ms step_avg:403.86ms
step:546/1775 n_predict=3 lr=1.0000 bs=131072 train_time:220505ms step_avg:403.86ms
step:547/1775 n_predict=3 lr=1.0000 bs=131072 train_time:220905ms step_avg:403.85ms
step:548/1775 n_predict=3 lr=1.0000 bs=131072 train_time:221307ms step_avg:403.84ms
step:549/1775 n_predict=3 lr=1.0000 bs=131072 train_time:221710ms step_avg:403.84ms
step:550/1775 n_predict=3 lr=1.0000 bs=131072 train_time:222117ms step_avg:403.85ms
step:551/1775 n_predict=3 lr=1.0000 bs=131072 train_time:222519ms step_avg:403.85ms
step:552/1775 n_predict=3 lr=1.0000 bs=131072 train_time:222922ms step_avg:403.84ms
step:553/1775 n_predict=3 lr=1.0000 bs=131072 train_time:223326ms step_avg:403.84ms
step:554/1775 n_predict=3 lr=1.0000 bs=131072 train_time:223730ms step_avg:403.85ms
step:555/1775 n_predict=3 lr=1.0000 bs=131072 train_time:224136ms step_avg:403.85ms
step:556/1775 n_predict=3 lr=1.0000 bs=131072 train_time:224541ms step_avg:403.85ms
step:557/1775 n_predict=3 lr=1.0000 bs=131072 train_time:224947ms step_avg:403.85ms
step:558/1775 n_predict=3 lr=1.0000 bs=131072 train_time:225351ms step_avg:403.86ms
step:559/1775 n_predict=3 lr=1.0000 bs=131072 train_time:225751ms step_avg:403.85ms
step:560/1775 n_predict=3 lr=1.0000 bs=131072 train_time:226155ms step_avg:403.85ms
step:561/1775 n_predict=3 lr=1.0000 bs=131072 train_time:226558ms step_avg:403.85ms
step:562/1775 n_predict=3 lr=1.0000 bs=131072 train_time:226965ms step_avg:403.85ms
step:563/1775 n_predict=3 lr=1.0000 bs=131072 train_time:227366ms step_avg:403.85ms
step:564/1775 n_predict=3 lr=1.0000 bs=131072 train_time:227771ms step_avg:403.85ms
step:565/1775 n_predict=3 lr=1.0000 bs=131072 train_time:228178ms step_avg:403.85ms
step:566/1775 n_predict=3 lr=1.0000 bs=131072 train_time:228581ms step_avg:403.85ms
step:567/1775 n_predict=3 lr=1.0000 bs=131072 train_time:228985ms step_avg:403.85ms
step:568/1775 n_predict=3 lr=1.0000 bs=131072 train_time:229390ms step_avg:403.86ms
step:569/1775 n_predict=3 lr=1.0000 bs=131072 train_time:229797ms step_avg:403.86ms
step:570/1775 n_predict=3 lr=1.0000 bs=131072 train_time:230203ms step_avg:403.86ms
step:571/1775 n_predict=3 lr=1.0000 bs=131072 train_time:230601ms step_avg:403.85ms
step:572/1775 n_predict=3 lr=1.0000 bs=131072 train_time:231004ms step_avg:403.85ms
step:573/1775 n_predict=3 lr=1.0000 bs=131072 train_time:231403ms step_avg:403.84ms
step:574/1775 n_predict=3 lr=1.0000 bs=131072 train_time:231805ms step_avg:403.84ms
step:575/1775 n_predict=3 lr=1.0000 bs=131072 train_time:232206ms step_avg:403.84ms
step:576/1775 n_predict=3 lr=1.0000 bs=131072 train_time:232607ms step_avg:403.83ms
step:577/1775 n_predict=3 lr=1.0000 bs=131072 train_time:233009ms step_avg:403.83ms
step:578/1775 n_predict=3 lr=1.0000 bs=131072 train_time:233417ms step_avg:403.84ms
step:579/1775 n_predict=3 lr=1.0000 bs=131072 train_time:233820ms step_avg:403.83ms
step:580/1775 n_predict=2 lr=1.5200 bs=262144 train_time:350723ms step_avg:604.70ms
step:581/1775 n_predict=2 lr=1.5200 bs=262144 train_time:351453ms step_avg:604.91ms
step:582/1775 n_predict=2 lr=1.5200 bs=262144 train_time:352180ms step_avg:605.12ms
step:583/1775 n_predict=2 lr=1.5200 bs=262144 train_time:352899ms step_avg:605.32ms
step:584/1775 n_predict=2 lr=1.5200 bs=262144 train_time:353629ms step_avg:605.53ms
step:585/1775 n_predict=2 lr=1.5200 bs=262144 train_time:354357ms step_avg:605.74ms
step:586/1775 n_predict=2 lr=1.5200 bs=262144 train_time:355084ms step_avg:605.94ms
step:587/1775 n_predict=2 lr=1.5200 bs=262144 train_time:355808ms step_avg:606.15ms
step:588/1775 n_predict=2 lr=1.5200 bs=262144 train_time:356534ms step_avg:606.35ms
step:589/1775 n_predict=2 lr=1.5200 bs=262144 train_time:357264ms step_avg:606.56ms
step:590/1775 n_predict=2 lr=1.5200 bs=262144 train_time:357991ms step_avg:606.76ms
step:591/1775 n_predict=2 lr=1.5200 bs=262144 train_time:358720ms step_avg:606.97ms
step:592/1775 n_predict=2 lr=1.5200 bs=262144 train_time:359447ms step_avg:607.17ms
step:593/1775 n_predict=2 lr=1.5200 bs=262144 train_time:360170ms step_avg:607.37ms
step:594/1775 n_predict=2 lr=1.5200 bs=262144 train_time:360903ms step_avg:607.58ms
step:595/1775 n_predict=2 lr=1.5200 bs=262144 train_time:361630ms step_avg:607.78ms
step:596/1775 n_predict=2 lr=1.5200 bs=262144 train_time:362362ms step_avg:607.99ms
step:597/1775 n_predict=2 lr=1.5200 bs=262144 train_time:363090ms step_avg:608.19ms
step:598/1775 n_predict=2 lr=1.5200 bs=262144 train_time:363825ms step_avg:608.40ms
step:599/1775 n_predict=2 lr=1.5200 bs=262144 train_time:364552ms step_avg:608.60ms
step:600/1775 n_predict=2 lr=1.5200 bs=262144 train_time:365282ms step_avg:608.80ms
step:601/1775 n_predict=2 lr=1.5200 bs=262144 train_time:366011ms step_avg:609.00ms
step:602/1775 n_predict=2 lr=1.5200 bs=262144 train_time:366742ms step_avg:609.21ms
step:603/1775 n_predict=2 lr=1.5200 bs=262144 train_time:367470ms step_avg:609.40ms
step:604/1775 n_predict=2 lr=1.5200 bs=262144 train_time:368205ms step_avg:609.61ms
step:605/1775 n_predict=2 lr=1.5200 bs=262144 train_time:368933ms step_avg:609.81ms
step:606/1775 n_predict=2 lr=1.5200 bs=262144 train_time:369665ms step_avg:610.01ms
step:607/1775 n_predict=2 lr=1.5200 bs=262144 train_time:370393ms step_avg:610.20ms
step:608/1775 n_predict=2 lr=1.5200 bs=262144 train_time:371127ms step_avg:610.41ms
step:609/1775 n_predict=2 lr=1.5200 bs=262144 train_time:371855ms step_avg:610.60ms
step:610/1775 n_predict=2 lr=1.5200 bs=262144 train_time:372583ms step_avg:610.79ms
step:611/1775 n_predict=2 lr=1.5200 bs=262144 train_time:373312ms step_avg:610.99ms
step:612/1775 n_predict=2 lr=1.5200 bs=262144 train_time:374047ms step_avg:611.19ms
step:613/1775 n_predict=2 lr=1.5200 bs=262144 train_time:374772ms step_avg:611.37ms
step:614/1775 n_predict=2 lr=1.5200 bs=262144 train_time:375503ms step_avg:611.57ms
step:615/1775 n_predict=2 lr=1.5200 bs=262144 train_time:376234ms step_avg:611.76ms
step:616/1775 n_predict=2 lr=1.5200 bs=262144 train_time:376967ms step_avg:611.96ms
step:617/1775 n_predict=2 lr=1.5200 bs=262144 train_time:377692ms step_avg:612.14ms
step:618/1775 n_predict=2 lr=1.5200 bs=262144 train_time:378432ms step_avg:612.35ms
step:619/1775 n_predict=2 lr=1.5200 bs=262144 train_time:379166ms step_avg:612.55ms
step:620/1775 n_predict=2 lr=1.5200 bs=262144 train_time:379894ms step_avg:612.73ms
step:621/1775 n_predict=2 lr=1.5200 bs=262144 train_time:380629ms step_avg:612.93ms
step:622/1775 n_predict=2 lr=1.5200 bs=262144 train_time:381367ms step_avg:613.13ms
step:623/1775 n_predict=2 lr=1.5200 bs=262144 train_time:382094ms step_avg:613.31ms
step:624/1775 n_predict=2 lr=1.5200 bs=262144 train_time:382829ms step_avg:613.51ms
step:625/1775 n_predict=2 lr=1.5200 bs=262144 train_time:383562ms step_avg:613.70ms
step:626/1775 n_predict=2 lr=1.5200 bs=262144 train_time:384293ms step_avg:613.89ms
step:627/1775 n_predict=2 lr=1.5200 bs=262144 train_time:385026ms step_avg:614.08ms
step:628/1775 n_predict=2 lr=1.5200 bs=262144 train_time:385761ms step_avg:614.27ms
step:629/1775 n_predict=2 lr=1.5200 bs=262144 train_time:386491ms step_avg:614.45ms
step:630/1775 n_predict=2 lr=1.5200 bs=262144 train_time:387228ms step_avg:614.65ms
step:631/1775 n_predict=2 lr=1.5200 bs=262144 train_time:387962ms step_avg:614.84ms
step:632/1775 n_predict=2 lr=1.5200 bs=262144 train_time:388694ms step_avg:615.02ms
step:633/1775 n_predict=2 lr=1.5200 bs=262144 train_time:389428ms step_avg:615.21ms
step:634/1775 n_predict=2 lr=1.5200 bs=262144 train_time:390166ms step_avg:615.40ms
step:635/1775 n_predict=2 lr=1.5200 bs=262144 train_time:390895ms step_avg:615.58ms
step:636/1775 n_predict=2 lr=1.5200 bs=262144 train_time:391635ms step_avg:615.78ms
step:637/1775 n_predict=2 lr=1.5200 bs=262144 train_time:392367ms step_avg:615.96ms
step:638/1775 n_predict=2 lr=1.5200 bs=262144 train_time:393100ms step_avg:616.14ms
step:639/1775 n_predict=2 lr=1.5200 bs=262144 train_time:393834ms step_avg:616.33ms
step:640/1775 n_predict=2 lr=1.5200 bs=262144 train_time:394569ms step_avg:616.51ms
step:641/1775 n_predict=2 lr=1.5200 bs=262144 train_time:395303ms step_avg:616.70ms
step:642/1775 n_predict=2 lr=1.5200 bs=262144 train_time:396037ms step_avg:616.88ms
step:643/1775 n_predict=2 lr=1.5200 bs=262144 train_time:396772ms step_avg:617.06ms
step:644/1775 n_predict=2 lr=1.5200 bs=262144 train_time:397510ms step_avg:617.25ms
step:645/1775 n_predict=2 lr=1.5200 bs=262144 train_time:398240ms step_avg:617.43ms
step:646/1775 n_predict=2 lr=1.5200 bs=262144 train_time:398976ms step_avg:617.61ms
step:647/1775 n_predict=2 lr=1.5200 bs=262144 train_time:399713ms step_avg:617.79ms
step:648/1775 n_predict=2 lr=1.5200 bs=262144 train_time:400446ms step_avg:617.97ms
step:649/1775 n_predict=2 lr=1.5200 bs=262144 train_time:401179ms step_avg:618.15ms
step:650/1775 n_predict=2 lr=1.5200 bs=262144 train_time:401917ms step_avg:618.33ms
step:651/1775 n_predict=2 lr=1.5200 bs=262144 train_time:402650ms step_avg:618.51ms
step:652/1775 n_predict=2 lr=1.5200 bs=262144 train_time:403388ms step_avg:618.69ms
step:653/1775 n_predict=2 lr=1.5200 bs=262144 train_time:404123ms step_avg:618.87ms
step:654/1775 n_predict=2 lr=1.5200 bs=262144 train_time:404859ms step_avg:619.05ms
step:655/1775 n_predict=2 lr=1.5200 bs=262144 train_time:405591ms step_avg:619.22ms
step:656/1775 n_predict=2 lr=1.5200 bs=262144 train_time:406331ms step_avg:619.41ms
step:657/1775 n_predict=2 lr=1.5200 bs=262144 train_time:407067ms step_avg:619.58ms
step:658/1775 n_predict=2 lr=1.5200 bs=262144 train_time:407800ms step_avg:619.76ms
step:659/1775 n_predict=2 lr=1.5200 bs=262144 train_time:408535ms step_avg:619.93ms
step:660/1775 n_predict=2 lr=1.5200 bs=262144 train_time:409273ms step_avg:620.11ms
step:661/1775 n_predict=2 lr=1.5200 bs=262144 train_time:410008ms step_avg:620.28ms
step:662/1775 n_predict=2 lr=1.5200 bs=262144 train_time:410745ms step_avg:620.46ms
step:663/1775 n_predict=2 lr=1.5200 bs=262144 train_time:411477ms step_avg:620.63ms
step:664/1775 n_predict=2 lr=1.5200 bs=262144 train_time:412212ms step_avg:620.80ms
step:665/1775 n_predict=2 lr=1.5200 bs=262144 train_time:412945ms step_avg:620.97ms
step:666/1775 n_predict=2 lr=1.5200 bs=262144 train_time:413685ms step_avg:621.15ms
step:667/1775 n_predict=2 lr=1.5200 bs=262144 train_time:414420ms step_avg:621.32ms
step:668/1775 n_predict=2 lr=1.5200 bs=262144 train_time:415159ms step_avg:621.50ms
step:669/1775 n_predict=2 lr=1.5200 bs=262144 train_time:415894ms step_avg:621.66ms
step:670/1775 n_predict=2 lr=1.5200 bs=262144 train_time:416634ms step_avg:621.84ms
step:671/1775 n_predict=2 lr=1.5200 bs=262144 train_time:417368ms step_avg:622.01ms
step:672/1775 n_predict=2 lr=1.5200 bs=262144 train_time:418107ms step_avg:622.18ms
step:673/1775 n_predict=2 lr=1.5200 bs=262144 train_time:418845ms step_avg:622.35ms
step:674/1775 n_predict=2 lr=1.5200 bs=262144 train_time:419583ms step_avg:622.53ms
step:675/1775 n_predict=2 lr=1.5200 bs=262144 train_time:420319ms step_avg:622.69ms
step:676/1775 n_predict=2 lr=1.5200 bs=262144 train_time:421059ms step_avg:622.87ms
step:677/1775 n_predict=2 lr=1.5200 bs=262144 train_time:421793ms step_avg:623.03ms
step:678/1775 n_predict=2 lr=1.5200 bs=262144 train_time:422536ms step_avg:623.21ms
step:679/1775 n_predict=2 lr=1.5200 bs=262144 train_time:423271ms step_avg:623.37ms
step:680/1775 n_predict=2 lr=1.5200 bs=262144 train_time:424013ms step_avg:623.55ms
step:681/1775 n_predict=2 lr=1.5200 bs=262144 train_time:424749ms step_avg:623.71ms
step:682/1775 n_predict=2 lr=1.5200 bs=262144 train_time:425488ms step_avg:623.88ms
step:683/1775 n_predict=2 lr=1.5200 bs=262144 train_time:426224ms step_avg:624.05ms
step:684/1775 n_predict=2 lr=1.5200 bs=262144 train_time:426966ms step_avg:624.22ms
step:685/1775 n_predict=2 lr=1.5200 bs=262144 train_time:427698ms step_avg:624.38ms
step:686/1775 n_predict=2 lr=1.5200 bs=262144 train_time:428439ms step_avg:624.55ms
step:687/1775 n_predict=2 lr=1.5200 bs=262144 train_time:429177ms step_avg:624.71ms
step:688/1775 n_predict=2 lr=1.5200 bs=262144 train_time:429916ms step_avg:624.88ms
step:689/1775 n_predict=2 lr=1.5200 bs=262144 train_time:430653ms step_avg:625.04ms
step:690/1775 n_predict=2 lr=1.5200 bs=262144 train_time:431394ms step_avg:625.21ms
step:691/1775 n_predict=2 lr=1.5200 bs=262144 train_time:432133ms step_avg:625.37ms
step:692/1775 n_predict=2 lr=1.5200 bs=262144 train_time:432873ms step_avg:625.54ms
step:693/1775 n_predict=2 lr=1.5200 bs=262144 train_time:433611ms step_avg:625.70ms
step:694/1775 n_predict=2 lr=1.5200 bs=262144 train_time:434350ms step_avg:625.86ms
step:695/1775 n_predict=2 lr=1.5200 bs=262144 train_time:435089ms step_avg:626.03ms
step:696/1775 n_predict=2 lr=1.5200 bs=262144 train_time:435832ms step_avg:626.20ms
step:697/1775 n_predict=2 lr=1.5200 bs=262144 train_time:436571ms step_avg:626.36ms
step:698/1775 n_predict=2 lr=1.5200 bs=262144 train_time:437311ms step_avg:626.52ms
step:699/1775 n_predict=2 lr=1.5200 bs=262144 train_time:438052ms step_avg:626.68ms
step:700/1775 n_predict=2 lr=1.5200 bs=262144 train_time:438789ms step_avg:626.84ms
step:701/1775 n_predict=2 lr=1.5200 bs=262144 train_time:439529ms step_avg:627.00ms
step:702/1775 n_predict=2 lr=1.5200 bs=262144 train_time:440270ms step_avg:627.16ms
step:703/1775 n_predict=2 lr=1.5200 bs=262144 train_time:441007ms step_avg:627.32ms
step:704/1775 n_predict=2 lr=1.5200 bs=262144 train_time:441747ms step_avg:627.48ms
step:705/1775 n_predict=2 lr=1.5200 bs=262144 train_time:442485ms step_avg:627.64ms
step:706/1775 n_predict=2 lr=1.5200 bs=262144 train_time:443225ms step_avg:627.80ms
step:707/1775 n_predict=2 lr=1.5200 bs=262144 train_time:443963ms step_avg:627.95ms
step:708/1775 n_predict=2 lr=1.5200 bs=262144 train_time:444699ms step_avg:628.11ms
step:709/1775 n_predict=2 lr=1.5200 bs=262144 train_time:445438ms step_avg:628.26ms
step:710/1775 n_predict=2 lr=1.5200 bs=262144 train_time:446182ms step_avg:628.43ms
step:711/1775 n_predict=2 lr=1.5200 bs=262144 train_time:446920ms step_avg:628.58ms
step:712/1775 n_predict=2 lr=1.5200 bs=262144 train_time:447661ms step_avg:628.74ms
step:713/1775 n_predict=2 lr=1.5200 bs=262144 train_time:448396ms step_avg:628.89ms
step:714/1775 n_predict=2 lr=1.5200 bs=262144 train_time:449139ms step_avg:629.05ms
step:715/1775 n_predict=2 lr=1.5200 bs=262144 train_time:449880ms step_avg:629.20ms
step:716/1775 n_predict=2 lr=1.5200 bs=262144 train_time:450621ms step_avg:629.36ms
step:717/1775 n_predict=2 lr=1.5200 bs=262144 train_time:451362ms step_avg:629.51ms
step:718/1775 n_predict=2 lr=1.5200 bs=262144 train_time:452101ms step_avg:629.67ms
step:719/1775 n_predict=2 lr=1.5200 bs=262144 train_time:452840ms step_avg:629.82ms
step:720/1775 n_predict=2 lr=1.5200 bs=262144 train_time:453581ms step_avg:629.97ms
step:721/1775 n_predict=2 lr=1.5200 bs=262144 train_time:454316ms step_avg:630.12ms
step:722/1775 n_predict=2 lr=1.5200 bs=262144 train_time:455060ms step_avg:630.28ms
step:723/1775 n_predict=2 lr=1.5200 bs=262144 train_time:455796ms step_avg:630.42ms
step:724/1775 n_predict=2 lr=1.5200 bs=262144 train_time:456538ms step_avg:630.58ms
step:725/1775 n_predict=2 lr=1.5200 bs=262144 train_time:457278ms step_avg:630.73ms
step:726/1775 n_predict=2 lr=1.5200 bs=262144 train_time:458021ms step_avg:630.88ms
step:727/1775 n_predict=2 lr=1.5200 bs=262144 train_time:458760ms step_avg:631.03ms
step:728/1775 n_predict=2 lr=1.5200 bs=262144 train_time:459496ms step_avg:631.18ms
step:729/1775 n_predict=2 lr=1.5200 bs=262144 train_time:460237ms step_avg:631.33ms
step:730/1775 n_predict=2 lr=1.5200 bs=262144 train_time:460979ms step_avg:631.48ms
step:731/1775 n_predict=2 lr=1.5200 bs=262144 train_time:461717ms step_avg:631.62ms
step:732/1775 n_predict=2 lr=1.5200 bs=262144 train_time:462459ms step_avg:631.77ms
step:733/1775 n_predict=2 lr=1.5200 bs=262144 train_time:463198ms step_avg:631.92ms
step:734/1775 n_predict=2 lr=1.5200 bs=262144 train_time:463939ms step_avg:632.07ms
step:735/1775 n_predict=2 lr=1.5200 bs=262144 train_time:464676ms step_avg:632.21ms
step:736/1775 n_predict=2 lr=1.5200 bs=262144 train_time:465418ms step_avg:632.36ms
step:737/1775 n_predict=2 lr=1.5200 bs=262144 train_time:466156ms step_avg:632.50ms
step:738/1775 n_predict=2 lr=1.5200 bs=262144 train_time:466898ms step_avg:632.65ms
step:739/1775 n_predict=2 lr=1.5200 bs=262144 train_time:467635ms step_avg:632.79ms
step:740/1775 n_predict=2 lr=1.5200 bs=262144 train_time:468379ms step_avg:632.94ms
step:741/1775 n_predict=2 lr=1.5200 bs=262144 train_time:469118ms step_avg:633.09ms
step:742/1775 n_predict=2 lr=1.5200 bs=262144 train_time:469859ms step_avg:633.23ms
step:743/1775 n_predict=2 lr=1.5200 bs=262144 train_time:470596ms step_avg:633.37ms
step:744/1775 n_predict=2 lr=1.5200 bs=262144 train_time:471340ms step_avg:633.52ms
step:745/1775 n_predict=2 lr=1.5200 bs=262144 train_time:472077ms step_avg:633.66ms
step:746/1775 n_predict=2 lr=1.5200 bs=262144 train_time:472818ms step_avg:633.80ms
step:747/1775 n_predict=2 lr=1.5200 bs=262144 train_time:473558ms step_avg:633.95ms
step:748/1775 n_predict=2 lr=1.5200 bs=262144 train_time:474298ms step_avg:634.09ms
step:749/1775 n_predict=2 lr=1.5200 bs=262144 train_time:475041ms step_avg:634.23ms
step:750/1775 n_predict=2 lr=1.5200 bs=262144 train_time:475783ms step_avg:634.38ms
step:750/1775 lr=1.5200 bs=262144 n_predict=2 val_loss:4.0045 val_malbo_loss:4.0823 train_time:475845ms step_avg:634.46ms
step:751/1775 n_predict=2 lr=1.5200 bs=262144 train_time:476509ms step_avg:634.50ms
step:752/1775 n_predict=2 lr=1.5200 bs=262144 train_time:477236ms step_avg:634.62ms
step:753/1775 n_predict=2 lr=1.5200 bs=262144 train_time:477963ms step_avg:634.74ms
step:754/1775 n_predict=2 lr=1.5200 bs=262144 train_time:478691ms step_avg:634.87ms
step:755/1775 n_predict=2 lr=1.5200 bs=262144 train_time:479414ms step_avg:634.99ms
step:756/1775 n_predict=2 lr=1.5200 bs=262144 train_time:480145ms step_avg:635.11ms
step:757/1775 n_predict=2 lr=1.5200 bs=262144 train_time:480873ms step_avg:635.23ms
step:758/1775 n_predict=2 lr=1.5200 bs=262144 train_time:481602ms step_avg:635.36ms
step:759/1775 n_predict=2 lr=1.5200 bs=262144 train_time:482326ms step_avg:635.48ms
step:760/1775 n_predict=2 lr=1.5200 bs=262144 train_time:483057ms step_avg:635.60ms
step:761/1775 n_predict=2 lr=1.5200 bs=262144 train_time:483785ms step_avg:635.72ms
step:762/1775 n_predict=2 lr=1.5200 bs=262144 train_time:484517ms step_avg:635.85ms
step:763/1775 n_predict=2 lr=1.5200 bs=262144 train_time:485247ms step_avg:635.97ms
step:764/1775 n_predict=2 lr=1.5200 bs=262144 train_time:485974ms step_avg:636.09ms
step:765/1775 n_predict=2 lr=1.5200 bs=262144 train_time:486705ms step_avg:636.22ms
step:766/1775 n_predict=2 lr=1.5200 bs=262144 train_time:487436ms step_avg:636.34ms
step:767/1775 n_predict=2 lr=1.5200 bs=262144 train_time:488164ms step_avg:636.46ms
step:768/1775 n_predict=2 lr=1.5200 bs=262144 train_time:488895ms step_avg:636.58ms
step:769/1775 n_predict=2 lr=1.5200 bs=262144 train_time:489626ms step_avg:636.70ms
step:770/1775 n_predict=2 lr=1.5200 bs=262144 train_time:490361ms step_avg:636.83ms
step:771/1775 n_predict=2 lr=1.5200 bs=262144 train_time:491088ms step_avg:636.95ms
step:772/1775 n_predict=2 lr=1.5200 bs=262144 train_time:491819ms step_avg:637.07ms
step:773/1775 n_predict=2 lr=1.5200 bs=262144 train_time:492549ms step_avg:637.19ms
step:774/1775 n_predict=2 lr=1.5200 bs=262144 train_time:493280ms step_avg:637.31ms
step:775/1775 n_predict=2 lr=1.5200 bs=262144 train_time:494011ms step_avg:637.43ms
step:776/1775 n_predict=2 lr=1.5200 bs=262144 train_time:494739ms step_avg:637.55ms
step:777/1775 n_predict=2 lr=1.5200 bs=262144 train_time:495470ms step_avg:637.67ms
step:778/1775 n_predict=2 lr=1.5200 bs=262144 train_time:496205ms step_avg:637.80ms
step:779/1775 n_predict=2 lr=1.5200 bs=262144 train_time:496931ms step_avg:637.91ms
step:780/1775 n_predict=2 lr=1.5200 bs=262144 train_time:497666ms step_avg:638.03ms
step:781/1775 n_predict=2 lr=1.5200 bs=262144 train_time:498393ms step_avg:638.15ms
step:782/1775 n_predict=2 lr=1.5200 bs=262144 train_time:499127ms step_avg:638.27ms
step:783/1775 n_predict=2 lr=1.5200 bs=262144 train_time:499855ms step_avg:638.38ms
step:784/1775 n_predict=2 lr=1.5200 bs=262144 train_time:500591ms step_avg:638.51ms
step:785/1775 n_predict=2 lr=1.5200 bs=262144 train_time:501323ms step_avg:638.63ms
step:786/1775 n_predict=2 lr=1.5200 bs=262144 train_time:502056ms step_avg:638.75ms
step:787/1775 n_predict=2 lr=1.5200 bs=262144 train_time:502789ms step_avg:638.87ms
step:788/1775 n_predict=2 lr=1.5200 bs=262144 train_time:503524ms step_avg:638.99ms
step:789/1775 n_predict=2 lr=1.5200 bs=262144 train_time:504259ms step_avg:639.11ms
step:790/1775 n_predict=2 lr=1.5200 bs=262144 train_time:504991ms step_avg:639.23ms
step:791/1775 n_predict=2 lr=1.5200 bs=262144 train_time:505722ms step_avg:639.35ms
step:792/1775 n_predict=2 lr=1.5200 bs=262144 train_time:506460ms step_avg:639.47ms
step:793/1775 n_predict=2 lr=1.5200 bs=262144 train_time:507190ms step_avg:639.58ms
step:794/1775 n_predict=2 lr=1.5200 bs=262144 train_time:507924ms step_avg:639.70ms
step:795/1775 n_predict=2 lr=1.5200 bs=262144 train_time:508658ms step_avg:639.82ms
step:796/1775 n_predict=2 lr=1.5200 bs=262144 train_time:509394ms step_avg:639.94ms
step:797/1775 n_predict=2 lr=1.5200 bs=262144 train_time:510126ms step_avg:640.06ms
step:798/1775 n_predict=2 lr=1.5200 bs=262144 train_time:510863ms step_avg:640.18ms
step:799/1775 n_predict=2 lr=1.5200 bs=262144 train_time:511595ms step_avg:640.29ms
step:800/1775 n_predict=2 lr=1.5200 bs=262144 train_time:512331ms step_avg:640.41ms
step:801/1775 n_predict=2 lr=1.5200 bs=262144 train_time:513067ms step_avg:640.53ms
step:802/1775 n_predict=2 lr=1.5200 bs=262144 train_time:513801ms step_avg:640.65ms
step:803/1775 n_predict=2 lr=1.5200 bs=262144 train_time:514535ms step_avg:640.77ms
step:804/1775 n_predict=2 lr=1.5200 bs=262144 train_time:515275ms step_avg:640.89ms
step:805/1775 n_predict=2 lr=1.5200 bs=262144 train_time:516008ms step_avg:641.00ms
step:806/1775 n_predict=2 lr=1.5200 bs=262144 train_time:516741ms step_avg:641.12ms
step:807/1775 n_predict=2 lr=1.5200 bs=262144 train_time:517478ms step_avg:641.24ms
step:808/1775 n_predict=2 lr=1.5200 bs=262144 train_time:518214ms step_avg:641.35ms
step:809/1775 n_predict=2 lr=1.5200 bs=262144 train_time:518951ms step_avg:641.47ms
step:810/1775 n_predict=2 lr=1.5200 bs=262144 train_time:519687ms step_avg:641.59ms
step:811/1775 n_predict=2 lr=1.5200 bs=262144 train_time:520421ms step_avg:641.70ms
step:812/1775 n_predict=2 lr=1.5200 bs=262144 train_time:521159ms step_avg:641.82ms
step:813/1775 n_predict=2 lr=1.5200 bs=262144 train_time:521892ms step_avg:641.93ms
step:814/1775 n_predict=2 lr=1.5200 bs=262144 train_time:522633ms step_avg:642.06ms
step:815/1775 n_predict=2 lr=1.5200 bs=262144 train_time:523366ms step_avg:642.17ms
step:816/1775 n_predict=2 lr=1.5200 bs=262144 train_time:524099ms step_avg:642.28ms
step:817/1775 n_predict=2 lr=1.5200 bs=262144 train_time:524836ms step_avg:642.39ms
step:818/1775 n_predict=2 lr=1.5200 bs=262144 train_time:525572ms step_avg:642.51ms
step:819/1775 n_predict=2 lr=1.5200 bs=262144 train_time:526308ms step_avg:642.62ms
step:820/1775 n_predict=2 lr=1.5200 bs=262144 train_time:527042ms step_avg:642.73ms
step:821/1775 n_predict=2 lr=1.5200 bs=262144 train_time:527775ms step_avg:642.84ms
step:822/1775 n_predict=2 lr=1.5200 bs=262144 train_time:528512ms step_avg:642.96ms
step:823/1775 n_predict=2 lr=1.5200 bs=262144 train_time:529250ms step_avg:643.07ms
step:824/1775 n_predict=2 lr=1.5200 bs=262144 train_time:529983ms step_avg:643.18ms
step:825/1775 n_predict=2 lr=1.5200 bs=262144 train_time:530716ms step_avg:643.29ms
step:826/1775 n_predict=2 lr=1.5200 bs=262144 train_time:531456ms step_avg:643.41ms
step:827/1775 n_predict=2 lr=1.5200 bs=262144 train_time:532190ms step_avg:643.52ms
step:828/1775 n_predict=2 lr=1.5200 bs=262144 train_time:532924ms step_avg:643.63ms
step:829/1775 n_predict=2 lr=1.5200 bs=262144 train_time:533660ms step_avg:643.74ms
step:830/1775 n_predict=2 lr=1.5200 bs=262144 train_time:534398ms step_avg:643.85ms
step:831/1775 n_predict=2 lr=1.5200 bs=262144 train_time:535136ms step_avg:643.97ms
step:832/1775 n_predict=2 lr=1.5200 bs=262144 train_time:535872ms step_avg:644.08ms
step:833/1775 n_predict=2 lr=1.5200 bs=262144 train_time:536611ms step_avg:644.19ms
step:834/1775 n_predict=2 lr=1.5200 bs=262144 train_time:537349ms step_avg:644.30ms
step:835/1775 n_predict=2 lr=1.5200 bs=262144 train_time:538085ms step_avg:644.41ms
step:836/1775 n_predict=2 lr=1.5200 bs=262144 train_time:538820ms step_avg:644.52ms
step:837/1775 n_predict=2 lr=1.5200 bs=262144 train_time:539558ms step_avg:644.63ms
step:838/1775 n_predict=2 lr=1.5200 bs=262144 train_time:540295ms step_avg:644.74ms
step:839/1775 n_predict=2 lr=1.5200 bs=262144 train_time:541029ms step_avg:644.85ms
step:840/1775 n_predict=2 lr=1.5200 bs=262144 train_time:541771ms step_avg:644.97ms
step:841/1775 n_predict=2 lr=1.5200 bs=262144 train_time:542505ms step_avg:645.07ms
step:842/1775 n_predict=2 lr=1.5200 bs=262144 train_time:543241ms step_avg:645.18ms
step:843/1775 n_predict=2 lr=1.5200 bs=262144 train_time:543978ms step_avg:645.29ms
step:844/1775 n_predict=2 lr=1.5200 bs=262144 train_time:544716ms step_avg:645.40ms
step:845/1775 n_predict=2 lr=1.5200 bs=262144 train_time:545453ms step_avg:645.51ms
step:846/1775 n_predict=2 lr=1.5200 bs=262144 train_time:546194ms step_avg:645.62ms
step:847/1775 n_predict=2 lr=1.5200 bs=262144 train_time:546929ms step_avg:645.72ms
step:848/1775 n_predict=2 lr=1.5200 bs=262144 train_time:547670ms step_avg:645.84ms
step:849/1775 n_predict=2 lr=1.5200 bs=262144 train_time:548403ms step_avg:645.94ms
step:850/1775 n_predict=2 lr=1.5200 bs=262144 train_time:549144ms step_avg:646.05ms
step:851/1775 n_predict=2 lr=1.5200 bs=262144 train_time:549883ms step_avg:646.16ms
step:852/1775 n_predict=2 lr=1.5200 bs=262144 train_time:550618ms step_avg:646.26ms
step:853/1775 n_predict=2 lr=1.5200 bs=262144 train_time:551360ms step_avg:646.38ms
step:854/1775 n_predict=2 lr=1.5200 bs=262144 train_time:552098ms step_avg:646.48ms
step:855/1775 n_predict=2 lr=1.5200 bs=262144 train_time:552836ms step_avg:646.59ms
step:856/1775 n_predict=2 lr=1.5200 bs=262144 train_time:553573ms step_avg:646.70ms
step:857/1775 n_predict=2 lr=1.5200 bs=262144 train_time:554310ms step_avg:646.80ms
step:858/1775 n_predict=2 lr=1.5200 bs=262144 train_time:555046ms step_avg:646.91ms
step:859/1775 n_predict=2 lr=1.5200 bs=262144 train_time:555783ms step_avg:647.01ms
step:860/1775 n_predict=2 lr=1.5200 bs=262144 train_time:556520ms step_avg:647.12ms
step:861/1775 n_predict=2 lr=1.5200 bs=262144 train_time:557256ms step_avg:647.22ms
step:862/1775 n_predict=2 lr=1.5200 bs=262144 train_time:557997ms step_avg:647.33ms
step:863/1775 n_predict=2 lr=1.5200 bs=262144 train_time:558738ms step_avg:647.44ms
step:864/1775 n_predict=2 lr=1.5200 bs=262144 train_time:559476ms step_avg:647.54ms
step:865/1775 n_predict=2 lr=1.5200 bs=262144 train_time:560212ms step_avg:647.64ms
step:866/1775 n_predict=2 lr=1.5200 bs=262144 train_time:560953ms step_avg:647.75ms
step:867/1775 n_predict=2 lr=1.5200 bs=262144 train_time:561695ms step_avg:647.86ms
step:868/1775 n_predict=2 lr=1.5200 bs=262144 train_time:562434ms step_avg:647.97ms
step:869/1775 n_predict=2 lr=1.5192 bs=262144 train_time:563172ms step_avg:648.07ms
step:870/1775 n_predict=2 lr=1.5175 bs=262144 train_time:563910ms step_avg:648.17ms
step:871/1775 n_predict=2 lr=1.5159 bs=262144 train_time:564648ms step_avg:648.28ms
step:872/1775 n_predict=2 lr=1.5143 bs=262144 train_time:565388ms step_avg:648.38ms
step:873/1775 n_predict=2 lr=1.5126 bs=262144 train_time:566118ms step_avg:648.47ms
step:874/1775 n_predict=2 lr=1.5110 bs=262144 train_time:566863ms step_avg:648.58ms
step:875/1775 n_predict=2 lr=1.5094 bs=262144 train_time:567601ms step_avg:648.69ms
step:876/1775 n_predict=2 lr=1.5077 bs=262144 train_time:568341ms step_avg:648.79ms
step:877/1775 n_predict=2 lr=1.5061 bs=262144 train_time:569078ms step_avg:648.89ms
step:878/1775 n_predict=2 lr=1.5044 bs=262144 train_time:569817ms step_avg:648.99ms
step:879/1775 n_predict=2 lr=1.5028 bs=262144 train_time:570558ms step_avg:649.10ms
step:880/1775 n_predict=2 lr=1.5012 bs=262144 train_time:571298ms step_avg:649.20ms
step:881/1775 n_predict=2 lr=1.4995 bs=262144 train_time:572035ms step_avg:649.30ms
step:882/1775 n_predict=2 lr=1.4979 bs=262144 train_time:572774ms step_avg:649.40ms
step:883/1775 n_predict=2 lr=1.4963 bs=262144 train_time:573513ms step_avg:649.50ms
step:884/1775 n_predict=2 lr=1.4946 bs=262144 train_time:574254ms step_avg:649.61ms
step:885/1775 n_predict=2 lr=1.4930 bs=262144 train_time:574993ms step_avg:649.71ms
step:886/1775 n_predict=2 lr=1.4914 bs=262144 train_time:575735ms step_avg:649.81ms
step:887/1775 n_predict=2 lr=1.4897 bs=262144 train_time:576472ms step_avg:649.91ms
step:888/1775 n_predict=2 lr=1.4881 bs=262144 train_time:577217ms step_avg:650.02ms
step:889/1775 n_predict=2 lr=1.4864 bs=262144 train_time:577954ms step_avg:650.12ms
step:890/1775 n_predict=2 lr=1.4848 bs=262144 train_time:578693ms step_avg:650.22ms
step:891/1775 n_predict=2 lr=1.4832 bs=262144 train_time:579433ms step_avg:650.32ms
step:892/1775 n_predict=2 lr=1.4815 bs=262144 train_time:580178ms step_avg:650.42ms
step:893/1775 n_predict=2 lr=1.4799 bs=262144 train_time:580912ms step_avg:650.52ms
step:894/1775 n_predict=2 lr=1.4783 bs=262144 train_time:581652ms step_avg:650.62ms
step:895/1775 n_predict=2 lr=1.4766 bs=262144 train_time:582391ms step_avg:650.72ms
step:896/1775 n_predict=2 lr=1.4750 bs=262144 train_time:583131ms step_avg:650.82ms
step:897/1775 n_predict=2 lr=1.4733 bs=262144 train_time:583871ms step_avg:650.92ms
step:898/1775 n_predict=2 lr=1.4717 bs=262144 train_time:584612ms step_avg:651.02ms
step:899/1775 n_predict=2 lr=1.4701 bs=262144 train_time:585350ms step_avg:651.11ms
step:900/1775 n_predict=2 lr=1.4684 bs=262144 train_time:586090ms step_avg:651.21ms
step:901/1775 n_predict=2 lr=1.4668 bs=262144 train_time:586825ms step_avg:651.30ms
step:902/1775 n_predict=2 lr=1.4652 bs=262144 train_time:587568ms step_avg:651.41ms
step:903/1775 n_predict=2 lr=1.4635 bs=262144 train_time:588303ms step_avg:651.50ms
step:904/1775 n_predict=2 lr=1.4619 bs=262144 train_time:589047ms step_avg:651.60ms
step:905/1775 n_predict=2 lr=1.4603 bs=262144 train_time:589781ms step_avg:651.69ms
step:906/1775 n_predict=2 lr=1.4586 bs=262144 train_time:590521ms step_avg:651.79ms
step:907/1775 n_predict=2 lr=1.4570 bs=262144 train_time:591263ms step_avg:651.89ms
step:908/1775 n_predict=2 lr=1.4553 bs=262144 train_time:592006ms step_avg:651.99ms
step:909/1775 n_predict=2 lr=1.4537 bs=262144 train_time:592749ms step_avg:652.09ms
step:910/1775 n_predict=2 lr=1.4521 bs=262144 train_time:593484ms step_avg:652.18ms
step:911/1775 n_predict=2 lr=1.4504 bs=262144 train_time:594219ms step_avg:652.27ms
step:912/1775 n_predict=2 lr=1.4488 bs=262144 train_time:594964ms step_avg:652.37ms
step:913/1775 n_predict=2 lr=1.4472 bs=262144 train_time:595702ms step_avg:652.47ms
step:914/1775 n_predict=2 lr=1.4455 bs=262144 train_time:596441ms step_avg:652.56ms
step:915/1775 n_predict=2 lr=1.4439 bs=262144 train_time:597183ms step_avg:652.66ms
step:916/1775 n_predict=2 lr=1.4422 bs=262144 train_time:597921ms step_avg:652.75ms
step:917/1775 n_predict=2 lr=1.4406 bs=262144 train_time:598665ms step_avg:652.85ms
step:918/1775 n_predict=2 lr=1.4390 bs=262144 train_time:599403ms step_avg:652.94ms
step:919/1775 n_predict=2 lr=1.4373 bs=262144 train_time:600145ms step_avg:653.04ms
step:920/1775 n_predict=2 lr=1.4357 bs=262144 train_time:600887ms step_avg:653.14ms
step:921/1775 n_predict=2 lr=1.4341 bs=262144 train_time:601625ms step_avg:653.23ms
step:922/1775 n_predict=2 lr=1.4324 bs=262144 train_time:602366ms step_avg:653.32ms
step:923/1775 n_predict=2 lr=1.4308 bs=262144 train_time:603102ms step_avg:653.42ms
step:924/1775 n_predict=2 lr=1.4292 bs=262144 train_time:603849ms step_avg:653.52ms
step:925/1775 n_predict=2 lr=1.4275 bs=262144 train_time:604588ms step_avg:653.61ms
step:926/1775 n_predict=2 lr=1.4259 bs=262144 train_time:605328ms step_avg:653.70ms
step:927/1775 n_predict=2 lr=1.4242 bs=262144 train_time:606066ms step_avg:653.79ms
step:928/1775 n_predict=2 lr=1.4226 bs=262144 train_time:606807ms step_avg:653.89ms
step:929/1775 n_predict=2 lr=1.4210 bs=262144 train_time:607545ms step_avg:653.98ms
step:930/1775 n_predict=2 lr=1.4193 bs=262144 train_time:608284ms step_avg:654.07ms
step:931/1775 n_predict=2 lr=1.4177 bs=262144 train_time:609020ms step_avg:654.16ms
step:932/1775 n_predict=2 lr=1.4161 bs=262144 train_time:609765ms step_avg:654.25ms
step:933/1775 n_predict=2 lr=1.4144 bs=262144 train_time:610506ms step_avg:654.35ms
step:934/1775 n_predict=2 lr=1.4128 bs=262144 train_time:611244ms step_avg:654.44ms
step:935/1775 n_predict=2 lr=1.4111 bs=262144 train_time:611986ms step_avg:654.53ms
step:936/1775 n_predict=2 lr=1.4095 bs=262144 train_time:612729ms step_avg:654.63ms
step:937/1775 n_predict=2 lr=1.4079 bs=262144 train_time:613469ms step_avg:654.72ms
step:938/1775 n_predict=2 lr=1.4062 bs=262144 train_time:614207ms step_avg:654.80ms
step:939/1775 n_predict=2 lr=1.4046 bs=262144 train_time:614948ms step_avg:654.90ms
step:940/1775 n_predict=2 lr=1.4030 bs=262144 train_time:615691ms step_avg:654.99ms
step:941/1775 n_predict=2 lr=1.4013 bs=262144 train_time:616428ms step_avg:655.08ms
step:942/1775 n_predict=2 lr=1.3997 bs=262144 train_time:617166ms step_avg:655.17ms
step:943/1775 n_predict=2 lr=1.3981 bs=262144 train_time:617905ms step_avg:655.25ms
step:944/1775 n_predict=2 lr=1.3964 bs=262144 train_time:618649ms step_avg:655.35ms
step:945/1775 n_predict=2 lr=1.3948 bs=262144 train_time:619388ms step_avg:655.44ms
step:946/1775 n_predict=2 lr=1.3931 bs=262144 train_time:620129ms step_avg:655.53ms
step:947/1775 n_predict=2 lr=1.3915 bs=262144 train_time:620870ms step_avg:655.62ms
step:948/1775 n_predict=2 lr=1.3899 bs=262144 train_time:621614ms step_avg:655.71ms
step:949/1775 n_predict=2 lr=1.3882 bs=262144 train_time:622355ms step_avg:655.80ms
step:950/1775 n_predict=2 lr=1.3866 bs=262144 train_time:623095ms step_avg:655.89ms
step:951/1775 n_predict=2 lr=1.3850 bs=262144 train_time:623836ms step_avg:655.98ms
step:952/1775 n_predict=2 lr=1.3833 bs=262144 train_time:624577ms step_avg:656.07ms
step:953/1775 n_predict=2 lr=1.3817 bs=262144 train_time:625317ms step_avg:656.16ms
step:954/1775 n_predict=2 lr=1.3800 bs=262144 train_time:626059ms step_avg:656.25ms
step:955/1775 n_predict=2 lr=1.3784 bs=262144 train_time:626797ms step_avg:656.33ms
step:956/1775 n_predict=2 lr=1.3768 bs=262144 train_time:627541ms step_avg:656.42ms
step:957/1775 n_predict=2 lr=1.3751 bs=262144 train_time:628281ms step_avg:656.51ms
step:958/1775 n_predict=2 lr=1.3735 bs=262144 train_time:629018ms step_avg:656.60ms
step:959/1775 n_predict=2 lr=1.3719 bs=262144 train_time:629763ms step_avg:656.69ms
step:960/1775 n_predict=2 lr=1.3702 bs=262144 train_time:630506ms step_avg:656.78ms
step:961/1775 n_predict=2 lr=1.3686 bs=262144 train_time:631245ms step_avg:656.86ms
step:962/1775 n_predict=2 lr=1.3670 bs=262144 train_time:631987ms step_avg:656.95ms
step:963/1775 n_predict=2 lr=1.3653 bs=262144 train_time:632725ms step_avg:657.04ms
step:964/1775 n_predict=2 lr=1.3637 bs=262144 train_time:633468ms step_avg:657.12ms
step:965/1775 n_predict=2 lr=1.3620 bs=262144 train_time:634205ms step_avg:657.21ms
step:966/1775 n_predict=2 lr=1.3604 bs=262144 train_time:634946ms step_avg:657.29ms
step:967/1775 n_predict=2 lr=1.3588 bs=262144 train_time:635684ms step_avg:657.38ms
step:968/1775 n_predict=2 lr=1.3571 bs=262144 train_time:636423ms step_avg:657.46ms
step:969/1775 n_predict=2 lr=1.3555 bs=262144 train_time:637163ms step_avg:657.55ms
step:970/1775 n_predict=2 lr=1.3539 bs=262144 train_time:637905ms step_avg:657.63ms
step:971/1775 n_predict=2 lr=1.3522 bs=262144 train_time:638646ms step_avg:657.72ms
step:972/1775 n_predict=2 lr=1.3506 bs=262144 train_time:639383ms step_avg:657.80ms
step:973/1775 n_predict=2 lr=1.3489 bs=262144 train_time:640125ms step_avg:657.89ms
step:974/1775 n_predict=2 lr=1.3473 bs=262144 train_time:640867ms step_avg:657.97ms
step:975/1775 n_predict=2 lr=1.3457 bs=262144 train_time:641605ms step_avg:658.06ms
step:976/1775 n_predict=2 lr=1.3440 bs=262144 train_time:642352ms step_avg:658.15ms
step:977/1775 n_predict=2 lr=1.3424 bs=262144 train_time:643086ms step_avg:658.23ms
step:978/1775 n_predict=2 lr=1.3408 bs=262144 train_time:643827ms step_avg:658.31ms
step:979/1775 n_predict=2 lr=1.3391 bs=262144 train_time:644566ms step_avg:658.39ms
step:980/1775 n_predict=2 lr=1.3375 bs=262144 train_time:645305ms step_avg:658.47ms
step:981/1775 n_predict=2 lr=1.3359 bs=262144 train_time:646044ms step_avg:658.56ms
step:982/1775 n_predict=2 lr=1.3342 bs=262144 train_time:646791ms step_avg:658.65ms
step:983/1775 n_predict=2 lr=1.3326 bs=262144 train_time:647530ms step_avg:658.73ms
step:984/1775 n_predict=2 lr=1.3309 bs=262144 train_time:648273ms step_avg:658.81ms
step:985/1775 n_predict=2 lr=1.3293 bs=262144 train_time:649013ms step_avg:658.90ms
step:986/1775 n_predict=2 lr=1.3277 bs=262144 train_time:649756ms step_avg:658.98ms
step:987/1775 n_predict=2 lr=1.3260 bs=262144 train_time:650497ms step_avg:659.06ms
step:988/1775 n_predict=2 lr=1.3244 bs=262144 train_time:651240ms step_avg:659.15ms
step:989/1775 n_predict=2 lr=1.3228 bs=262144 train_time:651979ms step_avg:659.23ms
step:990/1775 n_predict=2 lr=1.3211 bs=262144 train_time:652722ms step_avg:659.32ms
step:991/1775 n_predict=2 lr=1.3195 bs=262144 train_time:653465ms step_avg:659.40ms
step:992/1775 n_predict=2 lr=1.3178 bs=262144 train_time:654206ms step_avg:659.48ms
step:993/1775 n_predict=2 lr=1.3162 bs=262144 train_time:654948ms step_avg:659.56ms
step:994/1775 n_predict=2 lr=1.3146 bs=262144 train_time:655689ms step_avg:659.65ms
step:995/1775 n_predict=2 lr=1.3129 bs=262144 train_time:656428ms step_avg:659.73ms
step:996/1775 n_predict=2 lr=1.3113 bs=262144 train_time:657165ms step_avg:659.80ms
step:997/1775 n_predict=2 lr=1.3097 bs=262144 train_time:657904ms step_avg:659.88ms
step:998/1775 n_predict=2 lr=1.3080 bs=262144 train_time:658652ms step_avg:659.97ms
step:999/1775 n_predict=2 lr=1.3064 bs=262144 train_time:659392ms step_avg:660.05ms
step:1000/1775 n_predict=2 lr=1.3047 bs=262144 train_time:660131ms step_avg:660.13ms
step:1000/1775 lr=1.3031 bs=262144 n_predict=2 val_loss:3.7384 val_malbo_loss:3.8180 train_time:660194ms step_avg:660.19ms
step:1001/1775 n_predict=2 lr=1.3031 bs=262144 train_time:660871ms step_avg:660.21ms
step:1002/1775 n_predict=2 lr=1.3015 bs=262144 train_time:661612ms step_avg:660.29ms
step:1003/1775 n_predict=2 lr=1.2998 bs=262144 train_time:662351ms step_avg:660.37ms
step:1004/1775 n_predict=2 lr=1.2982 bs=262144 train_time:663093ms step_avg:660.45ms
step:1005/1775 n_predict=2 lr=1.2966 bs=262144 train_time:663834ms step_avg:660.53ms
step:1006/1775 n_predict=2 lr=1.2949 bs=262144 train_time:664578ms step_avg:660.61ms
step:1007/1775 n_predict=2 lr=1.2933 bs=262144 train_time:665315ms step_avg:660.69ms
step:1008/1775 n_predict=2 lr=1.2917 bs=262144 train_time:666054ms step_avg:660.77ms
step:1009/1775 n_predict=2 lr=1.2900 bs=262144 train_time:666796ms step_avg:660.85ms
step:1010/1775 n_predict=2 lr=1.2884 bs=262144 train_time:667541ms step_avg:660.93ms
step:1011/1775 n_predict=2 lr=1.2867 bs=262144 train_time:668279ms step_avg:661.01ms
step:1012/1775 n_predict=2 lr=1.2851 bs=262144 train_time:669023ms step_avg:661.09ms
step:1013/1775 n_predict=2 lr=1.2835 bs=262144 train_time:669767ms step_avg:661.17ms
step:1014/1775 n_predict=2 lr=1.2818 bs=262144 train_time:670506ms step_avg:661.25ms
step:1015/1775 n_predict=2 lr=1.2802 bs=262144 train_time:671244ms step_avg:661.32ms
step:1016/1775 n_predict=2 lr=1.2786 bs=262144 train_time:671985ms step_avg:661.40ms
step:1017/1775 n_predict=2 lr=1.2769 bs=262144 train_time:672728ms step_avg:661.48ms
step:1018/1775 n_predict=2 lr=1.2753 bs=262144 train_time:673471ms step_avg:661.56ms
step:1019/1775 n_predict=2 lr=1.2736 bs=262144 train_time:674209ms step_avg:661.64ms
step:1020/1775 n_predict=2 lr=1.2720 bs=262144 train_time:674951ms step_avg:661.72ms
step:1021/1775 n_predict=2 lr=1.2704 bs=262144 train_time:675692ms step_avg:661.79ms
step:1022/1775 n_predict=2 lr=1.2687 bs=262144 train_time:676438ms step_avg:661.88ms
step:1023/1775 n_predict=2 lr=1.2671 bs=262144 train_time:677175ms step_avg:661.95ms
step:1024/1775 n_predict=2 lr=1.2655 bs=262144 train_time:677916ms step_avg:662.03ms
step:1025/1775 n_predict=2 lr=1.2638 bs=262144 train_time:678659ms step_avg:662.11ms
step:1026/1775 n_predict=2 lr=1.2622 bs=262144 train_time:679401ms step_avg:662.18ms
step:1027/1775 n_predict=2 lr=1.2606 bs=262144 train_time:680141ms step_avg:662.26ms
step:1028/1775 n_predict=2 lr=1.2589 bs=262144 train_time:680884ms step_avg:662.34ms
step:1029/1775 n_predict=2 lr=1.2573 bs=262144 train_time:681623ms step_avg:662.41ms
step:1030/1775 n_predict=2 lr=1.2556 bs=262144 train_time:682366ms step_avg:662.49ms
step:1031/1775 n_predict=2 lr=1.2540 bs=262144 train_time:683100ms step_avg:662.56ms
step:1032/1775 n_predict=2 lr=1.2524 bs=262144 train_time:683845ms step_avg:662.64ms
step:1033/1775 n_predict=2 lr=1.2507 bs=262144 train_time:684585ms step_avg:662.72ms
step:1034/1775 n_predict=2 lr=1.2491 bs=262144 train_time:685326ms step_avg:662.79ms
step:1035/1775 n_predict=2 lr=1.2475 bs=262144 train_time:686067ms step_avg:662.87ms
step:1036/1775 n_predict=2 lr=1.2458 bs=262144 train_time:686809ms step_avg:662.94ms
step:1037/1775 n_predict=2 lr=1.2442 bs=262144 train_time:687551ms step_avg:663.02ms
step:1038/1775 n_predict=2 lr=1.2425 bs=262144 train_time:688293ms step_avg:663.10ms
step:1039/1775 n_predict=2 lr=1.2409 bs=262144 train_time:689031ms step_avg:663.17ms
step:1040/1775 n_predict=2 lr=1.2393 bs=262144 train_time:689772ms step_avg:663.24ms
step:1041/1775 n_predict=2 lr=1.2376 bs=262144 train_time:690510ms step_avg:663.31ms
step:1042/1775 n_predict=2 lr=1.2360 bs=262144 train_time:691254ms step_avg:663.39ms
step:1043/1775 n_predict=2 lr=1.2344 bs=262144 train_time:691996ms step_avg:663.47ms
step:1044/1775 n_predict=2 lr=1.2327 bs=262144 train_time:692738ms step_avg:663.54ms
step:1045/1775 n_predict=2 lr=1.2311 bs=262144 train_time:693479ms step_avg:663.62ms
step:1046/1775 n_predict=2 lr=1.2295 bs=262144 train_time:694219ms step_avg:663.69ms
step:1047/1775 n_predict=2 lr=1.2278 bs=262144 train_time:694960ms step_avg:663.76ms
step:1048/1775 n_predict=2 lr=1.2262 bs=262144 train_time:695700ms step_avg:663.84ms
step:1049/1775 n_predict=2 lr=1.2245 bs=262144 train_time:696440ms step_avg:663.91ms
step:1050/1775 n_predict=2 lr=1.2229 bs=262144 train_time:697186ms step_avg:663.99ms
step:1051/1775 n_predict=2 lr=1.2213 bs=262144 train_time:697926ms step_avg:664.06ms
step:1052/1775 n_predict=2 lr=1.2196 bs=262144 train_time:698667ms step_avg:664.13ms
step:1053/1775 n_predict=2 lr=1.2180 bs=262144 train_time:699407ms step_avg:664.20ms
step:1054/1775 n_predict=2 lr=1.2164 bs=262144 train_time:700147ms step_avg:664.28ms
step:1055/1775 n_predict=2 lr=1.2147 bs=262144 train_time:700885ms step_avg:664.35ms
step:1056/1775 n_predict=2 lr=1.2131 bs=262144 train_time:701627ms step_avg:664.42ms
step:1057/1775 n_predict=2 lr=1.2114 bs=262144 train_time:702371ms step_avg:664.49ms
step:1058/1775 n_predict=2 lr=1.2098 bs=262144 train_time:703106ms step_avg:664.56ms
step:1059/1775 n_predict=2 lr=1.2082 bs=262144 train_time:703845ms step_avg:664.63ms
step:1060/1775 n_predict=2 lr=1.2065 bs=262144 train_time:704585ms step_avg:664.70ms
step:1061/1775 n_predict=2 lr=1.2049 bs=262144 train_time:705327ms step_avg:664.78ms
step:1062/1775 n_predict=2 lr=1.2033 bs=262144 train_time:706070ms step_avg:664.85ms
step:1063/1775 n_predict=2 lr=1.2016 bs=262144 train_time:706807ms step_avg:664.92ms
step:1064/1775 n_predict=2 lr=1.2000 bs=262144 train_time:707548ms step_avg:664.99ms
step:1065/1775 n_predict=2 lr=1.1984 bs=262144 train_time:708285ms step_avg:665.06ms
step:1066/1775 n_predict=2 lr=1.1967 bs=262144 train_time:709029ms step_avg:665.13ms
step:1067/1775 n_predict=2 lr=1.1951 bs=262144 train_time:709769ms step_avg:665.20ms
step:1068/1775 n_predict=2 lr=1.1934 bs=262144 train_time:710507ms step_avg:665.27ms
step:1069/1775 n_predict=2 lr=1.1918 bs=262144 train_time:711247ms step_avg:665.34ms
step:1070/1775 n_predict=2 lr=1.1902 bs=262144 train_time:711988ms step_avg:665.41ms
step:1071/1775 n_predict=2 lr=1.1885 bs=262144 train_time:712729ms step_avg:665.48ms
step:1072/1775 n_predict=2 lr=1.1869 bs=262144 train_time:713466ms step_avg:665.55ms
step:1073/1775 n_predict=2 lr=1.1853 bs=262144 train_time:714202ms step_avg:665.61ms
step:1074/1775 n_predict=2 lr=1.1836 bs=262144 train_time:714946ms step_avg:665.68ms
step:1075/1775 n_predict=2 lr=1.1820 bs=262144 train_time:715683ms step_avg:665.75ms
step:1076/1775 n_predict=2 lr=1.1803 bs=262144 train_time:716422ms step_avg:665.82ms
step:1077/1775 n_predict=2 lr=1.1787 bs=262144 train_time:717162ms step_avg:665.89ms
step:1078/1775 n_predict=2 lr=1.1771 bs=262144 train_time:717903ms step_avg:665.96ms
step:1079/1775 n_predict=2 lr=1.1754 bs=262144 train_time:718645ms step_avg:666.03ms
step:1080/1775 n_predict=2 lr=1.1738 bs=262144 train_time:719386ms step_avg:666.10ms
step:1081/1775 n_predict=2 lr=1.1722 bs=262144 train_time:720126ms step_avg:666.17ms
step:1082/1775 n_predict=2 lr=1.1705 bs=262144 train_time:720870ms step_avg:666.24ms
step:1083/1775 n_predict=2 lr=1.1689 bs=262144 train_time:721606ms step_avg:666.30ms
step:1084/1775 n_predict=2 lr=1.1673 bs=262144 train_time:722348ms step_avg:666.37ms
step:1085/1775 n_predict=2 lr=1.1656 bs=262144 train_time:723085ms step_avg:666.44ms
step:1086/1775 n_predict=2 lr=1.1640 bs=262144 train_time:723826ms step_avg:666.51ms
step:1087/1775 n_predict=2 lr=1.1623 bs=262144 train_time:724568ms step_avg:666.58ms
step:1088/1775 n_predict=2 lr=1.1607 bs=262144 train_time:725307ms step_avg:666.64ms
step:1089/1775 n_predict=2 lr=1.1591 bs=262144 train_time:726042ms step_avg:666.70ms
step:1090/1775 n_predict=2 lr=1.1574 bs=262144 train_time:726784ms step_avg:666.77ms
step:1091/1775 n_predict=2 lr=1.1558 bs=262144 train_time:727527ms step_avg:666.84ms
step:1092/1775 n_predict=2 lr=1.1542 bs=262144 train_time:728268ms step_avg:666.91ms
step:1093/1775 n_predict=2 lr=1.1525 bs=262144 train_time:729006ms step_avg:666.98ms
step:1094/1775 n_predict=2 lr=1.1509 bs=262144 train_time:729747ms step_avg:667.04ms
step:1095/1775 n_predict=2 lr=1.1492 bs=262144 train_time:730484ms step_avg:667.11ms
step:1096/1775 n_predict=2 lr=1.1476 bs=262144 train_time:731224ms step_avg:667.18ms
step:1097/1775 n_predict=2 lr=1.1460 bs=262144 train_time:731961ms step_avg:667.24ms
step:1098/1775 n_predict=2 lr=1.1443 bs=262144 train_time:732702ms step_avg:667.31ms
step:1099/1775 n_predict=2 lr=1.1427 bs=262144 train_time:733443ms step_avg:667.37ms
step:1100/1775 n_predict=2 lr=1.1411 bs=262144 train_time:734183ms step_avg:667.44ms
step:1101/1775 n_predict=2 lr=1.1394 bs=262144 train_time:734924ms step_avg:667.51ms
step:1102/1775 n_predict=2 lr=1.1378 bs=262144 train_time:735671ms step_avg:667.58ms
step:1103/1775 n_predict=2 lr=1.1361 bs=262144 train_time:736407ms step_avg:667.64ms
step:1104/1775 n_predict=2 lr=1.1345 bs=262144 train_time:737147ms step_avg:667.71ms
step:1105/1775 n_predict=2 lr=1.1329 bs=262144 train_time:737885ms step_avg:667.77ms
step:1106/1775 n_predict=2 lr=1.1312 bs=262144 train_time:738630ms step_avg:667.84ms
step:1107/1775 n_predict=2 lr=1.1296 bs=262144 train_time:739367ms step_avg:667.90ms
step:1108/1775 n_predict=2 lr=1.1280 bs=262144 train_time:740108ms step_avg:667.97ms
step:1109/1775 n_predict=2 lr=1.1263 bs=262144 train_time:740848ms step_avg:668.03ms
step:1110/1775 n_predict=2 lr=1.1247 bs=262144 train_time:741588ms step_avg:668.10ms
step:1111/1775 n_predict=2 lr=1.1231 bs=262144 train_time:742327ms step_avg:668.16ms
step:1112/1775 n_predict=2 lr=1.1214 bs=262144 train_time:743067ms step_avg:668.23ms
step:1113/1775 n_predict=2 lr=1.1198 bs=262144 train_time:743802ms step_avg:668.29ms
step:1114/1775 n_predict=2 lr=1.1181 bs=262144 train_time:744544ms step_avg:668.35ms
step:1115/1775 n_predict=2 lr=1.1165 bs=262144 train_time:745283ms step_avg:668.42ms
step:1116/1775 n_predict=2 lr=1.1149 bs=262144 train_time:746025ms step_avg:668.48ms
step:1117/1775 n_predict=2 lr=1.1132 bs=262144 train_time:746763ms step_avg:668.54ms
step:1118/1775 n_predict=2 lr=1.1116 bs=262144 train_time:747501ms step_avg:668.61ms
step:1119/1775 n_predict=2 lr=1.1100 bs=262144 train_time:748244ms step_avg:668.67ms
step:1120/1775 n_predict=2 lr=1.1083 bs=262144 train_time:748985ms step_avg:668.74ms
step:1121/1775 n_predict=2 lr=1.1067 bs=262144 train_time:749725ms step_avg:668.80ms
step:1122/1775 n_predict=2 lr=1.1050 bs=262144 train_time:750470ms step_avg:668.87ms
step:1123/1775 n_predict=2 lr=1.1034 bs=262144 train_time:751209ms step_avg:668.93ms
step:1124/1775 n_predict=2 lr=1.1018 bs=262144 train_time:751951ms step_avg:669.00ms
step:1125/1775 n_predict=2 lr=1.1001 bs=262144 train_time:752688ms step_avg:669.06ms
step:1126/1775 n_predict=2 lr=1.0985 bs=262144 train_time:753430ms step_avg:669.12ms
step:1127/1775 n_predict=2 lr=1.0969 bs=262144 train_time:754169ms step_avg:669.18ms
step:1128/1775 n_predict=2 lr=1.0952 bs=262144 train_time:754905ms step_avg:669.24ms
step:1129/1775 n_predict=2 lr=1.0936 bs=262144 train_time:755648ms step_avg:669.31ms
step:1130/1775 n_predict=2 lr=1.0920 bs=262144 train_time:756389ms step_avg:669.37ms
step:1131/1775 n_predict=2 lr=1.0903 bs=262144 train_time:757127ms step_avg:669.43ms
step:1132/1775 n_predict=2 lr=1.0887 bs=262144 train_time:757868ms step_avg:669.49ms
step:1133/1775 n_predict=2 lr=1.0870 bs=262144 train_time:758608ms step_avg:669.56ms
step:1134/1775 n_predict=2 lr=1.0854 bs=262144 train_time:759347ms step_avg:669.62ms
step:1135/1775 n_predict=2 lr=1.0838 bs=262144 train_time:760085ms step_avg:669.68ms
step:1136/1775 n_predict=2 lr=1.0821 bs=262144 train_time:760826ms step_avg:669.74ms
step:1137/1775 n_predict=2 lr=1.0805 bs=262144 train_time:761569ms step_avg:669.81ms
step:1138/1775 n_predict=2 lr=1.0789 bs=262144 train_time:762306ms step_avg:669.86ms
step:1139/1775 n_predict=2 lr=1.0772 bs=262144 train_time:763044ms step_avg:669.92ms
step:1140/1775 n_predict=2 lr=1.0756 bs=262144 train_time:763787ms step_avg:669.99ms
step:1141/1775 n_predict=2 lr=1.0739 bs=262144 train_time:764527ms step_avg:670.05ms
step:1142/1775 n_predict=2 lr=1.0723 bs=262144 train_time:765267ms step_avg:670.11ms
step:1143/1775 n_predict=2 lr=1.0707 bs=262144 train_time:766005ms step_avg:670.17ms
step:1144/1775 n_predict=2 lr=1.0690 bs=262144 train_time:766747ms step_avg:670.23ms
step:1145/1775 n_predict=2 lr=1.0674 bs=262144 train_time:767486ms step_avg:670.29ms
step:1146/1775 n_predict=2 lr=1.0658 bs=262144 train_time:768230ms step_avg:670.36ms
step:1147/1775 n_predict=2 lr=1.0641 bs=262144 train_time:768970ms step_avg:670.42ms
step:1148/1775 n_predict=2 lr=1.0625 bs=262144 train_time:769709ms step_avg:670.48ms
step:1149/1775 n_predict=2 lr=1.0609 bs=262144 train_time:770448ms step_avg:670.54ms
step:1150/1775 n_predict=2 lr=1.0592 bs=262144 train_time:771187ms step_avg:670.60ms
step:1151/1775 n_predict=2 lr=1.0576 bs=262144 train_time:771927ms step_avg:670.66ms
step:1152/1775 n_predict=2 lr=1.0559 bs=262144 train_time:772668ms step_avg:670.72ms
step:1153/1775 n_predict=2 lr=1.0543 bs=262144 train_time:773407ms step_avg:670.78ms
step:1154/1775 n_predict=2 lr=1.0527 bs=262144 train_time:774149ms step_avg:670.84ms
step:1155/1775 n_predict=2 lr=1.0510 bs=262144 train_time:774887ms step_avg:670.90ms
step:1156/1775 n_predict=2 lr=1.0494 bs=262144 train_time:775628ms step_avg:670.96ms
step:1157/1775 n_predict=2 lr=1.0478 bs=262144 train_time:776370ms step_avg:671.02ms
step:1158/1775 n_predict=1 lr=1.1860 bs=393216 train_time:889564ms step_avg:768.19ms
step:1159/1775 n_predict=1 lr=1.1842 bs=393216 train_time:991851ms step_avg:855.78ms
step:1160/1775 n_predict=1 lr=1.1823 bs=393216 train_time:993461ms step_avg:856.43ms
step:1161/1775 n_predict=1 lr=1.1804 bs=393216 train_time:995075ms step_avg:857.08ms
step:1162/1775 n_predict=1 lr=1.1785 bs=393216 train_time:996776ms step_avg:857.81ms
step:1163/1775 n_predict=1 lr=1.1766 bs=393216 train_time:998434ms step_avg:858.50ms
step:1164/1775 n_predict=1 lr=1.1748 bs=393216 train_time:1000150ms step_avg:859.24ms
step:1165/1775 n_predict=1 lr=1.1729 bs=393216 train_time:1001775ms step_avg:859.89ms
step:1166/1775 n_predict=1 lr=1.1710 bs=393216 train_time:1003377ms step_avg:860.53ms
step:1167/1775 n_predict=1 lr=1.1691 bs=393216 train_time:1004974ms step_avg:861.16ms
step:1168/1775 n_predict=1 lr=1.1673 bs=393216 train_time:1006551ms step_avg:861.77ms
step:1169/1775 n_predict=1 lr=1.1654 bs=393216 train_time:1008167ms step_avg:862.42ms
step:1170/1775 n_predict=1 lr=1.1635 bs=393216 train_time:1009754ms step_avg:863.04ms
step:1171/1775 n_predict=1 lr=1.1616 bs=393216 train_time:1011371ms step_avg:863.68ms
step:1172/1775 n_predict=1 lr=1.1597 bs=393216 train_time:1012977ms step_avg:864.31ms
step:1173/1775 n_predict=1 lr=1.1579 bs=393216 train_time:1014567ms step_avg:864.93ms
step:1174/1775 n_predict=1 lr=1.1560 bs=393216 train_time:1016152ms step_avg:865.55ms
step:1175/1775 n_predict=1 lr=1.1541 bs=393216 train_time:1017751ms step_avg:866.17ms
step:1176/1775 n_predict=1 lr=1.1522 bs=393216 train_time:1019469ms step_avg:866.90ms
step:1177/1775 n_predict=1 lr=1.1503 bs=393216 train_time:1021074ms step_avg:867.52ms
step:1178/1775 n_predict=1 lr=1.1485 bs=393216 train_time:1022651ms step_avg:868.13ms
step:1179/1775 n_predict=1 lr=1.1466 bs=393216 train_time:1024274ms step_avg:868.77ms
step:1180/1775 n_predict=1 lr=1.1447 bs=393216 train_time:1025877ms step_avg:869.39ms
step:1181/1775 n_predict=1 lr=1.1428 bs=393216 train_time:1027474ms step_avg:870.00ms
step:1182/1775 n_predict=1 lr=1.1409 bs=393216 train_time:1029068ms step_avg:870.62ms
step:1183/1775 n_predict=1 lr=1.1391 bs=393216 train_time:1030649ms step_avg:871.22ms
step:1184/1775 n_predict=1 lr=1.1372 bs=393216 train_time:1032177ms step_avg:871.77ms
step:1185/1775 n_predict=1 lr=1.1353 bs=393216 train_time:1033776ms step_avg:872.39ms
step:1186/1775 n_predict=1 lr=1.1334 bs=393216 train_time:1035456ms step_avg:873.07ms
step:1187/1775 n_predict=1 lr=1.1316 bs=393216 train_time:1036975ms step_avg:873.61ms
step:1188/1775 n_predict=1 lr=1.1297 bs=393216 train_time:1038559ms step_avg:874.21ms
step:1189/1775 n_predict=1 lr=1.1278 bs=393216 train_time:1040175ms step_avg:874.83ms
step:1190/1775 n_predict=1 lr=1.1259 bs=393216 train_time:1041778ms step_avg:875.44ms
step:1191/1775 n_predict=1 lr=1.1240 bs=393216 train_time:1043374ms step_avg:876.05ms
step:1192/1775 n_predict=1 lr=1.1222 bs=393216 train_time:1044977ms step_avg:876.66ms
step:1193/1775 n_predict=1 lr=1.1203 bs=393216 train_time:1046675ms step_avg:877.35ms
step:1194/1775 n_predict=1 lr=1.1184 bs=393216 train_time:1048277ms step_avg:877.95ms
step:1195/1775 n_predict=1 lr=1.1165 bs=393216 train_time:1049974ms step_avg:878.64ms
step:1196/1775 n_predict=1 lr=1.1146 bs=393216 train_time:1051578ms step_avg:879.25ms
step:1197/1775 n_predict=1 lr=1.1128 bs=393216 train_time:1053235ms step_avg:879.90ms
step:1198/1775 n_predict=1 lr=1.1109 bs=393216 train_time:1054838ms step_avg:880.50ms
step:1199/1775 n_predict=1 lr=1.1090 bs=393216 train_time:1056453ms step_avg:881.11ms
step:1200/1775 n_predict=1 lr=1.1071 bs=393216 train_time:1058071ms step_avg:881.73ms
step:1201/1775 n_predict=1 lr=1.1052 bs=393216 train_time:1059750ms step_avg:882.39ms
step:1202/1775 n_predict=1 lr=1.1034 bs=393216 train_time:1061478ms step_avg:883.09ms
step:1203/1775 n_predict=1 lr=1.1015 bs=393216 train_time:1063176ms step_avg:883.77ms
step:1204/1775 n_predict=1 lr=1.0996 bs=393216 train_time:1064776ms step_avg:884.37ms
step:1205/1775 n_predict=1 lr=1.0977 bs=393216 train_time:1066452ms step_avg:885.02ms
step:1206/1775 n_predict=1 lr=1.0959 bs=393216 train_time:1068074ms step_avg:885.63ms
step:1207/1775 n_predict=1 lr=1.0940 bs=393216 train_time:1069763ms step_avg:886.30ms
step:1208/1775 n_predict=1 lr=1.0921 bs=393216 train_time:1071470ms step_avg:886.98ms
step:1209/1775 n_predict=1 lr=1.0902 bs=393216 train_time:1073076ms step_avg:887.57ms
step:1210/1775 n_predict=1 lr=1.0883 bs=393216 train_time:1074777ms step_avg:888.25ms
step:1211/1775 n_predict=1 lr=1.0865 bs=393216 train_time:1076464ms step_avg:888.90ms
step:1212/1775 n_predict=1 lr=1.0846 bs=393216 train_time:1078177ms step_avg:889.59ms
step:1213/1775 n_predict=1 lr=1.0827 bs=393216 train_time:1079875ms step_avg:890.25ms
step:1214/1775 n_predict=1 lr=1.0808 bs=393216 train_time:1081476ms step_avg:890.84ms
step:1215/1775 n_predict=1 lr=1.0789 bs=393216 train_time:1083161ms step_avg:891.49ms
step:1216/1775 n_predict=1 lr=1.0771 bs=393216 train_time:1084779ms step_avg:892.09ms
step:1217/1775 n_predict=1 lr=1.0752 bs=393216 train_time:1086472ms step_avg:892.75ms
step:1218/1775 n_predict=1 lr=1.0733 bs=393216 train_time:1088078ms step_avg:893.33ms
step:1219/1775 n_predict=1 lr=1.0714 bs=393216 train_time:1089775ms step_avg:893.99ms
step:1220/1775 n_predict=1 lr=1.0695 bs=393216 train_time:1091377ms step_avg:894.57ms
step:1221/1775 n_predict=1 lr=1.0677 bs=393216 train_time:1093052ms step_avg:895.21ms
step:1222/1775 n_predict=1 lr=1.0658 bs=393216 train_time:1094674ms step_avg:895.81ms
step:1223/1775 n_predict=1 lr=1.0639 bs=393216 train_time:1096350ms step_avg:896.44ms
step:1224/1775 n_predict=1 lr=1.0620 bs=393216 train_time:1098044ms step_avg:897.09ms
step:1225/1775 n_predict=1 lr=1.0601 bs=393216 train_time:1099676ms step_avg:897.69ms
step:1226/1775 n_predict=1 lr=1.0583 bs=393216 train_time:1101273ms step_avg:898.27ms
step:1227/1775 n_predict=1 lr=1.0564 bs=393216 train_time:1102875ms step_avg:898.84ms
step:1228/1775 n_predict=1 lr=1.0545 bs=393216 train_time:1104577ms step_avg:899.49ms
step:1229/1775 n_predict=1 lr=1.0526 bs=393216 train_time:1106244ms step_avg:900.12ms
step:1230/1775 n_predict=1 lr=1.0508 bs=393216 train_time:1107852ms step_avg:900.69ms
step:1231/1775 n_predict=1 lr=1.0489 bs=393216 train_time:1109564ms step_avg:901.35ms
step:1232/1775 n_predict=1 lr=1.0470 bs=393216 train_time:1111175ms step_avg:901.93ms
step:1233/1775 n_predict=1 lr=1.0451 bs=393216 train_time:1112781ms step_avg:902.50ms
step:1234/1775 n_predict=1 lr=1.0432 bs=393216 train_time:1114470ms step_avg:903.14ms
step:1235/1775 n_predict=1 lr=1.0414 bs=393216 train_time:1116075ms step_avg:903.70ms
step:1236/1775 n_predict=1 lr=1.0395 bs=393216 train_time:1117673ms step_avg:904.27ms
step:1237/1775 n_predict=1 lr=1.0376 bs=393216 train_time:1119259ms step_avg:904.82ms
step:1238/1775 n_predict=1 lr=1.0357 bs=393216 train_time:1120938ms step_avg:905.44ms
step:1239/1775 n_predict=1 lr=1.0338 bs=393216 train_time:1122548ms step_avg:906.01ms
step:1240/1775 n_predict=1 lr=1.0320 bs=393216 train_time:1124176ms step_avg:906.59ms
step:1241/1775 n_predict=1 lr=1.0301 bs=393216 train_time:1125853ms step_avg:907.21ms
step:1242/1775 n_predict=1 lr=1.0282 bs=393216 train_time:1127477ms step_avg:907.79ms
step:1243/1775 n_predict=1 lr=1.0263 bs=393216 train_time:1129072ms step_avg:908.34ms
step:1244/1775 n_predict=1 lr=1.0244 bs=393216 train_time:1130677ms step_avg:908.90ms
step:1245/1775 n_predict=1 lr=1.0226 bs=393216 train_time:1132275ms step_avg:909.46ms
step:1246/1775 n_predict=1 lr=1.0207 bs=393216 train_time:1133882ms step_avg:910.02ms
step:1247/1775 n_predict=1 lr=1.0188 bs=393216 train_time:1135472ms step_avg:910.56ms
step:1248/1775 n_predict=1 lr=1.0169 bs=393216 train_time:1137069ms step_avg:911.11ms
step:1249/1775 n_predict=1 lr=1.0151 bs=393216 train_time:1138775ms step_avg:911.75ms
step:1250/1775 n_predict=1 lr=1.0132 bs=393216 train_time:1140454ms step_avg:912.36ms
step:1250/1775 lr=1.0113 bs=393216 n_predict=1 val_loss:3.5143 val_malbo_loss:3.5943 train_time:1140537ms step_avg:912.43ms
step:1251/1775 n_predict=1 lr=1.0113 bs=393216 train_time:1142072ms step_avg:912.93ms
step:1252/1775 n_predict=1 lr=1.0094 bs=393216 train_time:1143751ms step_avg:913.54ms
step:1253/1775 n_predict=1 lr=1.0075 bs=393216 train_time:1145364ms step_avg:914.10ms
step:1254/1775 n_predict=1 lr=1.0057 bs=393216 train_time:1147076ms step_avg:914.73ms
step:1255/1775 n_predict=1 lr=1.0038 bs=393216 train_time:1148645ms step_avg:915.25ms
step:1256/1775 n_predict=1 lr=1.0019 bs=393216 train_time:1150274ms step_avg:915.82ms
step:1257/1775 n_predict=1 lr=1.0000 bs=393216 train_time:1151964ms step_avg:916.44ms
step:1258/1775 n_predict=1 lr=0.9981 bs=393216 train_time:1153575ms step_avg:916.99ms
step:1259/1775 n_predict=1 lr=0.9963 bs=393216 train_time:1155173ms step_avg:917.53ms
step:1260/1775 n_predict=1 lr=0.9944 bs=393216 train_time:1156874ms step_avg:918.15ms
step:1261/1775 n_predict=1 lr=0.9925 bs=393216 train_time:1158572ms step_avg:918.77ms
step:1262/1775 n_predict=1 lr=0.9906 bs=393216 train_time:1160175ms step_avg:919.31ms
step:1263/1775 n_predict=1 lr=0.9887 bs=393216 train_time:1161847ms step_avg:919.91ms
step:1264/1775 n_predict=1 lr=0.9869 bs=393216 train_time:1163563ms step_avg:920.54ms
step:1265/1775 n_predict=1 lr=0.9850 bs=393216 train_time:1165172ms step_avg:921.08ms
step:1266/1775 n_predict=1 lr=0.9831 bs=393216 train_time:1166877ms step_avg:921.70ms
step:1267/1775 n_predict=1 lr=0.9812 bs=393216 train_time:1168560ms step_avg:922.30ms
step:1268/1775 n_predict=1 lr=0.9794 bs=393216 train_time:1170253ms step_avg:922.91ms
step:1269/1775 n_predict=1 lr=0.9775 bs=393216 train_time:1171855ms step_avg:923.45ms
step:1270/1775 n_predict=1 lr=0.9756 bs=393216 train_time:1173454ms step_avg:923.98ms
step:1271/1775 n_predict=1 lr=0.9737 bs=393216 train_time:1175063ms step_avg:924.52ms
step:1272/1775 n_predict=1 lr=0.9718 bs=393216 train_time:1176722ms step_avg:925.10ms
step:1273/1775 n_predict=1 lr=0.9700 bs=393216 train_time:1178372ms step_avg:925.67ms
step:1274/1775 n_predict=1 lr=0.9681 bs=393216 train_time:1180051ms step_avg:926.26ms
step:1275/1775 n_predict=1 lr=0.9662 bs=393216 train_time:1181756ms step_avg:926.87ms
step:1276/1775 n_predict=1 lr=0.9643 bs=393216 train_time:1183474ms step_avg:927.49ms
step:1277/1775 n_predict=1 lr=0.9624 bs=393216 train_time:1185149ms step_avg:928.07ms
step:1278/1775 n_predict=1 lr=0.9606 bs=393216 train_time:1186875ms step_avg:928.70ms
step:1279/1775 n_predict=1 lr=0.9587 bs=393216 train_time:1188472ms step_avg:929.22ms
step:1280/1775 n_predict=1 lr=0.9568 bs=393216 train_time:1190075ms step_avg:929.75ms
step:1281/1775 n_predict=1 lr=0.9549 bs=393216 train_time:1191772ms step_avg:930.35ms
step:1282/1775 n_predict=1 lr=0.9530 bs=393216 train_time:1193557ms step_avg:931.01ms
step:1283/1775 n_predict=1 lr=0.9512 bs=393216 train_time:1195173ms step_avg:931.55ms
step:1284/1775 n_predict=1 lr=0.9493 bs=393216 train_time:1196851ms step_avg:932.13ms
step:1285/1775 n_predict=1 lr=0.9474 bs=393216 train_time:1198477ms step_avg:932.67ms
step:1286/1775 n_predict=1 lr=0.9455 bs=393216 train_time:1200179ms step_avg:933.26ms
step:1287/1775 n_predict=1 lr=0.9437 bs=393216 train_time:1201866ms step_avg:933.85ms
step:1288/1775 n_predict=1 lr=0.9418 bs=393216 train_time:1203577ms step_avg:934.45ms
step:1289/1775 n_predict=1 lr=0.9399 bs=393216 train_time:1205173ms step_avg:934.97ms
step:1290/1775 n_predict=1 lr=0.9380 bs=393216 train_time:1206761ms step_avg:935.47ms
step:1291/1775 n_predict=1 lr=0.9361 bs=393216 train_time:1208376ms step_avg:936.00ms
step:1292/1775 n_predict=1 lr=0.9343 bs=393216 train_time:1210050ms step_avg:936.57ms
step:1293/1775 n_predict=1 lr=0.9324 bs=393216 train_time:1211659ms step_avg:937.09ms
step:1294/1775 n_predict=1 lr=0.9305 bs=393216 train_time:1213475ms step_avg:937.77ms
step:1295/1775 n_predict=1 lr=0.9286 bs=393216 train_time:1215171ms step_avg:938.36ms
step:1296/1775 n_predict=1 lr=0.9267 bs=393216 train_time:1216877ms step_avg:938.95ms
step:1297/1775 n_predict=1 lr=0.9249 bs=393216 train_time:1218561ms step_avg:939.52ms
step:1298/1775 n_predict=1 lr=0.9230 bs=393216 train_time:1220175ms step_avg:940.04ms
step:1299/1775 n_predict=1 lr=0.9211 bs=393216 train_time:1221773ms step_avg:940.55ms
step:1300/1775 n_predict=1 lr=0.9192 bs=393216 train_time:1223375ms step_avg:941.06ms
step:1301/1775 n_predict=1 lr=0.9173 bs=393216 train_time:1225047ms step_avg:941.62ms
step:1302/1775 n_predict=1 lr=0.9155 bs=393216 train_time:1226770ms step_avg:942.22ms
step:1303/1775 n_predict=1 lr=0.9136 bs=393216 train_time:1228449ms step_avg:942.78ms
step:1304/1775 n_predict=1 lr=0.9117 bs=393216 train_time:1230075ms step_avg:943.31ms
step:1305/1775 n_predict=1 lr=0.9098 bs=393216 train_time:1231667ms step_avg:943.81ms
step:1306/1775 n_predict=1 lr=0.9080 bs=393216 train_time:1233266ms step_avg:944.31ms
step:1307/1775 n_predict=1 lr=0.9061 bs=393216 train_time:1234946ms step_avg:944.87ms
step:1308/1775 n_predict=1 lr=0.9042 bs=393216 train_time:1236550ms step_avg:945.37ms
step:1309/1775 n_predict=1 lr=0.9023 bs=393216 train_time:1238173ms step_avg:945.89ms
step:1310/1775 n_predict=1 lr=0.9004 bs=393216 train_time:1239873ms step_avg:946.47ms
step:1311/1775 n_predict=1 lr=0.8986 bs=393216 train_time:1241473ms step_avg:946.97ms
step:1312/1775 n_predict=1 lr=0.8967 bs=393216 train_time:1243086ms step_avg:947.47ms
step:1313/1775 n_predict=1 lr=0.8948 bs=393216 train_time:1244754ms step_avg:948.02ms
step:1314/1775 n_predict=1 lr=0.8929 bs=393216 train_time:1246375ms step_avg:948.53ms
step:1315/1775 n_predict=1 lr=0.8910 bs=393216 train_time:1248077ms step_avg:949.11ms
step:1316/1775 n_predict=1 lr=0.8892 bs=393216 train_time:1249675ms step_avg:949.60ms
step:1317/1775 n_predict=1 lr=0.8873 bs=393216 train_time:1251371ms step_avg:950.17ms
step:1318/1775 n_predict=1 lr=0.8854 bs=393216 train_time:1252976ms step_avg:950.66ms
step:1319/1775 n_predict=1 lr=0.8835 bs=393216 train_time:1254674ms step_avg:951.23ms
step:1320/1775 n_predict=1 lr=0.8816 bs=393216 train_time:1256380ms step_avg:951.80ms
step:1321/1775 n_predict=1 lr=0.8798 bs=393216 train_time:1258073ms step_avg:952.36ms
step:1322/1775 n_predict=1 lr=0.8779 bs=393216 train_time:1259675ms step_avg:952.86ms
step:1323/1775 n_predict=1 lr=0.8760 bs=393216 train_time:1261273ms step_avg:953.34ms
step:1324/1775 n_predict=1 lr=0.8741 bs=393216 train_time:1262874ms step_avg:953.83ms
step:1325/1775 n_predict=1 lr=0.8723 bs=393216 train_time:1264472ms step_avg:954.32ms
step:1326/1775 n_predict=1 lr=0.8704 bs=393216 train_time:1266151ms step_avg:954.87ms
step:1327/1775 n_predict=1 lr=0.8685 bs=393216 train_time:1267770ms step_avg:955.37ms
step:1328/1775 n_predict=1 lr=0.8666 bs=393216 train_time:1269376ms step_avg:955.86ms
step:1329/1775 n_predict=1 lr=0.8647 bs=393216 train_time:1270973ms step_avg:956.34ms
step:1330/1775 n_predict=1 lr=0.8629 bs=393216 train_time:1272675ms step_avg:956.90ms
step:1331/1775 n_predict=1 lr=0.8610 bs=393216 train_time:1274360ms step_avg:957.45ms
step:1332/1775 n_predict=1 lr=0.8591 bs=393216 train_time:1276052ms step_avg:958.00ms
step:1333/1775 n_predict=1 lr=0.8572 bs=393216 train_time:1277649ms step_avg:958.48ms
step:1334/1775 n_predict=1 lr=0.8553 bs=393216 train_time:1279250ms step_avg:958.96ms
step:1335/1775 n_predict=1 lr=0.8535 bs=393216 train_time:1280941ms step_avg:959.51ms
step:1336/1775 n_predict=1 lr=0.8516 bs=393216 train_time:1282575ms step_avg:960.01ms
step:1337/1775 n_predict=1 lr=0.8497 bs=393216 train_time:1284259ms step_avg:960.55ms
step:1338/1775 n_predict=1 lr=0.8478 bs=393216 train_time:1285955ms step_avg:961.10ms
step:1339/1775 n_predict=1 lr=0.8459 bs=393216 train_time:1287572ms step_avg:961.59ms
step:1340/1775 n_predict=1 lr=0.8441 bs=393216 train_time:1289173ms step_avg:962.07ms
step:1341/1775 n_predict=1 lr=0.8422 bs=393216 train_time:1290873ms step_avg:962.62ms
step:1342/1775 n_predict=1 lr=0.8403 bs=393216 train_time:1292568ms step_avg:963.17ms
step:1343/1775 n_predict=1 lr=0.8384 bs=393216 train_time:1294150ms step_avg:963.63ms
step:1344/1775 n_predict=1 lr=0.8366 bs=393216 train_time:1295874ms step_avg:964.19ms
step:1345/1775 n_predict=1 lr=0.8347 bs=393216 train_time:1297572ms step_avg:964.74ms
step:1346/1775 n_predict=1 lr=0.8328 bs=393216 train_time:1299267ms step_avg:965.28ms
step:1347/1775 n_predict=1 lr=0.8309 bs=393216 train_time:1300872ms step_avg:965.76ms
step:1348/1775 n_predict=1 lr=0.8290 bs=393216 train_time:1302467ms step_avg:966.22ms
step:1349/1775 n_predict=1 lr=0.8272 bs=393216 train_time:1304053ms step_avg:966.68ms
step:1350/1775 n_predict=1 lr=0.8253 bs=393216 train_time:1305776ms step_avg:967.24ms
step:1351/1775 n_predict=1 lr=0.8234 bs=393216 train_time:1307473ms step_avg:967.78ms
step:1352/1775 n_predict=1 lr=0.8215 bs=393216 train_time:1309074ms step_avg:968.25ms
step:1353/1775 n_predict=1 lr=0.8196 bs=393216 train_time:1310768ms step_avg:968.79ms
step:1354/1775 n_predict=1 lr=0.8178 bs=393216 train_time:1312375ms step_avg:969.26ms
step:1355/1775 n_predict=1 lr=0.8159 bs=393216 train_time:1313962ms step_avg:969.71ms
step:1356/1775 n_predict=1 lr=0.8140 bs=393216 train_time:1315672ms step_avg:970.26ms
step:1357/1775 n_predict=1 lr=0.8121 bs=393216 train_time:1317359ms step_avg:970.79ms
step:1358/1775 n_predict=1 lr=0.8102 bs=393216 train_time:1319076ms step_avg:971.34ms
step:1359/1775 n_predict=1 lr=0.8084 bs=393216 train_time:1320673ms step_avg:971.80ms
step:1360/1775 n_predict=1 lr=0.8065 bs=393216 train_time:1322264ms step_avg:972.25ms
step:1361/1775 n_predict=1 lr=0.8046 bs=393216 train_time:1324050ms step_avg:972.85ms
step:1362/1775 n_predict=1 lr=0.8027 bs=393216 train_time:1325671ms step_avg:973.33ms
step:1363/1775 n_predict=1 lr=0.8009 bs=393216 train_time:1327266ms step_avg:973.78ms
step:1364/1775 n_predict=1 lr=0.7990 bs=393216 train_time:1328875ms step_avg:974.25ms
step:1365/1775 n_predict=1 lr=0.7971 bs=393216 train_time:1330473ms step_avg:974.71ms
step:1366/1775 n_predict=1 lr=0.7952 bs=393216 train_time:1332076ms step_avg:975.17ms
step:1367/1775 n_predict=1 lr=0.7933 bs=393216 train_time:1333773ms step_avg:975.69ms
step:1368/1775 n_predict=1 lr=0.7915 bs=393216 train_time:1335375ms step_avg:976.15ms
step:1369/1775 n_predict=1 lr=0.7896 bs=393216 train_time:1337073ms step_avg:976.68ms
step:1370/1775 n_predict=1 lr=0.7877 bs=393216 train_time:1338676ms step_avg:977.14ms
step:1371/1775 n_predict=1 lr=0.7858 bs=393216 train_time:1340473ms step_avg:977.73ms
step:1372/1775 n_predict=1 lr=0.7839 bs=393216 train_time:1342173ms step_avg:978.26ms
step:1373/1775 n_predict=1 lr=0.7821 bs=393216 train_time:1343951ms step_avg:978.84ms
step:1374/1775 n_predict=1 lr=0.7802 bs=393216 train_time:1345554ms step_avg:979.30ms
step:1375/1775 n_predict=1 lr=0.7783 bs=393216 train_time:1347173ms step_avg:979.76ms
step:1376/1775 n_predict=1 lr=0.7764 bs=393216 train_time:1348851ms step_avg:980.27ms
step:1377/1775 n_predict=1 lr=0.7745 bs=393216 train_time:1350450ms step_avg:980.72ms
step:1378/1775 n_predict=1 lr=0.7727 bs=393216 train_time:1352076ms step_avg:981.19ms
step:1379/1775 n_predict=1 lr=0.7708 bs=393216 train_time:1353674ms step_avg:981.63ms
step:1380/1775 n_predict=1 lr=0.7689 bs=393216 train_time:1355366ms step_avg:982.15ms
step:1381/1775 n_predict=1 lr=0.7670 bs=393216 train_time:1357064ms step_avg:982.67ms
step:1382/1775 n_predict=1 lr=0.7652 bs=393216 train_time:1358675ms step_avg:983.12ms
step:1383/1775 n_predict=1 lr=0.7633 bs=393216 train_time:1360354ms step_avg:983.63ms
step:1384/1775 n_predict=1 lr=0.7614 bs=393216 train_time:1361975ms step_avg:984.09ms
step:1385/1775 n_predict=1 lr=0.7595 bs=393216 train_time:1363573ms step_avg:984.53ms
step:1386/1775 n_predict=1 lr=0.7576 bs=393216 train_time:1365252ms step_avg:985.03ms
step:1387/1775 n_predict=1 lr=0.7558 bs=393216 train_time:1366874ms step_avg:985.49ms
step:1388/1775 n_predict=1 lr=0.7539 bs=393216 train_time:1368461ms step_avg:985.92ms
step:1389/1775 n_predict=1 lr=0.7520 bs=393216 train_time:1370073ms step_avg:986.37ms
step:1390/1775 n_predict=1 lr=0.7501 bs=393216 train_time:1371753ms step_avg:986.87ms
step:1391/1775 n_predict=1 lr=0.7482 bs=393216 train_time:1373349ms step_avg:987.31ms
step:1392/1775 n_predict=1 lr=0.7464 bs=393216 train_time:1374967ms step_avg:987.76ms
step:1393/1775 n_predict=1 lr=0.7445 bs=393216 train_time:1376648ms step_avg:988.26ms
step:1394/1775 n_predict=1 lr=0.7426 bs=393216 train_time:1378373ms step_avg:988.79ms
step:1395/1775 n_predict=1 lr=0.7407 bs=393216 train_time:1380060ms step_avg:989.29ms
step:1396/1775 n_predict=1 lr=0.7388 bs=393216 train_time:1381667ms step_avg:989.73ms
step:1397/1775 n_predict=1 lr=0.7370 bs=393216 train_time:1383366ms step_avg:990.24ms
step:1398/1775 n_predict=1 lr=0.7351 bs=393216 train_time:1384975ms step_avg:990.68ms
step:1399/1775 n_predict=1 lr=0.7332 bs=393216 train_time:1386572ms step_avg:991.12ms
step:1400/1775 n_predict=1 lr=0.7313 bs=393216 train_time:1388275ms step_avg:991.63ms
step:1401/1775 n_predict=1 lr=0.7295 bs=393216 train_time:1389965ms step_avg:992.12ms
step:1402/1775 n_predict=1 lr=0.7276 bs=393216 train_time:1391575ms step_avg:992.56ms
step:1403/1775 n_predict=1 lr=0.7257 bs=393216 train_time:1393232ms step_avg:993.04ms
step:1404/1775 n_predict=1 lr=0.7238 bs=393216 train_time:1394873ms step_avg:993.50ms
step:1405/1775 n_predict=1 lr=0.7219 bs=393216 train_time:1396564ms step_avg:994.00ms
step:1406/1775 n_predict=1 lr=0.7201 bs=393216 train_time:1398176ms step_avg:994.44ms
step:1407/1775 n_predict=1 lr=0.7182 bs=393216 train_time:1399848ms step_avg:994.92ms
step:1408/1775 n_predict=1 lr=0.7163 bs=393216 train_time:1401572ms step_avg:995.43ms
step:1409/1775 n_predict=1 lr=0.7144 bs=393216 train_time:1403172ms step_avg:995.86ms
step:1410/1775 n_predict=1 lr=0.7125 bs=393216 train_time:1404864ms step_avg:996.36ms
step:1411/1775 n_predict=1 lr=0.7107 bs=393216 train_time:1406450ms step_avg:996.78ms
step:1412/1775 n_predict=1 lr=0.7088 bs=393216 train_time:1408077ms step_avg:997.22ms
step:1413/1775 n_predict=1 lr=0.7069 bs=393216 train_time:1409673ms step_avg:997.65ms
step:1414/1775 n_predict=1 lr=0.7050 bs=393216 train_time:1411352ms step_avg:998.13ms
step:1415/1775 n_predict=1 lr=0.7031 bs=393216 train_time:1413049ms step_avg:998.62ms
step:1416/1775 n_predict=1 lr=0.7013 bs=393216 train_time:1414675ms step_avg:999.06ms
step:1417/1775 n_predict=1 lr=0.6994 bs=393216 train_time:1416273ms step_avg:999.49ms
step:1418/1775 n_predict=1 lr=0.6975 bs=393216 train_time:1417874ms step_avg:999.91ms
step:1419/1775 n_predict=1 lr=0.6956 bs=393216 train_time:1419474ms step_avg:1000.33ms
step:1420/1775 n_predict=1 lr=0.6938 bs=393216 train_time:1421153ms step_avg:1000.81ms
step:1421/1775 n_predict=1 lr=0.6919 bs=393216 train_time:1422770ms step_avg:1001.25ms
step:1422/1775 n_predict=1 lr=0.6900 bs=393216 train_time:1424467ms step_avg:1001.73ms
step:1423/1775 n_predict=1 lr=0.6881 bs=393216 train_time:1426075ms step_avg:1002.16ms
step:1424/1775 n_predict=1 lr=0.6862 bs=393216 train_time:1427675ms step_avg:1002.58ms
step:1425/1775 n_predict=1 lr=0.6844 bs=393216 train_time:1429273ms step_avg:1003.00ms
step:1426/1775 n_predict=1 lr=0.6825 bs=393216 train_time:1430875ms step_avg:1003.42ms
step:1427/1775 n_predict=1 lr=0.6806 bs=393216 train_time:1432561ms step_avg:1003.90ms
step:1428/1775 n_predict=1 lr=0.6787 bs=393216 train_time:1434174ms step_avg:1004.32ms
step:1429/1775 n_predict=1 lr=0.6768 bs=393216 train_time:1435837ms step_avg:1004.78ms
step:1430/1775 n_predict=1 lr=0.6750 bs=393216 train_time:1437476ms step_avg:1005.23ms
step:1431/1775 n_predict=1 lr=0.6731 bs=393216 train_time:1439071ms step_avg:1005.64ms
step:1432/1775 n_predict=1 lr=0.6712 bs=393216 train_time:1440750ms step_avg:1006.11ms
step:1433/1775 n_predict=1 lr=0.6693 bs=393216 train_time:1442447ms step_avg:1006.59ms
step:1434/1775 n_predict=1 lr=0.6674 bs=393216 train_time:1444154ms step_avg:1007.08ms
step:1435/1775 n_predict=1 lr=0.6656 bs=393216 train_time:1445850ms step_avg:1007.56ms
step:1436/1775 n_predict=1 lr=0.6637 bs=393216 train_time:1447450ms step_avg:1007.97ms
step:1437/1775 n_predict=1 lr=0.6618 bs=393216 train_time:1449057ms step_avg:1008.39ms
step:1438/1775 n_predict=1 lr=0.6599 bs=393216 train_time:1450647ms step_avg:1008.79ms
step:1439/1775 n_predict=1 lr=0.6581 bs=393216 train_time:1452356ms step_avg:1009.28ms
step:1440/1775 n_predict=1 lr=0.6562 bs=393216 train_time:1453975ms step_avg:1009.70ms
step:1441/1775 n_predict=1 lr=0.6543 bs=393216 train_time:1455572ms step_avg:1010.11ms
step:1442/1775 n_predict=1 lr=0.6524 bs=393216 train_time:1457178ms step_avg:1010.53ms
step:1443/1775 n_predict=1 lr=0.6505 bs=393216 train_time:1458850ms step_avg:1010.98ms
step:1444/1775 n_predict=1 lr=0.6487 bs=393216 train_time:1460475ms step_avg:1011.41ms
step:1445/1775 n_predict=1 lr=0.6468 bs=393216 train_time:1462266ms step_avg:1011.95ms
step:1446/1775 n_predict=1 lr=0.6449 bs=393216 train_time:1463875ms step_avg:1012.36ms
step:1447/1775 n_predict=1 lr=0.6430 bs=393216 train_time:1465471ms step_avg:1012.77ms
step:1448/1775 n_predict=1 lr=0.6411 bs=393216 train_time:1467168ms step_avg:1013.24ms
step:1449/1775 n_predict=1 lr=0.6393 bs=393216 train_time:1468750ms step_avg:1013.63ms
step:1450/1775 n_predict=1 lr=0.6374 bs=393216 train_time:1470475ms step_avg:1014.12ms
step:1451/1775 n_predict=1 lr=0.6355 bs=393216 train_time:1472159ms step_avg:1014.58ms
step:1452/1775 n_predict=1 lr=0.6336 bs=393216 train_time:1473750ms step_avg:1014.98ms
step:1453/1775 n_predict=1 lr=0.6317 bs=393216 train_time:1475472ms step_avg:1015.47ms
step:1454/1775 n_predict=1 lr=0.6299 bs=393216 train_time:1477061ms step_avg:1015.86ms
step:1455/1775 n_predict=1 lr=0.6280 bs=393216 train_time:1478753ms step_avg:1016.33ms
step:1456/1775 n_predict=1 lr=0.6261 bs=393216 train_time:1480352ms step_avg:1016.73ms
step:1457/1775 n_predict=1 lr=0.6242 bs=393216 train_time:1481964ms step_avg:1017.13ms
step:1458/1775 n_predict=1 lr=0.6224 bs=393216 train_time:1483648ms step_avg:1017.59ms
step:1459/1775 n_predict=1 lr=0.6205 bs=393216 train_time:1485269ms step_avg:1018.00ms
step:1460/1775 n_predict=1 lr=0.6186 bs=393216 train_time:1486850ms step_avg:1018.39ms
step:1461/1775 n_predict=1 lr=0.6167 bs=393216 train_time:1488548ms step_avg:1018.86ms
step:1462/1775 n_predict=1 lr=0.6148 bs=393216 train_time:1490252ms step_avg:1019.32ms
step:1463/1775 n_predict=1 lr=0.6130 bs=393216 train_time:1491873ms step_avg:1019.74ms
step:1464/1775 n_predict=1 lr=0.6111 bs=393216 train_time:1493551ms step_avg:1020.18ms
step:1465/1775 n_predict=1 lr=0.6092 bs=393216 train_time:1495333ms step_avg:1020.71ms
step:1466/1775 n_predict=1 lr=0.6073 bs=393216 train_time:1496963ms step_avg:1021.12ms
step:1467/1775 n_predict=1 lr=0.6054 bs=393216 train_time:1498573ms step_avg:1021.52ms
step:1468/1775 n_predict=1 lr=0.6036 bs=393216 train_time:1500173ms step_avg:1021.92ms
step:1469/1775 n_predict=1 lr=0.6017 bs=393216 train_time:1501848ms step_avg:1022.36ms
step:1470/1775 n_predict=1 lr=0.5998 bs=393216 train_time:1503556ms step_avg:1022.83ms
step:1471/1775 n_predict=1 lr=0.5979 bs=393216 train_time:1505148ms step_avg:1023.21ms
step:1472/1775 n_predict=1 lr=0.5960 bs=393216 train_time:1506767ms step_avg:1023.62ms
step:1473/1775 n_predict=1 lr=0.5942 bs=393216 train_time:1508372ms step_avg:1024.01ms
step:1474/1775 n_predict=1 lr=0.5923 bs=393216 train_time:1510051ms step_avg:1024.46ms
step:1475/1775 n_predict=1 lr=0.5904 bs=393216 train_time:1511673ms step_avg:1024.86ms
step:1476/1775 n_predict=1 lr=0.5885 bs=393216 train_time:1513375ms step_avg:1025.32ms
step:1477/1775 n_predict=1 lr=0.5867 bs=393216 train_time:1515047ms step_avg:1025.76ms
step:1478/1775 n_predict=1 lr=0.5848 bs=393216 train_time:1516649ms step_avg:1026.15ms
step:1479/1775 n_predict=1 lr=0.5829 bs=393216 train_time:1518263ms step_avg:1026.55ms
step:1480/1775 n_predict=1 lr=0.5810 bs=393216 train_time:1519878ms step_avg:1026.94ms
step:1481/1775 n_predict=1 lr=0.5791 bs=393216 train_time:1521473ms step_avg:1027.33ms
step:1482/1775 n_predict=1 lr=0.5773 bs=393216 train_time:1523135ms step_avg:1027.76ms
step:1483/1775 n_predict=1 lr=0.5754 bs=393216 train_time:1524773ms step_avg:1028.17ms
step:1484/1775 n_predict=1 lr=0.5735 bs=393216 train_time:1526467ms step_avg:1028.62ms
step:1485/1775 n_predict=1 lr=0.5716 bs=393216 train_time:1528075ms step_avg:1029.01ms
step:1486/1775 n_predict=1 lr=0.5697 bs=393216 train_time:1529675ms step_avg:1029.39ms
step:1487/1775 n_predict=1 lr=0.5679 bs=393216 train_time:1531273ms step_avg:1029.77ms
step:1488/1775 n_predict=1 lr=0.5660 bs=393216 train_time:1532974ms step_avg:1030.22ms
step:1489/1775 n_predict=1 lr=0.5641 bs=393216 train_time:1534662ms step_avg:1030.67ms
step:1490/1775 n_predict=1 lr=0.5622 bs=393216 train_time:1536275ms step_avg:1031.06ms
step:1491/1775 n_predict=1 lr=0.5603 bs=393216 train_time:1537873ms step_avg:1031.44ms
step:1492/1775 n_predict=1 lr=0.5585 bs=393216 train_time:1539664ms step_avg:1031.95ms
step:1493/1775 n_predict=1 lr=0.5566 bs=393216 train_time:1541270ms step_avg:1032.33ms
step:1494/1775 n_predict=1 lr=0.5547 bs=393216 train_time:1542971ms step_avg:1032.78ms
step:1495/1775 n_predict=1 lr=0.5528 bs=393216 train_time:1544671ms step_avg:1033.22ms
step:1496/1775 n_predict=1 lr=0.5510 bs=393216 train_time:1546275ms step_avg:1033.61ms
step:1497/1775 n_predict=1 lr=0.5491 bs=393216 train_time:1547966ms step_avg:1034.05ms
step:1498/1775 n_predict=1 lr=0.5472 bs=393216 train_time:1549561ms step_avg:1034.42ms
step:1499/1775 n_predict=1 lr=0.5453 bs=393216 train_time:1551150ms step_avg:1034.79ms
step:1500/1775 n_predict=1 lr=0.5434 bs=393216 train_time:1552776ms step_avg:1035.18ms
step:1500/1775 lr=0.5416 bs=393216 n_predict=1 val_loss:3.3823 val_malbo_loss:3.4624 train_time:1552861ms step_avg:1035.24ms
step:1501/1775 n_predict=1 lr=0.5416 bs=393216 train_time:1554564ms step_avg:1035.69ms
step:1502/1775 n_predict=1 lr=0.5397 bs=393216 train_time:1556183ms step_avg:1036.07ms
step:1503/1775 n_predict=1 lr=0.5378 bs=393216 train_time:1557766ms step_avg:1036.44ms
step:1504/1775 n_predict=1 lr=0.5359 bs=393216 train_time:1559384ms step_avg:1036.82ms
step:1505/1775 n_predict=1 lr=0.5340 bs=393216 train_time:1560993ms step_avg:1037.20ms
step:1506/1775 n_predict=1 lr=0.5322 bs=393216 train_time:1562694ms step_avg:1037.65ms
step:1507/1775 n_predict=1 lr=0.5303 bs=393216 train_time:1564290ms step_avg:1038.02ms
step:1508/1775 n_predict=1 lr=0.5284 bs=393216 train_time:1565891ms step_avg:1038.39ms
step:1509/1775 n_predict=1 lr=0.5265 bs=393216 train_time:1567575ms step_avg:1038.82ms
step:1510/1775 n_predict=1 lr=0.5246 bs=393216 train_time:1569268ms step_avg:1039.25ms
step:1511/1775 n_predict=1 lr=0.5228 bs=393216 train_time:1570886ms step_avg:1039.63ms
step:1512/1775 n_predict=1 lr=0.5209 bs=393216 train_time:1572489ms step_avg:1040.01ms
step:1513/1775 n_predict=1 lr=0.5190 bs=393216 train_time:1574083ms step_avg:1040.37ms
step:1514/1775 n_predict=1 lr=0.5171 bs=393216 train_time:1575684ms step_avg:1040.74ms
step:1515/1775 n_predict=1 lr=0.5153 bs=393216 train_time:1577365ms step_avg:1041.16ms
step:1516/1775 n_predict=1 lr=0.5134 bs=393216 train_time:1579080ms step_avg:1041.61ms
step:1517/1775 n_predict=1 lr=0.5115 bs=393216 train_time:1580686ms step_avg:1041.98ms
step:1518/1775 n_predict=1 lr=0.5096 bs=393216 train_time:1582299ms step_avg:1042.36ms
step:1519/1775 n_predict=1 lr=0.5077 bs=393216 train_time:1583990ms step_avg:1042.78ms
step:1520/1775 n_predict=1 lr=0.5059 bs=393216 train_time:1585667ms step_avg:1043.20ms
step:1521/1775 n_predict=1 lr=0.5040 bs=393216 train_time:1587289ms step_avg:1043.58ms
step:1522/1775 n_predict=1 lr=0.5021 bs=393216 train_time:1588991ms step_avg:1044.02ms
step:1523/1775 n_predict=1 lr=0.5002 bs=393216 train_time:1590656ms step_avg:1044.42ms
step:1524/1775 n_predict=1 lr=0.4983 bs=393216 train_time:1592283ms step_avg:1044.80ms
step:1525/1775 n_predict=1 lr=0.4965 bs=393216 train_time:1593964ms step_avg:1045.22ms
step:1526/1775 n_predict=1 lr=0.4946 bs=393216 train_time:1595591ms step_avg:1045.60ms
step:1527/1775 n_predict=1 lr=0.4927 bs=393216 train_time:1597254ms step_avg:1046.01ms
step:1528/1775 n_predict=1 lr=0.4908 bs=393216 train_time:1598790ms step_avg:1046.33ms
step:1529/1775 n_predict=1 lr=0.4889 bs=393216 train_time:1600389ms step_avg:1046.69ms
step:1530/1775 n_predict=1 lr=0.4871 bs=393216 train_time:1602090ms step_avg:1047.12ms
step:1531/1775 n_predict=1 lr=0.4852 bs=393216 train_time:1603790ms step_avg:1047.54ms
step:1532/1775 n_predict=1 lr=0.4833 bs=393216 train_time:1605468ms step_avg:1047.96ms
step:1533/1775 n_predict=1 lr=0.4814 bs=393216 train_time:1607085ms step_avg:1048.33ms
step:1534/1775 n_predict=1 lr=0.4796 bs=393216 train_time:1608791ms step_avg:1048.76ms
step:1535/1775 n_predict=1 lr=0.4777 bs=393216 train_time:1610389ms step_avg:1049.11ms
step:1536/1775 n_predict=1 lr=0.4758 bs=393216 train_time:1611991ms step_avg:1049.47ms
step:1537/1775 n_predict=1 lr=0.4739 bs=393216 train_time:1613590ms step_avg:1049.83ms
step:1538/1775 n_predict=1 lr=0.4720 bs=393216 train_time:1615192ms step_avg:1050.19ms
step:1539/1775 n_predict=1 lr=0.4702 bs=393216 train_time:1616888ms step_avg:1050.61ms
step:1540/1775 n_predict=1 lr=0.4683 bs=393216 train_time:1618566ms step_avg:1051.02ms
step:1541/1775 n_predict=1 lr=0.4664 bs=393216 train_time:1620377ms step_avg:1051.51ms
step:1542/1775 n_predict=1 lr=0.4645 bs=393216 train_time:1622067ms step_avg:1051.92ms
step:1543/1775 n_predict=1 lr=0.4626 bs=393216 train_time:1623689ms step_avg:1052.29ms
step:1544/1775 n_predict=1 lr=0.4608 bs=393216 train_time:1625296ms step_avg:1052.65ms
step:1545/1775 n_predict=1 lr=0.4589 bs=393216 train_time:1626954ms step_avg:1053.04ms
step:1546/1775 n_predict=1 lr=0.4570 bs=393216 train_time:1628667ms step_avg:1053.47ms
step:1547/1775 n_predict=1 lr=0.4551 bs=393216 train_time:1630268ms step_avg:1053.83ms
step:1548/1775 n_predict=1 lr=0.4532 bs=393216 train_time:1631895ms step_avg:1054.20ms
step:1549/1775 n_predict=1 lr=0.4514 bs=393216 train_time:1633490ms step_avg:1054.54ms
step:1550/1775 n_predict=1 lr=0.4495 bs=393216 train_time:1635094ms step_avg:1054.90ms
step:1551/1775 n_predict=1 lr=0.4476 bs=393216 train_time:1636688ms step_avg:1055.25ms
step:1552/1775 n_predict=1 lr=0.4457 bs=393216 train_time:1638276ms step_avg:1055.59ms
step:1553/1775 n_predict=1 lr=0.4439 bs=393216 train_time:1639888ms step_avg:1055.95ms
step:1554/1775 n_predict=1 lr=0.4420 bs=393216 train_time:1641492ms step_avg:1056.30ms
step:1555/1775 n_predict=1 lr=0.4401 bs=393216 train_time:1643089ms step_avg:1056.65ms
step:1556/1775 n_predict=1 lr=0.4382 bs=393216 train_time:1644768ms step_avg:1057.05ms
step:1557/1775 n_predict=1 lr=0.4363 bs=393216 train_time:1646389ms step_avg:1057.41ms
step:1558/1775 n_predict=1 lr=0.4345 bs=393216 train_time:1647987ms step_avg:1057.76ms
step:1559/1775 n_predict=1 lr=0.4326 bs=393216 train_time:1649682ms step_avg:1058.17ms
step:1560/1775 n_predict=1 lr=0.4307 bs=393216 train_time:1651364ms step_avg:1058.57ms
step:1561/1775 n_predict=1 lr=0.4288 bs=393216 train_time:1652964ms step_avg:1058.91ms
step:1562/1775 n_predict=1 lr=0.4269 bs=393216 train_time:1654573ms step_avg:1059.27ms
step:1563/1775 n_predict=1 lr=0.4251 bs=393216 train_time:1656285ms step_avg:1059.68ms
step:1564/1775 n_predict=1 lr=0.4232 bs=393216 train_time:1657892ms step_avg:1060.03ms
step:1565/1775 n_predict=1 lr=0.4213 bs=393216 train_time:1659489ms step_avg:1060.38ms
step:1566/1775 n_predict=1 lr=0.4194 bs=393216 train_time:1661083ms step_avg:1060.72ms
step:1567/1775 n_predict=1 lr=0.4175 bs=393216 train_time:1662692ms step_avg:1061.07ms
step:1568/1775 n_predict=1 lr=0.4157 bs=393216 train_time:1664392ms step_avg:1061.47ms
step:1569/1775 n_predict=1 lr=0.4138 bs=393216 train_time:1666065ms step_avg:1061.86ms
step:1570/1775 n_predict=1 lr=0.4119 bs=393216 train_time:1667767ms step_avg:1062.27ms
step:1571/1775 n_predict=1 lr=0.4100 bs=393216 train_time:1669467ms step_avg:1062.68ms
step:1572/1775 n_predict=1 lr=0.4081 bs=393216 train_time:1671091ms step_avg:1063.03ms
step:1573/1775 n_predict=1 lr=0.4063 bs=393216 train_time:1672681ms step_avg:1063.37ms
step:1574/1775 n_predict=1 lr=0.4044 bs=393216 train_time:1674292ms step_avg:1063.72ms
step:1575/1775 n_predict=1 lr=0.4025 bs=393216 train_time:1675957ms step_avg:1064.10ms
step:1576/1775 n_predict=1 lr=0.4006 bs=393216 train_time:1677582ms step_avg:1064.46ms
step:1577/1775 n_predict=1 lr=0.3988 bs=393216 train_time:1679188ms step_avg:1064.80ms
step:1578/1775 n_predict=1 lr=0.3969 bs=393216 train_time:1680867ms step_avg:1065.19ms
step:1579/1775 n_predict=1 lr=0.3950 bs=393216 train_time:1682488ms step_avg:1065.54ms
step:1580/1775 n_predict=1 lr=0.3931 bs=393216 train_time:1684156ms step_avg:1065.92ms
step:1581/1775 n_predict=1 lr=0.3912 bs=393216 train_time:1685880ms step_avg:1066.34ms
step:1582/1775 n_predict=1 lr=0.3894 bs=393216 train_time:1687483ms step_avg:1066.68ms
step:1583/1775 n_predict=1 lr=0.3875 bs=393216 train_time:1689089ms step_avg:1067.02ms
step:1584/1775 n_predict=1 lr=0.3856 bs=393216 train_time:1690692ms step_avg:1067.36ms
step:1585/1775 n_predict=1 lr=0.3837 bs=393216 train_time:1692289ms step_avg:1067.69ms
step:1586/1775 n_predict=1 lr=0.3818 bs=393216 train_time:1693883ms step_avg:1068.02ms
step:1587/1775 n_predict=1 lr=0.3800 bs=393216 train_time:1695488ms step_avg:1068.36ms
step:1588/1775 n_predict=1 lr=0.3781 bs=393216 train_time:1697159ms step_avg:1068.74ms
step:1589/1775 n_predict=1 lr=0.3762 bs=393216 train_time:1698882ms step_avg:1069.15ms
step:1590/1775 n_predict=1 lr=0.3743 bs=393216 train_time:1700481ms step_avg:1069.49ms
step:1591/1775 n_predict=1 lr=0.3724 bs=393216 train_time:1702265ms step_avg:1069.93ms
step:1592/1775 n_predict=1 lr=0.3706 bs=393216 train_time:1703890ms step_avg:1070.28ms
step:1593/1775 n_predict=1 lr=0.3687 bs=393216 train_time:1705486ms step_avg:1070.61ms
step:1594/1775 n_predict=1 lr=0.3668 bs=393216 train_time:1707094ms step_avg:1070.95ms
step:1595/1775 n_predict=1 lr=0.3649 bs=393216 train_time:1708789ms step_avg:1071.34ms
step:1596/1775 n_predict=1 lr=0.3631 bs=393216 train_time:1710491ms step_avg:1071.74ms
step:1597/1775 n_predict=1 lr=0.3612 bs=393216 train_time:1712175ms step_avg:1072.12ms
step:1598/1775 n_predict=1 lr=0.3593 bs=393216 train_time:1713891ms step_avg:1072.52ms
step:1599/1775 n_predict=1 lr=0.3574 bs=393216 train_time:1715567ms step_avg:1072.90ms
step:1600/1775 n_predict=1 lr=0.3555 bs=393216 train_time:1717269ms step_avg:1073.29ms
step:1601/1775 n_predict=1 lr=0.3537 bs=393216 train_time:1718965ms step_avg:1073.68ms
step:1602/1775 n_predict=1 lr=0.3518 bs=393216 train_time:1720591ms step_avg:1074.03ms
step:1603/1775 n_predict=1 lr=0.3499 bs=393216 train_time:1722166ms step_avg:1074.34ms
step:1604/1775 n_predict=1 lr=0.3480 bs=393216 train_time:1723885ms step_avg:1074.74ms
step:1605/1775 n_predict=1 lr=0.3461 bs=393216 train_time:1725489ms step_avg:1075.07ms
step:1606/1775 n_predict=1 lr=0.3443 bs=393216 train_time:1727251ms step_avg:1075.50ms
step:1607/1775 n_predict=1 lr=0.3424 bs=393216 train_time:1728889ms step_avg:1075.85ms
step:1608/1775 n_predict=1 lr=0.3405 bs=393216 train_time:1730595ms step_avg:1076.24ms
step:1609/1775 n_predict=1 lr=0.3386 bs=393216 train_time:1732282ms step_avg:1076.62ms
step:1610/1775 n_predict=1 lr=0.3367 bs=393216 train_time:1733968ms step_avg:1077.00ms
step:1611/1775 n_predict=1 lr=0.3349 bs=393216 train_time:1735574ms step_avg:1077.33ms
step:1612/1775 n_predict=1 lr=0.3330 bs=393216 train_time:1737283ms step_avg:1077.72ms
step:1613/1775 n_predict=1 lr=0.3311 bs=393216 train_time:1738888ms step_avg:1078.05ms
step:1614/1775 n_predict=1 lr=0.3292 bs=393216 train_time:1740487ms step_avg:1078.37ms
step:1615/1775 n_predict=1 lr=0.3274 bs=393216 train_time:1742180ms step_avg:1078.75ms
step:1616/1775 n_predict=1 lr=0.3255 bs=393216 train_time:1743792ms step_avg:1079.08ms
step:1617/1775 n_predict=1 lr=0.3236 bs=393216 train_time:1745365ms step_avg:1079.38ms
step:1618/1775 n_predict=1 lr=0.3217 bs=393216 train_time:1747067ms step_avg:1079.77ms
step:1619/1775 n_predict=1 lr=0.3198 bs=393216 train_time:1748689ms step_avg:1080.10ms
step:1620/1775 n_predict=1 lr=0.3180 bs=393216 train_time:1750284ms step_avg:1080.42ms
step:1621/1775 n_predict=1 lr=0.3161 bs=393216 train_time:1751888ms step_avg:1080.75ms
step:1622/1775 n_predict=1 lr=0.3142 bs=393216 train_time:1753490ms step_avg:1081.07ms
step:1623/1775 n_predict=1 lr=0.3123 bs=393216 train_time:1755169ms step_avg:1081.43ms
step:1624/1775 n_predict=1 lr=0.3104 bs=393216 train_time:1756789ms step_avg:1081.77ms
step:1625/1775 n_predict=1 lr=0.3086 bs=393216 train_time:1758462ms step_avg:1082.13ms
step:1626/1775 n_predict=1 lr=0.3067 bs=393216 train_time:1760090ms step_avg:1082.47ms
step:1627/1775 n_predict=1 lr=0.3048 bs=393216 train_time:1761689ms step_avg:1082.78ms
step:1628/1775 n_predict=1 lr=0.3029 bs=393216 train_time:1763288ms step_avg:1083.10ms
step:1629/1775 n_predict=1 lr=0.3010 bs=393216 train_time:1764881ms step_avg:1083.41ms
step:1630/1775 n_predict=1 lr=0.2992 bs=393216 train_time:1766488ms step_avg:1083.73ms
step:1631/1775 n_predict=1 lr=0.2973 bs=393216 train_time:1768091ms step_avg:1084.05ms
step:1632/1775 n_predict=1 lr=0.2954 bs=393216 train_time:1769689ms step_avg:1084.37ms
step:1633/1775 n_predict=1 lr=0.2935 bs=393216 train_time:1771289ms step_avg:1084.68ms
step:1634/1775 n_predict=1 lr=0.2917 bs=393216 train_time:1772977ms step_avg:1085.05ms
step:1635/1775 n_predict=1 lr=0.2898 bs=393216 train_time:1774581ms step_avg:1085.37ms
step:1636/1775 n_predict=1 lr=0.2879 bs=393216 train_time:1776267ms step_avg:1085.74ms
step:1637/1775 n_predict=1 lr=0.2860 bs=393216 train_time:1778054ms step_avg:1086.17ms
step:1638/1775 n_predict=1 lr=0.2841 bs=393216 train_time:1779690ms step_avg:1086.50ms
step:1639/1775 n_predict=1 lr=0.2823 bs=393216 train_time:1781282ms step_avg:1086.81ms
step:1640/1775 n_predict=1 lr=0.2804 bs=393216 train_time:1782970ms step_avg:1087.18ms
step:1641/1775 n_predict=1 lr=0.2785 bs=393216 train_time:1784582ms step_avg:1087.50ms
step:1642/1775 n_predict=1 lr=0.2766 bs=393216 train_time:1786190ms step_avg:1087.81ms
step:1643/1775 n_predict=1 lr=0.2747 bs=393216 train_time:1787890ms step_avg:1088.19ms
step:1644/1775 n_predict=1 lr=0.2729 bs=393216 train_time:1789491ms step_avg:1088.50ms
step:1645/1775 n_predict=1 lr=0.2710 bs=393216 train_time:1791188ms step_avg:1088.87ms
step:1646/1775 n_predict=1 lr=0.2691 bs=393216 train_time:1792867ms step_avg:1089.23ms
step:1647/1775 n_predict=1 lr=0.2672 bs=393216 train_time:1794473ms step_avg:1089.54ms
step:1648/1775 n_predict=1 lr=0.2653 bs=393216 train_time:1796090ms step_avg:1089.86ms
step:1649/1775 n_predict=1 lr=0.2635 bs=393216 train_time:1797782ms step_avg:1090.23ms
step:1650/1775 n_predict=1 lr=0.2616 bs=393216 train_time:1799391ms step_avg:1090.54ms
step:1651/1775 n_predict=1 lr=0.2597 bs=393216 train_time:1801065ms step_avg:1090.89ms
step:1652/1775 n_predict=1 lr=0.2578 bs=393216 train_time:1802694ms step_avg:1091.22ms
step:1653/1775 n_predict=1 lr=0.2560 bs=393216 train_time:1804292ms step_avg:1091.53ms
step:1654/1775 n_predict=1 lr=0.2541 bs=393216 train_time:1805985ms step_avg:1091.89ms
step:1655/1775 n_predict=1 lr=0.2522 bs=393216 train_time:1807589ms step_avg:1092.20ms
step:1656/1775 n_predict=1 lr=0.2503 bs=393216 train_time:1809271ms step_avg:1092.55ms
step:1657/1775 n_predict=1 lr=0.2484 bs=393216 train_time:1810889ms step_avg:1092.87ms
step:1658/1775 n_predict=1 lr=0.2466 bs=393216 train_time:1812489ms step_avg:1093.18ms
step:1659/1775 n_predict=1 lr=0.2447 bs=393216 train_time:1814088ms step_avg:1093.48ms
step:1660/1775 n_predict=1 lr=0.2428 bs=393216 train_time:1815792ms step_avg:1093.85ms
step:1661/1775 n_predict=1 lr=0.2409 bs=393216 train_time:1817367ms step_avg:1094.14ms
step:1662/1775 n_predict=1 lr=0.2390 bs=393216 train_time:1818992ms step_avg:1094.46ms
step:1663/1775 n_predict=1 lr=0.2372 bs=393216 train_time:1820682ms step_avg:1094.82ms
step:1664/1775 n_predict=1 lr=0.2353 bs=393216 train_time:1822292ms step_avg:1095.13ms
step:1665/1775 n_predict=1 lr=0.2334 bs=393216 train_time:1824088ms step_avg:1095.55ms
step:1666/1775 n_predict=1 lr=0.2315 bs=393216 train_time:1825768ms step_avg:1095.90ms
step:1667/1775 n_predict=1 lr=0.2296 bs=393216 train_time:1827376ms step_avg:1096.21ms
step:1668/1775 n_predict=1 lr=0.2278 bs=393216 train_time:1829080ms step_avg:1096.57ms
step:1669/1775 n_predict=1 lr=0.2259 bs=393216 train_time:1830765ms step_avg:1096.92ms
step:1670/1775 n_predict=1 lr=0.2240 bs=393216 train_time:1832390ms step_avg:1097.24ms
step:1671/1775 n_predict=1 lr=0.2221 bs=393216 train_time:1833990ms step_avg:1097.54ms
step:1672/1775 n_predict=1 lr=0.2203 bs=393216 train_time:1835593ms step_avg:1097.84ms
step:1673/1775 n_predict=1 lr=0.2184 bs=393216 train_time:1837288ms step_avg:1098.20ms
step:1674/1775 n_predict=1 lr=0.2165 bs=393216 train_time:1838891ms step_avg:1098.50ms
step:1675/1775 n_predict=1 lr=0.2146 bs=393216 train_time:1840487ms step_avg:1098.80ms
step:1676/1775 n_predict=1 lr=0.2127 bs=393216 train_time:1842090ms step_avg:1099.10ms
step:1677/1775 n_predict=1 lr=0.2109 bs=393216 train_time:1843685ms step_avg:1099.39ms
step:1678/1775 n_predict=1 lr=0.2090 bs=393216 train_time:1845291ms step_avg:1099.70ms
step:1679/1775 n_predict=1 lr=0.2071 bs=393216 train_time:1846964ms step_avg:1100.04ms
step:1680/1775 n_predict=1 lr=0.2052 bs=393216 train_time:1848687ms step_avg:1100.41ms
step:1681/1775 n_predict=1 lr=0.2033 bs=393216 train_time:1850292ms step_avg:1100.71ms
step:1682/1775 n_predict=1 lr=0.2015 bs=393216 train_time:1851890ms step_avg:1101.00ms
step:1683/1775 n_predict=1 lr=0.1996 bs=393216 train_time:1853566ms step_avg:1101.35ms
step:1684/1775 n_predict=1 lr=0.1977 bs=393216 train_time:1855191ms step_avg:1101.66ms
step:1685/1775 n_predict=1 lr=0.1958 bs=393216 train_time:1856890ms step_avg:1102.01ms
step:1686/1775 n_predict=1 lr=0.1939 bs=393216 train_time:1858472ms step_avg:1102.30ms
step:1687/1775 n_predict=1 lr=0.1921 bs=393216 train_time:1860175ms step_avg:1102.65ms
step:1688/1775 n_predict=1 lr=0.1902 bs=393216 train_time:1861793ms step_avg:1102.96ms
step:1689/1775 n_predict=1 lr=0.1883 bs=393216 train_time:1863388ms step_avg:1103.25ms
step:1690/1775 n_predict=1 lr=0.1864 bs=393216 train_time:1864990ms step_avg:1103.54ms
step:1691/1775 n_predict=1 lr=0.1846 bs=393216 train_time:1866662ms step_avg:1103.88ms
step:1692/1775 n_predict=1 lr=0.1827 bs=393216 train_time:1868283ms step_avg:1104.19ms
step:1693/1775 n_predict=1 lr=0.1808 bs=393216 train_time:1869892ms step_avg:1104.48ms
step:1694/1775 n_predict=1 lr=0.1789 bs=393216 train_time:1871493ms step_avg:1104.78ms
step:1695/1775 n_predict=1 lr=0.1770 bs=393216 train_time:1873180ms step_avg:1105.12ms
step:1696/1775 n_predict=1 lr=0.1752 bs=393216 train_time:1874867ms step_avg:1105.46ms
step:1697/1775 n_predict=1 lr=0.1733 bs=393216 train_time:1876481ms step_avg:1105.76ms
step:1698/1775 n_predict=1 lr=0.1714 bs=393216 train_time:1878190ms step_avg:1106.12ms
step:1699/1775 n_predict=1 lr=0.1695 bs=393216 train_time:1879889ms step_avg:1106.47ms
step:1700/1775 n_predict=1 lr=0.1676 bs=393216 train_time:1881491ms step_avg:1106.76ms
step:1701/1775 n_predict=1 lr=0.1658 bs=393216 train_time:1883084ms step_avg:1107.05ms
step:1702/1775 n_predict=1 lr=0.1639 bs=393216 train_time:1884688ms step_avg:1107.34ms
step:1703/1775 n_predict=1 lr=0.1620 bs=393216 train_time:1886387ms step_avg:1107.68ms
step:1704/1775 n_predict=1 lr=0.1601 bs=393216 train_time:1888090ms step_avg:1108.03ms
step:1705/1775 n_predict=1 lr=0.1582 bs=393216 train_time:1889679ms step_avg:1108.32ms
step:1706/1775 n_predict=1 lr=0.1564 bs=393216 train_time:1891388ms step_avg:1108.67ms
step:1707/1775 n_predict=1 lr=0.1545 bs=393216 train_time:1892989ms step_avg:1108.96ms
step:1708/1775 n_predict=1 lr=0.1526 bs=393216 train_time:1894593ms step_avg:1109.25ms
step:1709/1775 n_predict=1 lr=0.1507 bs=393216 train_time:1896170ms step_avg:1109.52ms
step:1710/1775 n_predict=1 lr=0.1489 bs=393216 train_time:1897789ms step_avg:1109.82ms
step:1711/1775 n_predict=1 lr=0.1470 bs=393216 train_time:1899490ms step_avg:1110.16ms
step:1712/1775 n_predict=1 lr=0.1451 bs=393216 train_time:1901091ms step_avg:1110.45ms
step:1713/1775 n_predict=1 lr=0.1432 bs=393216 train_time:1902789ms step_avg:1110.79ms
step:1714/1775 n_predict=1 lr=0.1413 bs=393216 train_time:1904470ms step_avg:1111.13ms
step:1715/1775 n_predict=1 lr=0.1395 bs=393216 train_time:1906086ms step_avg:1111.42ms
step:1716/1775 n_predict=1 lr=0.1376 bs=393216 train_time:1907778ms step_avg:1111.76ms
step:1717/1775 n_predict=1 lr=0.1357 bs=393216 train_time:1909489ms step_avg:1112.11ms
step:1718/1775 n_predict=1 lr=0.1338 bs=393216 train_time:1911087ms step_avg:1112.39ms
step:1719/1775 n_predict=1 lr=0.1319 bs=393216 train_time:1912664ms step_avg:1112.66ms
step:1720/1775 n_predict=1 lr=0.1301 bs=393216 train_time:1914291ms step_avg:1112.96ms
step:1721/1775 n_predict=1 lr=0.1282 bs=393216 train_time:1915968ms step_avg:1113.29ms
step:1722/1775 n_predict=1 lr=0.1263 bs=393216 train_time:1917591ms step_avg:1113.58ms
step:1723/1775 n_predict=1 lr=0.1244 bs=393216 train_time:1919189ms step_avg:1113.86ms
step:1724/1775 n_predict=1 lr=0.1225 bs=393216 train_time:1920891ms step_avg:1114.21ms
step:1725/1775 n_predict=1 lr=0.1207 bs=393216 train_time:1922490ms step_avg:1114.49ms
step:1726/1775 n_predict=1 lr=0.1188 bs=393216 train_time:1924085ms step_avg:1114.77ms
step:1727/1775 n_predict=1 lr=0.1169 bs=393216 train_time:1925693ms step_avg:1115.05ms
step:1728/1775 n_predict=1 lr=0.1150 bs=393216 train_time:1927368ms step_avg:1115.38ms
step:1729/1775 n_predict=1 lr=0.1132 bs=393216 train_time:1929065ms step_avg:1115.71ms
step:1730/1775 n_predict=1 lr=0.1113 bs=393216 train_time:1930782ms step_avg:1116.06ms
step:1731/1775 n_predict=1 lr=0.1094 bs=393216 train_time:1932489ms step_avg:1116.40ms
step:1732/1775 n_predict=1 lr=0.1075 bs=393216 train_time:1934092ms step_avg:1116.68ms
step:1733/1775 n_predict=1 lr=0.1056 bs=393216 train_time:1935688ms step_avg:1116.96ms
step:1734/1775 n_predict=1 lr=0.1038 bs=393216 train_time:1937290ms step_avg:1117.24ms
step:1735/1775 n_predict=1 lr=0.1019 bs=393216 train_time:1938964ms step_avg:1117.56ms
step:1736/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2035868ms step_avg:1172.73ms
step:1737/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2037486ms step_avg:1172.99ms
step:1738/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2039090ms step_avg:1173.24ms
step:1739/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2040787ms step_avg:1173.54ms
step:1740/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2042485ms step_avg:1173.84ms
step:1741/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2044089ms step_avg:1174.09ms
step:1742/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2045690ms step_avg:1174.33ms
step:1743/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2047382ms step_avg:1174.63ms
step:1744/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2048994ms step_avg:1174.88ms
step:1745/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2050763ms step_avg:1175.22ms
step:1746/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2052451ms step_avg:1175.52ms
step:1747/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2054082ms step_avg:1175.78ms
step:1748/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2055678ms step_avg:1176.02ms
step:1749/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2057286ms step_avg:1176.26ms
step:1750/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2058897ms step_avg:1176.51ms
step:1750/1775 lr=0.1000 bs=393216 n_predict=1 val_loss:3.2910 val_malbo_loss:3.3707 train_time:2058974ms step_avg:1176.56ms
step:1751/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2060595ms step_avg:1176.81ms
step:1752/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2062302ms step_avg:1177.11ms
step:1753/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2063971ms step_avg:1177.39ms
step:1754/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2065598ms step_avg:1177.65ms
step:1755/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2067195ms step_avg:1177.89ms
step:1756/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2068968ms step_avg:1178.23ms
step:1757/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2070579ms step_avg:1178.47ms
step:1758/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2072196ms step_avg:1178.72ms
step:1759/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2073869ms step_avg:1179.00ms
step:1760/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2075558ms step_avg:1179.29ms
step:1761/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2077273ms step_avg:1179.60ms
step:1762/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2078870ms step_avg:1179.84ms
step:1763/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2080482ms step_avg:1180.08ms
step:1764/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2082200ms step_avg:1180.39ms
step:1765/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2083995ms step_avg:1180.73ms
step:1766/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2085669ms step_avg:1181.01ms
step:1767/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2087387ms step_avg:1181.32ms
step:1768/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2088998ms step_avg:1181.56ms
step:1769/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2090692ms step_avg:1181.85ms
step:1770/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2092376ms step_avg:1182.13ms
step:1771/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2093970ms step_avg:1182.37ms
step:1772/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2095587ms step_avg:1182.61ms
step:1773/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2097195ms step_avg:1182.85ms
step:1774/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2098890ms step_avg:1183.14ms
step:1775/1775 n_predict=1 lr=0.1000 bs=393216 train_time:2100492ms step_avg:1183.38ms
step:1775/1775 lr=0.1000 bs=393216 n_predict=1 val_loss:3.2846 val_malbo_loss:3.3644 train_time:2100577ms step_avg:1183.42ms
peak memory allocated: 30169 MiB reserved: 30618 MiB
