import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

from malbo import compute_malbo_parameters

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int, use_malbo: bool):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

        self.use_malbo = use_malbo

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0

            loss = (cross_entropy * mtp_weights).sum()
            if self.use_malbo:
                T, K = logits_flat.shape
                with torch.no_grad():
                    mask = torch.ones_like(cross_entropy)
                    for k in range(1, n_predict):  # zero out preds past end of sequence
                        mask[-k:, k] = 0
                    vhat, kappa, gamma = compute_malbo_parameters((-cross_entropy).float().exp().T, mask.T, K)
                    weights_transposed = kappa * gamma

                malbo_loss = T * (cross_entropy * weights_transposed.T * mtp_weights).sum()

                if not torch.isfinite(loss) or not torch.isfinite(malbo_loss):
                    raise RuntimeError(f"Non-finite loss (1): {loss} {malbo_loss}")
            else:
                malbo_loss = loss
        elif self.training:
            if self.use_malbo:
                logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
                T, K = logits_flat.shape
                cross_entropy = F.cross_entropy(logits_flat, target_seq, reduction="none")
                loss = cross_entropy.sum()

                with torch.no_grad():
                    mask = torch.ones_like(cross_entropy)
                    vhat, kappa, gamma = compute_malbo_parameters((-cross_entropy).float().exp().unsqueeze(0), mask.unsqueeze(0), K)
                    weights = (kappa * gamma).squeeze(0)

                malbo_loss = T * (weights * cross_entropy).sum()

                if not torch.isfinite(loss) or not torch.isfinite(malbo_loss):
                    raise RuntimeError(f"Non-finite loss (2): {loss} {malbo_loss}")
            else:
                loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
                malbo_loss = loss
        else:
            if self.use_malbo:
                K = logits_for_loss.size(-1)
                cross_entropy = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="none")
                loss = cross_entropy.mean()

                with torch.no_grad():
                    mask = torch.ones_like(cross_entropy)
                    vhat, kappa, gamma = compute_malbo_parameters((-cross_entropy).float().exp().unsqueeze(0), mask, K)
                    weights = (kappa * gamma).squeeze(0)

                malbo_loss = (weights * cross_entropy).sum()

                if not torch.isfinite(loss) or not torch.isfinite(malbo_loss):
                    raise RuntimeError(f"Non-finite loss (3): {loss} {malbo_loss}")
            else:
                loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
                malbo_loss = loss

        return loss, malbo_loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008*args.lr_fac, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023*args.lr_fac, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

    lr_fac: float = 1.0

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size),
    use_malbo=os.environ.get('use_malbo', 'True') == 'True',
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=False)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args())[1] / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
    break
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss, val_malbo_loss = 0, 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                this_val_loss, this_val_malbo_loss = model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
                val_loss += this_val_loss
                val_malbo_loss += this_val_malbo_loss
        val_loss /= val_steps
        val_malbo_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        dist.reduce(val_malbo_loss, 0, op=dist.ReduceOp.AVG)
        n_predict = training_manager.mtp_weights_schedule[step].size(0)
        lr = get_lr(step)
        bs = get_bs(step)
        print0(f"step:{step}/{train_steps} {lr=:.4f} {bs=} {n_predict=} val_loss:{val_loss:.4f} val_malbo_loss:{val_malbo_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args())[1] / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    n_predict = training_manager.mtp_weights_schedule[step].size(0)
    lr = get_lr(step)
    bs = get_bs(step)
    print0(f"step:{step+1}/{train_steps} {n_predict=} {lr=:.4f} {bs=} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.19 (main, Oct 21 2025, 16:43:05) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Tue Jan 20 03:26:49 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.195.03             Driver Version: 570.195.03     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:00:0F.0 Off |                    0 |
| N/A   30C    P0             73W /  310W |    1103MiB /  81559MiB |      4%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A           37739      C   .../envs/speedrun/bin/python3.10       1094MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 lr=1.0000 bs=131072 n_predict=3 val_loss:10.8307 val_malbo_loss:10.8208 train_time:0ms step_avg:0.04ms
step:1/1775 n_predict=3 lr=1.0000 bs=131072 train_time:540ms step_avg:539.92ms
step:2/1775 n_predict=3 lr=1.0000 bs=131072 train_time:1713ms step_avg:856.42ms
step:3/1775 n_predict=3 lr=1.0000 bs=131072 train_time:2112ms step_avg:703.91ms
step:4/1775 n_predict=3 lr=1.0000 bs=131072 train_time:2536ms step_avg:634.11ms
step:5/1775 n_predict=3 lr=1.0000 bs=131072 train_time:2962ms step_avg:592.33ms
step:6/1775 n_predict=3 lr=1.0000 bs=131072 train_time:3387ms step_avg:564.51ms
step:7/1775 n_predict=3 lr=1.0000 bs=131072 train_time:3818ms step_avg:545.37ms
step:8/1775 n_predict=3 lr=1.0000 bs=131072 train_time:4245ms step_avg:530.57ms
step:9/1775 n_predict=3 lr=1.0000 bs=131072 train_time:4668ms step_avg:518.68ms
step:10/1775 n_predict=3 lr=1.0000 bs=131072 train_time:5098ms step_avg:509.84ms
step:11/1775 n_predict=3 lr=1.0000 bs=131072 train_time:5525ms step_avg:502.23ms
step:12/1775 n_predict=3 lr=1.0000 bs=131072 train_time:5954ms step_avg:496.18ms
step:13/1775 n_predict=3 lr=1.0000 bs=131072 train_time:6382ms step_avg:490.92ms
step:14/1775 n_predict=3 lr=1.0000 bs=131072 train_time:6812ms step_avg:486.55ms
step:15/1775 n_predict=3 lr=1.0000 bs=131072 train_time:7238ms step_avg:482.55ms
step:16/1775 n_predict=3 lr=1.0000 bs=131072 train_time:7665ms step_avg:479.08ms
step:17/1775 n_predict=3 lr=1.0000 bs=131072 train_time:8094ms step_avg:476.11ms
step:18/1775 n_predict=3 lr=1.0000 bs=131072 train_time:8525ms step_avg:473.60ms
step:19/1775 n_predict=3 lr=1.0000 bs=131072 train_time:8951ms step_avg:471.12ms
step:20/1775 n_predict=3 lr=1.0000 bs=131072 train_time:9382ms step_avg:469.11ms
step:21/1775 n_predict=3 lr=1.0000 bs=131072 train_time:9814ms step_avg:467.32ms
step:22/1775 n_predict=3 lr=1.0000 bs=131072 train_time:10238ms step_avg:465.38ms
step:23/1775 n_predict=3 lr=1.0000 bs=131072 train_time:10670ms step_avg:463.92ms
step:24/1775 n_predict=3 lr=1.0000 bs=131072 train_time:11098ms step_avg:462.41ms
step:25/1775 n_predict=3 lr=1.0000 bs=131072 train_time:11527ms step_avg:461.07ms
step:26/1775 n_predict=3 lr=1.0000 bs=131072 train_time:11957ms step_avg:459.87ms
step:27/1775 n_predict=3 lr=1.0000 bs=131072 train_time:12387ms step_avg:458.79ms
step:28/1775 n_predict=3 lr=1.0000 bs=131072 train_time:12824ms step_avg:457.99ms
step:29/1775 n_predict=3 lr=1.0000 bs=131072 train_time:13246ms step_avg:456.76ms
step:30/1775 n_predict=3 lr=1.0000 bs=131072 train_time:13679ms step_avg:455.97ms
step:31/1775 n_predict=3 lr=1.0000 bs=131072 train_time:14115ms step_avg:455.31ms
step:32/1775 n_predict=3 lr=1.0000 bs=131072 train_time:14538ms step_avg:454.32ms
step:33/1775 n_predict=3 lr=1.0000 bs=131072 train_time:14970ms step_avg:453.63ms
step:34/1775 n_predict=3 lr=1.0000 bs=131072 train_time:15402ms step_avg:453.01ms
step:35/1775 n_predict=3 lr=1.0000 bs=131072 train_time:15833ms step_avg:452.36ms
step:36/1775 n_predict=3 lr=1.0000 bs=131072 train_time:16260ms step_avg:451.67ms
step:37/1775 n_predict=3 lr=1.0000 bs=131072 train_time:16688ms step_avg:451.03ms
step:38/1775 n_predict=3 lr=1.0000 bs=131072 train_time:17127ms step_avg:450.71ms
step:39/1775 n_predict=3 lr=1.0000 bs=131072 train_time:17549ms step_avg:449.97ms
step:40/1775 n_predict=3 lr=1.0000 bs=131072 train_time:17979ms step_avg:449.48ms
step:41/1775 n_predict=3 lr=1.0000 bs=131072 train_time:18409ms step_avg:449.01ms
step:42/1775 n_predict=3 lr=1.0000 bs=131072 train_time:18835ms step_avg:448.45ms
step:43/1775 n_predict=3 lr=1.0000 bs=131072 train_time:19263ms step_avg:447.98ms
step:44/1775 n_predict=3 lr=1.0000 bs=131072 train_time:19695ms step_avg:447.61ms
step:45/1775 n_predict=3 lr=1.0000 bs=131072 train_time:20123ms step_avg:447.17ms
step:46/1775 n_predict=3 lr=1.0000 bs=131072 train_time:20555ms step_avg:446.85ms
step:47/1775 n_predict=3 lr=1.0000 bs=131072 train_time:20985ms step_avg:446.49ms
step:48/1775 n_predict=3 lr=1.0000 bs=131072 train_time:21416ms step_avg:446.18ms
step:49/1775 n_predict=3 lr=1.0000 bs=131072 train_time:21838ms step_avg:445.68ms
step:50/1775 n_predict=3 lr=1.0000 bs=131072 train_time:22270ms step_avg:445.40ms
step:51/1775 n_predict=3 lr=1.0000 bs=131072 train_time:22697ms step_avg:445.04ms
step:52/1775 n_predict=3 lr=1.0000 bs=131072 train_time:23128ms step_avg:444.77ms
step:53/1775 n_predict=3 lr=1.0000 bs=131072 train_time:23555ms step_avg:444.44ms
step:54/1775 n_predict=3 lr=1.0000 bs=131072 train_time:23988ms step_avg:444.22ms
step:55/1775 n_predict=3 lr=1.0000 bs=131072 train_time:24413ms step_avg:443.86ms
step:56/1775 n_predict=3 lr=1.0000 bs=131072 train_time:24847ms step_avg:443.70ms
step:57/1775 n_predict=3 lr=1.0000 bs=131072 train_time:25270ms step_avg:443.33ms
step:58/1775 n_predict=3 lr=1.0000 bs=131072 train_time:25701ms step_avg:443.12ms
step:59/1775 n_predict=3 lr=1.0000 bs=131072 train_time:26126ms step_avg:442.82ms
step:60/1775 n_predict=3 lr=1.0000 bs=131072 train_time:26555ms step_avg:442.58ms
step:61/1775 n_predict=3 lr=1.0000 bs=131072 train_time:26982ms step_avg:442.33ms
step:62/1775 n_predict=3 lr=1.0000 bs=131072 train_time:27409ms step_avg:442.09ms
step:63/1775 n_predict=3 lr=1.0000 bs=131072 train_time:27836ms step_avg:441.84ms
step:64/1775 n_predict=3 lr=1.0000 bs=131072 train_time:28266ms step_avg:441.66ms
step:65/1775 n_predict=3 lr=1.0000 bs=131072 train_time:28696ms step_avg:441.48ms
step:66/1775 n_predict=3 lr=1.0000 bs=131072 train_time:29125ms step_avg:441.29ms
step:67/1775 n_predict=3 lr=1.0000 bs=131072 train_time:29550ms step_avg:441.05ms
step:68/1775 n_predict=3 lr=1.0000 bs=131072 train_time:29978ms step_avg:440.86ms
step:69/1775 n_predict=3 lr=1.0000 bs=131072 train_time:30411ms step_avg:440.74ms
step:70/1775 n_predict=3 lr=1.0000 bs=131072 train_time:30840ms step_avg:440.57ms
step:71/1775 n_predict=3 lr=1.0000 bs=131072 train_time:31270ms step_avg:440.42ms
step:72/1775 n_predict=3 lr=1.0000 bs=131072 train_time:31701ms step_avg:440.30ms
step:73/1775 n_predict=3 lr=1.0000 bs=131072 train_time:32130ms step_avg:440.14ms
step:74/1775 n_predict=3 lr=1.0000 bs=131072 train_time:32562ms step_avg:440.02ms
step:75/1775 n_predict=3 lr=1.0000 bs=131072 train_time:32990ms step_avg:439.87ms
step:76/1775 n_predict=3 lr=1.0000 bs=131072 train_time:33425ms step_avg:439.81ms
step:77/1775 n_predict=3 lr=1.0000 bs=131072 train_time:33853ms step_avg:439.64ms
step:78/1775 n_predict=3 lr=1.0000 bs=131072 train_time:34279ms step_avg:439.47ms
step:79/1775 n_predict=3 lr=1.0000 bs=131072 train_time:34712ms step_avg:439.40ms
step:80/1775 n_predict=3 lr=1.0000 bs=131072 train_time:35145ms step_avg:439.31ms
step:81/1775 n_predict=3 lr=1.0000 bs=131072 train_time:35569ms step_avg:439.12ms
step:82/1775 n_predict=3 lr=1.0000 bs=131072 train_time:36002ms step_avg:439.05ms
step:83/1775 n_predict=3 lr=1.0000 bs=131072 train_time:36429ms step_avg:438.90ms
step:84/1775 n_predict=3 lr=1.0000 bs=131072 train_time:36861ms step_avg:438.82ms
step:85/1775 n_predict=3 lr=1.0000 bs=131072 train_time:37290ms step_avg:438.70ms
step:86/1775 n_predict=3 lr=1.0000 bs=131072 train_time:37720ms step_avg:438.60ms
step:87/1775 n_predict=3 lr=1.0000 bs=131072 train_time:38148ms step_avg:438.48ms
step:88/1775 n_predict=3 lr=1.0000 bs=131072 train_time:38578ms step_avg:438.38ms
step:89/1775 n_predict=3 lr=1.0000 bs=131072 train_time:39006ms step_avg:438.27ms
step:90/1775 n_predict=3 lr=1.0000 bs=131072 train_time:39436ms step_avg:438.18ms
step:91/1775 n_predict=3 lr=1.0000 bs=131072 train_time:39865ms step_avg:438.08ms
step:92/1775 n_predict=3 lr=1.0000 bs=131072 train_time:40295ms step_avg:437.99ms
step:93/1775 n_predict=3 lr=1.0000 bs=131072 train_time:40727ms step_avg:437.93ms
step:94/1775 n_predict=3 lr=1.0000 bs=131072 train_time:41155ms step_avg:437.82ms
step:95/1775 n_predict=3 lr=1.0000 bs=131072 train_time:41584ms step_avg:437.73ms
step:96/1775 n_predict=3 lr=1.0000 bs=131072 train_time:42014ms step_avg:437.64ms
step:97/1775 n_predict=3 lr=1.0000 bs=131072 train_time:42444ms step_avg:437.57ms
step:98/1775 n_predict=3 lr=1.0000 bs=131072 train_time:42875ms step_avg:437.50ms
step:99/1775 n_predict=3 lr=1.0000 bs=131072 train_time:43305ms step_avg:437.42ms
step:100/1775 n_predict=3 lr=1.0000 bs=131072 train_time:43738ms step_avg:437.38ms
step:101/1775 n_predict=3 lr=1.0000 bs=131072 train_time:44166ms step_avg:437.29ms
step:102/1775 n_predict=3 lr=1.0000 bs=131072 train_time:44598ms step_avg:437.23ms
step:103/1775 n_predict=3 lr=1.0000 bs=131072 train_time:45028ms step_avg:437.16ms
step:104/1775 n_predict=3 lr=1.0000 bs=131072 train_time:45460ms step_avg:437.12ms
step:105/1775 n_predict=3 lr=1.0000 bs=131072 train_time:45891ms step_avg:437.05ms
step:106/1775 n_predict=3 lr=1.0000 bs=131072 train_time:46320ms step_avg:436.98ms
step:107/1775 n_predict=3 lr=1.0000 bs=131072 train_time:46749ms step_avg:436.91ms
step:108/1775 n_predict=3 lr=1.0000 bs=131072 train_time:47180ms step_avg:436.85ms
step:109/1775 n_predict=3 lr=1.0000 bs=131072 train_time:47609ms step_avg:436.78ms
step:110/1775 n_predict=3 lr=1.0000 bs=131072 train_time:48041ms step_avg:436.74ms
step:111/1775 n_predict=3 lr=1.0000 bs=131072 train_time:48470ms step_avg:436.67ms
step:112/1775 n_predict=3 lr=1.0000 bs=131072 train_time:48903ms step_avg:436.63ms
step:113/1775 n_predict=3 lr=1.0000 bs=131072 train_time:49333ms step_avg:436.57ms
step:114/1775 n_predict=3 lr=1.0000 bs=131072 train_time:49763ms step_avg:436.52ms
step:115/1775 n_predict=3 lr=1.0000 bs=131072 train_time:50194ms step_avg:436.47ms
step:116/1775 n_predict=3 lr=1.0000 bs=131072 train_time:50633ms step_avg:436.49ms
step:117/1775 n_predict=3 lr=1.0000 bs=131072 train_time:51060ms step_avg:436.41ms
step:118/1775 n_predict=3 lr=1.0000 bs=131072 train_time:51490ms step_avg:436.36ms
step:119/1775 n_predict=3 lr=1.0000 bs=131072 train_time:51920ms step_avg:436.31ms
step:120/1775 n_predict=3 lr=1.0000 bs=131072 train_time:52354ms step_avg:436.28ms
step:121/1775 n_predict=3 lr=1.0000 bs=131072 train_time:52783ms step_avg:436.22ms
step:122/1775 n_predict=3 lr=1.0000 bs=131072 train_time:53211ms step_avg:436.15ms
step:123/1775 n_predict=3 lr=1.0000 bs=131072 train_time:53641ms step_avg:436.11ms
step:124/1775 n_predict=3 lr=1.0000 bs=131072 train_time:54074ms step_avg:436.08ms
step:125/1775 n_predict=3 lr=1.0000 bs=131072 train_time:54503ms step_avg:436.03ms
step:126/1775 n_predict=3 lr=1.0000 bs=131072 train_time:54936ms step_avg:436.00ms
step:127/1775 n_predict=3 lr=1.0000 bs=131072 train_time:55365ms step_avg:435.94ms
step:128/1775 n_predict=3 lr=1.0000 bs=131072 train_time:55796ms step_avg:435.91ms
step:129/1775 n_predict=3 lr=1.0000 bs=131072 train_time:56226ms step_avg:435.86ms
step:130/1775 n_predict=3 lr=1.0000 bs=131072 train_time:56658ms step_avg:435.83ms
step:131/1775 n_predict=3 lr=1.0000 bs=131072 train_time:57089ms step_avg:435.79ms
step:132/1775 n_predict=3 lr=1.0000 bs=131072 train_time:57520ms step_avg:435.76ms
step:133/1775 n_predict=3 lr=1.0000 bs=131072 train_time:57953ms step_avg:435.74ms
step:134/1775 n_predict=3 lr=1.0000 bs=131072 train_time:58383ms step_avg:435.70ms
step:135/1775 n_predict=3 lr=1.0000 bs=131072 train_time:58811ms step_avg:435.64ms
step:136/1775 n_predict=3 lr=1.0000 bs=131072 train_time:59244ms step_avg:435.62ms
step:137/1775 n_predict=3 lr=1.0000 bs=131072 train_time:59673ms step_avg:435.57ms
step:138/1775 n_predict=3 lr=1.0000 bs=131072 train_time:60107ms step_avg:435.55ms
step:139/1775 n_predict=3 lr=1.0000 bs=131072 train_time:60537ms step_avg:435.52ms
step:140/1775 n_predict=3 lr=1.0000 bs=131072 train_time:60969ms step_avg:435.49ms
step:141/1775 n_predict=3 lr=1.0000 bs=131072 train_time:61399ms step_avg:435.45ms
step:142/1775 n_predict=3 lr=1.0000 bs=131072 train_time:61833ms step_avg:435.44ms
step:143/1775 n_predict=3 lr=1.0000 bs=131072 train_time:62259ms step_avg:435.37ms
step:144/1775 n_predict=3 lr=1.0000 bs=131072 train_time:62691ms step_avg:435.35ms
step:145/1775 n_predict=3 lr=1.0000 bs=131072 train_time:63122ms step_avg:435.32ms
step:146/1775 n_predict=3 lr=1.0000 bs=131072 train_time:63553ms step_avg:435.29ms
step:147/1775 n_predict=3 lr=1.0000 bs=131072 train_time:63982ms step_avg:435.25ms
step:148/1775 n_predict=3 lr=1.0000 bs=131072 train_time:64412ms step_avg:435.22ms
step:149/1775 n_predict=3 lr=1.0000 bs=131072 train_time:64843ms step_avg:435.19ms
step:150/1775 n_predict=3 lr=1.0000 bs=131072 train_time:65275ms step_avg:435.17ms
step:151/1775 n_predict=3 lr=1.0000 bs=131072 train_time:65704ms step_avg:435.12ms
step:152/1775 n_predict=3 lr=1.0000 bs=131072 train_time:66140ms step_avg:435.13ms
step:153/1775 n_predict=3 lr=1.0000 bs=131072 train_time:66566ms step_avg:435.07ms
step:154/1775 n_predict=3 lr=1.0000 bs=131072 train_time:67001ms step_avg:435.07ms
step:155/1775 n_predict=3 lr=1.0000 bs=131072 train_time:67430ms step_avg:435.04ms
step:156/1775 n_predict=3 lr=1.0000 bs=131072 train_time:67865ms step_avg:435.03ms
step:157/1775 n_predict=3 lr=1.0000 bs=131072 train_time:68298ms step_avg:435.02ms
step:158/1775 n_predict=3 lr=1.0000 bs=131072 train_time:68731ms step_avg:435.00ms
step:159/1775 n_predict=3 lr=1.0000 bs=131072 train_time:69161ms step_avg:434.98ms
step:160/1775 n_predict=3 lr=1.0000 bs=131072 train_time:69593ms step_avg:434.96ms
step:161/1775 n_predict=3 lr=1.0000 bs=131072 train_time:70025ms step_avg:434.94ms
step:162/1775 n_predict=3 lr=1.0000 bs=131072 train_time:70456ms step_avg:434.91ms
step:163/1775 n_predict=3 lr=1.0000 bs=131072 train_time:70887ms step_avg:434.89ms
step:164/1775 n_predict=3 lr=1.0000 bs=131072 train_time:71320ms step_avg:434.88ms
step:165/1775 n_predict=3 lr=1.0000 bs=131072 train_time:71750ms step_avg:434.85ms
step:166/1775 n_predict=3 lr=1.0000 bs=131072 train_time:72182ms step_avg:434.83ms
step:167/1775 n_predict=3 lr=1.0000 bs=131072 train_time:72610ms step_avg:434.79ms
step:168/1775 n_predict=3 lr=1.0000 bs=131072 train_time:73045ms step_avg:434.79ms
step:169/1775 n_predict=3 lr=1.0000 bs=131072 train_time:73477ms step_avg:434.78ms
step:170/1775 n_predict=3 lr=1.0000 bs=131072 train_time:73908ms step_avg:434.75ms
step:171/1775 n_predict=3 lr=1.0000 bs=131072 train_time:74339ms step_avg:434.73ms
step:172/1775 n_predict=3 lr=1.0000 bs=131072 train_time:74771ms step_avg:434.71ms
step:173/1775 n_predict=3 lr=1.0000 bs=131072 train_time:75201ms step_avg:434.69ms
step:174/1775 n_predict=3 lr=1.0000 bs=131072 train_time:75633ms step_avg:434.67ms
step:175/1775 n_predict=3 lr=1.0000 bs=131072 train_time:76062ms step_avg:434.64ms
step:176/1775 n_predict=3 lr=1.0000 bs=131072 train_time:76495ms step_avg:434.63ms
step:177/1775 n_predict=3 lr=1.0000 bs=131072 train_time:76927ms step_avg:434.62ms
step:178/1775 n_predict=3 lr=1.0000 bs=131072 train_time:77357ms step_avg:434.59ms
step:179/1775 n_predict=3 lr=1.0000 bs=131072 train_time:77789ms step_avg:434.58ms
step:180/1775 n_predict=3 lr=1.0000 bs=131072 train_time:78222ms step_avg:434.57ms
step:181/1775 n_predict=3 lr=1.0000 bs=131072 train_time:78651ms step_avg:434.53ms
step:182/1775 n_predict=3 lr=1.0000 bs=131072 train_time:79083ms step_avg:434.52ms
step:183/1775 n_predict=3 lr=1.0000 bs=131072 train_time:79513ms step_avg:434.50ms
step:184/1775 n_predict=3 lr=1.0000 bs=131072 train_time:79944ms step_avg:434.48ms
step:185/1775 n_predict=3 lr=1.0000 bs=131072 train_time:80376ms step_avg:434.46ms
step:186/1775 n_predict=3 lr=1.0000 bs=131072 train_time:80807ms step_avg:434.44ms
step:187/1775 n_predict=3 lr=1.0000 bs=131072 train_time:81237ms step_avg:434.42ms
step:188/1775 n_predict=3 lr=1.0000 bs=131072 train_time:81670ms step_avg:434.42ms
step:189/1775 n_predict=3 lr=1.0000 bs=131072 train_time:82099ms step_avg:434.39ms
step:190/1775 n_predict=3 lr=1.0000 bs=131072 train_time:82531ms step_avg:434.37ms
step:191/1775 n_predict=3 lr=1.0000 bs=131072 train_time:82960ms step_avg:434.35ms
step:192/1775 n_predict=3 lr=1.0000 bs=131072 train_time:83393ms step_avg:434.34ms
step:193/1775 n_predict=3 lr=1.0000 bs=131072 train_time:83827ms step_avg:434.34ms
step:194/1775 n_predict=3 lr=1.0000 bs=131072 train_time:84255ms step_avg:434.31ms
step:195/1775 n_predict=3 lr=1.0000 bs=131072 train_time:84685ms step_avg:434.28ms
step:196/1775 n_predict=3 lr=1.0000 bs=131072 train_time:85114ms step_avg:434.25ms
step:197/1775 n_predict=3 lr=1.0000 bs=131072 train_time:85546ms step_avg:434.24ms
step:198/1775 n_predict=3 lr=1.0000 bs=131072 train_time:85978ms step_avg:434.23ms
step:199/1775 n_predict=3 lr=1.0000 bs=131072 train_time:86407ms step_avg:434.21ms
step:200/1775 n_predict=3 lr=1.0000 bs=131072 train_time:86841ms step_avg:434.20ms
step:201/1775 n_predict=3 lr=1.0000 bs=131072 train_time:87270ms step_avg:434.18ms
step:202/1775 n_predict=3 lr=1.0000 bs=131072 train_time:87702ms step_avg:434.17ms
step:203/1775 n_predict=3 lr=1.0000 bs=131072 train_time:88130ms step_avg:434.14ms
step:204/1775 n_predict=3 lr=1.0000 bs=131072 train_time:88561ms step_avg:434.12ms
step:205/1775 n_predict=3 lr=1.0000 bs=131072 train_time:88991ms step_avg:434.10ms
step:206/1775 n_predict=3 lr=1.0000 bs=131072 train_time:89421ms step_avg:434.08ms
step:207/1775 n_predict=3 lr=1.0000 bs=131072 train_time:89851ms step_avg:434.06ms
step:208/1775 n_predict=3 lr=1.0000 bs=131072 train_time:90281ms step_avg:434.04ms
step:209/1775 n_predict=3 lr=1.0000 bs=131072 train_time:90710ms step_avg:434.02ms
step:210/1775 n_predict=3 lr=1.0000 bs=131072 train_time:91143ms step_avg:434.01ms
step:211/1775 n_predict=3 lr=1.0000 bs=131072 train_time:91571ms step_avg:433.99ms
step:212/1775 n_predict=3 lr=1.0000 bs=131072 train_time:92004ms step_avg:433.98ms
step:213/1775 n_predict=3 lr=1.0000 bs=131072 train_time:92433ms step_avg:433.96ms
step:214/1775 n_predict=3 lr=1.0000 bs=131072 train_time:92865ms step_avg:433.95ms
step:215/1775 n_predict=3 lr=1.0000 bs=131072 train_time:93294ms step_avg:433.92ms
step:216/1775 n_predict=3 lr=1.0000 bs=131072 train_time:93727ms step_avg:433.92ms
step:217/1775 n_predict=3 lr=1.0000 bs=131072 train_time:94155ms step_avg:433.90ms
step:218/1775 n_predict=3 lr=1.0000 bs=131072 train_time:94587ms step_avg:433.88ms
step:219/1775 n_predict=3 lr=1.0000 bs=131072 train_time:95014ms step_avg:433.85ms
step:220/1775 n_predict=3 lr=1.0000 bs=131072 train_time:95444ms step_avg:433.84ms
step:221/1775 n_predict=3 lr=1.0000 bs=131072 train_time:95877ms step_avg:433.83ms
step:222/1775 n_predict=3 lr=1.0000 bs=131072 train_time:96305ms step_avg:433.80ms
step:223/1775 n_predict=3 lr=1.0000 bs=131072 train_time:96733ms step_avg:433.78ms
step:224/1775 n_predict=3 lr=1.0000 bs=131072 train_time:97163ms step_avg:433.76ms
step:225/1775 n_predict=3 lr=1.0000 bs=131072 train_time:97591ms step_avg:433.74ms
step:226/1775 n_predict=3 lr=1.0000 bs=131072 train_time:98023ms step_avg:433.73ms
step:227/1775 n_predict=3 lr=1.0000 bs=131072 train_time:98450ms step_avg:433.70ms
step:228/1775 n_predict=3 lr=1.0000 bs=131072 train_time:98883ms step_avg:433.70ms
step:229/1775 n_predict=3 lr=1.0000 bs=131072 train_time:99311ms step_avg:433.67ms
step:230/1775 n_predict=3 lr=1.0000 bs=131072 train_time:99743ms step_avg:433.66ms
step:231/1775 n_predict=3 lr=1.0000 bs=131072 train_time:100175ms step_avg:433.66ms
step:232/1775 n_predict=3 lr=1.0000 bs=131072 train_time:100605ms step_avg:433.64ms
step:233/1775 n_predict=3 lr=1.0000 bs=131072 train_time:101036ms step_avg:433.63ms
step:234/1775 n_predict=3 lr=1.0000 bs=131072 train_time:101472ms step_avg:433.64ms
step:235/1775 n_predict=3 lr=1.0000 bs=131072 train_time:101898ms step_avg:433.61ms
step:236/1775 n_predict=3 lr=1.0000 bs=131072 train_time:102330ms step_avg:433.60ms
step:237/1775 n_predict=3 lr=1.0000 bs=131072 train_time:102757ms step_avg:433.57ms
step:238/1775 n_predict=3 lr=1.0000 bs=131072 train_time:103190ms step_avg:433.57ms
step:239/1775 n_predict=3 lr=1.0000 bs=131072 train_time:103620ms step_avg:433.56ms
step:240/1775 n_predict=3 lr=1.0000 bs=131072 train_time:104052ms step_avg:433.55ms
step:241/1775 n_predict=3 lr=1.0000 bs=131072 train_time:104483ms step_avg:433.54ms
step:242/1775 n_predict=3 lr=1.0000 bs=131072 train_time:104913ms step_avg:433.52ms
step:243/1775 n_predict=3 lr=1.0000 bs=131072 train_time:105344ms step_avg:433.51ms
step:244/1775 n_predict=3 lr=1.0000 bs=131072 train_time:105776ms step_avg:433.51ms
step:245/1775 n_predict=3 lr=1.0000 bs=131072 train_time:106206ms step_avg:433.49ms
step:246/1775 n_predict=3 lr=1.0000 bs=131072 train_time:106639ms step_avg:433.49ms
step:247/1775 n_predict=3 lr=1.0000 bs=131072 train_time:107068ms step_avg:433.48ms
step:248/1775 n_predict=3 lr=1.0000 bs=131072 train_time:107500ms step_avg:433.47ms
step:249/1775 n_predict=3 lr=1.0000 bs=131072 train_time:107930ms step_avg:433.45ms
step:250/1775 n_predict=3 lr=1.0000 bs=131072 train_time:108361ms step_avg:433.44ms
step:250/1775 lr=1.0000 bs=131072 n_predict=3 val_loss:4.6139 val_malbo_loss:4.6372 train_time:108390ms step_avg:433.56ms
step:251/1775 n_predict=3 lr=1.0000 bs=131072 train_time:108792ms step_avg:433.43ms
step:252/1775 n_predict=3 lr=1.0000 bs=131072 train_time:109227ms step_avg:433.44ms
step:253/1775 n_predict=3 lr=1.0000 bs=131072 train_time:109652ms step_avg:433.41ms
step:254/1775 n_predict=3 lr=1.0000 bs=131072 train_time:110080ms step_avg:433.39ms
step:255/1775 n_predict=3 lr=1.0000 bs=131072 train_time:110509ms step_avg:433.37ms
step:256/1775 n_predict=3 lr=1.0000 bs=131072 train_time:110939ms step_avg:433.36ms
step:257/1775 n_predict=3 lr=1.0000 bs=131072 train_time:111372ms step_avg:433.35ms
step:258/1775 n_predict=3 lr=1.0000 bs=131072 train_time:111800ms step_avg:433.33ms
step:259/1775 n_predict=3 lr=1.0000 bs=131072 train_time:112229ms step_avg:433.32ms
step:260/1775 n_predict=3 lr=1.0000 bs=131072 train_time:112658ms step_avg:433.30ms
step:261/1775 n_predict=3 lr=1.0000 bs=131072 train_time:113088ms step_avg:433.29ms
step:262/1775 n_predict=3 lr=1.0000 bs=131072 train_time:113521ms step_avg:433.28ms
step:263/1775 n_predict=3 lr=1.0000 bs=131072 train_time:113947ms step_avg:433.26ms
step:264/1775 n_predict=3 lr=1.0000 bs=131072 train_time:114377ms step_avg:433.25ms
step:265/1775 n_predict=3 lr=1.0000 bs=131072 train_time:114804ms step_avg:433.22ms
step:266/1775 n_predict=3 lr=1.0000 bs=131072 train_time:115237ms step_avg:433.22ms
step:267/1775 n_predict=3 lr=1.0000 bs=131072 train_time:115666ms step_avg:433.20ms
step:268/1775 n_predict=3 lr=1.0000 bs=131072 train_time:116098ms step_avg:433.20ms
step:269/1775 n_predict=3 lr=1.0000 bs=131072 train_time:116525ms step_avg:433.18ms
step:270/1775 n_predict=3 lr=1.0000 bs=131072 train_time:116955ms step_avg:433.17ms
step:271/1775 n_predict=3 lr=1.0000 bs=131072 train_time:117383ms step_avg:433.15ms
step:272/1775 n_predict=3 lr=1.0000 bs=131072 train_time:117814ms step_avg:433.14ms
step:273/1775 n_predict=3 lr=1.0000 bs=131072 train_time:118243ms step_avg:433.12ms
step:274/1775 n_predict=3 lr=1.0000 bs=131072 train_time:118675ms step_avg:433.12ms
step:275/1775 n_predict=3 lr=1.0000 bs=131072 train_time:119102ms step_avg:433.10ms
step:276/1775 n_predict=3 lr=1.0000 bs=131072 train_time:119535ms step_avg:433.10ms
step:277/1775 n_predict=3 lr=1.0000 bs=131072 train_time:119965ms step_avg:433.09ms
step:278/1775 n_predict=3 lr=1.0000 bs=131072 train_time:120395ms step_avg:433.08ms
step:279/1775 n_predict=3 lr=1.0000 bs=131072 train_time:120823ms step_avg:433.06ms
step:280/1775 n_predict=3 lr=1.0000 bs=131072 train_time:121256ms step_avg:433.06ms
step:281/1775 n_predict=3 lr=1.0000 bs=131072 train_time:121684ms step_avg:433.04ms
step:282/1775 n_predict=3 lr=1.0000 bs=131072 train_time:122116ms step_avg:433.04ms
step:283/1775 n_predict=3 lr=1.0000 bs=131072 train_time:122545ms step_avg:433.02ms
step:284/1775 n_predict=3 lr=1.0000 bs=131072 train_time:122977ms step_avg:433.02ms
step:285/1775 n_predict=3 lr=1.0000 bs=131072 train_time:123405ms step_avg:433.00ms
step:286/1775 n_predict=3 lr=1.0000 bs=131072 train_time:123838ms step_avg:433.00ms
step:287/1775 n_predict=3 lr=1.0000 bs=131072 train_time:124271ms step_avg:433.00ms
step:288/1775 n_predict=3 lr=1.0000 bs=131072 train_time:124699ms step_avg:432.98ms
step:289/1775 n_predict=3 lr=1.0000 bs=131072 train_time:125129ms step_avg:432.97ms
step:290/1775 n_predict=3 lr=1.0000 bs=131072 train_time:125560ms step_avg:432.97ms
step:291/1775 n_predict=3 lr=1.0000 bs=131072 train_time:125988ms step_avg:432.95ms
step:292/1775 n_predict=3 lr=1.0000 bs=131072 train_time:126422ms step_avg:432.95ms
step:293/1775 n_predict=3 lr=1.0000 bs=131072 train_time:126850ms step_avg:432.94ms
step:294/1775 n_predict=3 lr=1.0000 bs=131072 train_time:127280ms step_avg:432.93ms
step:295/1775 n_predict=3 lr=1.0000 bs=131072 train_time:127709ms step_avg:432.91ms
step:296/1775 n_predict=3 lr=1.0000 bs=131072 train_time:128141ms step_avg:432.91ms
step:297/1775 n_predict=3 lr=1.0000 bs=131072 train_time:128572ms step_avg:432.90ms
step:298/1775 n_predict=3 lr=1.0000 bs=131072 train_time:129001ms step_avg:432.89ms
step:299/1775 n_predict=3 lr=1.0000 bs=131072 train_time:129431ms step_avg:432.88ms
step:300/1775 n_predict=3 lr=1.0000 bs=131072 train_time:129862ms step_avg:432.87ms
step:301/1775 n_predict=3 lr=1.0000 bs=131072 train_time:130293ms step_avg:432.87ms
step:302/1775 n_predict=3 lr=1.0000 bs=131072 train_time:130725ms step_avg:432.86ms
step:303/1775 n_predict=3 lr=1.0000 bs=131072 train_time:131152ms step_avg:432.84ms
step:304/1775 n_predict=3 lr=1.0000 bs=131072 train_time:131582ms step_avg:432.84ms
step:305/1775 n_predict=3 lr=1.0000 bs=131072 train_time:132011ms step_avg:432.82ms
step:306/1775 n_predict=3 lr=1.0000 bs=131072 train_time:132444ms step_avg:432.82ms
step:307/1775 n_predict=3 lr=1.0000 bs=131072 train_time:132874ms step_avg:432.81ms
step:308/1775 n_predict=3 lr=1.0000 bs=131072 train_time:133304ms step_avg:432.80ms
step:309/1775 n_predict=3 lr=1.0000 bs=131072 train_time:133734ms step_avg:432.80ms
step:310/1775 n_predict=3 lr=1.0000 bs=131072 train_time:134164ms step_avg:432.79ms
step:311/1775 n_predict=3 lr=1.0000 bs=131072 train_time:134593ms step_avg:432.77ms
step:312/1775 n_predict=3 lr=1.0000 bs=131072 train_time:135025ms step_avg:432.77ms
step:313/1775 n_predict=3 lr=1.0000 bs=131072 train_time:135454ms step_avg:432.76ms
step:314/1775 n_predict=3 lr=1.0000 bs=131072 train_time:135886ms step_avg:432.76ms
step:315/1775 n_predict=3 lr=1.0000 bs=131072 train_time:136316ms step_avg:432.75ms
step:316/1775 n_predict=3 lr=1.0000 bs=131072 train_time:136746ms step_avg:432.74ms
step:317/1775 n_predict=3 lr=1.0000 bs=131072 train_time:137173ms step_avg:432.72ms
step:318/1775 n_predict=3 lr=1.0000 bs=131072 train_time:137600ms step_avg:432.70ms
step:319/1775 n_predict=3 lr=1.0000 bs=131072 train_time:138029ms step_avg:432.69ms
step:320/1775 n_predict=3 lr=1.0000 bs=131072 train_time:138455ms step_avg:432.67ms
step:321/1775 n_predict=3 lr=1.0000 bs=131072 train_time:138883ms step_avg:432.66ms
step:322/1775 n_predict=3 lr=1.0000 bs=131072 train_time:139311ms step_avg:432.64ms
step:323/1775 n_predict=3 lr=1.0000 bs=131072 train_time:139739ms step_avg:432.63ms
step:324/1775 n_predict=3 lr=1.0000 bs=131072 train_time:140169ms step_avg:432.62ms
step:325/1775 n_predict=3 lr=1.0000 bs=131072 train_time:140596ms step_avg:432.60ms
step:326/1775 n_predict=3 lr=1.0000 bs=131072 train_time:141024ms step_avg:432.59ms
step:327/1775 n_predict=3 lr=1.0000 bs=131072 train_time:141453ms step_avg:432.58ms
step:328/1775 n_predict=3 lr=1.0000 bs=131072 train_time:141881ms step_avg:432.56ms
step:329/1775 n_predict=3 lr=1.0000 bs=131072 train_time:142307ms step_avg:432.54ms
step:330/1775 n_predict=3 lr=1.0000 bs=131072 train_time:142736ms step_avg:432.53ms
step:331/1775 n_predict=3 lr=1.0000 bs=131072 train_time:143165ms step_avg:432.52ms
step:332/1775 n_predict=3 lr=1.0000 bs=131072 train_time:143601ms step_avg:432.53ms
step:333/1775 n_predict=3 lr=1.0000 bs=131072 train_time:144024ms step_avg:432.50ms
step:334/1775 n_predict=3 lr=1.0000 bs=131072 train_time:144454ms step_avg:432.50ms
step:335/1775 n_predict=3 lr=1.0000 bs=131072 train_time:144884ms step_avg:432.49ms
step:336/1775 n_predict=3 lr=1.0000 bs=131072 train_time:145315ms step_avg:432.48ms
step:337/1775 n_predict=3 lr=1.0000 bs=131072 train_time:145744ms step_avg:432.47ms
step:338/1775 n_predict=3 lr=1.0000 bs=131072 train_time:146176ms step_avg:432.47ms
step:339/1775 n_predict=3 lr=1.0000 bs=131072 train_time:146604ms step_avg:432.46ms
step:340/1775 n_predict=3 lr=1.0000 bs=131072 train_time:147036ms step_avg:432.46ms
step:341/1775 n_predict=3 lr=1.0000 bs=131072 train_time:147466ms step_avg:432.45ms
step:342/1775 n_predict=3 lr=1.0000 bs=131072 train_time:147897ms step_avg:432.45ms
step:343/1775 n_predict=3 lr=1.0000 bs=131072 train_time:148325ms step_avg:432.44ms
step:344/1775 n_predict=3 lr=1.0000 bs=131072 train_time:148756ms step_avg:432.43ms
step:345/1775 n_predict=3 lr=1.0000 bs=131072 train_time:149187ms step_avg:432.43ms
step:346/1775 n_predict=3 lr=1.0000 bs=131072 train_time:149619ms step_avg:432.43ms
step:347/1775 n_predict=3 lr=1.0000 bs=131072 train_time:150050ms step_avg:432.42ms
step:348/1775 n_predict=3 lr=1.0000 bs=131072 train_time:150480ms step_avg:432.41ms
step:349/1775 n_predict=3 lr=1.0000 bs=131072 train_time:150911ms step_avg:432.41ms
step:350/1775 n_predict=3 lr=1.0000 bs=131072 train_time:151344ms step_avg:432.41ms
step:351/1775 n_predict=3 lr=1.0000 bs=131072 train_time:151773ms step_avg:432.40ms
step:352/1775 n_predict=3 lr=1.0000 bs=131072 train_time:152204ms step_avg:432.40ms
step:353/1775 n_predict=3 lr=1.0000 bs=131072 train_time:152635ms step_avg:432.39ms
step:354/1775 n_predict=3 lr=1.0000 bs=131072 train_time:153066ms step_avg:432.39ms
step:355/1775 n_predict=3 lr=1.0000 bs=131072 train_time:153497ms step_avg:432.39ms
step:356/1775 n_predict=3 lr=1.0000 bs=131072 train_time:153929ms step_avg:432.38ms
step:357/1775 n_predict=3 lr=1.0000 bs=131072 train_time:154358ms step_avg:432.37ms
step:358/1775 n_predict=3 lr=1.0000 bs=131072 train_time:154791ms step_avg:432.38ms
step:359/1775 n_predict=3 lr=1.0000 bs=131072 train_time:155221ms step_avg:432.37ms
step:360/1775 n_predict=3 lr=1.0000 bs=131072 train_time:155654ms step_avg:432.37ms
step:361/1775 n_predict=3 lr=1.0000 bs=131072 train_time:156085ms step_avg:432.37ms
step:362/1775 n_predict=3 lr=1.0000 bs=131072 train_time:156516ms step_avg:432.36ms
step:363/1775 n_predict=3 lr=1.0000 bs=131072 train_time:156945ms step_avg:432.36ms
step:364/1775 n_predict=3 lr=1.0000 bs=131072 train_time:157376ms step_avg:432.35ms
step:365/1775 n_predict=3 lr=1.0000 bs=131072 train_time:157804ms step_avg:432.34ms
step:366/1775 n_predict=3 lr=1.0000 bs=131072 train_time:158236ms step_avg:432.34ms
step:367/1775 n_predict=3 lr=1.0000 bs=131072 train_time:158667ms step_avg:432.33ms
step:368/1775 n_predict=3 lr=1.0000 bs=131072 train_time:159100ms step_avg:432.34ms
step:369/1775 n_predict=3 lr=1.0000 bs=131072 train_time:159531ms step_avg:432.33ms
step:370/1775 n_predict=3 lr=1.0000 bs=131072 train_time:159960ms step_avg:432.32ms
step:371/1775 n_predict=3 lr=1.0000 bs=131072 train_time:160391ms step_avg:432.32ms
step:372/1775 n_predict=3 lr=1.0000 bs=131072 train_time:160822ms step_avg:432.32ms
step:373/1775 n_predict=3 lr=1.0000 bs=131072 train_time:161252ms step_avg:432.31ms
step:374/1775 n_predict=3 lr=1.0000 bs=131072 train_time:161684ms step_avg:432.31ms
step:375/1775 n_predict=3 lr=1.0000 bs=131072 train_time:162118ms step_avg:432.31ms
step:376/1775 n_predict=3 lr=1.0000 bs=131072 train_time:162547ms step_avg:432.30ms
step:377/1775 n_predict=3 lr=1.0000 bs=131072 train_time:162977ms step_avg:432.30ms
step:378/1775 n_predict=3 lr=1.0000 bs=131072 train_time:163409ms step_avg:432.30ms
step:379/1775 n_predict=3 lr=1.0000 bs=131072 train_time:163839ms step_avg:432.29ms
step:380/1775 n_predict=3 lr=1.0000 bs=131072 train_time:164273ms step_avg:432.30ms
step:381/1775 n_predict=3 lr=1.0000 bs=131072 train_time:164701ms step_avg:432.29ms
step:382/1775 n_predict=3 lr=1.0000 bs=131072 train_time:165136ms step_avg:432.29ms
step:383/1775 n_predict=3 lr=1.0000 bs=131072 train_time:165568ms step_avg:432.29ms
step:384/1775 n_predict=3 lr=1.0000 bs=131072 train_time:165998ms step_avg:432.29ms
step:385/1775 n_predict=3 lr=1.0000 bs=131072 train_time:166428ms step_avg:432.28ms
step:386/1775 n_predict=3 lr=1.0000 bs=131072 train_time:166858ms step_avg:432.27ms
step:387/1775 n_predict=3 lr=1.0000 bs=131072 train_time:167293ms step_avg:432.28ms
step:388/1775 n_predict=3 lr=1.0000 bs=131072 train_time:167721ms step_avg:432.27ms
step:389/1775 n_predict=3 lr=1.0000 bs=131072 train_time:168150ms step_avg:432.26ms
step:390/1775 n_predict=3 lr=1.0000 bs=131072 train_time:168581ms step_avg:432.26ms
step:391/1775 n_predict=3 lr=1.0000 bs=131072 train_time:169011ms step_avg:432.25ms
step:392/1775 n_predict=3 lr=1.0000 bs=131072 train_time:169444ms step_avg:432.25ms
step:393/1775 n_predict=3 lr=1.0000 bs=131072 train_time:169873ms step_avg:432.25ms
step:394/1775 n_predict=3 lr=1.0000 bs=131072 train_time:170304ms step_avg:432.24ms
step:395/1775 n_predict=3 lr=1.0000 bs=131072 train_time:170733ms step_avg:432.24ms
step:396/1775 n_predict=3 lr=1.0000 bs=131072 train_time:171163ms step_avg:432.23ms
step:397/1775 n_predict=3 lr=1.0000 bs=131072 train_time:171593ms step_avg:432.22ms
step:398/1775 n_predict=3 lr=1.0000 bs=131072 train_time:172024ms step_avg:432.22ms
step:399/1775 n_predict=3 lr=1.0000 bs=131072 train_time:172453ms step_avg:432.21ms
step:400/1775 n_predict=3 lr=1.0000 bs=131072 train_time:172885ms step_avg:432.21ms
step:401/1775 n_predict=3 lr=1.0000 bs=131072 train_time:173315ms step_avg:432.21ms
step:402/1775 n_predict=3 lr=1.0000 bs=131072 train_time:173752ms step_avg:432.22ms
step:403/1775 n_predict=3 lr=1.0000 bs=131072 train_time:174182ms step_avg:432.21ms
step:404/1775 n_predict=3 lr=1.0000 bs=131072 train_time:174609ms step_avg:432.20ms
step:405/1775 n_predict=3 lr=1.0000 bs=131072 train_time:175037ms step_avg:432.19ms
step:406/1775 n_predict=3 lr=1.0000 bs=131072 train_time:175472ms step_avg:432.20ms
step:407/1775 n_predict=3 lr=1.0000 bs=131072 train_time:175901ms step_avg:432.19ms
step:408/1775 n_predict=3 lr=1.0000 bs=131072 train_time:176336ms step_avg:432.19ms
step:409/1775 n_predict=3 lr=1.0000 bs=131072 train_time:176764ms step_avg:432.19ms
step:410/1775 n_predict=3 lr=1.0000 bs=131072 train_time:177195ms step_avg:432.18ms
step:411/1775 n_predict=3 lr=1.0000 bs=131072 train_time:177626ms step_avg:432.18ms
step:412/1775 n_predict=3 lr=1.0000 bs=131072 train_time:178057ms step_avg:432.18ms
step:413/1775 n_predict=3 lr=1.0000 bs=131072 train_time:178489ms step_avg:432.18ms
step:414/1775 n_predict=3 lr=1.0000 bs=131072 train_time:178922ms step_avg:432.18ms
step:415/1775 n_predict=3 lr=1.0000 bs=131072 train_time:179351ms step_avg:432.17ms
step:416/1775 n_predict=3 lr=1.0000 bs=131072 train_time:179787ms step_avg:432.18ms
step:417/1775 n_predict=3 lr=1.0000 bs=131072 train_time:180216ms step_avg:432.17ms
step:418/1775 n_predict=3 lr=1.0000 bs=131072 train_time:180651ms step_avg:432.18ms
step:419/1775 n_predict=3 lr=1.0000 bs=131072 train_time:181078ms step_avg:432.17ms
step:420/1775 n_predict=3 lr=1.0000 bs=131072 train_time:181510ms step_avg:432.17ms
step:421/1775 n_predict=3 lr=1.0000 bs=131072 train_time:181941ms step_avg:432.16ms
step:422/1775 n_predict=3 lr=1.0000 bs=131072 train_time:182376ms step_avg:432.17ms
step:423/1775 n_predict=3 lr=1.0000 bs=131072 train_time:182805ms step_avg:432.16ms
step:424/1775 n_predict=3 lr=1.0000 bs=131072 train_time:183238ms step_avg:432.17ms
step:425/1775 n_predict=3 lr=1.0000 bs=131072 train_time:183671ms step_avg:432.17ms
step:426/1775 n_predict=3 lr=1.0000 bs=131072 train_time:184100ms step_avg:432.16ms
step:427/1775 n_predict=3 lr=1.0000 bs=131072 train_time:184531ms step_avg:432.16ms
step:428/1775 n_predict=3 lr=1.0000 bs=131072 train_time:184961ms step_avg:432.15ms
step:429/1775 n_predict=3 lr=1.0000 bs=131072 train_time:185393ms step_avg:432.15ms
step:430/1775 n_predict=3 lr=1.0000 bs=131072 train_time:185825ms step_avg:432.15ms
step:431/1775 n_predict=3 lr=1.0000 bs=131072 train_time:186253ms step_avg:432.14ms
step:432/1775 n_predict=3 lr=1.0000 bs=131072 train_time:186685ms step_avg:432.14ms
step:433/1775 n_predict=3 lr=1.0000 bs=131072 train_time:187114ms step_avg:432.13ms
step:434/1775 n_predict=3 lr=1.0000 bs=131072 train_time:187547ms step_avg:432.14ms
step:435/1775 n_predict=3 lr=1.0000 bs=131072 train_time:187975ms step_avg:432.13ms
step:436/1775 n_predict=3 lr=1.0000 bs=131072 train_time:188405ms step_avg:432.12ms
step:437/1775 n_predict=3 lr=1.0000 bs=131072 train_time:188839ms step_avg:432.12ms
step:438/1775 n_predict=3 lr=1.0000 bs=131072 train_time:189274ms step_avg:432.13ms
step:439/1775 n_predict=3 lr=1.0000 bs=131072 train_time:189702ms step_avg:432.12ms
step:440/1775 n_predict=3 lr=1.0000 bs=131072 train_time:190136ms step_avg:432.13ms
step:441/1775 n_predict=3 lr=1.0000 bs=131072 train_time:190564ms step_avg:432.12ms
step:442/1775 n_predict=3 lr=1.0000 bs=131072 train_time:190998ms step_avg:432.12ms
step:443/1775 n_predict=3 lr=1.0000 bs=131072 train_time:191426ms step_avg:432.11ms
step:444/1775 n_predict=3 lr=1.0000 bs=131072 train_time:191852ms step_avg:432.10ms
step:445/1775 n_predict=3 lr=1.0000 bs=131072 train_time:192281ms step_avg:432.09ms
step:446/1775 n_predict=3 lr=1.0000 bs=131072 train_time:192711ms step_avg:432.09ms
step:447/1775 n_predict=3 lr=1.0000 bs=131072 train_time:193139ms step_avg:432.08ms
step:448/1775 n_predict=3 lr=1.0000 bs=131072 train_time:193572ms step_avg:432.08ms
step:449/1775 n_predict=3 lr=1.0000 bs=131072 train_time:193999ms step_avg:432.07ms
step:450/1775 n_predict=3 lr=1.0000 bs=131072 train_time:194429ms step_avg:432.06ms
step:451/1775 n_predict=3 lr=1.0000 bs=131072 train_time:194856ms step_avg:432.05ms
step:452/1775 n_predict=3 lr=1.0000 bs=131072 train_time:195290ms step_avg:432.06ms
step:453/1775 n_predict=3 lr=1.0000 bs=131072 train_time:195716ms step_avg:432.04ms
step:454/1775 n_predict=3 lr=1.0000 bs=131072 train_time:196146ms step_avg:432.04ms
step:455/1775 n_predict=3 lr=1.0000 bs=131072 train_time:196575ms step_avg:432.03ms
step:456/1775 n_predict=3 lr=1.0000 bs=131072 train_time:197003ms step_avg:432.02ms
step:457/1775 n_predict=3 lr=1.0000 bs=131072 train_time:197433ms step_avg:432.02ms
step:458/1775 n_predict=3 lr=1.0000 bs=131072 train_time:197861ms step_avg:432.01ms
step:459/1775 n_predict=3 lr=1.0000 bs=131072 train_time:198292ms step_avg:432.01ms
step:460/1775 n_predict=3 lr=1.0000 bs=131072 train_time:198725ms step_avg:432.01ms
step:461/1775 n_predict=3 lr=1.0000 bs=131072 train_time:199154ms step_avg:432.01ms
step:462/1775 n_predict=3 lr=1.0000 bs=131072 train_time:199589ms step_avg:432.01ms
step:463/1775 n_predict=3 lr=1.0000 bs=131072 train_time:200019ms step_avg:432.01ms
step:464/1775 n_predict=3 lr=1.0000 bs=131072 train_time:200453ms step_avg:432.01ms
step:465/1775 n_predict=3 lr=1.0000 bs=131072 train_time:200883ms step_avg:432.01ms
step:466/1775 n_predict=3 lr=1.0000 bs=131072 train_time:201312ms step_avg:432.00ms
step:467/1775 n_predict=3 lr=1.0000 bs=131072 train_time:201744ms step_avg:432.00ms
step:468/1775 n_predict=3 lr=1.0000 bs=131072 train_time:202177ms step_avg:432.00ms
step:469/1775 n_predict=3 lr=1.0000 bs=131072 train_time:202607ms step_avg:432.00ms
step:470/1775 n_predict=3 lr=1.0000 bs=131072 train_time:203038ms step_avg:432.00ms
step:471/1775 n_predict=3 lr=1.0000 bs=131072 train_time:203469ms step_avg:431.99ms
step:472/1775 n_predict=3 lr=1.0000 bs=131072 train_time:203900ms step_avg:431.99ms
step:473/1775 n_predict=3 lr=1.0000 bs=131072 train_time:204329ms step_avg:431.98ms
step:474/1775 n_predict=3 lr=1.0000 bs=131072 train_time:204759ms step_avg:431.98ms
step:475/1775 n_predict=3 lr=1.0000 bs=131072 train_time:205191ms step_avg:431.98ms
step:476/1775 n_predict=3 lr=1.0000 bs=131072 train_time:205628ms step_avg:431.99ms
step:477/1775 n_predict=3 lr=1.0000 bs=131072 train_time:206054ms step_avg:431.98ms
step:478/1775 n_predict=3 lr=1.0000 bs=131072 train_time:206485ms step_avg:431.98ms
step:479/1775 n_predict=3 lr=1.0000 bs=131072 train_time:206914ms step_avg:431.97ms
step:480/1775 n_predict=3 lr=1.0000 bs=131072 train_time:207344ms step_avg:431.97ms
step:481/1775 n_predict=3 lr=1.0000 bs=131072 train_time:207774ms step_avg:431.96ms
step:482/1775 n_predict=3 lr=1.0000 bs=131072 train_time:208203ms step_avg:431.96ms
step:483/1775 n_predict=3 lr=1.0000 bs=131072 train_time:208636ms step_avg:431.96ms
step:484/1775 n_predict=3 lr=1.0000 bs=131072 train_time:209068ms step_avg:431.96ms
step:485/1775 n_predict=3 lr=1.0000 bs=131072 train_time:209498ms step_avg:431.96ms
step:486/1775 n_predict=3 lr=1.0000 bs=131072 train_time:209932ms step_avg:431.96ms
step:487/1775 n_predict=3 lr=1.0000 bs=131072 train_time:210359ms step_avg:431.95ms
step:488/1775 n_predict=3 lr=1.0000 bs=131072 train_time:210795ms step_avg:431.96ms
step:489/1775 n_predict=3 lr=1.0000 bs=131072 train_time:211225ms step_avg:431.95ms
step:490/1775 n_predict=3 lr=1.0000 bs=131072 train_time:211657ms step_avg:431.95ms
step:491/1775 n_predict=3 lr=1.0000 bs=131072 train_time:212090ms step_avg:431.96ms
step:492/1775 n_predict=3 lr=1.0000 bs=131072 train_time:212522ms step_avg:431.96ms
step:493/1775 n_predict=3 lr=1.0000 bs=131072 train_time:212954ms step_avg:431.96ms
step:494/1775 n_predict=3 lr=1.0000 bs=131072 train_time:213387ms step_avg:431.96ms
step:495/1775 n_predict=3 lr=1.0000 bs=131072 train_time:213818ms step_avg:431.96ms
step:496/1775 n_predict=3 lr=1.0000 bs=131072 train_time:214251ms step_avg:431.96ms
step:497/1775 n_predict=3 lr=1.0000 bs=131072 train_time:214689ms step_avg:431.97ms
step:498/1775 n_predict=3 lr=1.0000 bs=131072 train_time:215118ms step_avg:431.96ms
step:499/1775 n_predict=3 lr=1.0000 bs=131072 train_time:215547ms step_avg:431.96ms
step:500/1775 n_predict=3 lr=1.0000 bs=131072 train_time:215979ms step_avg:431.96ms
step:500/1775 lr=1.0000 bs=131072 n_predict=3 val_loss:4.2755 val_malbo_loss:4.2988 train_time:216008ms step_avg:432.02ms
step:501/1775 n_predict=3 lr=1.0000 bs=131072 train_time:216404ms step_avg:431.94ms
step:502/1775 n_predict=3 lr=1.0000 bs=131072 train_time:216842ms step_avg:431.96ms
step:503/1775 n_predict=3 lr=1.0000 bs=131072 train_time:217269ms step_avg:431.95ms
step:504/1775 n_predict=3 lr=1.0000 bs=131072 train_time:217699ms step_avg:431.94ms
step:505/1775 n_predict=3 lr=1.0000 bs=131072 train_time:218126ms step_avg:431.93ms
step:506/1775 n_predict=3 lr=1.0000 bs=131072 train_time:218558ms step_avg:431.93ms
step:507/1775 n_predict=3 lr=1.0000 bs=131072 train_time:218987ms step_avg:431.93ms
step:508/1775 n_predict=3 lr=1.0000 bs=131072 train_time:219416ms step_avg:431.92ms
step:509/1775 n_predict=3 lr=1.0000 bs=131072 train_time:219844ms step_avg:431.91ms
step:510/1775 n_predict=3 lr=1.0000 bs=131072 train_time:220280ms step_avg:431.92ms
step:511/1775 n_predict=3 lr=1.0000 bs=131072 train_time:220706ms step_avg:431.91ms
step:512/1775 n_predict=3 lr=1.0000 bs=131072 train_time:221137ms step_avg:431.91ms
step:513/1775 n_predict=3 lr=1.0000 bs=131072 train_time:221568ms step_avg:431.91ms
step:514/1775 n_predict=3 lr=1.0000 bs=131072 train_time:221996ms step_avg:431.90ms
step:515/1775 n_predict=3 lr=1.0000 bs=131072 train_time:222423ms step_avg:431.89ms
step:516/1775 n_predict=3 lr=1.0000 bs=131072 train_time:222854ms step_avg:431.89ms
step:517/1775 n_predict=3 lr=1.0000 bs=131072 train_time:223280ms step_avg:431.88ms
step:518/1775 n_predict=3 lr=1.0000 bs=131072 train_time:223711ms step_avg:431.87ms
step:519/1775 n_predict=3 lr=1.0000 bs=131072 train_time:224143ms step_avg:431.87ms
step:520/1775 n_predict=3 lr=1.0000 bs=131072 train_time:224571ms step_avg:431.87ms
step:521/1775 n_predict=3 lr=1.0000 bs=131072 train_time:224999ms step_avg:431.86ms
step:522/1775 n_predict=3 lr=1.0000 bs=131072 train_time:225430ms step_avg:431.86ms
step:523/1775 n_predict=3 lr=1.0000 bs=131072 train_time:225859ms step_avg:431.85ms
step:524/1775 n_predict=3 lr=1.0000 bs=131072 train_time:226288ms step_avg:431.85ms
step:525/1775 n_predict=3 lr=1.0000 bs=131072 train_time:226717ms step_avg:431.84ms
step:526/1775 n_predict=3 lr=1.0000 bs=131072 train_time:227147ms step_avg:431.84ms
step:527/1775 n_predict=3 lr=1.0000 bs=131072 train_time:227576ms step_avg:431.83ms
step:528/1775 n_predict=3 lr=1.0000 bs=131072 train_time:228007ms step_avg:431.83ms
step:529/1775 n_predict=3 lr=1.0000 bs=131072 train_time:228439ms step_avg:431.83ms
step:530/1775 n_predict=3 lr=1.0000 bs=131072 train_time:228869ms step_avg:431.83ms
step:531/1775 n_predict=3 lr=1.0000 bs=131072 train_time:229296ms step_avg:431.82ms
step:532/1775 n_predict=3 lr=1.0000 bs=131072 train_time:229724ms step_avg:431.81ms
step:533/1775 n_predict=3 lr=1.0000 bs=131072 train_time:230155ms step_avg:431.81ms
step:534/1775 n_predict=3 lr=1.0000 bs=131072 train_time:230587ms step_avg:431.81ms
step:535/1775 n_predict=3 lr=1.0000 bs=131072 train_time:231016ms step_avg:431.81ms
step:536/1775 n_predict=3 lr=1.0000 bs=131072 train_time:231447ms step_avg:431.80ms
step:537/1775 n_predict=3 lr=1.0000 bs=131072 train_time:231875ms step_avg:431.80ms
step:538/1775 n_predict=3 lr=1.0000 bs=131072 train_time:232305ms step_avg:431.79ms
step:539/1775 n_predict=3 lr=1.0000 bs=131072 train_time:232738ms step_avg:431.80ms
step:540/1775 n_predict=3 lr=1.0000 bs=131072 train_time:233167ms step_avg:431.79ms
step:541/1775 n_predict=3 lr=1.0000 bs=131072 train_time:233597ms step_avg:431.79ms
step:542/1775 n_predict=3 lr=1.0000 bs=131072 train_time:234025ms step_avg:431.78ms
step:543/1775 n_predict=3 lr=1.0000 bs=131072 train_time:234455ms step_avg:431.78ms
step:544/1775 n_predict=3 lr=1.0000 bs=131072 train_time:234887ms step_avg:431.78ms
step:545/1775 n_predict=3 lr=1.0000 bs=131072 train_time:235316ms step_avg:431.77ms
step:546/1775 n_predict=3 lr=1.0000 bs=131072 train_time:235749ms step_avg:431.77ms
step:547/1775 n_predict=3 lr=1.0000 bs=131072 train_time:236178ms step_avg:431.77ms
step:548/1775 n_predict=3 lr=1.0000 bs=131072 train_time:236609ms step_avg:431.77ms
step:549/1775 n_predict=3 lr=1.0000 bs=131072 train_time:237040ms step_avg:431.77ms
step:550/1775 n_predict=3 lr=1.0000 bs=131072 train_time:237469ms step_avg:431.76ms
step:551/1775 n_predict=3 lr=1.0000 bs=131072 train_time:237898ms step_avg:431.76ms
step:552/1775 n_predict=3 lr=1.0000 bs=131072 train_time:238330ms step_avg:431.76ms
step:553/1775 n_predict=3 lr=1.0000 bs=131072 train_time:238760ms step_avg:431.75ms
step:554/1775 n_predict=3 lr=1.0000 bs=131072 train_time:239190ms step_avg:431.75ms
step:555/1775 n_predict=3 lr=1.0000 bs=131072 train_time:239617ms step_avg:431.74ms
step:556/1775 n_predict=3 lr=1.0000 bs=131072 train_time:240049ms step_avg:431.74ms
step:557/1775 n_predict=3 lr=1.0000 bs=131072 train_time:240478ms step_avg:431.74ms
step:558/1775 n_predict=3 lr=1.0000 bs=131072 train_time:240910ms step_avg:431.74ms
step:559/1775 n_predict=3 lr=1.0000 bs=131072 train_time:241339ms step_avg:431.73ms
step:560/1775 n_predict=3 lr=1.0000 bs=131072 train_time:241769ms step_avg:431.73ms
step:561/1775 n_predict=3 lr=1.0000 bs=131072 train_time:242200ms step_avg:431.73ms
step:562/1775 n_predict=3 lr=1.0000 bs=131072 train_time:242630ms step_avg:431.73ms
step:563/1775 n_predict=3 lr=1.0000 bs=131072 train_time:243060ms step_avg:431.72ms
step:564/1775 n_predict=3 lr=1.0000 bs=131072 train_time:243489ms step_avg:431.72ms
step:565/1775 n_predict=3 lr=1.0000 bs=131072 train_time:243919ms step_avg:431.72ms
step:566/1775 n_predict=3 lr=1.0000 bs=131072 train_time:244352ms step_avg:431.72ms
step:567/1775 n_predict=3 lr=1.0000 bs=131072 train_time:244781ms step_avg:431.71ms
step:568/1775 n_predict=3 lr=1.0000 bs=131072 train_time:245214ms step_avg:431.71ms
step:569/1775 n_predict=3 lr=1.0000 bs=131072 train_time:245641ms step_avg:431.71ms
step:570/1775 n_predict=3 lr=1.0000 bs=131072 train_time:246072ms step_avg:431.71ms
step:571/1775 n_predict=3 lr=1.0000 bs=131072 train_time:246502ms step_avg:431.70ms
step:572/1775 n_predict=3 lr=1.0000 bs=131072 train_time:246935ms step_avg:431.70ms
step:573/1775 n_predict=3 lr=1.0000 bs=131072 train_time:247363ms step_avg:431.70ms
step:574/1775 n_predict=3 lr=1.0000 bs=131072 train_time:247794ms step_avg:431.70ms
step:575/1775 n_predict=3 lr=1.0000 bs=131072 train_time:248221ms step_avg:431.69ms
step:576/1775 n_predict=3 lr=1.0000 bs=131072 train_time:248654ms step_avg:431.69ms
step:577/1775 n_predict=3 lr=1.0000 bs=131072 train_time:249084ms step_avg:431.69ms
step:578/1775 n_predict=3 lr=1.0000 bs=131072 train_time:249516ms step_avg:431.69ms
step:579/1775 n_predict=3 lr=1.0000 bs=131072 train_time:249945ms step_avg:431.68ms
step:580/1775 n_predict=2 lr=1.5200 bs=262144 train_time:272270ms step_avg:469.43ms
step:581/1775 n_predict=2 lr=1.5200 bs=262144 train_time:273044ms step_avg:469.96ms
step:582/1775 n_predict=2 lr=1.5200 bs=262144 train_time:273826ms step_avg:470.49ms
step:583/1775 n_predict=2 lr=1.5200 bs=262144 train_time:274605ms step_avg:471.02ms
step:584/1775 n_predict=2 lr=1.5200 bs=262144 train_time:275384ms step_avg:471.55ms
step:585/1775 n_predict=2 lr=1.5200 bs=262144 train_time:276162ms step_avg:472.07ms
step:586/1775 n_predict=2 lr=1.5200 bs=262144 train_time:276941ms step_avg:472.60ms
step:587/1775 n_predict=2 lr=1.5200 bs=262144 train_time:277721ms step_avg:473.12ms
step:588/1775 n_predict=2 lr=1.5200 bs=262144 train_time:278506ms step_avg:473.65ms
step:589/1775 n_predict=2 lr=1.5200 bs=262144 train_time:279282ms step_avg:474.16ms
step:590/1775 n_predict=2 lr=1.5200 bs=262144 train_time:280059ms step_avg:474.68ms
step:591/1775 n_predict=2 lr=1.5200 bs=262144 train_time:280840ms step_avg:475.20ms
step:592/1775 n_predict=2 lr=1.5200 bs=262144 train_time:281629ms step_avg:475.72ms
step:593/1775 n_predict=2 lr=1.5200 bs=262144 train_time:282406ms step_avg:476.23ms
step:594/1775 n_predict=2 lr=1.5200 bs=262144 train_time:283188ms step_avg:476.75ms
step:595/1775 n_predict=2 lr=1.5200 bs=262144 train_time:283965ms step_avg:477.25ms
step:596/1775 n_predict=2 lr=1.5200 bs=262144 train_time:284745ms step_avg:477.76ms
step:597/1775 n_predict=2 lr=1.5200 bs=262144 train_time:285532ms step_avg:478.28ms
step:598/1775 n_predict=2 lr=1.5200 bs=262144 train_time:286314ms step_avg:478.79ms
step:599/1775 n_predict=2 lr=1.5200 bs=262144 train_time:287091ms step_avg:479.28ms
step:600/1775 n_predict=2 lr=1.5200 bs=262144 train_time:287871ms step_avg:479.78ms
step:601/1775 n_predict=2 lr=1.5200 bs=262144 train_time:288647ms step_avg:480.28ms
step:602/1775 n_predict=2 lr=1.5200 bs=262144 train_time:289436ms step_avg:480.79ms
step:603/1775 n_predict=2 lr=1.5200 bs=262144 train_time:290214ms step_avg:481.28ms
step:604/1775 n_predict=2 lr=1.5200 bs=262144 train_time:290995ms step_avg:481.78ms
step:605/1775 n_predict=2 lr=1.5200 bs=262144 train_time:291781ms step_avg:482.28ms
step:606/1775 n_predict=2 lr=1.5200 bs=262144 train_time:292554ms step_avg:482.76ms
step:607/1775 n_predict=2 lr=1.5200 bs=262144 train_time:293335ms step_avg:483.25ms
step:608/1775 n_predict=2 lr=1.5200 bs=262144 train_time:294115ms step_avg:483.74ms
step:609/1775 n_predict=2 lr=1.5200 bs=262144 train_time:294893ms step_avg:484.23ms
step:610/1775 n_predict=2 lr=1.5200 bs=262144 train_time:295674ms step_avg:484.71ms
step:611/1775 n_predict=2 lr=1.5200 bs=262144 train_time:296451ms step_avg:485.19ms
step:612/1775 n_predict=2 lr=1.5200 bs=262144 train_time:297241ms step_avg:485.69ms
step:613/1775 n_predict=2 lr=1.5200 bs=262144 train_time:298024ms step_avg:486.17ms
step:614/1775 n_predict=2 lr=1.5200 bs=262144 train_time:298804ms step_avg:486.65ms
step:615/1775 n_predict=2 lr=1.5200 bs=262144 train_time:299582ms step_avg:487.13ms
step:616/1775 n_predict=2 lr=1.5200 bs=262144 train_time:300368ms step_avg:487.61ms
step:617/1775 n_predict=2 lr=1.5200 bs=262144 train_time:301146ms step_avg:488.08ms
step:618/1775 n_predict=2 lr=1.5200 bs=262144 train_time:301934ms step_avg:488.57ms
step:619/1775 n_predict=2 lr=1.5200 bs=262144 train_time:302716ms step_avg:489.04ms
step:620/1775 n_predict=2 lr=1.5200 bs=262144 train_time:303499ms step_avg:489.51ms
step:621/1775 n_predict=2 lr=1.5200 bs=262144 train_time:304280ms step_avg:489.98ms
step:622/1775 n_predict=2 lr=1.5200 bs=262144 train_time:305066ms step_avg:490.46ms
step:623/1775 n_predict=2 lr=1.5200 bs=262144 train_time:305845ms step_avg:490.92ms
step:624/1775 n_predict=2 lr=1.5200 bs=262144 train_time:306641ms step_avg:491.41ms
step:625/1775 n_predict=2 lr=1.5200 bs=262144 train_time:307426ms step_avg:491.88ms
step:626/1775 n_predict=2 lr=1.5200 bs=262144 train_time:308208ms step_avg:492.34ms
step:627/1775 n_predict=2 lr=1.5200 bs=262144 train_time:308989ms step_avg:492.80ms
step:628/1775 n_predict=2 lr=1.5200 bs=262144 train_time:309772ms step_avg:493.27ms
step:629/1775 n_predict=2 lr=1.5200 bs=262144 train_time:310548ms step_avg:493.72ms
step:630/1775 n_predict=2 lr=1.5200 bs=262144 train_time:311340ms step_avg:494.19ms
step:631/1775 n_predict=2 lr=1.5200 bs=262144 train_time:312127ms step_avg:494.65ms
step:632/1775 n_predict=2 lr=1.5200 bs=262144 train_time:312911ms step_avg:495.11ms
step:633/1775 n_predict=2 lr=1.5200 bs=262144 train_time:313691ms step_avg:495.56ms
step:634/1775 n_predict=2 lr=1.5200 bs=262144 train_time:314472ms step_avg:496.01ms
step:635/1775 n_predict=2 lr=1.5200 bs=262144 train_time:315249ms step_avg:496.46ms
step:636/1775 n_predict=2 lr=1.5200 bs=262144 train_time:316042ms step_avg:496.92ms
step:637/1775 n_predict=2 lr=1.5200 bs=262144 train_time:316830ms step_avg:497.38ms
step:638/1775 n_predict=2 lr=1.5200 bs=262144 train_time:317613ms step_avg:497.83ms
step:639/1775 n_predict=2 lr=1.5200 bs=262144 train_time:318394ms step_avg:498.27ms
step:640/1775 n_predict=2 lr=1.5200 bs=262144 train_time:319181ms step_avg:498.72ms
step:641/1775 n_predict=2 lr=1.5200 bs=262144 train_time:319962ms step_avg:499.16ms
step:642/1775 n_predict=2 lr=1.5200 bs=262144 train_time:320748ms step_avg:499.61ms
step:643/1775 n_predict=2 lr=1.5200 bs=262144 train_time:321534ms step_avg:500.05ms
step:644/1775 n_predict=2 lr=1.5200 bs=262144 train_time:322318ms step_avg:500.49ms
step:645/1775 n_predict=2 lr=1.5200 bs=262144 train_time:323098ms step_avg:500.93ms
step:646/1775 n_predict=2 lr=1.5200 bs=262144 train_time:323883ms step_avg:501.37ms
step:647/1775 n_predict=2 lr=1.5200 bs=262144 train_time:324666ms step_avg:501.80ms
step:648/1775 n_predict=2 lr=1.5200 bs=262144 train_time:325449ms step_avg:502.24ms
step:649/1775 n_predict=2 lr=1.5200 bs=262144 train_time:326235ms step_avg:502.67ms
step:650/1775 n_predict=2 lr=1.5200 bs=262144 train_time:327019ms step_avg:503.11ms
step:651/1775 n_predict=2 lr=1.5200 bs=262144 train_time:327798ms step_avg:503.53ms
step:652/1775 n_predict=2 lr=1.5200 bs=262144 train_time:328583ms step_avg:503.96ms
step:653/1775 n_predict=2 lr=1.5200 bs=262144 train_time:329361ms step_avg:504.38ms
step:654/1775 n_predict=2 lr=1.5200 bs=262144 train_time:330143ms step_avg:504.81ms
step:655/1775 n_predict=2 lr=1.5200 bs=262144 train_time:330930ms step_avg:505.24ms
step:656/1775 n_predict=2 lr=1.5200 bs=262144 train_time:331711ms step_avg:505.66ms
step:657/1775 n_predict=2 lr=1.5200 bs=262144 train_time:332488ms step_avg:506.07ms
step:658/1775 n_predict=2 lr=1.5200 bs=262144 train_time:333271ms step_avg:506.49ms
step:659/1775 n_predict=2 lr=1.5200 bs=262144 train_time:334046ms step_avg:506.90ms
step:660/1775 n_predict=2 lr=1.5200 bs=262144 train_time:334839ms step_avg:507.33ms
step:661/1775 n_predict=2 lr=1.5200 bs=262144 train_time:335621ms step_avg:507.75ms
step:662/1775 n_predict=2 lr=1.5200 bs=262144 train_time:336407ms step_avg:508.17ms
step:663/1775 n_predict=2 lr=1.5200 bs=262144 train_time:337190ms step_avg:508.58ms
step:664/1775 n_predict=2 lr=1.5200 bs=262144 train_time:337974ms step_avg:509.00ms
step:665/1775 n_predict=2 lr=1.5200 bs=262144 train_time:338752ms step_avg:509.40ms
step:666/1775 n_predict=2 lr=1.5200 bs=262144 train_time:339542ms step_avg:509.82ms
step:667/1775 n_predict=2 lr=1.5200 bs=262144 train_time:340331ms step_avg:510.24ms
step:668/1775 n_predict=2 lr=1.5200 bs=262144 train_time:341114ms step_avg:510.65ms
step:669/1775 n_predict=2 lr=1.5200 bs=262144 train_time:341895ms step_avg:511.05ms
step:670/1775 n_predict=2 lr=1.5200 bs=262144 train_time:342679ms step_avg:511.46ms
step:671/1775 n_predict=2 lr=1.5200 bs=262144 train_time:343459ms step_avg:511.86ms
step:672/1775 n_predict=2 lr=1.5200 bs=262144 train_time:344245ms step_avg:512.27ms
step:673/1775 n_predict=2 lr=1.5200 bs=262144 train_time:345033ms step_avg:512.68ms
step:674/1775 n_predict=2 lr=1.5200 bs=262144 train_time:345819ms step_avg:513.08ms
step:675/1775 n_predict=2 lr=1.5200 bs=262144 train_time:346603ms step_avg:513.49ms
step:676/1775 n_predict=2 lr=1.5200 bs=262144 train_time:347388ms step_avg:513.89ms
step:677/1775 n_predict=2 lr=1.5200 bs=262144 train_time:348183ms step_avg:514.30ms
step:678/1775 n_predict=2 lr=1.5200 bs=262144 train_time:348950ms step_avg:514.68ms
step:679/1775 n_predict=2 lr=1.5200 bs=262144 train_time:349735ms step_avg:515.07ms
step:680/1775 n_predict=2 lr=1.5200 bs=262144 train_time:350516ms step_avg:515.46ms
step:681/1775 n_predict=2 lr=1.5200 bs=262144 train_time:351296ms step_avg:515.85ms
step:682/1775 n_predict=2 lr=1.5200 bs=262144 train_time:352078ms step_avg:516.24ms
step:683/1775 n_predict=2 lr=1.5200 bs=262144 train_time:352855ms step_avg:516.62ms
step:684/1775 n_predict=2 lr=1.5200 bs=262144 train_time:353640ms step_avg:517.02ms
step:685/1775 n_predict=2 lr=1.5200 bs=262144 train_time:354426ms step_avg:517.41ms
step:686/1775 n_predict=2 lr=1.5200 bs=262144 train_time:355210ms step_avg:517.80ms
step:687/1775 n_predict=2 lr=1.5200 bs=262144 train_time:355989ms step_avg:518.18ms
step:688/1775 n_predict=2 lr=1.5200 bs=262144 train_time:356770ms step_avg:518.56ms
step:689/1775 n_predict=2 lr=1.5200 bs=262144 train_time:357547ms step_avg:518.94ms
step:690/1775 n_predict=2 lr=1.5200 bs=262144 train_time:358341ms step_avg:519.33ms
step:691/1775 n_predict=2 lr=1.5200 bs=262144 train_time:359119ms step_avg:519.71ms
step:692/1775 n_predict=2 lr=1.5200 bs=262144 train_time:359900ms step_avg:520.09ms
step:693/1775 n_predict=2 lr=1.5200 bs=262144 train_time:360682ms step_avg:520.46ms
step:694/1775 n_predict=2 lr=1.5200 bs=262144 train_time:361467ms step_avg:520.85ms
step:695/1775 n_predict=2 lr=1.5200 bs=262144 train_time:362246ms step_avg:521.22ms
step:696/1775 n_predict=2 lr=1.5200 bs=262144 train_time:363039ms step_avg:521.61ms
step:697/1775 n_predict=2 lr=1.5200 bs=262144 train_time:363826ms step_avg:521.99ms
step:698/1775 n_predict=2 lr=1.5200 bs=262144 train_time:364614ms step_avg:522.37ms
step:699/1775 n_predict=2 lr=1.5200 bs=262144 train_time:365395ms step_avg:522.74ms
step:700/1775 n_predict=2 lr=1.5200 bs=262144 train_time:366181ms step_avg:523.12ms
step:701/1775 n_predict=2 lr=1.5200 bs=262144 train_time:366959ms step_avg:523.48ms
step:702/1775 n_predict=2 lr=1.5200 bs=262144 train_time:367749ms step_avg:523.86ms
step:703/1775 n_predict=2 lr=1.5200 bs=262144 train_time:368541ms step_avg:524.24ms
step:704/1775 n_predict=2 lr=1.5200 bs=262144 train_time:369323ms step_avg:524.61ms
step:705/1775 n_predict=2 lr=1.5200 bs=262144 train_time:370116ms step_avg:524.99ms
step:706/1775 n_predict=2 lr=1.5200 bs=262144 train_time:370898ms step_avg:525.35ms
step:707/1775 n_predict=2 lr=1.5200 bs=262144 train_time:371679ms step_avg:525.71ms
step:708/1775 n_predict=2 lr=1.5200 bs=262144 train_time:372467ms step_avg:526.08ms
step:709/1775 n_predict=2 lr=1.5200 bs=262144 train_time:373246ms step_avg:526.44ms
step:710/1775 n_predict=2 lr=1.5200 bs=262144 train_time:374046ms step_avg:526.83ms
step:711/1775 n_predict=2 lr=1.5200 bs=262144 train_time:374841ms step_avg:527.20ms
step:712/1775 n_predict=2 lr=1.5200 bs=262144 train_time:375629ms step_avg:527.57ms
step:713/1775 n_predict=2 lr=1.5200 bs=262144 train_time:376411ms step_avg:527.93ms
step:714/1775 n_predict=2 lr=1.5200 bs=262144 train_time:377204ms step_avg:528.30ms
step:715/1775 n_predict=2 lr=1.5200 bs=262144 train_time:377979ms step_avg:528.64ms
step:716/1775 n_predict=2 lr=1.5200 bs=262144 train_time:378765ms step_avg:529.00ms
step:717/1775 n_predict=2 lr=1.5200 bs=262144 train_time:379555ms step_avg:529.37ms
step:718/1775 n_predict=2 lr=1.5200 bs=262144 train_time:380348ms step_avg:529.73ms
step:719/1775 n_predict=2 lr=1.5200 bs=262144 train_time:381137ms step_avg:530.09ms
step:720/1775 n_predict=2 lr=1.5200 bs=262144 train_time:381924ms step_avg:530.45ms
step:721/1775 n_predict=2 lr=1.5200 bs=262144 train_time:382712ms step_avg:530.81ms
step:722/1775 n_predict=2 lr=1.5200 bs=262144 train_time:383497ms step_avg:531.16ms
step:723/1775 n_predict=2 lr=1.5200 bs=262144 train_time:384279ms step_avg:531.51ms
step:724/1775 n_predict=2 lr=1.5200 bs=262144 train_time:385063ms step_avg:531.86ms
step:725/1775 n_predict=2 lr=1.5200 bs=262144 train_time:385847ms step_avg:532.20ms
step:726/1775 n_predict=2 lr=1.5200 bs=262144 train_time:386644ms step_avg:532.57ms
step:727/1775 n_predict=2 lr=1.5200 bs=262144 train_time:387429ms step_avg:532.92ms
step:728/1775 n_predict=2 lr=1.5200 bs=262144 train_time:388219ms step_avg:533.27ms
step:729/1775 n_predict=2 lr=1.5200 bs=262144 train_time:388996ms step_avg:533.60ms
step:730/1775 n_predict=2 lr=1.5200 bs=262144 train_time:389783ms step_avg:533.95ms
step:731/1775 n_predict=2 lr=1.5200 bs=262144 train_time:390571ms step_avg:534.30ms
step:732/1775 n_predict=2 lr=1.5200 bs=262144 train_time:391353ms step_avg:534.63ms
step:733/1775 n_predict=2 lr=1.5200 bs=262144 train_time:392138ms step_avg:534.98ms
step:734/1775 n_predict=2 lr=1.5200 bs=262144 train_time:392925ms step_avg:535.32ms
step:735/1775 n_predict=2 lr=1.5200 bs=262144 train_time:393714ms step_avg:535.67ms
step:736/1775 n_predict=2 lr=1.5200 bs=262144 train_time:394495ms step_avg:536.00ms
step:737/1775 n_predict=2 lr=1.5200 bs=262144 train_time:395279ms step_avg:536.34ms
step:738/1775 n_predict=2 lr=1.5200 bs=262144 train_time:396067ms step_avg:536.68ms
step:739/1775 n_predict=2 lr=1.5200 bs=262144 train_time:396848ms step_avg:537.01ms
step:740/1775 n_predict=2 lr=1.5200 bs=262144 train_time:397644ms step_avg:537.36ms
step:741/1775 n_predict=2 lr=1.5200 bs=262144 train_time:398425ms step_avg:537.69ms
step:742/1775 n_predict=2 lr=1.5200 bs=262144 train_time:399212ms step_avg:538.02ms
step:743/1775 n_predict=2 lr=1.5200 bs=262144 train_time:399994ms step_avg:538.35ms
step:744/1775 n_predict=2 lr=1.5200 bs=262144 train_time:400778ms step_avg:538.68ms
step:745/1775 n_predict=2 lr=1.5200 bs=262144 train_time:401559ms step_avg:539.00ms
step:746/1775 n_predict=2 lr=1.5200 bs=262144 train_time:402348ms step_avg:539.34ms
step:747/1775 n_predict=2 lr=1.5200 bs=262144 train_time:403138ms step_avg:539.68ms
step:748/1775 n_predict=2 lr=1.5200 bs=262144 train_time:403922ms step_avg:540.00ms
step:749/1775 n_predict=2 lr=1.5200 bs=262144 train_time:404708ms step_avg:540.33ms
step:750/1775 n_predict=2 lr=1.5200 bs=262144 train_time:405498ms step_avg:540.66ms
step:750/1775 lr=1.5200 bs=262144 n_predict=2 val_loss:3.9936 val_malbo_loss:4.0197 train_time:405551ms step_avg:540.73ms
step:751/1775 n_predict=2 lr=1.5200 bs=262144 train_time:406266ms step_avg:540.97ms
step:752/1775 n_predict=2 lr=1.5200 bs=262144 train_time:407044ms step_avg:541.28ms
step:753/1775 n_predict=2 lr=1.5200 bs=262144 train_time:407824ms step_avg:541.60ms
step:754/1775 n_predict=2 lr=1.5200 bs=262144 train_time:408603ms step_avg:541.91ms
step:755/1775 n_predict=2 lr=1.5200 bs=262144 train_time:409379ms step_avg:542.22ms
step:756/1775 n_predict=2 lr=1.5200 bs=262144 train_time:410155ms step_avg:542.53ms
step:757/1775 n_predict=2 lr=1.5200 bs=262144 train_time:410928ms step_avg:542.84ms
step:758/1775 n_predict=2 lr=1.5200 bs=262144 train_time:411717ms step_avg:543.16ms
step:759/1775 n_predict=2 lr=1.5200 bs=262144 train_time:412494ms step_avg:543.47ms
step:760/1775 n_predict=2 lr=1.5200 bs=262144 train_time:413270ms step_avg:543.78ms
step:761/1775 n_predict=2 lr=1.5200 bs=262144 train_time:414052ms step_avg:544.09ms
step:762/1775 n_predict=2 lr=1.5200 bs=262144 train_time:414829ms step_avg:544.39ms
step:763/1775 n_predict=2 lr=1.5200 bs=262144 train_time:415611ms step_avg:544.71ms
step:764/1775 n_predict=2 lr=1.5200 bs=262144 train_time:416391ms step_avg:545.01ms
step:765/1775 n_predict=2 lr=1.5200 bs=262144 train_time:417169ms step_avg:545.32ms
step:766/1775 n_predict=2 lr=1.5200 bs=262144 train_time:417949ms step_avg:545.62ms
step:767/1775 n_predict=2 lr=1.5200 bs=262144 train_time:418728ms step_avg:545.93ms
step:768/1775 n_predict=2 lr=1.5200 bs=262144 train_time:419512ms step_avg:546.24ms
step:769/1775 n_predict=2 lr=1.5200 bs=262144 train_time:420291ms step_avg:546.54ms
step:770/1775 n_predict=2 lr=1.5200 bs=262144 train_time:421069ms step_avg:546.84ms
step:771/1775 n_predict=2 lr=1.5200 bs=262144 train_time:421848ms step_avg:547.14ms
step:772/1775 n_predict=2 lr=1.5200 bs=262144 train_time:422627ms step_avg:547.44ms
step:773/1775 n_predict=2 lr=1.5200 bs=262144 train_time:423410ms step_avg:547.75ms
step:774/1775 n_predict=2 lr=1.5200 bs=262144 train_time:424190ms step_avg:548.05ms
step:775/1775 n_predict=2 lr=1.5200 bs=262144 train_time:424967ms step_avg:548.34ms
step:776/1775 n_predict=2 lr=1.5200 bs=262144 train_time:425748ms step_avg:548.64ms
step:777/1775 n_predict=2 lr=1.5200 bs=262144 train_time:426526ms step_avg:548.94ms
step:778/1775 n_predict=2 lr=1.5200 bs=262144 train_time:427312ms step_avg:549.24ms
step:779/1775 n_predict=2 lr=1.5200 bs=262144 train_time:428088ms step_avg:549.54ms
step:780/1775 n_predict=2 lr=1.5200 bs=262144 train_time:428865ms step_avg:549.83ms
step:781/1775 n_predict=2 lr=1.5200 bs=262144 train_time:429642ms step_avg:550.12ms
step:782/1775 n_predict=2 lr=1.5200 bs=262144 train_time:430422ms step_avg:550.41ms
step:783/1775 n_predict=2 lr=1.5200 bs=262144 train_time:431202ms step_avg:550.71ms
step:784/1775 n_predict=2 lr=1.5200 bs=262144 train_time:431984ms step_avg:551.00ms
step:785/1775 n_predict=2 lr=1.5200 bs=262144 train_time:432759ms step_avg:551.29ms
step:786/1775 n_predict=2 lr=1.5200 bs=262144 train_time:433539ms step_avg:551.58ms
step:787/1775 n_predict=2 lr=1.5200 bs=262144 train_time:434321ms step_avg:551.87ms
step:788/1775 n_predict=2 lr=1.5200 bs=262144 train_time:435107ms step_avg:552.17ms
step:789/1775 n_predict=2 lr=1.5200 bs=262144 train_time:435888ms step_avg:552.46ms
step:790/1775 n_predict=2 lr=1.5200 bs=262144 train_time:436673ms step_avg:552.75ms
step:791/1775 n_predict=2 lr=1.5200 bs=262144 train_time:437448ms step_avg:553.03ms
step:792/1775 n_predict=2 lr=1.5200 bs=262144 train_time:438228ms step_avg:553.32ms
step:793/1775 n_predict=2 lr=1.5200 bs=262144 train_time:439016ms step_avg:553.61ms
step:794/1775 n_predict=2 lr=1.5200 bs=262144 train_time:439796ms step_avg:553.90ms
step:795/1775 n_predict=2 lr=1.5200 bs=262144 train_time:440573ms step_avg:554.18ms
step:796/1775 n_predict=2 lr=1.5200 bs=262144 train_time:441353ms step_avg:554.46ms
step:797/1775 n_predict=2 lr=1.5200 bs=262144 train_time:442129ms step_avg:554.74ms
step:798/1775 n_predict=2 lr=1.5200 bs=262144 train_time:442915ms step_avg:555.03ms
step:799/1775 n_predict=2 lr=1.5200 bs=262144 train_time:443694ms step_avg:555.31ms
step:800/1775 n_predict=2 lr=1.5200 bs=262144 train_time:444474ms step_avg:555.59ms
step:801/1775 n_predict=2 lr=1.5200 bs=262144 train_time:445254ms step_avg:555.87ms
step:802/1775 n_predict=2 lr=1.5200 bs=262144 train_time:446031ms step_avg:556.15ms
step:803/1775 n_predict=2 lr=1.5200 bs=262144 train_time:446819ms step_avg:556.44ms
step:804/1775 n_predict=2 lr=1.5200 bs=262144 train_time:447603ms step_avg:556.72ms
step:805/1775 n_predict=2 lr=1.5200 bs=262144 train_time:448383ms step_avg:557.00ms
step:806/1775 n_predict=2 lr=1.5200 bs=262144 train_time:449166ms step_avg:557.28ms
step:807/1775 n_predict=2 lr=1.5200 bs=262144 train_time:449945ms step_avg:557.55ms
step:808/1775 n_predict=2 lr=1.5200 bs=262144 train_time:450727ms step_avg:557.83ms
step:809/1775 n_predict=2 lr=1.5200 bs=262144 train_time:451517ms step_avg:558.12ms
step:810/1775 n_predict=2 lr=1.5200 bs=262144 train_time:452297ms step_avg:558.39ms
step:811/1775 n_predict=2 lr=1.5200 bs=262144 train_time:453075ms step_avg:558.66ms
step:812/1775 n_predict=2 lr=1.5200 bs=262144 train_time:453856ms step_avg:558.94ms
step:813/1775 n_predict=2 lr=1.5200 bs=262144 train_time:454637ms step_avg:559.21ms
step:814/1775 n_predict=2 lr=1.5200 bs=262144 train_time:455423ms step_avg:559.49ms
step:815/1775 n_predict=2 lr=1.5200 bs=262144 train_time:456210ms step_avg:559.77ms
step:816/1775 n_predict=2 lr=1.5200 bs=262144 train_time:456991ms step_avg:560.04ms
step:817/1775 n_predict=2 lr=1.5200 bs=262144 train_time:457771ms step_avg:560.31ms
step:818/1775 n_predict=2 lr=1.5200 bs=262144 train_time:458554ms step_avg:560.58ms
step:819/1775 n_predict=2 lr=1.5200 bs=262144 train_time:459330ms step_avg:560.84ms
step:820/1775 n_predict=2 lr=1.5200 bs=262144 train_time:460118ms step_avg:561.12ms
step:821/1775 n_predict=2 lr=1.5200 bs=262144 train_time:460898ms step_avg:561.39ms
step:822/1775 n_predict=2 lr=1.5200 bs=262144 train_time:461680ms step_avg:561.65ms
step:823/1775 n_predict=2 lr=1.5200 bs=262144 train_time:462461ms step_avg:561.92ms
step:824/1775 n_predict=2 lr=1.5200 bs=262144 train_time:463241ms step_avg:562.19ms
step:825/1775 n_predict=2 lr=1.5200 bs=262144 train_time:464025ms step_avg:562.45ms
step:826/1775 n_predict=2 lr=1.5200 bs=262144 train_time:464813ms step_avg:562.73ms
step:827/1775 n_predict=2 lr=1.5200 bs=262144 train_time:465593ms step_avg:562.99ms
step:828/1775 n_predict=2 lr=1.5200 bs=262144 train_time:466373ms step_avg:563.25ms
step:829/1775 n_predict=2 lr=1.5200 bs=262144 train_time:467154ms step_avg:563.51ms
step:830/1775 n_predict=2 lr=1.5200 bs=262144 train_time:467935ms step_avg:563.78ms
step:831/1775 n_predict=2 lr=1.5200 bs=262144 train_time:468721ms step_avg:564.05ms
step:832/1775 n_predict=2 lr=1.5200 bs=262144 train_time:469507ms step_avg:564.31ms
step:833/1775 n_predict=2 lr=1.5200 bs=262144 train_time:470291ms step_avg:564.58ms
step:834/1775 n_predict=2 lr=1.5200 bs=262144 train_time:471075ms step_avg:564.84ms
step:835/1775 n_predict=2 lr=1.5200 bs=262144 train_time:471856ms step_avg:565.10ms
step:836/1775 n_predict=2 lr=1.5200 bs=262144 train_time:472633ms step_avg:565.35ms
step:837/1775 n_predict=2 lr=1.5200 bs=262144 train_time:473420ms step_avg:565.62ms
step:838/1775 n_predict=2 lr=1.5200 bs=262144 train_time:474204ms step_avg:565.88ms
step:839/1775 n_predict=2 lr=1.5200 bs=262144 train_time:474988ms step_avg:566.14ms
step:840/1775 n_predict=2 lr=1.5200 bs=262144 train_time:475772ms step_avg:566.40ms
step:841/1775 n_predict=2 lr=1.5200 bs=262144 train_time:476552ms step_avg:566.65ms
step:842/1775 n_predict=2 lr=1.5200 bs=262144 train_time:477333ms step_avg:566.90ms
step:843/1775 n_predict=2 lr=1.5200 bs=262144 train_time:478122ms step_avg:567.17ms
step:844/1775 n_predict=2 lr=1.5200 bs=262144 train_time:478910ms step_avg:567.43ms
step:845/1775 n_predict=2 lr=1.5200 bs=262144 train_time:479693ms step_avg:567.68ms
step:846/1775 n_predict=2 lr=1.5200 bs=262144 train_time:480478ms step_avg:567.94ms
step:847/1775 n_predict=2 lr=1.5200 bs=262144 train_time:481257ms step_avg:568.19ms
step:848/1775 n_predict=2 lr=1.5200 bs=262144 train_time:482038ms step_avg:568.44ms
step:849/1775 n_predict=2 lr=1.5200 bs=262144 train_time:482825ms step_avg:568.70ms
step:850/1775 n_predict=2 lr=1.5200 bs=262144 train_time:483619ms step_avg:568.96ms
step:851/1775 n_predict=2 lr=1.5200 bs=262144 train_time:484400ms step_avg:569.21ms
step:852/1775 n_predict=2 lr=1.5200 bs=262144 train_time:485183ms step_avg:569.46ms
step:853/1775 n_predict=2 lr=1.5200 bs=262144 train_time:485966ms step_avg:569.71ms
step:854/1775 n_predict=2 lr=1.5200 bs=262144 train_time:486752ms step_avg:569.97ms
step:855/1775 n_predict=2 lr=1.5200 bs=262144 train_time:487532ms step_avg:570.21ms
step:856/1775 n_predict=2 lr=1.5200 bs=262144 train_time:488325ms step_avg:570.47ms
step:857/1775 n_predict=2 lr=1.5200 bs=262144 train_time:489115ms step_avg:570.73ms
step:858/1775 n_predict=2 lr=1.5200 bs=262144 train_time:489899ms step_avg:570.98ms
step:859/1775 n_predict=2 lr=1.5200 bs=262144 train_time:490677ms step_avg:571.22ms
step:860/1775 n_predict=2 lr=1.5200 bs=262144 train_time:491461ms step_avg:571.47ms
step:861/1775 n_predict=2 lr=1.5200 bs=262144 train_time:492241ms step_avg:571.71ms
step:862/1775 n_predict=2 lr=1.5200 bs=262144 train_time:493030ms step_avg:571.96ms
step:863/1775 n_predict=2 lr=1.5200 bs=262144 train_time:493821ms step_avg:572.21ms
step:864/1775 n_predict=2 lr=1.5200 bs=262144 train_time:494605ms step_avg:572.46ms
step:865/1775 n_predict=2 lr=1.5200 bs=262144 train_time:495388ms step_avg:572.70ms
step:866/1775 n_predict=2 lr=1.5200 bs=262144 train_time:496172ms step_avg:572.95ms
step:867/1775 n_predict=2 lr=1.5200 bs=262144 train_time:496954ms step_avg:573.19ms
step:868/1775 n_predict=2 lr=1.5200 bs=262144 train_time:497733ms step_avg:573.43ms
step:869/1775 n_predict=2 lr=1.5192 bs=262144 train_time:498522ms step_avg:573.67ms
step:870/1775 n_predict=2 lr=1.5175 bs=262144 train_time:499308ms step_avg:573.92ms
step:871/1775 n_predict=2 lr=1.5159 bs=262144 train_time:500094ms step_avg:574.16ms
step:872/1775 n_predict=2 lr=1.5143 bs=262144 train_time:500876ms step_avg:574.40ms
step:873/1775 n_predict=2 lr=1.5126 bs=262144 train_time:501659ms step_avg:574.64ms
step:874/1775 n_predict=2 lr=1.5110 bs=262144 train_time:502440ms step_avg:574.87ms
step:875/1775 n_predict=2 lr=1.5094 bs=262144 train_time:503227ms step_avg:575.12ms
step:876/1775 n_predict=2 lr=1.5077 bs=262144 train_time:504019ms step_avg:575.36ms
step:877/1775 n_predict=2 lr=1.5061 bs=262144 train_time:504802ms step_avg:575.60ms
step:878/1775 n_predict=2 lr=1.5044 bs=262144 train_time:505585ms step_avg:575.84ms
step:879/1775 n_predict=2 lr=1.5028 bs=262144 train_time:506367ms step_avg:576.07ms
step:880/1775 n_predict=2 lr=1.5012 bs=262144 train_time:507153ms step_avg:576.31ms
step:881/1775 n_predict=2 lr=1.4995 bs=262144 train_time:507931ms step_avg:576.54ms
step:882/1775 n_predict=2 lr=1.4979 bs=262144 train_time:508728ms step_avg:576.79ms
step:883/1775 n_predict=2 lr=1.4963 bs=262144 train_time:509506ms step_avg:577.02ms
step:884/1775 n_predict=2 lr=1.4946 bs=262144 train_time:510297ms step_avg:577.26ms
step:885/1775 n_predict=2 lr=1.4930 bs=262144 train_time:511073ms step_avg:577.48ms
step:886/1775 n_predict=2 lr=1.4914 bs=262144 train_time:511856ms step_avg:577.72ms
step:887/1775 n_predict=2 lr=1.4897 bs=262144 train_time:512635ms step_avg:577.94ms
step:888/1775 n_predict=2 lr=1.4881 bs=262144 train_time:513427ms step_avg:578.18ms
step:889/1775 n_predict=2 lr=1.4864 bs=262144 train_time:514216ms step_avg:578.42ms
step:890/1775 n_predict=2 lr=1.4848 bs=262144 train_time:514997ms step_avg:578.65ms
step:891/1775 n_predict=2 lr=1.4832 bs=262144 train_time:515777ms step_avg:578.87ms
step:892/1775 n_predict=2 lr=1.4815 bs=262144 train_time:516562ms step_avg:579.11ms
step:893/1775 n_predict=2 lr=1.4799 bs=262144 train_time:517345ms step_avg:579.33ms
step:894/1775 n_predict=2 lr=1.4783 bs=262144 train_time:518129ms step_avg:579.56ms
step:895/1775 n_predict=2 lr=1.4766 bs=262144 train_time:518918ms step_avg:579.80ms
step:896/1775 n_predict=2 lr=1.4750 bs=262144 train_time:519704ms step_avg:580.03ms
step:897/1775 n_predict=2 lr=1.4733 bs=262144 train_time:520485ms step_avg:580.25ms
step:898/1775 n_predict=2 lr=1.4717 bs=262144 train_time:521271ms step_avg:580.48ms
step:899/1775 n_predict=2 lr=1.4701 bs=262144 train_time:522055ms step_avg:580.71ms
step:900/1775 n_predict=2 lr=1.4684 bs=262144 train_time:522835ms step_avg:580.93ms
step:901/1775 n_predict=2 lr=1.4668 bs=262144 train_time:523621ms step_avg:581.16ms
step:902/1775 n_predict=2 lr=1.4652 bs=262144 train_time:524413ms step_avg:581.39ms
step:903/1775 n_predict=2 lr=1.4635 bs=262144 train_time:525199ms step_avg:581.62ms
step:904/1775 n_predict=2 lr=1.4619 bs=262144 train_time:525981ms step_avg:581.84ms
step:905/1775 n_predict=2 lr=1.4603 bs=262144 train_time:526762ms step_avg:582.06ms
step:906/1775 n_predict=2 lr=1.4586 bs=262144 train_time:527549ms step_avg:582.28ms
step:907/1775 n_predict=2 lr=1.4570 bs=262144 train_time:528332ms step_avg:582.50ms
step:908/1775 n_predict=2 lr=1.4553 bs=262144 train_time:529123ms step_avg:582.73ms
step:909/1775 n_predict=2 lr=1.4537 bs=262144 train_time:529911ms step_avg:582.96ms
step:910/1775 n_predict=2 lr=1.4521 bs=262144 train_time:530697ms step_avg:583.18ms
step:911/1775 n_predict=2 lr=1.4504 bs=262144 train_time:531478ms step_avg:583.40ms
step:912/1775 n_predict=2 lr=1.4488 bs=262144 train_time:532262ms step_avg:583.62ms
step:913/1775 n_predict=2 lr=1.4472 bs=262144 train_time:533048ms step_avg:583.84ms
step:914/1775 n_predict=2 lr=1.4455 bs=262144 train_time:533833ms step_avg:584.06ms
step:915/1775 n_predict=2 lr=1.4439 bs=262144 train_time:534619ms step_avg:584.28ms
step:916/1775 n_predict=2 lr=1.4422 bs=262144 train_time:535405ms step_avg:584.50ms
step:917/1775 n_predict=2 lr=1.4406 bs=262144 train_time:536186ms step_avg:584.72ms
step:918/1775 n_predict=2 lr=1.4390 bs=262144 train_time:536970ms step_avg:584.93ms
step:919/1775 n_predict=2 lr=1.4373 bs=262144 train_time:537759ms step_avg:585.16ms
step:920/1775 n_predict=2 lr=1.4357 bs=262144 train_time:538541ms step_avg:585.37ms
step:921/1775 n_predict=2 lr=1.4341 bs=262144 train_time:539323ms step_avg:585.58ms
step:922/1775 n_predict=2 lr=1.4324 bs=262144 train_time:540128ms step_avg:585.82ms
step:923/1775 n_predict=2 lr=1.4308 bs=262144 train_time:540902ms step_avg:586.03ms
step:924/1775 n_predict=2 lr=1.4292 bs=262144 train_time:541685ms step_avg:586.24ms
step:925/1775 n_predict=2 lr=1.4275 bs=262144 train_time:542470ms step_avg:586.45ms
step:926/1775 n_predict=2 lr=1.4259 bs=262144 train_time:543264ms step_avg:586.68ms
step:927/1775 n_predict=2 lr=1.4242 bs=262144 train_time:544039ms step_avg:586.88ms
step:928/1775 n_predict=2 lr=1.4226 bs=262144 train_time:544825ms step_avg:587.10ms
step:929/1775 n_predict=2 lr=1.4210 bs=262144 train_time:545624ms step_avg:587.32ms
step:930/1775 n_predict=2 lr=1.4193 bs=262144 train_time:546409ms step_avg:587.54ms
step:931/1775 n_predict=2 lr=1.4177 bs=262144 train_time:547190ms step_avg:587.74ms
step:932/1775 n_predict=2 lr=1.4161 bs=262144 train_time:547980ms step_avg:587.96ms
step:933/1775 n_predict=2 lr=1.4144 bs=262144 train_time:548765ms step_avg:588.17ms
step:934/1775 n_predict=2 lr=1.4128 bs=262144 train_time:549546ms step_avg:588.38ms
step:935/1775 n_predict=2 lr=1.4111 bs=262144 train_time:550336ms step_avg:588.60ms
step:936/1775 n_predict=2 lr=1.4095 bs=262144 train_time:551127ms step_avg:588.81ms
step:937/1775 n_predict=2 lr=1.4079 bs=262144 train_time:551917ms step_avg:589.03ms
step:938/1775 n_predict=2 lr=1.4062 bs=262144 train_time:552699ms step_avg:589.23ms
step:939/1775 n_predict=2 lr=1.4046 bs=262144 train_time:553485ms step_avg:589.44ms
step:940/1775 n_predict=2 lr=1.4030 bs=262144 train_time:554267ms step_avg:589.65ms
step:941/1775 n_predict=2 lr=1.4013 bs=262144 train_time:555059ms step_avg:589.86ms
step:942/1775 n_predict=2 lr=1.3997 bs=262144 train_time:555840ms step_avg:590.06ms
step:943/1775 n_predict=2 lr=1.3981 bs=262144 train_time:556625ms step_avg:590.27ms
step:944/1775 n_predict=2 lr=1.3964 bs=262144 train_time:557415ms step_avg:590.48ms
step:945/1775 n_predict=2 lr=1.3948 bs=262144 train_time:558197ms step_avg:590.68ms
step:946/1775 n_predict=2 lr=1.3931 bs=262144 train_time:558979ms step_avg:590.89ms
step:947/1775 n_predict=2 lr=1.3915 bs=262144 train_time:559762ms step_avg:591.09ms
step:948/1775 n_predict=2 lr=1.3899 bs=262144 train_time:560546ms step_avg:591.29ms
step:949/1775 n_predict=2 lr=1.3882 bs=262144 train_time:561328ms step_avg:591.49ms
step:950/1775 n_predict=2 lr=1.3866 bs=262144 train_time:562118ms step_avg:591.70ms
step:951/1775 n_predict=2 lr=1.3850 bs=262144 train_time:562900ms step_avg:591.90ms
step:952/1775 n_predict=2 lr=1.3833 bs=262144 train_time:563686ms step_avg:592.11ms
step:953/1775 n_predict=2 lr=1.3817 bs=262144 train_time:564465ms step_avg:592.30ms
step:954/1775 n_predict=2 lr=1.3800 bs=262144 train_time:565252ms step_avg:592.51ms
step:955/1775 n_predict=2 lr=1.3784 bs=262144 train_time:566032ms step_avg:592.70ms
step:956/1775 n_predict=2 lr=1.3768 bs=262144 train_time:566826ms step_avg:592.91ms
step:957/1775 n_predict=2 lr=1.3751 bs=262144 train_time:567617ms step_avg:593.12ms
step:958/1775 n_predict=2 lr=1.3735 bs=262144 train_time:568401ms step_avg:593.32ms
step:959/1775 n_predict=2 lr=1.3719 bs=262144 train_time:569182ms step_avg:593.52ms
step:960/1775 n_predict=2 lr=1.3702 bs=262144 train_time:569965ms step_avg:593.71ms
step:961/1775 n_predict=2 lr=1.3686 bs=262144 train_time:570752ms step_avg:593.91ms
step:962/1775 n_predict=2 lr=1.3670 bs=262144 train_time:571536ms step_avg:594.11ms
step:963/1775 n_predict=2 lr=1.3653 bs=262144 train_time:572323ms step_avg:594.31ms
step:964/1775 n_predict=2 lr=1.3637 bs=262144 train_time:573114ms step_avg:594.52ms
step:965/1775 n_predict=2 lr=1.3620 bs=262144 train_time:573902ms step_avg:594.72ms
step:966/1775 n_predict=2 lr=1.3604 bs=262144 train_time:574680ms step_avg:594.91ms
step:967/1775 n_predict=2 lr=1.3588 bs=262144 train_time:575459ms step_avg:595.10ms
step:968/1775 n_predict=2 lr=1.3571 bs=262144 train_time:576241ms step_avg:595.29ms
step:969/1775 n_predict=2 lr=1.3555 bs=262144 train_time:577027ms step_avg:595.49ms
step:970/1775 n_predict=2 lr=1.3539 bs=262144 train_time:577816ms step_avg:595.69ms
step:971/1775 n_predict=2 lr=1.3522 bs=262144 train_time:578597ms step_avg:595.88ms
step:972/1775 n_predict=2 lr=1.3506 bs=262144 train_time:579378ms step_avg:596.07ms
step:973/1775 n_predict=2 lr=1.3489 bs=262144 train_time:580161ms step_avg:596.26ms
step:974/1775 n_predict=2 lr=1.3473 bs=262144 train_time:580942ms step_avg:596.45ms
step:975/1775 n_predict=2 lr=1.3457 bs=262144 train_time:581731ms step_avg:596.65ms
step:976/1775 n_predict=2 lr=1.3440 bs=262144 train_time:582522ms step_avg:596.85ms
step:977/1775 n_predict=2 lr=1.3424 bs=262144 train_time:583305ms step_avg:597.04ms
step:978/1775 n_predict=2 lr=1.3408 bs=262144 train_time:584092ms step_avg:597.23ms
step:979/1775 n_predict=2 lr=1.3391 bs=262144 train_time:584873ms step_avg:597.42ms
step:980/1775 n_predict=2 lr=1.3375 bs=262144 train_time:585657ms step_avg:597.61ms
step:981/1775 n_predict=2 lr=1.3359 bs=262144 train_time:586438ms step_avg:597.80ms
step:982/1775 n_predict=2 lr=1.3342 bs=262144 train_time:587225ms step_avg:597.99ms
step:983/1775 n_predict=2 lr=1.3326 bs=262144 train_time:588011ms step_avg:598.18ms
step:984/1775 n_predict=2 lr=1.3309 bs=262144 train_time:588798ms step_avg:598.37ms
step:985/1775 n_predict=2 lr=1.3293 bs=262144 train_time:589576ms step_avg:598.55ms
step:986/1775 n_predict=2 lr=1.3277 bs=262144 train_time:590360ms step_avg:598.74ms
step:987/1775 n_predict=2 lr=1.3260 bs=262144 train_time:591140ms step_avg:598.93ms
step:988/1775 n_predict=2 lr=1.3244 bs=262144 train_time:591928ms step_avg:599.12ms
step:989/1775 n_predict=2 lr=1.3228 bs=262144 train_time:592714ms step_avg:599.31ms
step:990/1775 n_predict=2 lr=1.3211 bs=262144 train_time:593497ms step_avg:599.49ms
step:991/1775 n_predict=2 lr=1.3195 bs=262144 train_time:594280ms step_avg:599.68ms
step:992/1775 n_predict=2 lr=1.3178 bs=262144 train_time:595065ms step_avg:599.86ms
step:993/1775 n_predict=2 lr=1.3162 bs=262144 train_time:595848ms step_avg:600.05ms
step:994/1775 n_predict=2 lr=1.3146 bs=262144 train_time:596633ms step_avg:600.23ms
step:995/1775 n_predict=2 lr=1.3129 bs=262144 train_time:597422ms step_avg:600.42ms
step:996/1775 n_predict=2 lr=1.3113 bs=262144 train_time:598208ms step_avg:600.61ms
step:997/1775 n_predict=2 lr=1.3097 bs=262144 train_time:598992ms step_avg:600.79ms
step:998/1775 n_predict=2 lr=1.3080 bs=262144 train_time:599776ms step_avg:600.98ms
step:999/1775 n_predict=2 lr=1.3064 bs=262144 train_time:600557ms step_avg:601.16ms
step:1000/1775 n_predict=2 lr=1.3047 bs=262144 train_time:601338ms step_avg:601.34ms
step:1000/1775 lr=1.3031 bs=262144 n_predict=2 val_loss:3.7316 val_malbo_loss:3.7578 train_time:601395ms step_avg:601.39ms
step:1001/1775 n_predict=2 lr=1.3031 bs=262144 train_time:602113ms step_avg:601.51ms
step:1002/1775 n_predict=2 lr=1.3015 bs=262144 train_time:602891ms step_avg:601.69ms
step:1003/1775 n_predict=2 lr=1.2998 bs=262144 train_time:603676ms step_avg:601.87ms
step:1004/1775 n_predict=2 lr=1.2982 bs=262144 train_time:604454ms step_avg:602.05ms
step:1005/1775 n_predict=2 lr=1.2966 bs=262144 train_time:605230ms step_avg:602.22ms
step:1006/1775 n_predict=2 lr=1.2949 bs=262144 train_time:606011ms step_avg:602.40ms
step:1007/1775 n_predict=2 lr=1.2933 bs=262144 train_time:606786ms step_avg:602.57ms
step:1008/1775 n_predict=2 lr=1.2917 bs=262144 train_time:607572ms step_avg:602.75ms
step:1009/1775 n_predict=2 lr=1.2900 bs=262144 train_time:608354ms step_avg:602.93ms
step:1010/1775 n_predict=2 lr=1.2884 bs=262144 train_time:609132ms step_avg:603.10ms
step:1011/1775 n_predict=2 lr=1.2867 bs=262144 train_time:609911ms step_avg:603.27ms
step:1012/1775 n_predict=2 lr=1.2851 bs=262144 train_time:610688ms step_avg:603.45ms
step:1013/1775 n_predict=2 lr=1.2835 bs=262144 train_time:611462ms step_avg:603.62ms
step:1014/1775 n_predict=2 lr=1.2818 bs=262144 train_time:612242ms step_avg:603.79ms
step:1015/1775 n_predict=2 lr=1.2802 bs=262144 train_time:613017ms step_avg:603.96ms
step:1016/1775 n_predict=2 lr=1.2786 bs=262144 train_time:613799ms step_avg:604.13ms
step:1017/1775 n_predict=2 lr=1.2769 bs=262144 train_time:614580ms step_avg:604.31ms
step:1018/1775 n_predict=2 lr=1.2753 bs=262144 train_time:615360ms step_avg:604.48ms
step:1019/1775 n_predict=2 lr=1.2736 bs=262144 train_time:616137ms step_avg:604.65ms
step:1020/1775 n_predict=2 lr=1.2720 bs=262144 train_time:616916ms step_avg:604.82ms
step:1021/1775 n_predict=2 lr=1.2704 bs=262144 train_time:617697ms step_avg:604.99ms
step:1022/1775 n_predict=2 lr=1.2687 bs=262144 train_time:618479ms step_avg:605.17ms
step:1023/1775 n_predict=2 lr=1.2671 bs=262144 train_time:619260ms step_avg:605.34ms
step:1024/1775 n_predict=2 lr=1.2655 bs=262144 train_time:620040ms step_avg:605.51ms
step:1025/1775 n_predict=2 lr=1.2638 bs=262144 train_time:620819ms step_avg:605.68ms
step:1026/1775 n_predict=2 lr=1.2622 bs=262144 train_time:621608ms step_avg:605.86ms
step:1027/1775 n_predict=2 lr=1.2606 bs=262144 train_time:622387ms step_avg:606.02ms
step:1028/1775 n_predict=2 lr=1.2589 bs=262144 train_time:623183ms step_avg:606.21ms
step:1029/1775 n_predict=2 lr=1.2573 bs=262144 train_time:623951ms step_avg:606.37ms
step:1030/1775 n_predict=2 lr=1.2556 bs=262144 train_time:624727ms step_avg:606.53ms
step:1031/1775 n_predict=2 lr=1.2540 bs=262144 train_time:625510ms step_avg:606.70ms
step:1032/1775 n_predict=2 lr=1.2524 bs=262144 train_time:626289ms step_avg:606.87ms
step:1033/1775 n_predict=2 lr=1.2507 bs=262144 train_time:627067ms step_avg:607.03ms
step:1034/1775 n_predict=2 lr=1.2491 bs=262144 train_time:627849ms step_avg:607.20ms
step:1035/1775 n_predict=2 lr=1.2475 bs=262144 train_time:628626ms step_avg:607.37ms
step:1036/1775 n_predict=2 lr=1.2458 bs=262144 train_time:629415ms step_avg:607.54ms
step:1037/1775 n_predict=2 lr=1.2442 bs=262144 train_time:630194ms step_avg:607.71ms
step:1038/1775 n_predict=2 lr=1.2425 bs=262144 train_time:630974ms step_avg:607.87ms
step:1039/1775 n_predict=2 lr=1.2409 bs=262144 train_time:631756ms step_avg:608.04ms
step:1040/1775 n_predict=2 lr=1.2393 bs=262144 train_time:632536ms step_avg:608.21ms
step:1041/1775 n_predict=2 lr=1.2376 bs=262144 train_time:633319ms step_avg:608.38ms
step:1042/1775 n_predict=2 lr=1.2360 bs=262144 train_time:634112ms step_avg:608.55ms
step:1043/1775 n_predict=2 lr=1.2344 bs=262144 train_time:634893ms step_avg:608.72ms
step:1044/1775 n_predict=2 lr=1.2327 bs=262144 train_time:635672ms step_avg:608.88ms
step:1045/1775 n_predict=2 lr=1.2311 bs=262144 train_time:636454ms step_avg:609.05ms
step:1046/1775 n_predict=2 lr=1.2295 bs=262144 train_time:637230ms step_avg:609.21ms
step:1047/1775 n_predict=2 lr=1.2278 bs=262144 train_time:638015ms step_avg:609.37ms
step:1048/1775 n_predict=2 lr=1.2262 bs=262144 train_time:638814ms step_avg:609.56ms
step:1049/1775 n_predict=2 lr=1.2245 bs=262144 train_time:639581ms step_avg:609.71ms
step:1050/1775 n_predict=2 lr=1.2229 bs=262144 train_time:640364ms step_avg:609.87ms
step:1051/1775 n_predict=2 lr=1.2213 bs=262144 train_time:641146ms step_avg:610.03ms
step:1052/1775 n_predict=2 lr=1.2196 bs=262144 train_time:641925ms step_avg:610.19ms
step:1053/1775 n_predict=2 lr=1.2180 bs=262144 train_time:642712ms step_avg:610.36ms
step:1054/1775 n_predict=2 lr=1.2164 bs=262144 train_time:643495ms step_avg:610.53ms
step:1055/1775 n_predict=2 lr=1.2147 bs=262144 train_time:644274ms step_avg:610.69ms
step:1056/1775 n_predict=2 lr=1.2131 bs=262144 train_time:645062ms step_avg:610.85ms
step:1057/1775 n_predict=2 lr=1.2114 bs=262144 train_time:645836ms step_avg:611.01ms
step:1058/1775 n_predict=2 lr=1.2098 bs=262144 train_time:646623ms step_avg:611.17ms
step:1059/1775 n_predict=2 lr=1.2082 bs=262144 train_time:647409ms step_avg:611.34ms
step:1060/1775 n_predict=2 lr=1.2065 bs=262144 train_time:648195ms step_avg:611.50ms
step:1061/1775 n_predict=2 lr=1.2049 bs=262144 train_time:648973ms step_avg:611.66ms
step:1062/1775 n_predict=2 lr=1.2033 bs=262144 train_time:649755ms step_avg:611.82ms
step:1063/1775 n_predict=2 lr=1.2016 bs=262144 train_time:650534ms step_avg:611.98ms
step:1064/1775 n_predict=2 lr=1.2000 bs=262144 train_time:651323ms step_avg:612.15ms
step:1065/1775 n_predict=2 lr=1.1984 bs=262144 train_time:652105ms step_avg:612.30ms
step:1066/1775 n_predict=2 lr=1.1967 bs=262144 train_time:652893ms step_avg:612.47ms
step:1067/1775 n_predict=2 lr=1.1951 bs=262144 train_time:653676ms step_avg:612.63ms
step:1068/1775 n_predict=2 lr=1.1934 bs=262144 train_time:654459ms step_avg:612.79ms
step:1069/1775 n_predict=2 lr=1.1918 bs=262144 train_time:655241ms step_avg:612.95ms
step:1070/1775 n_predict=2 lr=1.1902 bs=262144 train_time:656024ms step_avg:613.11ms
step:1071/1775 n_predict=2 lr=1.1885 bs=262144 train_time:656815ms step_avg:613.27ms
step:1072/1775 n_predict=2 lr=1.1869 bs=262144 train_time:657596ms step_avg:613.43ms
step:1073/1775 n_predict=2 lr=1.1853 bs=262144 train_time:658374ms step_avg:613.58ms
step:1074/1775 n_predict=2 lr=1.1836 bs=262144 train_time:659154ms step_avg:613.74ms
step:1075/1775 n_predict=2 lr=1.1820 bs=262144 train_time:659931ms step_avg:613.89ms
step:1076/1775 n_predict=2 lr=1.1803 bs=262144 train_time:660719ms step_avg:614.05ms
step:1077/1775 n_predict=2 lr=1.1787 bs=262144 train_time:661511ms step_avg:614.22ms
step:1078/1775 n_predict=2 lr=1.1771 bs=262144 train_time:662293ms step_avg:614.37ms
step:1079/1775 n_predict=2 lr=1.1754 bs=262144 train_time:663071ms step_avg:614.52ms
step:1080/1775 n_predict=2 lr=1.1738 bs=262144 train_time:663856ms step_avg:614.68ms
step:1081/1775 n_predict=2 lr=1.1722 bs=262144 train_time:664636ms step_avg:614.83ms
step:1082/1775 n_predict=2 lr=1.1705 bs=262144 train_time:665423ms step_avg:614.99ms
step:1083/1775 n_predict=2 lr=1.1689 bs=262144 train_time:666208ms step_avg:615.15ms
step:1084/1775 n_predict=2 lr=1.1673 bs=262144 train_time:666992ms step_avg:615.31ms
step:1085/1775 n_predict=2 lr=1.1656 bs=262144 train_time:667771ms step_avg:615.46ms
step:1086/1775 n_predict=2 lr=1.1640 bs=262144 train_time:668557ms step_avg:615.61ms
step:1087/1775 n_predict=2 lr=1.1623 bs=262144 train_time:669334ms step_avg:615.76ms
step:1088/1775 n_predict=2 lr=1.1607 bs=262144 train_time:670124ms step_avg:615.92ms
step:1089/1775 n_predict=2 lr=1.1591 bs=262144 train_time:670915ms step_avg:616.08ms
step:1090/1775 n_predict=2 lr=1.1574 bs=262144 train_time:671698ms step_avg:616.24ms
step:1091/1775 n_predict=2 lr=1.1558 bs=262144 train_time:672475ms step_avg:616.38ms
step:1092/1775 n_predict=2 lr=1.1542 bs=262144 train_time:673259ms step_avg:616.54ms
step:1093/1775 n_predict=2 lr=1.1525 bs=262144 train_time:674040ms step_avg:616.69ms
step:1094/1775 n_predict=2 lr=1.1509 bs=262144 train_time:674823ms step_avg:616.84ms
step:1095/1775 n_predict=2 lr=1.1492 bs=262144 train_time:675614ms step_avg:617.00ms
step:1096/1775 n_predict=2 lr=1.1476 bs=262144 train_time:676406ms step_avg:617.16ms
step:1097/1775 n_predict=2 lr=1.1460 bs=262144 train_time:677181ms step_avg:617.30ms
step:1098/1775 n_predict=2 lr=1.1443 bs=262144 train_time:677967ms step_avg:617.46ms
step:1099/1775 n_predict=2 lr=1.1427 bs=262144 train_time:678749ms step_avg:617.61ms
step:1100/1775 n_predict=2 lr=1.1411 bs=262144 train_time:679528ms step_avg:617.75ms
step:1101/1775 n_predict=2 lr=1.1394 bs=262144 train_time:680313ms step_avg:617.90ms
step:1102/1775 n_predict=2 lr=1.1378 bs=262144 train_time:681102ms step_avg:618.06ms
step:1103/1775 n_predict=2 lr=1.1361 bs=262144 train_time:681880ms step_avg:618.21ms
step:1104/1775 n_predict=2 lr=1.1345 bs=262144 train_time:682664ms step_avg:618.36ms
step:1105/1775 n_predict=2 lr=1.1329 bs=262144 train_time:683451ms step_avg:618.51ms
step:1106/1775 n_predict=2 lr=1.1312 bs=262144 train_time:684231ms step_avg:618.65ms
step:1107/1775 n_predict=2 lr=1.1296 bs=262144 train_time:685014ms step_avg:618.80ms
step:1108/1775 n_predict=2 lr=1.1280 bs=262144 train_time:685802ms step_avg:618.95ms
step:1109/1775 n_predict=2 lr=1.1263 bs=262144 train_time:686583ms step_avg:619.10ms
step:1110/1775 n_predict=2 lr=1.1247 bs=262144 train_time:687368ms step_avg:619.25ms
step:1111/1775 n_predict=2 lr=1.1231 bs=262144 train_time:688150ms step_avg:619.40ms
step:1112/1775 n_predict=2 lr=1.1214 bs=262144 train_time:688930ms step_avg:619.54ms
step:1113/1775 n_predict=2 lr=1.1198 bs=262144 train_time:689719ms step_avg:619.69ms
step:1114/1775 n_predict=2 lr=1.1181 bs=262144 train_time:690502ms step_avg:619.84ms
step:1115/1775 n_predict=2 lr=1.1165 bs=262144 train_time:691287ms step_avg:619.99ms
step:1116/1775 n_predict=2 lr=1.1149 bs=262144 train_time:692071ms step_avg:620.14ms
step:1117/1775 n_predict=2 lr=1.1132 bs=262144 train_time:692850ms step_avg:620.28ms
step:1118/1775 n_predict=2 lr=1.1116 bs=262144 train_time:693630ms step_avg:620.42ms
step:1119/1775 n_predict=2 lr=1.1100 bs=262144 train_time:694419ms step_avg:620.57ms
step:1120/1775 n_predict=2 lr=1.1083 bs=262144 train_time:695207ms step_avg:620.72ms
step:1121/1775 n_predict=2 lr=1.1067 bs=262144 train_time:695992ms step_avg:620.87ms
step:1122/1775 n_predict=2 lr=1.1050 bs=262144 train_time:696774ms step_avg:621.01ms
step:1123/1775 n_predict=2 lr=1.1034 bs=262144 train_time:697556ms step_avg:621.15ms
step:1124/1775 n_predict=2 lr=1.1018 bs=262144 train_time:698337ms step_avg:621.30ms
step:1125/1775 n_predict=2 lr=1.1001 bs=262144 train_time:699124ms step_avg:621.44ms
step:1126/1775 n_predict=2 lr=1.0985 bs=262144 train_time:699913ms step_avg:621.59ms
step:1127/1775 n_predict=2 lr=1.0969 bs=262144 train_time:700696ms step_avg:621.74ms
step:1128/1775 n_predict=2 lr=1.0952 bs=262144 train_time:701480ms step_avg:621.88ms
step:1129/1775 n_predict=2 lr=1.0936 bs=262144 train_time:702261ms step_avg:622.02ms
step:1130/1775 n_predict=2 lr=1.0920 bs=262144 train_time:703048ms step_avg:622.17ms
step:1131/1775 n_predict=2 lr=1.0903 bs=262144 train_time:703827ms step_avg:622.30ms
step:1132/1775 n_predict=2 lr=1.0887 bs=262144 train_time:704620ms step_avg:622.46ms
step:1133/1775 n_predict=2 lr=1.0870 bs=262144 train_time:705404ms step_avg:622.60ms
step:1134/1775 n_predict=2 lr=1.0854 bs=262144 train_time:706192ms step_avg:622.74ms
step:1135/1775 n_predict=2 lr=1.0838 bs=262144 train_time:706971ms step_avg:622.88ms
step:1136/1775 n_predict=2 lr=1.0821 bs=262144 train_time:707754ms step_avg:623.02ms
step:1137/1775 n_predict=2 lr=1.0805 bs=262144 train_time:708536ms step_avg:623.16ms
step:1138/1775 n_predict=2 lr=1.0789 bs=262144 train_time:709323ms step_avg:623.31ms
step:1139/1775 n_predict=2 lr=1.0772 bs=262144 train_time:710110ms step_avg:623.45ms
step:1140/1775 n_predict=2 lr=1.0756 bs=262144 train_time:710893ms step_avg:623.59ms
step:1141/1775 n_predict=2 lr=1.0739 bs=262144 train_time:711672ms step_avg:623.73ms
step:1142/1775 n_predict=2 lr=1.0723 bs=262144 train_time:712458ms step_avg:623.87ms
step:1143/1775 n_predict=2 lr=1.0707 bs=262144 train_time:713236ms step_avg:624.00ms
step:1144/1775 n_predict=2 lr=1.0690 bs=262144 train_time:714022ms step_avg:624.15ms
step:1145/1775 n_predict=2 lr=1.0674 bs=262144 train_time:714808ms step_avg:624.29ms
step:1146/1775 n_predict=2 lr=1.0658 bs=262144 train_time:715590ms step_avg:624.42ms
step:1147/1775 n_predict=2 lr=1.0641 bs=262144 train_time:716369ms step_avg:624.56ms
step:1148/1775 n_predict=2 lr=1.0625 bs=262144 train_time:717148ms step_avg:624.69ms
step:1149/1775 n_predict=2 lr=1.0609 bs=262144 train_time:717924ms step_avg:624.83ms
step:1150/1775 n_predict=2 lr=1.0592 bs=262144 train_time:718721ms step_avg:624.97ms
step:1151/1775 n_predict=2 lr=1.0576 bs=262144 train_time:719495ms step_avg:625.10ms
step:1152/1775 n_predict=2 lr=1.0559 bs=262144 train_time:720280ms step_avg:625.24ms
step:1153/1775 n_predict=2 lr=1.0543 bs=262144 train_time:721060ms step_avg:625.38ms
step:1154/1775 n_predict=2 lr=1.0527 bs=262144 train_time:721854ms step_avg:625.52ms
step:1155/1775 n_predict=2 lr=1.0510 bs=262144 train_time:722627ms step_avg:625.65ms
step:1156/1775 n_predict=2 lr=1.0494 bs=262144 train_time:723421ms step_avg:625.80ms
step:1157/1775 n_predict=2 lr=1.0478 bs=262144 train_time:724198ms step_avg:625.93ms
step:1158/1775 n_predict=1 lr=1.1860 bs=393216 train_time:746781ms step_avg:644.89ms
step:1159/1775 n_predict=1 lr=1.1842 bs=393216 train_time:768879ms step_avg:663.40ms
step:1160/1775 n_predict=1 lr=1.1823 bs=393216 train_time:770574ms step_avg:664.29ms
step:1161/1775 n_predict=1 lr=1.1804 bs=393216 train_time:772180ms step_avg:665.10ms
step:1162/1775 n_predict=1 lr=1.1785 bs=393216 train_time:773882ms step_avg:665.99ms
step:1163/1775 n_predict=1 lr=1.1766 bs=393216 train_time:775548ms step_avg:666.85ms
step:1164/1775 n_predict=1 lr=1.1748 bs=393216 train_time:777185ms step_avg:667.68ms
step:1165/1775 n_predict=1 lr=1.1729 bs=393216 train_time:778879ms step_avg:668.57ms
step:1166/1775 n_predict=1 lr=1.1710 bs=393216 train_time:780741ms step_avg:669.59ms
step:1167/1775 n_predict=1 lr=1.1691 bs=393216 train_time:782354ms step_avg:670.40ms
step:1168/1775 n_predict=1 lr=1.1673 bs=393216 train_time:784051ms step_avg:671.28ms
step:1169/1775 n_predict=1 lr=1.1654 bs=393216 train_time:785678ms step_avg:672.09ms
step:1170/1775 n_predict=1 lr=1.1635 bs=393216 train_time:787283ms step_avg:672.89ms
step:1171/1775 n_predict=1 lr=1.1616 bs=393216 train_time:788878ms step_avg:673.68ms
step:1172/1775 n_predict=1 lr=1.1597 bs=393216 train_time:790481ms step_avg:674.47ms
step:1173/1775 n_predict=1 lr=1.1579 bs=393216 train_time:792171ms step_avg:675.34ms
step:1174/1775 n_predict=1 lr=1.1560 bs=393216 train_time:793882ms step_avg:676.22ms
step:1175/1775 n_predict=1 lr=1.1541 bs=393216 train_time:795657ms step_avg:677.15ms
step:1176/1775 n_predict=1 lr=1.1522 bs=393216 train_time:797274ms step_avg:677.95ms
step:1177/1775 n_predict=1 lr=1.1503 bs=393216 train_time:798846ms step_avg:678.71ms
step:1178/1775 n_predict=1 lr=1.1485 bs=393216 train_time:800582ms step_avg:679.61ms
step:1179/1775 n_predict=1 lr=1.1466 bs=393216 train_time:802178ms step_avg:680.39ms
step:1180/1775 n_predict=1 lr=1.1447 bs=393216 train_time:803850ms step_avg:681.23ms
step:1181/1775 n_predict=1 lr=1.1428 bs=393216 train_time:805576ms step_avg:682.11ms
step:1182/1775 n_predict=1 lr=1.1409 bs=393216 train_time:807282ms step_avg:682.98ms
step:1183/1775 n_predict=1 lr=1.1391 bs=393216 train_time:808949ms step_avg:683.81ms
step:1184/1775 n_predict=1 lr=1.1372 bs=393216 train_time:810581ms step_avg:684.61ms
step:1185/1775 n_predict=1 lr=1.1353 bs=393216 train_time:812179ms step_avg:685.38ms
step:1186/1775 n_predict=1 lr=1.1334 bs=393216 train_time:813778ms step_avg:686.15ms
step:1187/1775 n_predict=1 lr=1.1316 bs=393216 train_time:815448ms step_avg:686.98ms
step:1188/1775 n_predict=1 lr=1.1297 bs=393216 train_time:817085ms step_avg:687.78ms
step:1189/1775 n_predict=1 lr=1.1278 bs=393216 train_time:818783ms step_avg:688.63ms
step:1190/1775 n_predict=1 lr=1.1259 bs=393216 train_time:820377ms step_avg:689.39ms
step:1191/1775 n_predict=1 lr=1.1240 bs=393216 train_time:822080ms step_avg:690.24ms
step:1192/1775 n_predict=1 lr=1.1222 bs=393216 train_time:823751ms step_avg:691.07ms
step:1193/1775 n_predict=1 lr=1.1203 bs=393216 train_time:825555ms step_avg:692.00ms
step:1194/1775 n_predict=1 lr=1.1184 bs=393216 train_time:827270ms step_avg:692.86ms
step:1195/1775 n_predict=1 lr=1.1165 bs=393216 train_time:828868ms step_avg:693.61ms
step:1196/1775 n_predict=1 lr=1.1146 bs=393216 train_time:830668ms step_avg:694.54ms
step:1197/1775 n_predict=1 lr=1.1128 bs=393216 train_time:832280ms step_avg:695.30ms
step:1198/1775 n_predict=1 lr=1.1109 bs=393216 train_time:833934ms step_avg:696.11ms
step:1199/1775 n_predict=1 lr=1.1090 bs=393216 train_time:835662ms step_avg:696.97ms
step:1200/1775 n_predict=1 lr=1.1071 bs=393216 train_time:837382ms step_avg:697.82ms
step:1201/1775 n_predict=1 lr=1.1052 bs=393216 train_time:839079ms step_avg:698.65ms
step:1202/1775 n_predict=1 lr=1.1034 bs=393216 train_time:840834ms step_avg:699.53ms
step:1203/1775 n_predict=1 lr=1.1015 bs=393216 train_time:842532ms step_avg:700.36ms
step:1204/1775 n_predict=1 lr=1.0996 bs=393216 train_time:844253ms step_avg:701.21ms
step:1205/1775 n_predict=1 lr=1.0977 bs=393216 train_time:845979ms step_avg:702.06ms
step:1206/1775 n_predict=1 lr=1.0959 bs=393216 train_time:847582ms step_avg:702.80ms
step:1207/1775 n_predict=1 lr=1.0940 bs=393216 train_time:849471ms step_avg:703.79ms
step:1208/1775 n_predict=1 lr=1.0921 bs=393216 train_time:851182ms step_avg:704.62ms
step:1209/1775 n_predict=1 lr=1.0902 bs=393216 train_time:852862ms step_avg:705.43ms
step:1210/1775 n_predict=1 lr=1.0883 bs=393216 train_time:854581ms step_avg:706.27ms
step:1211/1775 n_predict=1 lr=1.0865 bs=393216 train_time:856275ms step_avg:707.08ms
step:1212/1775 n_predict=1 lr=1.0846 bs=393216 train_time:857934ms step_avg:707.87ms
step:1213/1775 n_predict=1 lr=1.0827 bs=393216 train_time:859568ms step_avg:708.63ms
step:1214/1775 n_predict=1 lr=1.0808 bs=393216 train_time:861181ms step_avg:709.37ms
step:1215/1775 n_predict=1 lr=1.0789 bs=393216 train_time:862879ms step_avg:710.19ms
step:1216/1775 n_predict=1 lr=1.0771 bs=393216 train_time:864567ms step_avg:710.99ms
step:1217/1775 n_predict=1 lr=1.0752 bs=393216 train_time:866179ms step_avg:711.73ms
step:1218/1775 n_predict=1 lr=1.0733 bs=393216 train_time:867883ms step_avg:712.55ms
step:1219/1775 n_predict=1 lr=1.0714 bs=393216 train_time:869579ms step_avg:713.35ms
step:1220/1775 n_predict=1 lr=1.0695 bs=393216 train_time:871281ms step_avg:714.16ms
step:1221/1775 n_predict=1 lr=1.0677 bs=393216 train_time:872980ms step_avg:714.97ms
step:1222/1775 n_predict=1 lr=1.0658 bs=393216 train_time:874588ms step_avg:715.70ms
step:1223/1775 n_predict=1 lr=1.0639 bs=393216 train_time:876276ms step_avg:716.50ms
step:1224/1775 n_predict=1 lr=1.0620 bs=393216 train_time:877849ms step_avg:717.20ms
step:1225/1775 n_predict=1 lr=1.0601 bs=393216 train_time:879566ms step_avg:718.01ms
step:1226/1775 n_predict=1 lr=1.0583 bs=393216 train_time:881273ms step_avg:718.82ms
step:1227/1775 n_predict=1 lr=1.0564 bs=393216 train_time:882981ms step_avg:719.63ms
step:1228/1775 n_predict=1 lr=1.0545 bs=393216 train_time:884779ms step_avg:720.50ms
step:1229/1775 n_predict=1 lr=1.0526 bs=393216 train_time:886478ms step_avg:721.30ms
step:1230/1775 n_predict=1 lr=1.0508 bs=393216 train_time:888183ms step_avg:722.10ms
step:1231/1775 n_predict=1 lr=1.0489 bs=393216 train_time:889878ms step_avg:722.89ms
step:1232/1775 n_predict=1 lr=1.0470 bs=393216 train_time:891581ms step_avg:723.69ms
step:1233/1775 n_predict=1 lr=1.0451 bs=393216 train_time:893179ms step_avg:724.40ms
step:1234/1775 n_predict=1 lr=1.0432 bs=393216 train_time:894950ms step_avg:725.24ms
step:1235/1775 n_predict=1 lr=1.0414 bs=393216 train_time:896576ms step_avg:725.97ms
step:1236/1775 n_predict=1 lr=1.0395 bs=393216 train_time:898282ms step_avg:726.77ms
step:1237/1775 n_predict=1 lr=1.0376 bs=393216 train_time:899980ms step_avg:727.55ms
step:1238/1775 n_predict=1 lr=1.0357 bs=393216 train_time:901656ms step_avg:728.32ms
step:1239/1775 n_predict=1 lr=1.0338 bs=393216 train_time:903338ms step_avg:729.09ms
step:1240/1775 n_predict=1 lr=1.0320 bs=393216 train_time:905082ms step_avg:729.90ms
step:1241/1775 n_predict=1 lr=1.0301 bs=393216 train_time:906754ms step_avg:730.66ms
step:1242/1775 n_predict=1 lr=1.0282 bs=393216 train_time:908381ms step_avg:731.39ms
step:1243/1775 n_predict=1 lr=1.0263 bs=393216 train_time:909979ms step_avg:732.08ms
step:1244/1775 n_predict=1 lr=1.0244 bs=393216 train_time:911750ms step_avg:732.92ms
step:1245/1775 n_predict=1 lr=1.0226 bs=393216 train_time:913382ms step_avg:733.64ms
step:1246/1775 n_predict=1 lr=1.0207 bs=393216 train_time:915089ms step_avg:734.42ms
step:1247/1775 n_predict=1 lr=1.0188 bs=393216 train_time:916779ms step_avg:735.19ms
step:1248/1775 n_predict=1 lr=1.0169 bs=393216 train_time:918372ms step_avg:735.88ms
step:1249/1775 n_predict=1 lr=1.0151 bs=393216 train_time:919980ms step_avg:736.57ms
step:1250/1775 n_predict=1 lr=1.0132 bs=393216 train_time:921671ms step_avg:737.34ms
step:1250/1775 lr=1.0113 bs=393216 n_predict=1 val_loss:3.5026 val_malbo_loss:3.5286 train_time:921751ms step_avg:737.40ms
step:1251/1775 n_predict=1 lr=1.0113 bs=393216 train_time:923436ms step_avg:738.16ms
step:1252/1775 n_predict=1 lr=1.0094 bs=393216 train_time:925040ms step_avg:738.85ms
step:1253/1775 n_predict=1 lr=1.0075 bs=393216 train_time:926707ms step_avg:739.59ms
step:1254/1775 n_predict=1 lr=1.0057 bs=393216 train_time:928399ms step_avg:740.35ms
step:1255/1775 n_predict=1 lr=1.0038 bs=393216 train_time:930036ms step_avg:741.06ms
step:1256/1775 n_predict=1 lr=1.0019 bs=393216 train_time:931708ms step_avg:741.81ms
step:1257/1775 n_predict=1 lr=1.0000 bs=393216 train_time:933436ms step_avg:742.59ms
step:1258/1775 n_predict=1 lr=0.9981 bs=393216 train_time:935132ms step_avg:743.35ms
step:1259/1775 n_predict=1 lr=0.9963 bs=393216 train_time:936825ms step_avg:744.10ms
step:1260/1775 n_predict=1 lr=0.9944 bs=393216 train_time:938538ms step_avg:744.87ms
step:1261/1775 n_predict=1 lr=0.9925 bs=393216 train_time:940212ms step_avg:745.61ms
step:1262/1775 n_predict=1 lr=0.9906 bs=393216 train_time:941938ms step_avg:746.39ms
step:1263/1775 n_predict=1 lr=0.9887 bs=393216 train_time:943636ms step_avg:747.14ms
step:1264/1775 n_predict=1 lr=0.9869 bs=393216 train_time:945239ms step_avg:747.82ms
step:1265/1775 n_predict=1 lr=0.9850 bs=393216 train_time:946876ms step_avg:748.52ms
step:1266/1775 n_predict=1 lr=0.9831 bs=393216 train_time:948542ms step_avg:749.24ms
step:1267/1775 n_predict=1 lr=0.9812 bs=393216 train_time:950240ms step_avg:749.99ms
step:1268/1775 n_predict=1 lr=0.9794 bs=393216 train_time:951942ms step_avg:750.74ms
step:1269/1775 n_predict=1 lr=0.9775 bs=393216 train_time:953605ms step_avg:751.46ms
step:1270/1775 n_predict=1 lr=0.9756 bs=393216 train_time:955235ms step_avg:752.15ms
step:1271/1775 n_predict=1 lr=0.9737 bs=393216 train_time:957003ms step_avg:752.95ms
step:1272/1775 n_predict=1 lr=0.9718 bs=393216 train_time:958739ms step_avg:753.73ms
step:1273/1775 n_predict=1 lr=0.9700 bs=393216 train_time:960336ms step_avg:754.39ms
step:1274/1775 n_predict=1 lr=0.9681 bs=393216 train_time:962039ms step_avg:755.13ms
step:1275/1775 n_predict=1 lr=0.9662 bs=393216 train_time:963737ms step_avg:755.87ms
step:1276/1775 n_predict=1 lr=0.9643 bs=393216 train_time:965336ms step_avg:756.53ms
step:1277/1775 n_predict=1 lr=0.9624 bs=393216 train_time:967039ms step_avg:757.27ms
step:1278/1775 n_predict=1 lr=0.9606 bs=393216 train_time:968798ms step_avg:758.06ms
step:1279/1775 n_predict=1 lr=0.9587 bs=393216 train_time:970436ms step_avg:758.75ms
step:1280/1775 n_predict=1 lr=0.9568 bs=393216 train_time:972139ms step_avg:759.48ms
step:1281/1775 n_predict=1 lr=0.9549 bs=393216 train_time:973734ms step_avg:760.14ms
step:1282/1775 n_predict=1 lr=0.9530 bs=393216 train_time:975327ms step_avg:760.79ms
step:1283/1775 n_predict=1 lr=0.9512 bs=393216 train_time:976997ms step_avg:761.49ms
step:1284/1775 n_predict=1 lr=0.9493 bs=393216 train_time:978621ms step_avg:762.17ms
step:1285/1775 n_predict=1 lr=0.9474 bs=393216 train_time:980241ms step_avg:762.83ms
step:1286/1775 n_predict=1 lr=0.9455 bs=393216 train_time:982031ms step_avg:763.63ms
step:1287/1775 n_predict=1 lr=0.9437 bs=393216 train_time:983639ms step_avg:764.29ms
step:1288/1775 n_predict=1 lr=0.9418 bs=393216 train_time:985237ms step_avg:764.94ms
step:1289/1775 n_predict=1 lr=0.9399 bs=393216 train_time:986836ms step_avg:765.58ms
step:1290/1775 n_predict=1 lr=0.9380 bs=393216 train_time:988614ms step_avg:766.37ms
step:1291/1775 n_predict=1 lr=0.9361 bs=393216 train_time:990205ms step_avg:767.01ms
step:1292/1775 n_predict=1 lr=0.9343 bs=393216 train_time:991839ms step_avg:767.68ms
step:1293/1775 n_predict=1 lr=0.9324 bs=393216 train_time:993498ms step_avg:768.37ms
step:1294/1775 n_predict=1 lr=0.9305 bs=393216 train_time:995191ms step_avg:769.08ms
step:1295/1775 n_predict=1 lr=0.9286 bs=393216 train_time:996836ms step_avg:769.76ms
step:1296/1775 n_predict=1 lr=0.9267 bs=393216 train_time:998538ms step_avg:770.48ms
step:1297/1775 n_predict=1 lr=0.9249 bs=393216 train_time:1000136ms step_avg:771.11ms
step:1298/1775 n_predict=1 lr=0.9230 bs=393216 train_time:1001841ms step_avg:771.83ms
step:1299/1775 n_predict=1 lr=0.9211 bs=393216 train_time:1003527ms step_avg:772.54ms
step:1300/1775 n_predict=1 lr=0.9192 bs=393216 train_time:1005106ms step_avg:773.16ms
step:1301/1775 n_predict=1 lr=0.9173 bs=393216 train_time:1006737ms step_avg:773.82ms
step:1302/1775 n_predict=1 lr=0.9155 bs=393216 train_time:1008527ms step_avg:774.60ms
step:1303/1775 n_predict=1 lr=0.9136 bs=393216 train_time:1010237ms step_avg:775.32ms
step:1304/1775 n_predict=1 lr=0.9117 bs=393216 train_time:1011939ms step_avg:776.03ms
step:1305/1775 n_predict=1 lr=0.9098 bs=393216 train_time:1013607ms step_avg:776.71ms
step:1306/1775 n_predict=1 lr=0.9080 bs=393216 train_time:1015238ms step_avg:777.36ms
step:1307/1775 n_predict=1 lr=0.9061 bs=393216 train_time:1016936ms step_avg:778.07ms
step:1308/1775 n_predict=1 lr=0.9042 bs=393216 train_time:1018594ms step_avg:778.74ms
step:1309/1775 n_predict=1 lr=0.9023 bs=393216 train_time:1020236ms step_avg:779.40ms
step:1310/1775 n_predict=1 lr=0.9004 bs=393216 train_time:1021921ms step_avg:780.09ms
step:1311/1775 n_predict=1 lr=0.8986 bs=393216 train_time:1023622ms step_avg:780.80ms
step:1312/1775 n_predict=1 lr=0.8967 bs=393216 train_time:1025338ms step_avg:781.51ms
step:1313/1775 n_predict=1 lr=0.8948 bs=393216 train_time:1027036ms step_avg:782.21ms
step:1314/1775 n_predict=1 lr=0.8929 bs=393216 train_time:1028739ms step_avg:782.91ms
step:1315/1775 n_predict=1 lr=0.8910 bs=393216 train_time:1030336ms step_avg:783.53ms
step:1316/1775 n_predict=1 lr=0.8892 bs=393216 train_time:1032105ms step_avg:784.27ms
step:1317/1775 n_predict=1 lr=0.8873 bs=393216 train_time:1033890ms step_avg:785.03ms
step:1318/1775 n_predict=1 lr=0.8854 bs=393216 train_time:1035639ms step_avg:785.77ms
step:1319/1775 n_predict=1 lr=0.8835 bs=393216 train_time:1037305ms step_avg:786.43ms
step:1320/1775 n_predict=1 lr=0.8816 bs=393216 train_time:1038939ms step_avg:787.08ms
step:1321/1775 n_predict=1 lr=0.8798 bs=393216 train_time:1040636ms step_avg:787.76ms
step:1322/1775 n_predict=1 lr=0.8779 bs=393216 train_time:1042409ms step_avg:788.51ms
step:1323/1775 n_predict=1 lr=0.8760 bs=393216 train_time:1044026ms step_avg:789.14ms
step:1324/1775 n_predict=1 lr=0.8741 bs=393216 train_time:1045631ms step_avg:789.75ms
step:1325/1775 n_predict=1 lr=0.8723 bs=393216 train_time:1047297ms step_avg:790.41ms
step:1326/1775 n_predict=1 lr=0.8704 bs=393216 train_time:1048937ms step_avg:791.05ms
step:1327/1775 n_predict=1 lr=0.8685 bs=393216 train_time:1050636ms step_avg:791.74ms
step:1328/1775 n_predict=1 lr=0.8666 bs=393216 train_time:1052339ms step_avg:792.42ms
step:1329/1775 n_predict=1 lr=0.8647 bs=393216 train_time:1054110ms step_avg:793.16ms
step:1330/1775 n_predict=1 lr=0.8629 bs=393216 train_time:1055832ms step_avg:793.86ms
step:1331/1775 n_predict=1 lr=0.8610 bs=393216 train_time:1057437ms step_avg:794.47ms
step:1332/1775 n_predict=1 lr=0.8591 bs=393216 train_time:1059033ms step_avg:795.07ms
step:1333/1775 n_predict=1 lr=0.8572 bs=393216 train_time:1060635ms step_avg:795.68ms
step:1334/1775 n_predict=1 lr=0.8553 bs=393216 train_time:1062332ms step_avg:796.35ms
step:1335/1775 n_predict=1 lr=0.8535 bs=393216 train_time:1064111ms step_avg:797.09ms
step:1336/1775 n_predict=1 lr=0.8516 bs=393216 train_time:1065738ms step_avg:797.71ms
step:1337/1775 n_predict=1 lr=0.8497 bs=393216 train_time:1067436ms step_avg:798.38ms
step:1338/1775 n_predict=1 lr=0.8478 bs=393216 train_time:1069141ms step_avg:799.06ms
step:1339/1775 n_predict=1 lr=0.8459 bs=393216 train_time:1070834ms step_avg:799.73ms
step:1340/1775 n_predict=1 lr=0.8441 bs=393216 train_time:1072538ms step_avg:800.40ms
step:1341/1775 n_predict=1 lr=0.8422 bs=393216 train_time:1074206ms step_avg:801.05ms
step:1342/1775 n_predict=1 lr=0.8403 bs=393216 train_time:1075839ms step_avg:801.67ms
step:1343/1775 n_predict=1 lr=0.8384 bs=393216 train_time:1077603ms step_avg:802.39ms
step:1344/1775 n_predict=1 lr=0.8366 bs=393216 train_time:1079227ms step_avg:803.00ms
step:1345/1775 n_predict=1 lr=0.8347 bs=393216 train_time:1080836ms step_avg:803.60ms
step:1346/1775 n_predict=1 lr=0.8328 bs=393216 train_time:1082539ms step_avg:804.26ms
step:1347/1775 n_predict=1 lr=0.8309 bs=393216 train_time:1084237ms step_avg:804.93ms
step:1348/1775 n_predict=1 lr=0.8290 bs=393216 train_time:1085832ms step_avg:805.51ms
step:1349/1775 n_predict=1 lr=0.8272 bs=393216 train_time:1087506ms step_avg:806.16ms
step:1350/1775 n_predict=1 lr=0.8253 bs=393216 train_time:1089207ms step_avg:806.82ms
step:1351/1775 n_predict=1 lr=0.8234 bs=393216 train_time:1090935ms step_avg:807.50ms
step:1352/1775 n_predict=1 lr=0.8215 bs=393216 train_time:1092640ms step_avg:808.17ms
step:1353/1775 n_predict=1 lr=0.8196 bs=393216 train_time:1094337ms step_avg:808.82ms
step:1354/1775 n_predict=1 lr=0.8178 bs=393216 train_time:1096008ms step_avg:809.46ms
step:1355/1775 n_predict=1 lr=0.8159 bs=393216 train_time:1097720ms step_avg:810.13ms
step:1356/1775 n_predict=1 lr=0.8140 bs=393216 train_time:1099436ms step_avg:810.79ms
step:1357/1775 n_predict=1 lr=0.8121 bs=393216 train_time:1101137ms step_avg:811.45ms
step:1358/1775 n_predict=1 lr=0.8102 bs=393216 train_time:1102841ms step_avg:812.11ms
step:1359/1775 n_predict=1 lr=0.8084 bs=393216 train_time:1104436ms step_avg:812.68ms
step:1360/1775 n_predict=1 lr=0.8065 bs=393216 train_time:1106108ms step_avg:813.31ms
step:1361/1775 n_predict=1 lr=0.8046 bs=393216 train_time:1107736ms step_avg:813.91ms
step:1362/1775 n_predict=1 lr=0.8027 bs=393216 train_time:1109492ms step_avg:814.60ms
step:1363/1775 n_predict=1 lr=0.8009 bs=393216 train_time:1111205ms step_avg:815.26ms
step:1364/1775 n_predict=1 lr=0.7990 bs=393216 train_time:1112808ms step_avg:815.84ms
step:1365/1775 n_predict=1 lr=0.7971 bs=393216 train_time:1114437ms step_avg:816.44ms
step:1366/1775 n_predict=1 lr=0.7952 bs=393216 train_time:1116036ms step_avg:817.01ms
step:1367/1775 n_predict=1 lr=0.7933 bs=393216 train_time:1117637ms step_avg:817.58ms
step:1368/1775 n_predict=1 lr=0.7915 bs=393216 train_time:1119228ms step_avg:818.15ms
step:1369/1775 n_predict=1 lr=0.7896 bs=393216 train_time:1120936ms step_avg:818.80ms
step:1370/1775 n_predict=1 lr=0.7877 bs=393216 train_time:1122638ms step_avg:819.44ms
step:1371/1775 n_predict=1 lr=0.7858 bs=393216 train_time:1124337ms step_avg:820.09ms
step:1372/1775 n_predict=1 lr=0.7839 bs=393216 train_time:1126039ms step_avg:820.73ms
step:1373/1775 n_predict=1 lr=0.7821 bs=393216 train_time:1127635ms step_avg:821.29ms
step:1374/1775 n_predict=1 lr=0.7802 bs=393216 train_time:1129342ms step_avg:821.94ms
step:1375/1775 n_predict=1 lr=0.7783 bs=393216 train_time:1131037ms step_avg:822.57ms
step:1376/1775 n_predict=1 lr=0.7764 bs=393216 train_time:1132621ms step_avg:823.13ms
step:1377/1775 n_predict=1 lr=0.7745 bs=393216 train_time:1134288ms step_avg:823.74ms
step:1378/1775 n_predict=1 lr=0.7727 bs=393216 train_time:1136038ms step_avg:824.41ms
step:1379/1775 n_predict=1 lr=0.7708 bs=393216 train_time:1137631ms step_avg:824.97ms
step:1380/1775 n_predict=1 lr=0.7689 bs=393216 train_time:1139339ms step_avg:825.61ms
step:1381/1775 n_predict=1 lr=0.7670 bs=393216 train_time:1140936ms step_avg:826.17ms
step:1382/1775 n_predict=1 lr=0.7652 bs=393216 train_time:1142710ms step_avg:826.85ms
step:1383/1775 n_predict=1 lr=0.7633 bs=393216 train_time:1144337ms step_avg:827.43ms
step:1384/1775 n_predict=1 lr=0.7614 bs=393216 train_time:1146040ms step_avg:828.06ms
step:1385/1775 n_predict=1 lr=0.7595 bs=393216 train_time:1147637ms step_avg:828.62ms
step:1386/1775 n_predict=1 lr=0.7576 bs=393216 train_time:1149338ms step_avg:829.25ms
step:1387/1775 n_predict=1 lr=0.7558 bs=393216 train_time:1151106ms step_avg:829.92ms
step:1388/1775 n_predict=1 lr=0.7539 bs=393216 train_time:1152808ms step_avg:830.55ms
step:1389/1775 n_predict=1 lr=0.7520 bs=393216 train_time:1154426ms step_avg:831.12ms
step:1390/1775 n_predict=1 lr=0.7501 bs=393216 train_time:1156108ms step_avg:831.73ms
step:1391/1775 n_predict=1 lr=0.7482 bs=393216 train_time:1157736ms step_avg:832.31ms
step:1392/1775 n_predict=1 lr=0.7464 bs=393216 train_time:1159339ms step_avg:832.86ms
step:1393/1775 n_predict=1 lr=0.7445 bs=393216 train_time:1161134ms step_avg:833.55ms
step:1394/1775 n_predict=1 lr=0.7426 bs=393216 train_time:1162821ms step_avg:834.16ms
step:1395/1775 n_predict=1 lr=0.7407 bs=393216 train_time:1164439ms step_avg:834.72ms
step:1396/1775 n_predict=1 lr=0.7388 bs=393216 train_time:1166007ms step_avg:835.25ms
step:1397/1775 n_predict=1 lr=0.7370 bs=393216 train_time:1167709ms step_avg:835.87ms
step:1398/1775 n_predict=1 lr=0.7351 bs=393216 train_time:1169437ms step_avg:836.51ms
step:1399/1775 n_predict=1 lr=0.7332 bs=393216 train_time:1171037ms step_avg:837.05ms
step:1400/1775 n_predict=1 lr=0.7313 bs=393216 train_time:1172706ms step_avg:837.65ms
step:1401/1775 n_predict=1 lr=0.7295 bs=393216 train_time:1174437ms step_avg:838.28ms
step:1402/1775 n_predict=1 lr=0.7276 bs=393216 train_time:1176039ms step_avg:838.83ms
step:1403/1775 n_predict=1 lr=0.7257 bs=393216 train_time:1177736ms step_avg:839.44ms
step:1404/1775 n_predict=1 lr=0.7238 bs=393216 train_time:1179342ms step_avg:839.99ms
step:1405/1775 n_predict=1 lr=0.7219 bs=393216 train_time:1181004ms step_avg:840.57ms
step:1406/1775 n_predict=1 lr=0.7201 bs=393216 train_time:1182727ms step_avg:841.20ms
step:1407/1775 n_predict=1 lr=0.7182 bs=393216 train_time:1184404ms step_avg:841.79ms
step:1408/1775 n_predict=1 lr=0.7163 bs=393216 train_time:1186127ms step_avg:842.42ms
step:1409/1775 n_predict=1 lr=0.7144 bs=393216 train_time:1187811ms step_avg:843.02ms
step:1410/1775 n_predict=1 lr=0.7125 bs=393216 train_time:1189720ms step_avg:843.77ms
step:1411/1775 n_predict=1 lr=0.7107 bs=393216 train_time:1191427ms step_avg:844.38ms
step:1412/1775 n_predict=1 lr=0.7088 bs=393216 train_time:1193106ms step_avg:844.98ms
step:1413/1775 n_predict=1 lr=0.7069 bs=393216 train_time:1194734ms step_avg:845.53ms
step:1414/1775 n_predict=1 lr=0.7050 bs=393216 train_time:1196336ms step_avg:846.07ms
step:1415/1775 n_predict=1 lr=0.7031 bs=393216 train_time:1198039ms step_avg:846.67ms
step:1416/1775 n_predict=1 lr=0.7013 bs=393216 train_time:1199638ms step_avg:847.20ms
step:1417/1775 n_predict=1 lr=0.6994 bs=393216 train_time:1201326ms step_avg:847.80ms
step:1418/1775 n_predict=1 lr=0.6975 bs=393216 train_time:1202941ms step_avg:848.34ms
step:1419/1775 n_predict=1 lr=0.6956 bs=393216 train_time:1204540ms step_avg:848.87ms
step:1420/1775 n_predict=1 lr=0.6938 bs=393216 train_time:1206210ms step_avg:849.44ms
step:1421/1775 n_predict=1 lr=0.6919 bs=393216 train_time:1207939ms step_avg:850.06ms
step:1422/1775 n_predict=1 lr=0.6900 bs=393216 train_time:1209539ms step_avg:850.59ms
step:1423/1775 n_predict=1 lr=0.6881 bs=393216 train_time:1211327ms step_avg:851.25ms
step:1424/1775 n_predict=1 lr=0.6862 bs=393216 train_time:1213038ms step_avg:851.85ms
step:1425/1775 n_predict=1 lr=0.6844 bs=393216 train_time:1214628ms step_avg:852.37ms
step:1426/1775 n_predict=1 lr=0.6825 bs=393216 train_time:1216338ms step_avg:852.97ms
step:1427/1775 n_predict=1 lr=0.6806 bs=393216 train_time:1217936ms step_avg:853.49ms
step:1428/1775 n_predict=1 lr=0.6787 bs=393216 train_time:1219538ms step_avg:854.02ms
step:1429/1775 n_predict=1 lr=0.6768 bs=393216 train_time:1221137ms step_avg:854.54ms
step:1430/1775 n_predict=1 lr=0.6750 bs=393216 train_time:1222741ms step_avg:855.06ms
step:1431/1775 n_predict=1 lr=0.6731 bs=393216 train_time:1224436ms step_avg:855.65ms
step:1432/1775 n_predict=1 lr=0.6712 bs=393216 train_time:1226006ms step_avg:856.15ms
step:1433/1775 n_predict=1 lr=0.6693 bs=393216 train_time:1227712ms step_avg:856.74ms
step:1434/1775 n_predict=1 lr=0.6674 bs=393216 train_time:1229435ms step_avg:857.35ms
step:1435/1775 n_predict=1 lr=0.6656 bs=393216 train_time:1231136ms step_avg:857.93ms
step:1436/1775 n_predict=1 lr=0.6637 bs=393216 train_time:1232707ms step_avg:858.43ms
step:1437/1775 n_predict=1 lr=0.6618 bs=393216 train_time:1234305ms step_avg:858.95ms
step:1438/1775 n_predict=1 lr=0.6599 bs=393216 train_time:1235936ms step_avg:859.48ms
step:1439/1775 n_predict=1 lr=0.6581 bs=393216 train_time:1237527ms step_avg:859.99ms
step:1440/1775 n_predict=1 lr=0.6562 bs=393216 train_time:1239234ms step_avg:860.58ms
step:1441/1775 n_predict=1 lr=0.6543 bs=393216 train_time:1241090ms step_avg:861.27ms
step:1442/1775 n_predict=1 lr=0.6524 bs=393216 train_time:1242792ms step_avg:861.85ms
step:1443/1775 n_predict=1 lr=0.6505 bs=393216 train_time:1244425ms step_avg:862.39ms
step:1444/1775 n_predict=1 lr=0.6487 bs=393216 train_time:1246129ms step_avg:862.97ms
step:1445/1775 n_predict=1 lr=0.6468 bs=393216 train_time:1248037ms step_avg:863.69ms
step:1446/1775 n_predict=1 lr=0.6449 bs=393216 train_time:1249738ms step_avg:864.27ms
step:1447/1775 n_predict=1 lr=0.6430 bs=393216 train_time:1251505ms step_avg:864.90ms
step:1448/1775 n_predict=1 lr=0.6411 bs=393216 train_time:1253321ms step_avg:865.55ms
step:1449/1775 n_predict=1 lr=0.6393 bs=393216 train_time:1255035ms step_avg:866.14ms
step:1450/1775 n_predict=1 lr=0.6374 bs=393216 train_time:1256829ms step_avg:866.78ms
step:1451/1775 n_predict=1 lr=0.6355 bs=393216 train_time:1258598ms step_avg:867.40ms
step:1452/1775 n_predict=1 lr=0.6336 bs=393216 train_time:1260440ms step_avg:868.07ms
step:1453/1775 n_predict=1 lr=0.6317 bs=393216 train_time:1262126ms step_avg:868.63ms
step:1454/1775 n_predict=1 lr=0.6299 bs=393216 train_time:1263942ms step_avg:869.29ms
step:1455/1775 n_predict=1 lr=0.6280 bs=393216 train_time:1265641ms step_avg:869.86ms
step:1456/1775 n_predict=1 lr=0.6261 bs=393216 train_time:1267426ms step_avg:870.48ms
step:1457/1775 n_predict=1 lr=0.6242 bs=393216 train_time:1269103ms step_avg:871.04ms
step:1458/1775 n_predict=1 lr=0.6224 bs=393216 train_time:1270836ms step_avg:871.63ms
step:1459/1775 n_predict=1 lr=0.6205 bs=393216 train_time:1272537ms step_avg:872.20ms
step:1460/1775 n_predict=1 lr=0.6186 bs=393216 train_time:1274337ms step_avg:872.83ms
step:1461/1775 n_predict=1 lr=0.6167 bs=393216 train_time:1276006ms step_avg:873.38ms
step:1462/1775 n_predict=1 lr=0.6148 bs=393216 train_time:1277739ms step_avg:873.97ms
step:1463/1775 n_predict=1 lr=0.6130 bs=393216 train_time:1279405ms step_avg:874.51ms
step:1464/1775 n_predict=1 lr=0.6111 bs=393216 train_time:1281041ms step_avg:875.03ms
step:1465/1775 n_predict=1 lr=0.6092 bs=393216 train_time:1282837ms step_avg:875.66ms
step:1466/1775 n_predict=1 lr=0.6073 bs=393216 train_time:1284439ms step_avg:876.15ms
step:1467/1775 n_predict=1 lr=0.6054 bs=393216 train_time:1286037ms step_avg:876.64ms
step:1468/1775 n_predict=1 lr=0.6036 bs=393216 train_time:1287642ms step_avg:877.14ms
step:1469/1775 n_predict=1 lr=0.6017 bs=393216 train_time:1289424ms step_avg:877.76ms
step:1470/1775 n_predict=1 lr=0.5998 bs=393216 train_time:1291039ms step_avg:878.26ms
step:1471/1775 n_predict=1 lr=0.5979 bs=393216 train_time:1292636ms step_avg:878.75ms
step:1472/1775 n_predict=1 lr=0.5960 bs=393216 train_time:1294238ms step_avg:879.24ms
step:1473/1775 n_predict=1 lr=0.5942 bs=393216 train_time:1296020ms step_avg:879.85ms
step:1474/1775 n_predict=1 lr=0.5923 bs=393216 train_time:1297638ms step_avg:880.35ms
step:1475/1775 n_predict=1 lr=0.5904 bs=393216 train_time:1299297ms step_avg:880.88ms
step:1476/1775 n_predict=1 lr=0.5885 bs=393216 train_time:1301028ms step_avg:881.46ms
step:1477/1775 n_predict=1 lr=0.5867 bs=393216 train_time:1302727ms step_avg:882.01ms
step:1478/1775 n_predict=1 lr=0.5848 bs=393216 train_time:1304338ms step_avg:882.50ms
step:1479/1775 n_predict=1 lr=0.5829 bs=393216 train_time:1306108ms step_avg:883.10ms
step:1480/1775 n_predict=1 lr=0.5810 bs=393216 train_time:1307739ms step_avg:883.61ms
step:1481/1775 n_predict=1 lr=0.5791 bs=393216 train_time:1309376ms step_avg:884.12ms
step:1482/1775 n_predict=1 lr=0.5773 bs=393216 train_time:1311037ms step_avg:884.64ms
step:1483/1775 n_predict=1 lr=0.5754 bs=393216 train_time:1312636ms step_avg:885.12ms
step:1484/1775 n_predict=1 lr=0.5735 bs=393216 train_time:1314338ms step_avg:885.67ms
step:1485/1775 n_predict=1 lr=0.5716 bs=393216 train_time:1316037ms step_avg:886.22ms
step:1486/1775 n_predict=1 lr=0.5697 bs=393216 train_time:1317637ms step_avg:886.70ms
step:1487/1775 n_predict=1 lr=0.5679 bs=393216 train_time:1319290ms step_avg:887.22ms
step:1488/1775 n_predict=1 lr=0.5660 bs=393216 train_time:1320891ms step_avg:887.70ms
step:1489/1775 n_predict=1 lr=0.5641 bs=393216 train_time:1322529ms step_avg:888.20ms
step:1490/1775 n_predict=1 lr=0.5622 bs=393216 train_time:1324126ms step_avg:888.68ms
step:1491/1775 n_predict=1 lr=0.5603 bs=393216 train_time:1325808ms step_avg:889.21ms
step:1492/1775 n_predict=1 lr=0.5585 bs=393216 train_time:1327442ms step_avg:889.71ms
step:1493/1775 n_predict=1 lr=0.5566 bs=393216 train_time:1329034ms step_avg:890.18ms
step:1494/1775 n_predict=1 lr=0.5547 bs=393216 train_time:1330708ms step_avg:890.70ms
step:1495/1775 n_predict=1 lr=0.5528 bs=393216 train_time:1332336ms step_avg:891.19ms
step:1496/1775 n_predict=1 lr=0.5510 bs=393216 train_time:1334030ms step_avg:891.73ms
step:1497/1775 n_predict=1 lr=0.5491 bs=393216 train_time:1335637ms step_avg:892.21ms
step:1498/1775 n_predict=1 lr=0.5472 bs=393216 train_time:1337439ms step_avg:892.82ms
step:1499/1775 n_predict=1 lr=0.5453 bs=393216 train_time:1339144ms step_avg:893.36ms
step:1500/1775 n_predict=1 lr=0.5434 bs=393216 train_time:1340827ms step_avg:893.88ms
step:1500/1775 lr=0.5416 bs=393216 n_predict=1 val_loss:3.3759 val_malbo_loss:3.4024 train_time:1340907ms step_avg:893.94ms
step:1501/1775 n_predict=1 lr=0.5416 bs=393216 train_time:1342488ms step_avg:894.40ms
step:1502/1775 n_predict=1 lr=0.5397 bs=393216 train_time:1344166ms step_avg:894.92ms
step:1503/1775 n_predict=1 lr=0.5378 bs=393216 train_time:1345795ms step_avg:895.41ms
step:1504/1775 n_predict=1 lr=0.5359 bs=393216 train_time:1347459ms step_avg:895.92ms
step:1505/1775 n_predict=1 lr=0.5340 bs=393216 train_time:1349094ms step_avg:896.41ms
step:1506/1775 n_predict=1 lr=0.5322 bs=393216 train_time:1350779ms step_avg:896.93ms
step:1507/1775 n_predict=1 lr=0.5303 bs=393216 train_time:1352394ms step_avg:897.41ms
step:1508/1775 n_predict=1 lr=0.5284 bs=393216 train_time:1354098ms step_avg:897.94ms
step:1509/1775 n_predict=1 lr=0.5265 bs=393216 train_time:1355894ms step_avg:898.54ms
step:1510/1775 n_predict=1 lr=0.5246 bs=393216 train_time:1357650ms step_avg:899.11ms
step:1511/1775 n_predict=1 lr=0.5228 bs=393216 train_time:1359294ms step_avg:899.60ms
step:1512/1775 n_predict=1 lr=0.5209 bs=393216 train_time:1361069ms step_avg:900.18ms
step:1513/1775 n_predict=1 lr=0.5190 bs=393216 train_time:1362695ms step_avg:900.66ms
step:1514/1775 n_predict=1 lr=0.5171 bs=393216 train_time:1364392ms step_avg:901.18ms
step:1515/1775 n_predict=1 lr=0.5153 bs=393216 train_time:1366064ms step_avg:901.69ms
step:1516/1775 n_predict=1 lr=0.5134 bs=393216 train_time:1367699ms step_avg:902.18ms
step:1517/1775 n_predict=1 lr=0.5115 bs=393216 train_time:1369389ms step_avg:902.70ms
step:1518/1775 n_predict=1 lr=0.5096 bs=393216 train_time:1371081ms step_avg:903.22ms
step:1519/1775 n_predict=1 lr=0.5077 bs=393216 train_time:1372847ms step_avg:903.78ms
step:1520/1775 n_predict=1 lr=0.5059 bs=393216 train_time:1374550ms step_avg:904.31ms
step:1521/1775 n_predict=1 lr=0.5040 bs=393216 train_time:1376193ms step_avg:904.79ms
step:1522/1775 n_predict=1 lr=0.5021 bs=393216 train_time:1377887ms step_avg:905.31ms
step:1523/1775 n_predict=1 lr=0.5002 bs=393216 train_time:1379647ms step_avg:905.87ms
step:1524/1775 n_predict=1 lr=0.4983 bs=393216 train_time:1381366ms step_avg:906.41ms
step:1525/1775 n_predict=1 lr=0.4965 bs=393216 train_time:1383000ms step_avg:906.89ms
step:1526/1775 n_predict=1 lr=0.4946 bs=393216 train_time:1384666ms step_avg:907.38ms
step:1527/1775 n_predict=1 lr=0.4927 bs=393216 train_time:1386295ms step_avg:907.86ms
step:1528/1775 n_predict=1 lr=0.4908 bs=393216 train_time:1387897ms step_avg:908.31ms
step:1529/1775 n_predict=1 lr=0.4889 bs=393216 train_time:1389563ms step_avg:908.80ms
step:1530/1775 n_predict=1 lr=0.4871 bs=393216 train_time:1391337ms step_avg:909.37ms
step:1531/1775 n_predict=1 lr=0.4852 bs=393216 train_time:1392895ms step_avg:909.79ms
step:1532/1775 n_predict=1 lr=0.4833 bs=393216 train_time:1394567ms step_avg:910.29ms
step:1533/1775 n_predict=1 lr=0.4814 bs=393216 train_time:1396192ms step_avg:910.76ms
step:1534/1775 n_predict=1 lr=0.4796 bs=393216 train_time:1397862ms step_avg:911.25ms
step:1535/1775 n_predict=1 lr=0.4777 bs=393216 train_time:1399483ms step_avg:911.72ms
step:1536/1775 n_predict=1 lr=0.4758 bs=393216 train_time:1401200ms step_avg:912.24ms
step:1537/1775 n_predict=1 lr=0.4739 bs=393216 train_time:1402877ms step_avg:912.74ms
step:1538/1775 n_predict=1 lr=0.4720 bs=393216 train_time:1404497ms step_avg:913.20ms
step:1539/1775 n_predict=1 lr=0.4702 bs=393216 train_time:1406185ms step_avg:913.70ms
step:1540/1775 n_predict=1 lr=0.4683 bs=393216 train_time:1407897ms step_avg:914.22ms
step:1541/1775 n_predict=1 lr=0.4664 bs=393216 train_time:1409685ms step_avg:914.79ms
step:1542/1775 n_predict=1 lr=0.4645 bs=393216 train_time:1411296ms step_avg:915.24ms
step:1543/1775 n_predict=1 lr=0.4626 bs=393216 train_time:1412994ms step_avg:915.74ms
step:1544/1775 n_predict=1 lr=0.4608 bs=393216 train_time:1414596ms step_avg:916.19ms
step:1545/1775 n_predict=1 lr=0.4589 bs=393216 train_time:1416264ms step_avg:916.68ms
step:1546/1775 n_predict=1 lr=0.4570 bs=393216 train_time:1418054ms step_avg:917.24ms
step:1547/1775 n_predict=1 lr=0.4551 bs=393216 train_time:1419683ms step_avg:917.70ms
step:1548/1775 n_predict=1 lr=0.4532 bs=393216 train_time:1421398ms step_avg:918.22ms
step:1549/1775 n_predict=1 lr=0.4514 bs=393216 train_time:1422990ms step_avg:918.65ms
step:1550/1775 n_predict=1 lr=0.4495 bs=393216 train_time:1424596ms step_avg:919.09ms
step:1551/1775 n_predict=1 lr=0.4476 bs=393216 train_time:1426296ms step_avg:919.60ms
step:1552/1775 n_predict=1 lr=0.4457 bs=393216 train_time:1427962ms step_avg:920.08ms
step:1553/1775 n_predict=1 lr=0.4439 bs=393216 train_time:1429594ms step_avg:920.54ms
step:1554/1775 n_predict=1 lr=0.4420 bs=393216 train_time:1431295ms step_avg:921.04ms
step:1555/1775 n_predict=1 lr=0.4401 bs=393216 train_time:1432882ms step_avg:921.47ms
step:1556/1775 n_predict=1 lr=0.4382 bs=393216 train_time:1434555ms step_avg:921.95ms
step:1557/1775 n_predict=1 lr=0.4363 bs=393216 train_time:1436292ms step_avg:922.47ms
step:1558/1775 n_predict=1 lr=0.4345 bs=393216 train_time:1437895ms step_avg:922.91ms
step:1559/1775 n_predict=1 lr=0.4326 bs=393216 train_time:1439562ms step_avg:923.39ms
step:1560/1775 n_predict=1 lr=0.4307 bs=393216 train_time:1441196ms step_avg:923.84ms
step:1561/1775 n_predict=1 lr=0.4288 bs=393216 train_time:1442894ms step_avg:924.34ms
step:1562/1775 n_predict=1 lr=0.4269 bs=393216 train_time:1444474ms step_avg:924.76ms
step:1563/1775 n_predict=1 lr=0.4251 bs=393216 train_time:1446190ms step_avg:925.27ms
step:1564/1775 n_predict=1 lr=0.4232 bs=393216 train_time:1447795ms step_avg:925.70ms
step:1565/1775 n_predict=1 lr=0.4213 bs=393216 train_time:1449494ms step_avg:926.19ms
step:1566/1775 n_predict=1 lr=0.4194 bs=393216 train_time:1451191ms step_avg:926.69ms
step:1567/1775 n_predict=1 lr=0.4175 bs=393216 train_time:1452793ms step_avg:927.12ms
step:1568/1775 n_predict=1 lr=0.4157 bs=393216 train_time:1454397ms step_avg:927.55ms
step:1569/1775 n_predict=1 lr=0.4138 bs=393216 train_time:1455990ms step_avg:927.97ms
step:1570/1775 n_predict=1 lr=0.4119 bs=393216 train_time:1457552ms step_avg:928.38ms
step:1571/1775 n_predict=1 lr=0.4100 bs=393216 train_time:1459193ms step_avg:928.83ms
step:1572/1775 n_predict=1 lr=0.4081 bs=393216 train_time:1460796ms step_avg:929.26ms
step:1573/1775 n_predict=1 lr=0.4063 bs=393216 train_time:1462461ms step_avg:929.73ms
step:1574/1775 n_predict=1 lr=0.4044 bs=393216 train_time:1464096ms step_avg:930.18ms
step:1575/1775 n_predict=1 lr=0.4025 bs=393216 train_time:1465693ms step_avg:930.60ms
step:1576/1775 n_predict=1 lr=0.4006 bs=393216 train_time:1467285ms step_avg:931.02ms
step:1577/1775 n_predict=1 lr=0.3988 bs=393216 train_time:1468962ms step_avg:931.49ms
step:1578/1775 n_predict=1 lr=0.3969 bs=393216 train_time:1470700ms step_avg:932.00ms
step:1579/1775 n_predict=1 lr=0.3950 bs=393216 train_time:1472284ms step_avg:932.42ms
step:1580/1775 n_predict=1 lr=0.3931 bs=393216 train_time:1473899ms step_avg:932.85ms
step:1581/1775 n_predict=1 lr=0.3912 bs=393216 train_time:1475491ms step_avg:933.26ms
step:1582/1775 n_predict=1 lr=0.3894 bs=393216 train_time:1477278ms step_avg:933.80ms
step:1583/1775 n_predict=1 lr=0.3875 bs=393216 train_time:1478994ms step_avg:934.30ms
step:1584/1775 n_predict=1 lr=0.3856 bs=393216 train_time:1480645ms step_avg:934.75ms
step:1585/1775 n_predict=1 lr=0.3837 bs=393216 train_time:1482293ms step_avg:935.20ms
step:1586/1775 n_predict=1 lr=0.3818 bs=393216 train_time:1483895ms step_avg:935.62ms
step:1587/1775 n_predict=1 lr=0.3800 bs=393216 train_time:1485494ms step_avg:936.04ms
step:1588/1775 n_predict=1 lr=0.3781 bs=393216 train_time:1487196ms step_avg:936.52ms
step:1589/1775 n_predict=1 lr=0.3762 bs=393216 train_time:1488894ms step_avg:937.00ms
step:1590/1775 n_predict=1 lr=0.3743 bs=393216 train_time:1490497ms step_avg:937.42ms
step:1591/1775 n_predict=1 lr=0.3724 bs=393216 train_time:1492090ms step_avg:937.83ms
step:1592/1775 n_predict=1 lr=0.3706 bs=393216 train_time:1493749ms step_avg:938.28ms
step:1593/1775 n_predict=1 lr=0.3687 bs=393216 train_time:1495395ms step_avg:938.73ms
step:1594/1775 n_predict=1 lr=0.3668 bs=393216 train_time:1497096ms step_avg:939.21ms
step:1595/1775 n_predict=1 lr=0.3649 bs=393216 train_time:1498795ms step_avg:939.68ms
step:1596/1775 n_predict=1 lr=0.3631 bs=393216 train_time:1500567ms step_avg:940.20ms
step:1597/1775 n_predict=1 lr=0.3612 bs=393216 train_time:1502194ms step_avg:940.63ms
step:1598/1775 n_predict=1 lr=0.3593 bs=393216 train_time:1503877ms step_avg:941.10ms
step:1599/1775 n_predict=1 lr=0.3574 bs=393216 train_time:1505586ms step_avg:941.58ms
step:1600/1775 n_predict=1 lr=0.3555 bs=393216 train_time:1507264ms step_avg:942.04ms
step:1601/1775 n_predict=1 lr=0.3537 bs=393216 train_time:1508864ms step_avg:942.45ms
step:1602/1775 n_predict=1 lr=0.3518 bs=393216 train_time:1510496ms step_avg:942.88ms
step:1603/1775 n_predict=1 lr=0.3499 bs=393216 train_time:1512168ms step_avg:943.34ms
step:1604/1775 n_predict=1 lr=0.3480 bs=393216 train_time:1513797ms step_avg:943.76ms
step:1605/1775 n_predict=1 lr=0.3461 bs=393216 train_time:1515394ms step_avg:944.17ms
step:1606/1775 n_predict=1 lr=0.3443 bs=393216 train_time:1517165ms step_avg:944.69ms
step:1607/1775 n_predict=1 lr=0.3424 bs=393216 train_time:1518965ms step_avg:945.22ms
step:1608/1775 n_predict=1 lr=0.3405 bs=393216 train_time:1520600ms step_avg:945.65ms
step:1609/1775 n_predict=1 lr=0.3386 bs=393216 train_time:1522247ms step_avg:946.08ms
step:1610/1775 n_predict=1 lr=0.3367 bs=393216 train_time:1524069ms step_avg:946.63ms
step:1611/1775 n_predict=1 lr=0.3349 bs=393216 train_time:1525785ms step_avg:947.10ms
step:1612/1775 n_predict=1 lr=0.3330 bs=393216 train_time:1527569ms step_avg:947.62ms
step:1613/1775 n_predict=1 lr=0.3311 bs=393216 train_time:1529194ms step_avg:948.04ms
step:1614/1775 n_predict=1 lr=0.3292 bs=393216 train_time:1530792ms step_avg:948.45ms
step:1615/1775 n_predict=1 lr=0.3274 bs=393216 train_time:1532395ms step_avg:948.85ms
step:1616/1775 n_predict=1 lr=0.3255 bs=393216 train_time:1534087ms step_avg:949.31ms
step:1617/1775 n_predict=1 lr=0.3236 bs=393216 train_time:1535794ms step_avg:949.78ms
step:1618/1775 n_predict=1 lr=0.3217 bs=393216 train_time:1537378ms step_avg:950.17ms
step:1619/1775 n_predict=1 lr=0.3198 bs=393216 train_time:1539095ms step_avg:950.65ms
step:1620/1775 n_predict=1 lr=0.3180 bs=393216 train_time:1540692ms step_avg:951.04ms
step:1621/1775 n_predict=1 lr=0.3161 bs=393216 train_time:1542362ms step_avg:951.49ms
step:1622/1775 n_predict=1 lr=0.3142 bs=393216 train_time:1543996ms step_avg:951.91ms
step:1623/1775 n_predict=1 lr=0.3123 bs=393216 train_time:1545585ms step_avg:952.30ms
step:1624/1775 n_predict=1 lr=0.3104 bs=393216 train_time:1547396ms step_avg:952.83ms
step:1625/1775 n_predict=1 lr=0.3086 bs=393216 train_time:1549072ms step_avg:953.28ms
step:1626/1775 n_predict=1 lr=0.3067 bs=393216 train_time:1550686ms step_avg:953.68ms
step:1627/1775 n_predict=1 lr=0.3048 bs=393216 train_time:1552391ms step_avg:954.14ms
step:1628/1775 n_predict=1 lr=0.3029 bs=393216 train_time:1553998ms step_avg:954.54ms
step:1629/1775 n_predict=1 lr=0.3010 bs=393216 train_time:1555692ms step_avg:955.00ms
step:1630/1775 n_predict=1 lr=0.2992 bs=393216 train_time:1557370ms step_avg:955.44ms
step:1631/1775 n_predict=1 lr=0.2973 bs=393216 train_time:1559094ms step_avg:955.91ms
step:1632/1775 n_predict=1 lr=0.2954 bs=393216 train_time:1560749ms step_avg:956.34ms
step:1633/1775 n_predict=1 lr=0.2935 bs=393216 train_time:1562495ms step_avg:956.82ms
step:1634/1775 n_predict=1 lr=0.2917 bs=393216 train_time:1564196ms step_avg:957.28ms
step:1635/1775 n_predict=1 lr=0.2898 bs=393216 train_time:1565970ms step_avg:957.78ms
step:1636/1775 n_predict=1 lr=0.2879 bs=393216 train_time:1567789ms step_avg:958.31ms
step:1637/1775 n_predict=1 lr=0.2860 bs=393216 train_time:1569495ms step_avg:958.76ms
step:1638/1775 n_predict=1 lr=0.2841 bs=393216 train_time:1571196ms step_avg:959.22ms
step:1639/1775 n_predict=1 lr=0.2823 bs=393216 train_time:1572794ms step_avg:959.61ms
step:1640/1775 n_predict=1 lr=0.2804 bs=393216 train_time:1574498ms step_avg:960.06ms
step:1641/1775 n_predict=1 lr=0.2785 bs=393216 train_time:1576192ms step_avg:960.51ms
step:1642/1775 n_predict=1 lr=0.2766 bs=393216 train_time:1577866ms step_avg:960.94ms
step:1643/1775 n_predict=1 lr=0.2747 bs=393216 train_time:1579562ms step_avg:961.39ms
step:1644/1775 n_predict=1 lr=0.2729 bs=393216 train_time:1581294ms step_avg:961.86ms
step:1645/1775 n_predict=1 lr=0.2710 bs=393216 train_time:1582995ms step_avg:962.31ms
step:1646/1775 n_predict=1 lr=0.2691 bs=393216 train_time:1584700ms step_avg:962.76ms
step:1647/1775 n_predict=1 lr=0.2672 bs=393216 train_time:1586294ms step_avg:963.14ms
step:1648/1775 n_predict=1 lr=0.2653 bs=393216 train_time:1587993ms step_avg:963.59ms
step:1649/1775 n_predict=1 lr=0.2635 bs=393216 train_time:1589594ms step_avg:963.97ms
step:1650/1775 n_predict=1 lr=0.2616 bs=393216 train_time:1591296ms step_avg:964.42ms
step:1651/1775 n_predict=1 lr=0.2597 bs=393216 train_time:1593063ms step_avg:964.91ms
step:1652/1775 n_predict=1 lr=0.2578 bs=393216 train_time:1594796ms step_avg:965.37ms
step:1653/1775 n_predict=1 lr=0.2560 bs=393216 train_time:1596394ms step_avg:965.76ms
step:1654/1775 n_predict=1 lr=0.2541 bs=393216 train_time:1597996ms step_avg:966.14ms
step:1655/1775 n_predict=1 lr=0.2522 bs=393216 train_time:1599756ms step_avg:966.62ms
step:1656/1775 n_predict=1 lr=0.2503 bs=393216 train_time:1601395ms step_avg:967.03ms
step:1657/1775 n_predict=1 lr=0.2484 bs=393216 train_time:1603195ms step_avg:967.53ms
step:1658/1775 n_predict=1 lr=0.2466 bs=393216 train_time:1604968ms step_avg:968.01ms
step:1659/1775 n_predict=1 lr=0.2447 bs=393216 train_time:1606592ms step_avg:968.41ms
step:1660/1775 n_predict=1 lr=0.2428 bs=393216 train_time:1608266ms step_avg:968.83ms
step:1661/1775 n_predict=1 lr=0.2409 bs=393216 train_time:1609963ms step_avg:969.27ms
step:1662/1775 n_predict=1 lr=0.2390 bs=393216 train_time:1611595ms step_avg:969.67ms
step:1663/1775 n_predict=1 lr=0.2372 bs=393216 train_time:1613294ms step_avg:970.11ms
step:1664/1775 n_predict=1 lr=0.2353 bs=393216 train_time:1614997ms step_avg:970.55ms
step:1665/1775 n_predict=1 lr=0.2334 bs=393216 train_time:1616697ms step_avg:970.99ms
step:1666/1775 n_predict=1 lr=0.2315 bs=393216 train_time:1618395ms step_avg:971.43ms
step:1667/1775 n_predict=1 lr=0.2296 bs=393216 train_time:1619982ms step_avg:971.80ms
step:1668/1775 n_predict=1 lr=0.2278 bs=393216 train_time:1621693ms step_avg:972.24ms
step:1669/1775 n_predict=1 lr=0.2259 bs=393216 train_time:1623294ms step_avg:972.61ms
step:1670/1775 n_predict=1 lr=0.2240 bs=393216 train_time:1624963ms step_avg:973.03ms
step:1671/1775 n_predict=1 lr=0.2221 bs=393216 train_time:1626647ms step_avg:973.46ms
step:1672/1775 n_predict=1 lr=0.2203 bs=393216 train_time:1628371ms step_avg:973.91ms
step:1673/1775 n_predict=1 lr=0.2184 bs=393216 train_time:1630094ms step_avg:974.35ms
step:1674/1775 n_predict=1 lr=0.2165 bs=393216 train_time:1631696ms step_avg:974.73ms
step:1675/1775 n_predict=1 lr=0.2146 bs=393216 train_time:1633297ms step_avg:975.10ms
step:1676/1775 n_predict=1 lr=0.2127 bs=393216 train_time:1634964ms step_avg:975.52ms
step:1677/1775 n_predict=1 lr=0.2109 bs=393216 train_time:1636679ms step_avg:975.96ms
step:1678/1775 n_predict=1 lr=0.2090 bs=393216 train_time:1638356ms step_avg:976.37ms
step:1679/1775 n_predict=1 lr=0.2071 bs=393216 train_time:1639983ms step_avg:976.76ms
step:1680/1775 n_predict=1 lr=0.2052 bs=393216 train_time:1641650ms step_avg:977.17ms
step:1681/1775 n_predict=1 lr=0.2033 bs=393216 train_time:1643292ms step_avg:977.57ms
step:1682/1775 n_predict=1 lr=0.2015 bs=393216 train_time:1644894ms step_avg:977.94ms
step:1683/1775 n_predict=1 lr=0.1996 bs=393216 train_time:1646694ms step_avg:978.43ms
step:1684/1775 n_predict=1 lr=0.1977 bs=393216 train_time:1648296ms step_avg:978.80ms
step:1685/1775 n_predict=1 lr=0.1958 bs=393216 train_time:1649891ms step_avg:979.16ms
step:1686/1775 n_predict=1 lr=0.1939 bs=393216 train_time:1651579ms step_avg:979.58ms
step:1687/1775 n_predict=1 lr=0.1921 bs=393216 train_time:1653290ms step_avg:980.02ms
step:1688/1775 n_predict=1 lr=0.1902 bs=393216 train_time:1654979ms step_avg:980.44ms
step:1689/1775 n_predict=1 lr=0.1883 bs=393216 train_time:1656664ms step_avg:980.86ms
step:1690/1775 n_predict=1 lr=0.1864 bs=393216 train_time:1658374ms step_avg:981.29ms
step:1691/1775 n_predict=1 lr=0.1846 bs=393216 train_time:1660152ms step_avg:981.76ms
step:1692/1775 n_predict=1 lr=0.1827 bs=393216 train_time:1661796ms step_avg:982.15ms
step:1693/1775 n_predict=1 lr=0.1808 bs=393216 train_time:1663390ms step_avg:982.51ms
step:1694/1775 n_predict=1 lr=0.1789 bs=393216 train_time:1665096ms step_avg:982.94ms
step:1695/1775 n_predict=1 lr=0.1770 bs=393216 train_time:1666762ms step_avg:983.34ms
step:1696/1775 n_predict=1 lr=0.1752 bs=393216 train_time:1668487ms step_avg:983.78ms
step:1697/1775 n_predict=1 lr=0.1733 bs=393216 train_time:1670193ms step_avg:984.20ms
step:1698/1775 n_predict=1 lr=0.1714 bs=393216 train_time:1671796ms step_avg:984.57ms
step:1699/1775 n_predict=1 lr=0.1695 bs=393216 train_time:1673474ms step_avg:984.98ms
step:1700/1775 n_predict=1 lr=0.1676 bs=393216 train_time:1675099ms step_avg:985.35ms
step:1701/1775 n_predict=1 lr=0.1658 bs=393216 train_time:1676852ms step_avg:985.80ms
step:1702/1775 n_predict=1 lr=0.1639 bs=393216 train_time:1678497ms step_avg:986.19ms
step:1703/1775 n_predict=1 lr=0.1620 bs=393216 train_time:1680095ms step_avg:986.55ms
step:1704/1775 n_predict=1 lr=0.1601 bs=393216 train_time:1681796ms step_avg:986.97ms
step:1705/1775 n_predict=1 lr=0.1582 bs=393216 train_time:1683394ms step_avg:987.33ms
step:1706/1775 n_predict=1 lr=0.1564 bs=393216 train_time:1684996ms step_avg:987.69ms
step:1707/1775 n_predict=1 lr=0.1545 bs=393216 train_time:1686594ms step_avg:988.05ms
step:1708/1775 n_predict=1 lr=0.1526 bs=393216 train_time:1688264ms step_avg:988.45ms
step:1709/1775 n_predict=1 lr=0.1507 bs=393216 train_time:1689885ms step_avg:988.81ms
step:1710/1775 n_predict=1 lr=0.1489 bs=393216 train_time:1691549ms step_avg:989.21ms
step:1711/1775 n_predict=1 lr=0.1470 bs=393216 train_time:1693195ms step_avg:989.59ms
step:1712/1775 n_predict=1 lr=0.1451 bs=393216 train_time:1694796ms step_avg:989.95ms
step:1713/1775 n_predict=1 lr=0.1432 bs=393216 train_time:1696393ms step_avg:990.31ms
step:1714/1775 n_predict=1 lr=0.1413 bs=393216 train_time:1697998ms step_avg:990.66ms
step:1715/1775 n_predict=1 lr=0.1395 bs=393216 train_time:1699594ms step_avg:991.02ms
step:1716/1775 n_predict=1 lr=0.1376 bs=393216 train_time:1701295ms step_avg:991.43ms
step:1717/1775 n_predict=1 lr=0.1357 bs=393216 train_time:1702992ms step_avg:991.84ms
step:1718/1775 n_predict=1 lr=0.1338 bs=393216 train_time:1704664ms step_avg:992.24ms
step:1719/1775 n_predict=1 lr=0.1319 bs=393216 train_time:1706364ms step_avg:992.65ms
step:1720/1775 n_predict=1 lr=0.1301 bs=393216 train_time:1707989ms step_avg:993.02ms
step:1721/1775 n_predict=1 lr=0.1282 bs=393216 train_time:1709594ms step_avg:993.37ms
step:1722/1775 n_predict=1 lr=0.1263 bs=393216 train_time:1711289ms step_avg:993.78ms
step:1723/1775 n_predict=1 lr=0.1244 bs=393216 train_time:1712894ms step_avg:994.13ms
step:1724/1775 n_predict=1 lr=0.1225 bs=393216 train_time:1714671ms step_avg:994.59ms
step:1725/1775 n_predict=1 lr=0.1207 bs=393216 train_time:1716294ms step_avg:994.95ms
step:1726/1775 n_predict=1 lr=0.1188 bs=393216 train_time:1717886ms step_avg:995.30ms
step:1727/1775 n_predict=1 lr=0.1169 bs=393216 train_time:1719467ms step_avg:995.64ms
step:1728/1775 n_predict=1 lr=0.1150 bs=393216 train_time:1721095ms step_avg:996.00ms
step:1729/1775 n_predict=1 lr=0.1132 bs=393216 train_time:1722678ms step_avg:996.34ms
step:1730/1775 n_predict=1 lr=0.1113 bs=393216 train_time:1724297ms step_avg:996.70ms
step:1731/1775 n_predict=1 lr=0.1094 bs=393216 train_time:1725883ms step_avg:997.04ms
step:1732/1775 n_predict=1 lr=0.1075 bs=393216 train_time:1727565ms step_avg:997.44ms
step:1733/1775 n_predict=1 lr=0.1056 bs=393216 train_time:1729194ms step_avg:997.80ms
step:1734/1775 n_predict=1 lr=0.1038 bs=393216 train_time:1730796ms step_avg:998.15ms
step:1735/1775 n_predict=1 lr=0.1019 bs=393216 train_time:1732383ms step_avg:998.49ms
step:1736/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1754197ms step_avg:1010.48ms
step:1737/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1755962ms step_avg:1010.92ms
step:1738/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1757679ms step_avg:1011.32ms
step:1739/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1759461ms step_avg:1011.77ms
step:1740/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1761089ms step_avg:1012.12ms
step:1741/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1762794ms step_avg:1012.52ms
step:1742/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1764496ms step_avg:1012.91ms
step:1743/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1766162ms step_avg:1013.29ms
step:1744/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1767786ms step_avg:1013.64ms
step:1745/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1769495ms step_avg:1014.04ms
step:1746/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1771277ms step_avg:1014.48ms
step:1747/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1772963ms step_avg:1014.86ms
step:1748/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1774597ms step_avg:1015.22ms
step:1749/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1776193ms step_avg:1015.55ms
step:1750/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1777895ms step_avg:1015.94ms
step:1750/1775 lr=0.1000 bs=393216 n_predict=1 val_loss:3.2844 val_malbo_loss:3.3112 train_time:1777976ms step_avg:1015.99ms
step:1751/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1779563ms step_avg:1016.31ms
step:1752/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1781266ms step_avg:1016.70ms
step:1753/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1782965ms step_avg:1017.09ms
step:1754/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1784936ms step_avg:1017.64ms
step:1755/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1786565ms step_avg:1017.99ms
step:1756/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1788350ms step_avg:1018.42ms
step:1757/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1790220ms step_avg:1018.91ms
step:1758/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1791868ms step_avg:1019.27ms
step:1759/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1793651ms step_avg:1019.70ms
step:1760/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1795373ms step_avg:1020.10ms
step:1761/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1797266ms step_avg:1020.59ms
step:1762/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1799070ms step_avg:1021.04ms
step:1763/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1800763ms step_avg:1021.42ms
step:1764/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1802423ms step_avg:1021.78ms
step:1765/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1804164ms step_avg:1022.19ms
step:1766/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1805837ms step_avg:1022.56ms
step:1767/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1807623ms step_avg:1022.99ms
step:1768/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1809330ms step_avg:1023.38ms
step:1769/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1810964ms step_avg:1023.72ms
step:1770/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1812667ms step_avg:1024.11ms
step:1771/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1814433ms step_avg:1024.52ms
step:1772/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1816069ms step_avg:1024.87ms
step:1773/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1817668ms step_avg:1025.19ms
step:1774/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1819337ms step_avg:1025.56ms
step:1775/1775 n_predict=1 lr=0.1000 bs=393216 train_time:1821034ms step_avg:1025.93ms
step:1775/1775 lr=0.1000 bs=393216 n_predict=1 val_loss:3.2781 val_malbo_loss:3.3049 train_time:1821113ms step_avg:1025.98ms
peak memory allocated: 30840 MiB reserved: 46440 MiB
