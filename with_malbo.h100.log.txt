import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

from malbo import compute_malbo_parameters

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        x = F.linear(x, self.c_fc.type_as(x))
        x = F.relu(x).square() # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977
        x = F.linear(x, self.c_proj.T.type_as(x))
        return x

class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int, use_malbo: bool):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

        self.use_malbo = use_malbo

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        logits = 23 * torch.sigmoid((logits + 5) / 7.5)
        logits_for_loss = logits.float() if not self.training else logits

        n_predict = mtp_weights.size(0) if mtp_weights is not None else 1
        if self.training and n_predict > 1:
            # Multi-token prediction: take loss of the weighted average of next n_predict tokens
            logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
            idx = F.pad(target_seq, (0, n_predict - 1)).unfold(0, n_predict, 1)  # [T, n_predict] of shifted targets
            target_logits = logits_flat.gather(1, idx)
            cross_entropy = torch.logsumexp(logits_flat, dim=-1).unsqueeze(1) - target_logits
            for k in range(1, n_predict):  # zero out preds past end of sequence
                cross_entropy[-k:, k] = 0

            loss = (cross_entropy * mtp_weights).sum()
            if self.use_malbo:
                T, K = logits_flat.shape
                with torch.no_grad():
                    vhat, kappa, gamma = compute_malbo_parameters((-cross_entropy).float().exp().T, K)
                    weights_transposed = kappa * gamma

                malbo_loss = T * (cross_entropy * weights_transposed.T * mtp_weights).sum()

                if not torch.isfinite(loss) or not torch.isfinite(malbo_loss):
                    raise RuntimeError(f"Non-finite loss (1): {loss} {malbo_loss}")
            else:
                malbo_loss = loss
        elif self.training:
            if self.use_malbo:
                logits_flat = logits_for_loss.view(-1, logits_for_loss.size(-1))
                T, K = logits_flat.shape
                cross_entropy = F.cross_entropy(logits_flat, target_seq, reduction="none")
                loss = cross_entropy.sum()

                with torch.no_grad():
                    vhat, kappa, gamma = compute_malbo_parameters((-cross_entropy).float().exp().unsqueeze(0), K)
                    weights = (kappa * gamma).squeeze(0)

                malbo_loss = T * (weights * cross_entropy).sum()

                if not torch.isfinite(loss) or not torch.isfinite(malbo_loss):
                    raise RuntimeError(f"Non-finite loss (2): {loss} {malbo_loss}")
            else:
                loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="sum")
                malbo_loss = loss
        else:
            if self.use_malbo:
                K = logits_for_loss.size(-1)
                cross_entropy = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="none")
                loss = cross_entropy.mean()

                with torch.no_grad():
                    vhat, kappa, gamma = compute_malbo_parameters((-cross_entropy).float().exp().unsqueeze(0), K)
                    weights = (kappa * gamma).squeeze(0)

                malbo_loss = (weights * cross_entropy).sum()

                if not torch.isfinite(loss) or not torch.isfinite(malbo_loss):
                    raise RuntimeError(f"Non-finite loss (3): {loss} {malbo_loss}")
            else:
                loss = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="mean")
                malbo_loss = loss

        return loss, malbo_loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 8
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size),
    use_malbo=os.environ.get('use_malbo', 'True') == 'True',
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=False)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args())[1] / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
    break
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss, val_malbo_loss = 0, 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                this_val_loss, this_val_malbo_loss = model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
                val_loss += this_val_loss
                val_malbo_loss += this_val_malbo_loss
        val_loss /= val_steps
        val_malbo_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        dist.reduce(val_malbo_loss, 0, op=dist.ReduceOp.AVG)
        print0(f"step:{step}/{train_steps} val_loss:{val_loss:.4f} val_malbo_loss:{val_malbo_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args())[1] / grad_accum_steps).backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    print0(f"step:{step+1}/{train_steps} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.19 (main, Oct 21 2025, 16:43:05) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Fri Jan 16 04:27:45 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.65.06              Driver Version: 580.65.06      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:57:00.0 Off |                    0 |
| N/A   27C    P0             71W /  310W |    1105MiB /  81559MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            3346      C   .../envs/speedrun/bin/python3.10       1096MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 val_loss:10.8321 val_malbo_loss:10.8222 train_time:0ms step_avg:0.03ms
step:1/1775 train_time:1084ms step_avg:1084.21ms
step:2/1775 train_time:5936ms step_avg:2967.92ms
step:3/1775 train_time:6439ms step_avg:2146.27ms
step:4/1775 train_time:6933ms step_avg:1733.30ms
step:5/1775 train_time:7418ms step_avg:1483.50ms
step:6/1775 train_time:7910ms step_avg:1318.33ms
step:7/1775 train_time:8384ms step_avg:1197.75ms
step:8/1775 train_time:8877ms step_avg:1109.59ms
step:9/1775 train_time:9352ms step_avg:1039.13ms
step:10/1775 train_time:9850ms step_avg:984.98ms
step:11/1775 train_time:10330ms step_avg:939.12ms
step:12/1775 train_time:10814ms step_avg:901.20ms
step:13/1775 train_time:11300ms step_avg:869.22ms
step:14/1775 train_time:11800ms step_avg:842.86ms
step:15/1775 train_time:12289ms step_avg:819.26ms
step:16/1775 train_time:12778ms step_avg:798.65ms
step:17/1775 train_time:13262ms step_avg:780.10ms
step:18/1775 train_time:13754ms step_avg:764.09ms
step:19/1775 train_time:14235ms step_avg:749.19ms
step:20/1775 train_time:14719ms step_avg:735.94ms
step:21/1775 train_time:15200ms step_avg:723.79ms
step:22/1775 train_time:15685ms step_avg:712.95ms
step:23/1775 train_time:16169ms step_avg:703.00ms
step:24/1775 train_time:16652ms step_avg:693.84ms
step:25/1775 train_time:17136ms step_avg:685.44ms
step:26/1775 train_time:17618ms step_avg:677.61ms
step:27/1775 train_time:18101ms step_avg:670.39ms
step:28/1775 train_time:18582ms step_avg:663.65ms
step:29/1775 train_time:19063ms step_avg:657.36ms
step:30/1775 train_time:19549ms step_avg:651.62ms
step:31/1775 train_time:20028ms step_avg:646.05ms
step:32/1775 train_time:20506ms step_avg:640.81ms
step:33/1775 train_time:20977ms step_avg:635.68ms
step:34/1775 train_time:21461ms step_avg:631.21ms
step:35/1775 train_time:21936ms step_avg:626.74ms
step:36/1775 train_time:22419ms step_avg:622.76ms
step:37/1775 train_time:22894ms step_avg:618.76ms
step:38/1775 train_time:23381ms step_avg:615.29ms
step:39/1775 train_time:23857ms step_avg:611.71ms
step:40/1775 train_time:24347ms step_avg:608.68ms
step:41/1775 train_time:24824ms step_avg:605.47ms
step:42/1775 train_time:25306ms step_avg:602.52ms
step:43/1775 train_time:25780ms step_avg:599.54ms
step:44/1775 train_time:26269ms step_avg:597.03ms
step:45/1775 train_time:26738ms step_avg:594.17ms
step:46/1775 train_time:27229ms step_avg:591.94ms
step:47/1775 train_time:27712ms step_avg:589.61ms
step:48/1775 train_time:28202ms step_avg:587.55ms
step:49/1775 train_time:28678ms step_avg:585.26ms
step:50/1775 train_time:29157ms step_avg:583.14ms
step:51/1775 train_time:29631ms step_avg:581.01ms
step:52/1775 train_time:30123ms step_avg:579.28ms
step:53/1775 train_time:30605ms step_avg:577.46ms
step:54/1775 train_time:31103ms step_avg:575.98ms
step:55/1775 train_time:31578ms step_avg:574.15ms
step:56/1775 train_time:32057ms step_avg:572.44ms
step:57/1775 train_time:32538ms step_avg:570.84ms
step:58/1775 train_time:33019ms step_avg:569.30ms
step:59/1775 train_time:33496ms step_avg:567.72ms
step:60/1775 train_time:33983ms step_avg:566.39ms
step:61/1775 train_time:34463ms step_avg:564.97ms
step:62/1775 train_time:34944ms step_avg:563.61ms
step:63/1775 train_time:35418ms step_avg:562.19ms
step:64/1775 train_time:35899ms step_avg:560.93ms
step:65/1775 train_time:36372ms step_avg:559.56ms
step:66/1775 train_time:36855ms step_avg:558.41ms
step:67/1775 train_time:37336ms step_avg:557.25ms
step:68/1775 train_time:37812ms step_avg:556.06ms
step:69/1775 train_time:38281ms step_avg:554.79ms
step:70/1775 train_time:38764ms step_avg:553.77ms
step:71/1775 train_time:39240ms step_avg:552.68ms
step:72/1775 train_time:39716ms step_avg:551.60ms
step:73/1775 train_time:40193ms step_avg:550.60ms
step:74/1775 train_time:40674ms step_avg:549.65ms
step:75/1775 train_time:41154ms step_avg:548.71ms
step:76/1775 train_time:41635ms step_avg:547.83ms
step:77/1775 train_time:42112ms step_avg:546.92ms
step:78/1775 train_time:42595ms step_avg:546.09ms
step:79/1775 train_time:43072ms step_avg:545.22ms
step:80/1775 train_time:43554ms step_avg:544.42ms
step:81/1775 train_time:44030ms step_avg:543.59ms
step:82/1775 train_time:44510ms step_avg:542.81ms
step:83/1775 train_time:45003ms step_avg:542.21ms
step:84/1775 train_time:45482ms step_avg:541.46ms
step:85/1775 train_time:45967ms step_avg:540.79ms
step:86/1775 train_time:46440ms step_avg:540.00ms
step:87/1775 train_time:46912ms step_avg:539.22ms
step:88/1775 train_time:47394ms step_avg:538.56ms
step:89/1775 train_time:47871ms step_avg:537.88ms
step:90/1775 train_time:48355ms step_avg:537.28ms
step:91/1775 train_time:48830ms step_avg:536.60ms
step:92/1775 train_time:49314ms step_avg:536.02ms
step:93/1775 train_time:49782ms step_avg:535.29ms
step:94/1775 train_time:50259ms step_avg:534.67ms
step:95/1775 train_time:50732ms step_avg:534.03ms
step:96/1775 train_time:51208ms step_avg:533.42ms
step:97/1775 train_time:51678ms step_avg:532.76ms
step:98/1775 train_time:52160ms step_avg:532.25ms
step:99/1775 train_time:52645ms step_avg:531.76ms
step:100/1775 train_time:53123ms step_avg:531.23ms
step:101/1775 train_time:53596ms step_avg:530.65ms
step:102/1775 train_time:54077ms step_avg:530.17ms
step:103/1775 train_time:54557ms step_avg:529.68ms
step:104/1775 train_time:55042ms step_avg:529.25ms
step:105/1775 train_time:55512ms step_avg:528.68ms
step:106/1775 train_time:55991ms step_avg:528.22ms
step:107/1775 train_time:56465ms step_avg:527.71ms
step:108/1775 train_time:56945ms step_avg:527.27ms
step:109/1775 train_time:57416ms step_avg:526.75ms
step:110/1775 train_time:57897ms step_avg:526.34ms
step:111/1775 train_time:58375ms step_avg:525.90ms
step:112/1775 train_time:58856ms step_avg:525.50ms
step:113/1775 train_time:59328ms step_avg:525.02ms
step:114/1775 train_time:59808ms step_avg:524.63ms
step:115/1775 train_time:60278ms step_avg:524.15ms
step:116/1775 train_time:60759ms step_avg:523.78ms
step:117/1775 train_time:61236ms step_avg:523.38ms
step:118/1775 train_time:61713ms step_avg:522.99ms
step:119/1775 train_time:62189ms step_avg:522.60ms
step:120/1775 train_time:62674ms step_avg:522.28ms
step:121/1775 train_time:63150ms step_avg:521.90ms
step:122/1775 train_time:63628ms step_avg:521.54ms
step:123/1775 train_time:64106ms step_avg:521.18ms
step:124/1775 train_time:64590ms step_avg:520.89ms
step:125/1775 train_time:65058ms step_avg:520.46ms
step:126/1775 train_time:65538ms step_avg:520.14ms
step:127/1775 train_time:66007ms step_avg:519.74ms
step:128/1775 train_time:66479ms step_avg:519.37ms
step:129/1775 train_time:66961ms step_avg:519.07ms
step:130/1775 train_time:67438ms step_avg:518.76ms
step:131/1775 train_time:68040ms step_avg:519.39ms
step:132/1775 train_time:68528ms step_avg:519.15ms
step:133/1775 train_time:69006ms step_avg:518.84ms
step:134/1775 train_time:69490ms step_avg:518.58ms
step:135/1775 train_time:69960ms step_avg:518.22ms
step:136/1775 train_time:70438ms step_avg:517.93ms
step:137/1775 train_time:70911ms step_avg:517.59ms
step:138/1775 train_time:71391ms step_avg:517.33ms
step:139/1775 train_time:71866ms step_avg:517.02ms
step:140/1775 train_time:72347ms step_avg:516.77ms
step:141/1775 train_time:72824ms step_avg:516.48ms
step:142/1775 train_time:73320ms step_avg:516.34ms
step:143/1775 train_time:73810ms step_avg:516.16ms
step:144/1775 train_time:74295ms step_avg:515.94ms
step:145/1775 train_time:74773ms step_avg:515.68ms
step:146/1775 train_time:75259ms step_avg:515.48ms
step:147/1775 train_time:75742ms step_avg:515.25ms
step:148/1775 train_time:76216ms step_avg:514.97ms
step:149/1775 train_time:76695ms step_avg:514.73ms
step:150/1775 train_time:77177ms step_avg:514.52ms
step:151/1775 train_time:77657ms step_avg:514.28ms
step:152/1775 train_time:78142ms step_avg:514.09ms
step:153/1775 train_time:78612ms step_avg:513.80ms
step:154/1775 train_time:79091ms step_avg:513.58ms
step:155/1775 train_time:79564ms step_avg:513.32ms
step:156/1775 train_time:80045ms step_avg:513.11ms
step:157/1775 train_time:80525ms step_avg:512.90ms
step:158/1775 train_time:81007ms step_avg:512.70ms
step:159/1775 train_time:81475ms step_avg:512.42ms
step:160/1775 train_time:81963ms step_avg:512.27ms
step:161/1775 train_time:82435ms step_avg:512.02ms
step:162/1775 train_time:82917ms step_avg:511.83ms
step:163/1775 train_time:83392ms step_avg:511.61ms
step:164/1775 train_time:83882ms step_avg:511.48ms
step:165/1775 train_time:84364ms step_avg:511.30ms
step:166/1775 train_time:84840ms step_avg:511.08ms
step:167/1775 train_time:85316ms step_avg:510.88ms
step:168/1775 train_time:85809ms step_avg:510.77ms
step:169/1775 train_time:86287ms step_avg:510.58ms
step:170/1775 train_time:86772ms step_avg:510.42ms
step:171/1775 train_time:87247ms step_avg:510.21ms
step:172/1775 train_time:87728ms step_avg:510.05ms
step:173/1775 train_time:88199ms step_avg:509.82ms
step:174/1775 train_time:88679ms step_avg:509.65ms
step:175/1775 train_time:89162ms step_avg:509.49ms
step:176/1775 train_time:89641ms step_avg:509.32ms
step:177/1775 train_time:90112ms step_avg:509.11ms
step:178/1775 train_time:90592ms step_avg:508.95ms
step:179/1775 train_time:91074ms step_avg:508.79ms
step:180/1775 train_time:91555ms step_avg:508.64ms
step:181/1775 train_time:92033ms step_avg:508.47ms
step:182/1775 train_time:92514ms step_avg:508.32ms
step:183/1775 train_time:92981ms step_avg:508.09ms
step:184/1775 train_time:93471ms step_avg:508.00ms
step:185/1775 train_time:93952ms step_avg:507.85ms
step:186/1775 train_time:94431ms step_avg:507.69ms
step:187/1775 train_time:94921ms step_avg:507.60ms
step:188/1775 train_time:95404ms step_avg:507.47ms
step:189/1775 train_time:95879ms step_avg:507.30ms
step:190/1775 train_time:96355ms step_avg:507.13ms
step:191/1775 train_time:96828ms step_avg:506.95ms
step:192/1775 train_time:97303ms step_avg:506.78ms
step:193/1775 train_time:97789ms step_avg:506.68ms
step:194/1775 train_time:98270ms step_avg:506.54ms
step:195/1775 train_time:98738ms step_avg:506.35ms
step:196/1775 train_time:99225ms step_avg:506.25ms
step:197/1775 train_time:99701ms step_avg:506.10ms
step:198/1775 train_time:100196ms step_avg:506.04ms
step:199/1775 train_time:100669ms step_avg:505.87ms
step:200/1775 train_time:101149ms step_avg:505.74ms
step:201/1775 train_time:101633ms step_avg:505.64ms
step:202/1775 train_time:102115ms step_avg:505.52ms
step:203/1775 train_time:102591ms step_avg:505.37ms
step:204/1775 train_time:103074ms step_avg:505.26ms
step:205/1775 train_time:103552ms step_avg:505.13ms
step:206/1775 train_time:104033ms step_avg:505.01ms
step:207/1775 train_time:104508ms step_avg:504.87ms
step:208/1775 train_time:104991ms step_avg:504.76ms
step:209/1775 train_time:105471ms step_avg:504.64ms
step:210/1775 train_time:105953ms step_avg:504.54ms
step:211/1775 train_time:106429ms step_avg:504.40ms
step:212/1775 train_time:106910ms step_avg:504.29ms
step:213/1775 train_time:107401ms step_avg:504.23ms
step:214/1775 train_time:107887ms step_avg:504.14ms
step:215/1775 train_time:108361ms step_avg:504.01ms
step:216/1775 train_time:108843ms step_avg:503.90ms
step:217/1775 train_time:109317ms step_avg:503.77ms
step:218/1775 train_time:109795ms step_avg:503.65ms
step:219/1775 train_time:110274ms step_avg:503.53ms
step:220/1775 train_time:110755ms step_avg:503.43ms
step:221/1775 train_time:111244ms step_avg:503.37ms
step:222/1775 train_time:111732ms step_avg:503.30ms
step:223/1775 train_time:112208ms step_avg:503.17ms
step:224/1775 train_time:112695ms step_avg:503.10ms
step:225/1775 train_time:113171ms step_avg:502.98ms
step:226/1775 train_time:113642ms step_avg:502.84ms
step:227/1775 train_time:114128ms step_avg:502.77ms
step:228/1775 train_time:114615ms step_avg:502.70ms
step:229/1775 train_time:115085ms step_avg:502.55ms
step:230/1775 train_time:115568ms step_avg:502.47ms
step:231/1775 train_time:116041ms step_avg:502.34ms
step:232/1775 train_time:116529ms step_avg:502.28ms
step:233/1775 train_time:117003ms step_avg:502.16ms
step:234/1775 train_time:117486ms step_avg:502.08ms
step:235/1775 train_time:117967ms step_avg:501.99ms
step:236/1775 train_time:118437ms step_avg:501.85ms
step:237/1775 train_time:118905ms step_avg:501.71ms
step:238/1775 train_time:119376ms step_avg:501.58ms
step:239/1775 train_time:119850ms step_avg:501.46ms
step:240/1775 train_time:120328ms step_avg:501.36ms
step:241/1775 train_time:120790ms step_avg:501.20ms
step:242/1775 train_time:121264ms step_avg:501.09ms
step:243/1775 train_time:121728ms step_avg:500.94ms
step:244/1775 train_time:122200ms step_avg:500.82ms
step:245/1775 train_time:122676ms step_avg:500.72ms
step:246/1775 train_time:123144ms step_avg:500.58ms
step:247/1775 train_time:123613ms step_avg:500.46ms
step:248/1775 train_time:124094ms step_avg:500.38ms
step:249/1775 train_time:124571ms step_avg:500.28ms
step:250/1775 train_time:125051ms step_avg:500.21ms
step:250/1775 val_loss:4.6187 val_malbo_loss:4.6429 train_time:125054ms step_avg:500.22ms
step:251/1775 train_time:125521ms step_avg:500.08ms
step:252/1775 train_time:125996ms step_avg:499.98ms
step:253/1775 train_time:126466ms step_avg:499.87ms
step:254/1775 train_time:126938ms step_avg:499.76ms
step:255/1775 train_time:127409ms step_avg:499.64ms
step:256/1775 train_time:127895ms step_avg:499.59ms
step:257/1775 train_time:128373ms step_avg:499.51ms
step:258/1775 train_time:128852ms step_avg:499.43ms
step:259/1775 train_time:129329ms step_avg:499.34ms
step:260/1775 train_time:129829ms step_avg:499.34ms
step:261/1775 train_time:130311ms step_avg:499.28ms
step:262/1775 train_time:130792ms step_avg:499.20ms
step:263/1775 train_time:131265ms step_avg:499.11ms
step:264/1775 train_time:131748ms step_avg:499.05ms
step:265/1775 train_time:132226ms step_avg:498.96ms
step:266/1775 train_time:132710ms step_avg:498.91ms
step:267/1775 train_time:133189ms step_avg:498.84ms
step:268/1775 train_time:133662ms step_avg:498.74ms
step:269/1775 train_time:134135ms step_avg:498.64ms
step:270/1775 train_time:134614ms step_avg:498.57ms
step:271/1775 train_time:135086ms step_avg:498.47ms
step:272/1775 train_time:135570ms step_avg:498.42ms
step:273/1775 train_time:136047ms step_avg:498.34ms
step:274/1775 train_time:136521ms step_avg:498.25ms
step:275/1775 train_time:136995ms step_avg:498.16ms
step:276/1775 train_time:137479ms step_avg:498.11ms
step:277/1775 train_time:137953ms step_avg:498.03ms
step:278/1775 train_time:138442ms step_avg:497.99ms
step:279/1775 train_time:138918ms step_avg:497.91ms
step:280/1775 train_time:139392ms step_avg:497.83ms
step:281/1775 train_time:139874ms step_avg:497.77ms
step:282/1775 train_time:140368ms step_avg:497.76ms
step:283/1775 train_time:140854ms step_avg:497.72ms
step:284/1775 train_time:141342ms step_avg:497.68ms
step:285/1775 train_time:141815ms step_avg:497.60ms
step:286/1775 train_time:142300ms step_avg:497.55ms
step:287/1775 train_time:142785ms step_avg:497.51ms
step:288/1775 train_time:143272ms step_avg:497.47ms
step:289/1775 train_time:143745ms step_avg:497.39ms
step:290/1775 train_time:144227ms step_avg:497.33ms
step:291/1775 train_time:144701ms step_avg:497.25ms
step:292/1775 train_time:145181ms step_avg:497.20ms
step:293/1775 train_time:145662ms step_avg:497.14ms
step:294/1775 train_time:146149ms step_avg:497.10ms
step:295/1775 train_time:146619ms step_avg:497.01ms
step:296/1775 train_time:147094ms step_avg:496.94ms
step:297/1775 train_time:147564ms step_avg:496.85ms
step:298/1775 train_time:148040ms step_avg:496.78ms
step:299/1775 train_time:148515ms step_avg:496.71ms
step:300/1775 train_time:148992ms step_avg:496.64ms
step:301/1775 train_time:149473ms step_avg:496.59ms
step:302/1775 train_time:149956ms step_avg:496.54ms
step:303/1775 train_time:150436ms step_avg:496.49ms
step:304/1775 train_time:150910ms step_avg:496.41ms
step:305/1775 train_time:151389ms step_avg:496.36ms
step:306/1775 train_time:151871ms step_avg:496.31ms
step:307/1775 train_time:152345ms step_avg:496.24ms
step:308/1775 train_time:152828ms step_avg:496.20ms
step:309/1775 train_time:153307ms step_avg:496.14ms
step:310/1775 train_time:153792ms step_avg:496.10ms
step:311/1775 train_time:154265ms step_avg:496.03ms
step:312/1775 train_time:154750ms step_avg:495.99ms
step:313/1775 train_time:155215ms step_avg:495.89ms
step:314/1775 train_time:155690ms step_avg:495.83ms
step:315/1775 train_time:156162ms step_avg:495.75ms
step:316/1775 train_time:156647ms step_avg:495.72ms
step:317/1775 train_time:157121ms step_avg:495.65ms
step:318/1775 train_time:157594ms step_avg:495.58ms
step:319/1775 train_time:158076ms step_avg:495.54ms
step:320/1775 train_time:158549ms step_avg:495.47ms
step:321/1775 train_time:159022ms step_avg:495.39ms
step:322/1775 train_time:159493ms step_avg:495.32ms
step:323/1775 train_time:159963ms step_avg:495.24ms
step:324/1775 train_time:160443ms step_avg:495.20ms
step:325/1775 train_time:160920ms step_avg:495.14ms
step:326/1775 train_time:161392ms step_avg:495.07ms
step:327/1775 train_time:161857ms step_avg:494.98ms
step:328/1775 train_time:162344ms step_avg:494.95ms
step:329/1775 train_time:162818ms step_avg:494.89ms
step:330/1775 train_time:163293ms step_avg:494.83ms
step:331/1775 train_time:163763ms step_avg:494.75ms
step:332/1775 train_time:164243ms step_avg:494.71ms
step:333/1775 train_time:164718ms step_avg:494.65ms
step:334/1775 train_time:165204ms step_avg:494.62ms
step:335/1775 train_time:165688ms step_avg:494.59ms
step:336/1775 train_time:166169ms step_avg:494.55ms
step:337/1775 train_time:166646ms step_avg:494.50ms
step:338/1775 train_time:167127ms step_avg:494.46ms
step:339/1775 train_time:167601ms step_avg:494.40ms
step:340/1775 train_time:168083ms step_avg:494.36ms
step:341/1775 train_time:168560ms step_avg:494.31ms
step:342/1775 train_time:169032ms step_avg:494.24ms
step:343/1775 train_time:169500ms step_avg:494.17ms
step:344/1775 train_time:169982ms step_avg:494.13ms
step:345/1775 train_time:170459ms step_avg:494.08ms
step:346/1775 train_time:170947ms step_avg:494.07ms
step:347/1775 train_time:171430ms step_avg:494.04ms
step:348/1775 train_time:171915ms step_avg:494.01ms
step:349/1775 train_time:172394ms step_avg:493.96ms
step:350/1775 train_time:172883ms step_avg:493.95ms
step:351/1775 train_time:173368ms step_avg:493.93ms
step:352/1775 train_time:173847ms step_avg:493.88ms
step:353/1775 train_time:174317ms step_avg:493.81ms
step:354/1775 train_time:174800ms step_avg:493.79ms
step:355/1775 train_time:175273ms step_avg:493.73ms
step:356/1775 train_time:175755ms step_avg:493.69ms
step:357/1775 train_time:176237ms step_avg:493.66ms
step:358/1775 train_time:176717ms step_avg:493.62ms
step:359/1775 train_time:177198ms step_avg:493.59ms
step:360/1775 train_time:177685ms step_avg:493.57ms
step:361/1775 train_time:178165ms step_avg:493.53ms
step:362/1775 train_time:178649ms step_avg:493.51ms
step:363/1775 train_time:179130ms step_avg:493.47ms
step:364/1775 train_time:179615ms step_avg:493.45ms
step:365/1775 train_time:180088ms step_avg:493.39ms
step:366/1775 train_time:180568ms step_avg:493.35ms
step:367/1775 train_time:181046ms step_avg:493.31ms
step:368/1775 train_time:181527ms step_avg:493.28ms
step:369/1775 train_time:182002ms step_avg:493.23ms
step:370/1775 train_time:182484ms step_avg:493.20ms
step:371/1775 train_time:182974ms step_avg:493.19ms
step:372/1775 train_time:183465ms step_avg:493.18ms
step:373/1775 train_time:183943ms step_avg:493.14ms
step:374/1775 train_time:184421ms step_avg:493.10ms
step:375/1775 train_time:184904ms step_avg:493.08ms
step:376/1775 train_time:185378ms step_avg:493.03ms
step:377/1775 train_time:185856ms step_avg:492.99ms
step:378/1775 train_time:186334ms step_avg:492.95ms
step:379/1775 train_time:186798ms step_avg:492.87ms
step:380/1775 train_time:187274ms step_avg:492.83ms
step:381/1775 train_time:187747ms step_avg:492.77ms
step:382/1775 train_time:188228ms step_avg:492.74ms
step:383/1775 train_time:188704ms step_avg:492.70ms
step:384/1775 train_time:189186ms step_avg:492.67ms
step:385/1775 train_time:189653ms step_avg:492.60ms
step:386/1775 train_time:190129ms step_avg:492.56ms
step:387/1775 train_time:190603ms step_avg:492.51ms
step:388/1775 train_time:191087ms step_avg:492.49ms
step:389/1775 train_time:191563ms step_avg:492.45ms
step:390/1775 train_time:192043ms step_avg:492.42ms
step:391/1775 train_time:192520ms step_avg:492.38ms
step:392/1775 train_time:193000ms step_avg:492.35ms
step:393/1775 train_time:193468ms step_avg:492.28ms
step:394/1775 train_time:193956ms step_avg:492.27ms
step:395/1775 train_time:194433ms step_avg:492.24ms
step:396/1775 train_time:194904ms step_avg:492.18ms
step:397/1775 train_time:195383ms step_avg:492.15ms
step:398/1775 train_time:195865ms step_avg:492.12ms
step:399/1775 train_time:196347ms step_avg:492.10ms
step:400/1775 train_time:196823ms step_avg:492.06ms
step:401/1775 train_time:197298ms step_avg:492.02ms
step:402/1775 train_time:197775ms step_avg:491.98ms
step:403/1775 train_time:198244ms step_avg:491.92ms
step:404/1775 train_time:198721ms step_avg:491.88ms
step:405/1775 train_time:199207ms step_avg:491.87ms
step:406/1775 train_time:199690ms step_avg:491.85ms
step:407/1775 train_time:200173ms step_avg:491.83ms
step:408/1775 train_time:200686ms step_avg:491.88ms
step:409/1775 train_time:201169ms step_avg:491.86ms
step:410/1775 train_time:201650ms step_avg:491.83ms
step:411/1775 train_time:202125ms step_avg:491.79ms
step:412/1775 train_time:202602ms step_avg:491.75ms
step:413/1775 train_time:203089ms step_avg:491.74ms
step:414/1775 train_time:203584ms step_avg:491.75ms
step:415/1775 train_time:204067ms step_avg:491.73ms
step:416/1775 train_time:204542ms step_avg:491.69ms
step:417/1775 train_time:205013ms step_avg:491.64ms
step:418/1775 train_time:205489ms step_avg:491.60ms
step:419/1775 train_time:205965ms step_avg:491.56ms
step:420/1775 train_time:206442ms step_avg:491.53ms
step:421/1775 train_time:206922ms step_avg:491.50ms
step:422/1775 train_time:207408ms step_avg:491.49ms
step:423/1775 train_time:207883ms step_avg:491.45ms
step:424/1775 train_time:208368ms step_avg:491.43ms
step:425/1775 train_time:208840ms step_avg:491.39ms
step:426/1775 train_time:209326ms step_avg:491.38ms
step:427/1775 train_time:209811ms step_avg:491.36ms
step:428/1775 train_time:210294ms step_avg:491.34ms
step:429/1775 train_time:210772ms step_avg:491.31ms
step:430/1775 train_time:211251ms step_avg:491.28ms
step:431/1775 train_time:211732ms step_avg:491.26ms
step:432/1775 train_time:212208ms step_avg:491.22ms
step:433/1775 train_time:212684ms step_avg:491.19ms
step:434/1775 train_time:213156ms step_avg:491.14ms
step:435/1775 train_time:213628ms step_avg:491.10ms
step:436/1775 train_time:214113ms step_avg:491.08ms
step:437/1775 train_time:214585ms step_avg:491.04ms
step:438/1775 train_time:215062ms step_avg:491.01ms
step:439/1775 train_time:215526ms step_avg:490.95ms
step:440/1775 train_time:216006ms step_avg:490.92ms
step:441/1775 train_time:216486ms step_avg:490.90ms
step:442/1775 train_time:216969ms step_avg:490.88ms
step:443/1775 train_time:217443ms step_avg:490.84ms
step:444/1775 train_time:217922ms step_avg:490.81ms
step:445/1775 train_time:218388ms step_avg:490.76ms
step:446/1775 train_time:218864ms step_avg:490.73ms
step:447/1775 train_time:219337ms step_avg:490.69ms
step:448/1775 train_time:219819ms step_avg:490.67ms
step:449/1775 train_time:220295ms step_avg:490.63ms
step:450/1775 train_time:220766ms step_avg:490.59ms
step:451/1775 train_time:221256ms step_avg:490.59ms
step:452/1775 train_time:221741ms step_avg:490.58ms
step:453/1775 train_time:222231ms step_avg:490.58ms
step:454/1775 train_time:222719ms step_avg:490.57ms
step:455/1775 train_time:223211ms step_avg:490.57ms
step:456/1775 train_time:223693ms step_avg:490.55ms
step:457/1775 train_time:224168ms step_avg:490.52ms
step:458/1775 train_time:224652ms step_avg:490.51ms
step:459/1775 train_time:225126ms step_avg:490.47ms
step:460/1775 train_time:225608ms step_avg:490.45ms
step:461/1775 train_time:226084ms step_avg:490.42ms
step:462/1775 train_time:226581ms step_avg:490.44ms
step:463/1775 train_time:227054ms step_avg:490.40ms
step:464/1775 train_time:227531ms step_avg:490.37ms
step:465/1775 train_time:228007ms step_avg:490.34ms
step:466/1775 train_time:228488ms step_avg:490.32ms
step:467/1775 train_time:228961ms step_avg:490.28ms
step:468/1775 train_time:229441ms step_avg:490.26ms
step:469/1775 train_time:229915ms step_avg:490.22ms
step:470/1775 train_time:230390ms step_avg:490.19ms
step:471/1775 train_time:230858ms step_avg:490.15ms
step:472/1775 train_time:231333ms step_avg:490.11ms
step:473/1775 train_time:231814ms step_avg:490.09ms
step:474/1775 train_time:232291ms step_avg:490.07ms
step:475/1775 train_time:232760ms step_avg:490.02ms
step:476/1775 train_time:233238ms step_avg:489.99ms
step:477/1775 train_time:233721ms step_avg:489.98ms
step:478/1775 train_time:234194ms step_avg:489.95ms
step:479/1775 train_time:234669ms step_avg:489.91ms
step:480/1775 train_time:235151ms step_avg:489.90ms
step:481/1775 train_time:235634ms step_avg:489.88ms
step:482/1775 train_time:236115ms step_avg:489.86ms
step:483/1775 train_time:236587ms step_avg:489.83ms
step:484/1775 train_time:237070ms step_avg:489.81ms
step:485/1775 train_time:237547ms step_avg:489.79ms
step:486/1775 train_time:238025ms step_avg:489.76ms
step:487/1775 train_time:238503ms step_avg:489.74ms
step:488/1775 train_time:238984ms step_avg:489.72ms
step:489/1775 train_time:239453ms step_avg:489.68ms
step:490/1775 train_time:239931ms step_avg:489.66ms
step:491/1775 train_time:240408ms step_avg:489.63ms
step:492/1775 train_time:240884ms step_avg:489.60ms
step:493/1775 train_time:241355ms step_avg:489.56ms
step:494/1775 train_time:241829ms step_avg:489.53ms
step:495/1775 train_time:242304ms step_avg:489.50ms
step:496/1775 train_time:242786ms step_avg:489.49ms
step:497/1775 train_time:243260ms step_avg:489.46ms
step:498/1775 train_time:243741ms step_avg:489.44ms
step:499/1775 train_time:244214ms step_avg:489.41ms
step:500/1775 train_time:244681ms step_avg:489.36ms
step:500/1775 val_loss:4.2819 val_malbo_loss:4.3069 train_time:244686ms step_avg:489.37ms
step:501/1775 train_time:245148ms step_avg:489.32ms
step:502/1775 train_time:245621ms step_avg:489.28ms
step:503/1775 train_time:246086ms step_avg:489.24ms
step:504/1775 train_time:246557ms step_avg:489.20ms
step:505/1775 train_time:247021ms step_avg:489.15ms
step:506/1775 train_time:247501ms step_avg:489.13ms
step:507/1775 train_time:247976ms step_avg:489.10ms
step:508/1775 train_time:248452ms step_avg:489.08ms
step:509/1775 train_time:248934ms step_avg:489.06ms
step:510/1775 train_time:249408ms step_avg:489.03ms
step:511/1775 train_time:249874ms step_avg:488.99ms
step:512/1775 train_time:250352ms step_avg:488.97ms
step:513/1775 train_time:250821ms step_avg:488.93ms
step:514/1775 train_time:251304ms step_avg:488.92ms
step:515/1775 train_time:251769ms step_avg:488.87ms
step:516/1775 train_time:252244ms step_avg:488.84ms
step:517/1775 train_time:252719ms step_avg:488.82ms
step:518/1775 train_time:253190ms step_avg:488.78ms
step:519/1775 train_time:253662ms step_avg:488.75ms
step:520/1775 train_time:254141ms step_avg:488.73ms
step:521/1775 train_time:254608ms step_avg:488.69ms
step:522/1775 train_time:255090ms step_avg:488.68ms
step:523/1775 train_time:255565ms step_avg:488.65ms
step:524/1775 train_time:256042ms step_avg:488.63ms
step:525/1775 train_time:256508ms step_avg:488.59ms
step:526/1775 train_time:256984ms step_avg:488.56ms
step:527/1775 train_time:257461ms step_avg:488.54ms
step:528/1775 train_time:257934ms step_avg:488.51ms
step:529/1775 train_time:258405ms step_avg:488.48ms
step:530/1775 train_time:258874ms step_avg:488.44ms
step:531/1775 train_time:259349ms step_avg:488.42ms
step:532/1775 train_time:259831ms step_avg:488.40ms
step:533/1775 train_time:260301ms step_avg:488.37ms
step:534/1775 train_time:260780ms step_avg:488.35ms
step:535/1775 train_time:261257ms step_avg:488.33ms
step:536/1775 train_time:261746ms step_avg:488.33ms
step:537/1775 train_time:262220ms step_avg:488.31ms
step:538/1775 train_time:262699ms step_avg:488.29ms
step:539/1775 train_time:263171ms step_avg:488.26ms
step:540/1775 train_time:263646ms step_avg:488.23ms
step:541/1775 train_time:264126ms step_avg:488.22ms
step:542/1775 train_time:264603ms step_avg:488.20ms
step:543/1775 train_time:265072ms step_avg:488.16ms
step:544/1775 train_time:265540ms step_avg:488.12ms
step:545/1775 train_time:266007ms step_avg:488.09ms
step:546/1775 train_time:266486ms step_avg:488.07ms
step:547/1775 train_time:266961ms step_avg:488.05ms
step:548/1775 train_time:267440ms step_avg:488.03ms
step:549/1775 train_time:267918ms step_avg:488.01ms
step:550/1775 train_time:268392ms step_avg:487.99ms
step:551/1775 train_time:268866ms step_avg:487.96ms
step:552/1775 train_time:269357ms step_avg:487.97ms
step:553/1775 train_time:269831ms step_avg:487.94ms
step:554/1775 train_time:270315ms step_avg:487.93ms
step:555/1775 train_time:270788ms step_avg:487.91ms
step:556/1775 train_time:271270ms step_avg:487.89ms
step:557/1775 train_time:271744ms step_avg:487.87ms
step:558/1775 train_time:272224ms step_avg:487.86ms
step:559/1775 train_time:272701ms step_avg:487.84ms
step:560/1775 train_time:273181ms step_avg:487.82ms
step:561/1775 train_time:273654ms step_avg:487.80ms
step:562/1775 train_time:274137ms step_avg:487.79ms
step:563/1775 train_time:274613ms step_avg:487.77ms
step:564/1775 train_time:275089ms step_avg:487.75ms
step:565/1775 train_time:275558ms step_avg:487.71ms
step:566/1775 train_time:276043ms step_avg:487.71ms
step:567/1775 train_time:276517ms step_avg:487.68ms
step:568/1775 train_time:277001ms step_avg:487.68ms
step:569/1775 train_time:277476ms step_avg:487.65ms
step:570/1775 train_time:277958ms step_avg:487.65ms
step:571/1775 train_time:278435ms step_avg:487.63ms
step:572/1775 train_time:278913ms step_avg:487.61ms
step:573/1775 train_time:279390ms step_avg:487.59ms
step:574/1775 train_time:279873ms step_avg:487.58ms
step:575/1775 train_time:280348ms step_avg:487.56ms
step:576/1775 train_time:280831ms step_avg:487.55ms
step:577/1775 train_time:281303ms step_avg:487.53ms
step:578/1775 train_time:281785ms step_avg:487.52ms
step:579/1775 train_time:282263ms step_avg:487.50ms
step:580/1775 train_time:440543ms step_avg:759.56ms
step:581/1775 train_time:441316ms step_avg:759.58ms
step:582/1775 train_time:442100ms step_avg:759.62ms
step:583/1775 train_time:442869ms step_avg:759.64ms
step:584/1775 train_time:443647ms step_avg:759.67ms
step:585/1775 train_time:444417ms step_avg:759.69ms
step:586/1775 train_time:445199ms step_avg:759.73ms
step:587/1775 train_time:445966ms step_avg:759.74ms
step:588/1775 train_time:446746ms step_avg:759.77ms
step:589/1775 train_time:447519ms step_avg:759.79ms
step:590/1775 train_time:448301ms step_avg:759.83ms
step:591/1775 train_time:449069ms step_avg:759.85ms
step:592/1775 train_time:449846ms step_avg:759.87ms
step:593/1775 train_time:450619ms step_avg:759.90ms
step:594/1775 train_time:451405ms step_avg:759.94ms
step:595/1775 train_time:452178ms step_avg:759.96ms
step:596/1775 train_time:452959ms step_avg:760.00ms
step:597/1775 train_time:453736ms step_avg:760.03ms
step:598/1775 train_time:454514ms step_avg:760.06ms
step:599/1775 train_time:455288ms step_avg:760.08ms
step:600/1775 train_time:456066ms step_avg:760.11ms
step:601/1775 train_time:456845ms step_avg:760.14ms
step:602/1775 train_time:457801ms step_avg:760.47ms
step:603/1775 train_time:458576ms step_avg:760.49ms
step:604/1775 train_time:459354ms step_avg:760.52ms
step:605/1775 train_time:460133ms step_avg:760.55ms
step:606/1775 train_time:460918ms step_avg:760.59ms
step:607/1775 train_time:461694ms step_avg:760.62ms
step:608/1775 train_time:462473ms step_avg:760.65ms
step:609/1775 train_time:463247ms step_avg:760.67ms
step:610/1775 train_time:464030ms step_avg:760.70ms
step:611/1775 train_time:464804ms step_avg:760.73ms
step:612/1775 train_time:465588ms step_avg:760.76ms
step:613/1775 train_time:466361ms step_avg:760.78ms
step:614/1775 train_time:467150ms step_avg:760.83ms
step:615/1775 train_time:467922ms step_avg:760.85ms
step:616/1775 train_time:468706ms step_avg:760.89ms
step:617/1775 train_time:469481ms step_avg:760.91ms
step:618/1775 train_time:470266ms step_avg:760.95ms
step:619/1775 train_time:471042ms step_avg:760.97ms
step:620/1775 train_time:471824ms step_avg:761.01ms
step:621/1775 train_time:472603ms step_avg:761.04ms
step:622/1775 train_time:473389ms step_avg:761.08ms
step:623/1775 train_time:474164ms step_avg:761.10ms
step:624/1775 train_time:474946ms step_avg:761.13ms
step:625/1775 train_time:475720ms step_avg:761.15ms
step:626/1775 train_time:476506ms step_avg:761.19ms
step:627/1775 train_time:477283ms step_avg:761.22ms
step:628/1775 train_time:478067ms step_avg:761.25ms
step:629/1775 train_time:478845ms step_avg:761.28ms
step:630/1775 train_time:479632ms step_avg:761.32ms
step:631/1775 train_time:480408ms step_avg:761.34ms
step:632/1775 train_time:481187ms step_avg:761.37ms
step:633/1775 train_time:481961ms step_avg:761.39ms
step:634/1775 train_time:482748ms step_avg:761.43ms
step:635/1775 train_time:483524ms step_avg:761.45ms
step:636/1775 train_time:484309ms step_avg:761.49ms
step:637/1775 train_time:485085ms step_avg:761.51ms
step:638/1775 train_time:485867ms step_avg:761.55ms
step:639/1775 train_time:486646ms step_avg:761.57ms
step:640/1775 train_time:487431ms step_avg:761.61ms
step:641/1775 train_time:488206ms step_avg:761.63ms
step:642/1775 train_time:488991ms step_avg:761.67ms
step:643/1775 train_time:489767ms step_avg:761.69ms
step:644/1775 train_time:490552ms step_avg:761.73ms
step:645/1775 train_time:491326ms step_avg:761.75ms
step:646/1775 train_time:492112ms step_avg:761.78ms
step:647/1775 train_time:492886ms step_avg:761.80ms
step:648/1775 train_time:493673ms step_avg:761.84ms
step:649/1775 train_time:494449ms step_avg:761.86ms
step:650/1775 train_time:495230ms step_avg:761.89ms
step:651/1775 train_time:496006ms step_avg:761.91ms
step:652/1775 train_time:496795ms step_avg:761.96ms
step:653/1775 train_time:497566ms step_avg:761.97ms
step:654/1775 train_time:498350ms step_avg:762.00ms
step:655/1775 train_time:499125ms step_avg:762.02ms
step:656/1775 train_time:499910ms step_avg:762.06ms
step:657/1775 train_time:500686ms step_avg:762.08ms
step:658/1775 train_time:501473ms step_avg:762.12ms
step:659/1775 train_time:502247ms step_avg:762.13ms
step:660/1775 train_time:503031ms step_avg:762.17ms
step:661/1775 train_time:503807ms step_avg:762.19ms
step:662/1775 train_time:504592ms step_avg:762.22ms
step:663/1775 train_time:505371ms step_avg:762.25ms
step:664/1775 train_time:506153ms step_avg:762.28ms
step:665/1775 train_time:506930ms step_avg:762.30ms
step:666/1775 train_time:507716ms step_avg:762.34ms
step:667/1775 train_time:508488ms step_avg:762.35ms
step:668/1775 train_time:509274ms step_avg:762.39ms
step:669/1775 train_time:510050ms step_avg:762.41ms
step:670/1775 train_time:510836ms step_avg:762.44ms
step:671/1775 train_time:511615ms step_avg:762.47ms
step:672/1775 train_time:512406ms step_avg:762.51ms
step:673/1775 train_time:513181ms step_avg:762.53ms
step:674/1775 train_time:513970ms step_avg:762.57ms
step:675/1775 train_time:514751ms step_avg:762.59ms
step:676/1775 train_time:515533ms step_avg:762.62ms
step:677/1775 train_time:516311ms step_avg:762.65ms
step:678/1775 train_time:517097ms step_avg:762.68ms
step:679/1775 train_time:517900ms step_avg:762.74ms
step:680/1775 train_time:518689ms step_avg:762.78ms
step:681/1775 train_time:519466ms step_avg:762.80ms
step:682/1775 train_time:520252ms step_avg:762.83ms
step:683/1775 train_time:521025ms step_avg:762.85ms
step:684/1775 train_time:521814ms step_avg:762.89ms
step:685/1775 train_time:522593ms step_avg:762.91ms
step:686/1775 train_time:523372ms step_avg:762.93ms
step:687/1775 train_time:524150ms step_avg:762.96ms
step:688/1775 train_time:524937ms step_avg:762.99ms
step:689/1775 train_time:525715ms step_avg:763.01ms
step:690/1775 train_time:526499ms step_avg:763.04ms
step:691/1775 train_time:527279ms step_avg:763.07ms
step:692/1775 train_time:528067ms step_avg:763.10ms
step:693/1775 train_time:528845ms step_avg:763.12ms
step:694/1775 train_time:529629ms step_avg:763.15ms
step:695/1775 train_time:530407ms step_avg:763.17ms
step:696/1775 train_time:531195ms step_avg:763.21ms
step:697/1775 train_time:531967ms step_avg:763.22ms
step:698/1775 train_time:532753ms step_avg:763.26ms
step:699/1775 train_time:533535ms step_avg:763.28ms
step:700/1775 train_time:534318ms step_avg:763.31ms
step:701/1775 train_time:535100ms step_avg:763.34ms
step:702/1775 train_time:535884ms step_avg:763.37ms
step:703/1775 train_time:536660ms step_avg:763.39ms
step:704/1775 train_time:537447ms step_avg:763.42ms
step:705/1775 train_time:538228ms step_avg:763.44ms
step:706/1775 train_time:539011ms step_avg:763.47ms
step:707/1775 train_time:539787ms step_avg:763.49ms
step:708/1775 train_time:540572ms step_avg:763.52ms
step:709/1775 train_time:541352ms step_avg:763.54ms
step:710/1775 train_time:542136ms step_avg:763.57ms
step:711/1775 train_time:542915ms step_avg:763.59ms
step:712/1775 train_time:543705ms step_avg:763.63ms
step:713/1775 train_time:544486ms step_avg:763.66ms
step:714/1775 train_time:545271ms step_avg:763.69ms
step:715/1775 train_time:546052ms step_avg:763.71ms
step:716/1775 train_time:546837ms step_avg:763.74ms
step:717/1775 train_time:547613ms step_avg:763.76ms
step:718/1775 train_time:548401ms step_avg:763.79ms
step:719/1775 train_time:549182ms step_avg:763.81ms
step:720/1775 train_time:549971ms step_avg:763.85ms
step:721/1775 train_time:550748ms step_avg:763.87ms
step:722/1775 train_time:551532ms step_avg:763.89ms
step:723/1775 train_time:552310ms step_avg:763.91ms
step:724/1775 train_time:553101ms step_avg:763.95ms
step:725/1775 train_time:553880ms step_avg:763.97ms
step:726/1775 train_time:554669ms step_avg:764.01ms
step:727/1775 train_time:555451ms step_avg:764.03ms
step:728/1775 train_time:556236ms step_avg:764.06ms
step:729/1775 train_time:557015ms step_avg:764.08ms
step:730/1775 train_time:557802ms step_avg:764.11ms
step:731/1775 train_time:558582ms step_avg:764.13ms
step:732/1775 train_time:559371ms step_avg:764.17ms
step:733/1775 train_time:560149ms step_avg:764.19ms
step:734/1775 train_time:560934ms step_avg:764.22ms
step:735/1775 train_time:561713ms step_avg:764.24ms
step:736/1775 train_time:562500ms step_avg:764.27ms
step:737/1775 train_time:563286ms step_avg:764.30ms
step:738/1775 train_time:564073ms step_avg:764.33ms
step:739/1775 train_time:564850ms step_avg:764.34ms
step:740/1775 train_time:565637ms step_avg:764.37ms
step:741/1775 train_time:566415ms step_avg:764.39ms
step:742/1775 train_time:567205ms step_avg:764.43ms
step:743/1775 train_time:567991ms step_avg:764.46ms
step:744/1775 train_time:568774ms step_avg:764.48ms
step:745/1775 train_time:569552ms step_avg:764.50ms
step:746/1775 train_time:570340ms step_avg:764.53ms
step:747/1775 train_time:571123ms step_avg:764.56ms
step:748/1775 train_time:571909ms step_avg:764.58ms
step:749/1775 train_time:572695ms step_avg:764.61ms
step:750/1775 train_time:573479ms step_avg:764.64ms
step:750/1775 val_loss:3.9904 val_malbo_loss:4.0166 train_time:573509ms step_avg:764.68ms
step:751/1775 train_time:574253ms step_avg:764.65ms
step:752/1775 train_time:575029ms step_avg:764.67ms
step:753/1775 train_time:575799ms step_avg:764.67ms
step:754/1775 train_time:576579ms step_avg:764.69ms
step:755/1775 train_time:577352ms step_avg:764.71ms
step:756/1775 train_time:578134ms step_avg:764.73ms
step:757/1775 train_time:578903ms step_avg:764.73ms
step:758/1775 train_time:579681ms step_avg:764.75ms
step:759/1775 train_time:580454ms step_avg:764.76ms
step:760/1775 train_time:581237ms step_avg:764.79ms
step:761/1775 train_time:582015ms step_avg:764.80ms
step:762/1775 train_time:582789ms step_avg:764.81ms
step:763/1775 train_time:583560ms step_avg:764.82ms
step:764/1775 train_time:584343ms step_avg:764.85ms
step:765/1775 train_time:585120ms step_avg:764.86ms
step:766/1775 train_time:585901ms step_avg:764.88ms
step:767/1775 train_time:586675ms step_avg:764.90ms
step:768/1775 train_time:587452ms step_avg:764.91ms
step:769/1775 train_time:588227ms step_avg:764.92ms
step:770/1775 train_time:589006ms step_avg:764.94ms
step:771/1775 train_time:589782ms step_avg:764.96ms
step:772/1775 train_time:590565ms step_avg:764.98ms
step:773/1775 train_time:591337ms step_avg:764.99ms
step:774/1775 train_time:592119ms step_avg:765.01ms
step:775/1775 train_time:592898ms step_avg:765.03ms
step:776/1775 train_time:593676ms step_avg:765.05ms
step:777/1775 train_time:594452ms step_avg:765.06ms
step:778/1775 train_time:595242ms step_avg:765.09ms
step:779/1775 train_time:596016ms step_avg:765.10ms
step:780/1775 train_time:596798ms step_avg:765.13ms
step:781/1775 train_time:597572ms step_avg:765.14ms
step:782/1775 train_time:598357ms step_avg:765.16ms
step:783/1775 train_time:599132ms step_avg:765.17ms
step:784/1775 train_time:599915ms step_avg:765.20ms
step:785/1775 train_time:600687ms step_avg:765.21ms
step:786/1775 train_time:601468ms step_avg:765.23ms
step:787/1775 train_time:602242ms step_avg:765.24ms
step:788/1775 train_time:603024ms step_avg:765.26ms
step:789/1775 train_time:603800ms step_avg:765.27ms
step:790/1775 train_time:604585ms step_avg:765.30ms
step:791/1775 train_time:605362ms step_avg:765.31ms
step:792/1775 train_time:606143ms step_avg:765.33ms
step:793/1775 train_time:606917ms step_avg:765.34ms
step:794/1775 train_time:607699ms step_avg:765.36ms
step:795/1775 train_time:608478ms step_avg:765.38ms
step:796/1775 train_time:609264ms step_avg:765.41ms
step:797/1775 train_time:610042ms step_avg:765.42ms
step:798/1775 train_time:610825ms step_avg:765.44ms
step:799/1775 train_time:611602ms step_avg:765.46ms
step:800/1775 train_time:612385ms step_avg:765.48ms
step:801/1775 train_time:613162ms step_avg:765.50ms
step:802/1775 train_time:613949ms step_avg:765.52ms
step:803/1775 train_time:614726ms step_avg:765.54ms
step:804/1775 train_time:615511ms step_avg:765.56ms
step:805/1775 train_time:616289ms step_avg:765.58ms
step:806/1775 train_time:617050ms step_avg:765.57ms
step:807/1775 train_time:617823ms step_avg:765.58ms
step:808/1775 train_time:618600ms step_avg:765.59ms
step:809/1775 train_time:619376ms step_avg:765.61ms
step:810/1775 train_time:620148ms step_avg:765.62ms
step:811/1775 train_time:620926ms step_avg:765.63ms
step:812/1775 train_time:621703ms step_avg:765.64ms
step:813/1775 train_time:622480ms step_avg:765.66ms
step:814/1775 train_time:623261ms step_avg:765.68ms
step:815/1775 train_time:624036ms step_avg:765.69ms
step:816/1775 train_time:624817ms step_avg:765.71ms
step:817/1775 train_time:625592ms step_avg:765.72ms
step:818/1775 train_time:626369ms step_avg:765.73ms
step:819/1775 train_time:627143ms step_avg:765.74ms
step:820/1775 train_time:627923ms step_avg:765.76ms
step:821/1775 train_time:628696ms step_avg:765.77ms
step:822/1775 train_time:629475ms step_avg:765.78ms
step:823/1775 train_time:630246ms step_avg:765.79ms
step:824/1775 train_time:631025ms step_avg:765.81ms
step:825/1775 train_time:631800ms step_avg:765.82ms
step:826/1775 train_time:632580ms step_avg:765.83ms
step:827/1775 train_time:633351ms step_avg:765.84ms
step:828/1775 train_time:634125ms step_avg:765.85ms
step:829/1775 train_time:634900ms step_avg:765.86ms
step:830/1775 train_time:635684ms step_avg:765.88ms
step:831/1775 train_time:636458ms step_avg:765.89ms
step:832/1775 train_time:637237ms step_avg:765.91ms
step:833/1775 train_time:638011ms step_avg:765.92ms
step:834/1775 train_time:638788ms step_avg:765.93ms
step:835/1775 train_time:639563ms step_avg:765.94ms
step:836/1775 train_time:640342ms step_avg:765.96ms
step:837/1775 train_time:641119ms step_avg:765.97ms
step:838/1775 train_time:641899ms step_avg:765.99ms
step:839/1775 train_time:642672ms step_avg:766.00ms
step:840/1775 train_time:643448ms step_avg:766.01ms
step:841/1775 train_time:644221ms step_avg:766.02ms
step:842/1775 train_time:645003ms step_avg:766.04ms
step:843/1775 train_time:645776ms step_avg:766.05ms
step:844/1775 train_time:646556ms step_avg:766.06ms
step:845/1775 train_time:647327ms step_avg:766.07ms
step:846/1775 train_time:648107ms step_avg:766.08ms
step:847/1775 train_time:648881ms step_avg:766.09ms
step:848/1775 train_time:649664ms step_avg:766.11ms
step:849/1775 train_time:650439ms step_avg:766.12ms
step:850/1775 train_time:651220ms step_avg:766.14ms
step:851/1775 train_time:651992ms step_avg:766.15ms
step:852/1775 train_time:652770ms step_avg:766.16ms
step:853/1775 train_time:653544ms step_avg:766.17ms
step:854/1775 train_time:654323ms step_avg:766.19ms
step:855/1775 train_time:655100ms step_avg:766.20ms
step:856/1775 train_time:655881ms step_avg:766.22ms
step:857/1775 train_time:656655ms step_avg:766.23ms
step:858/1775 train_time:657435ms step_avg:766.24ms
step:859/1775 train_time:658206ms step_avg:766.25ms
step:860/1775 train_time:658986ms step_avg:766.26ms
step:861/1775 train_time:659762ms step_avg:766.27ms
step:862/1775 train_time:660543ms step_avg:766.29ms
step:863/1775 train_time:661320ms step_avg:766.30ms
step:864/1775 train_time:662101ms step_avg:766.32ms
step:865/1775 train_time:662874ms step_avg:766.33ms
step:866/1775 train_time:663650ms step_avg:766.34ms
step:867/1775 train_time:664425ms step_avg:766.35ms
step:868/1775 train_time:665205ms step_avg:766.36ms
step:869/1775 train_time:665979ms step_avg:766.37ms
step:870/1775 train_time:666759ms step_avg:766.39ms
step:871/1775 train_time:667534ms step_avg:766.40ms
step:872/1775 train_time:668312ms step_avg:766.41ms
step:873/1775 train_time:669084ms step_avg:766.42ms
step:874/1775 train_time:669863ms step_avg:766.43ms
step:875/1775 train_time:670641ms step_avg:766.45ms
step:876/1775 train_time:671421ms step_avg:766.46ms
step:877/1775 train_time:672197ms step_avg:766.47ms
step:878/1775 train_time:672972ms step_avg:766.48ms
step:879/1775 train_time:673745ms step_avg:766.49ms
step:880/1775 train_time:674525ms step_avg:766.51ms
step:881/1775 train_time:675300ms step_avg:766.52ms
step:882/1775 train_time:676079ms step_avg:766.53ms
step:883/1775 train_time:676855ms step_avg:766.54ms
step:884/1775 train_time:677633ms step_avg:766.55ms
step:885/1775 train_time:678409ms step_avg:766.56ms
step:886/1775 train_time:679187ms step_avg:766.58ms
step:887/1775 train_time:679962ms step_avg:766.59ms
step:888/1775 train_time:680744ms step_avg:766.60ms
step:889/1775 train_time:681521ms step_avg:766.62ms
step:890/1775 train_time:682301ms step_avg:766.63ms
step:891/1775 train_time:683077ms step_avg:766.64ms
step:892/1775 train_time:683855ms step_avg:766.65ms
step:893/1775 train_time:684627ms step_avg:766.66ms
step:894/1775 train_time:685407ms step_avg:766.67ms
step:895/1775 train_time:686182ms step_avg:766.68ms
step:896/1775 train_time:686962ms step_avg:766.70ms
step:897/1775 train_time:687739ms step_avg:766.71ms
step:898/1775 train_time:688519ms step_avg:766.72ms
step:899/1775 train_time:689293ms step_avg:766.73ms
step:900/1775 train_time:690069ms step_avg:766.74ms
step:901/1775 train_time:690845ms step_avg:766.75ms
step:902/1775 train_time:691626ms step_avg:766.77ms
step:903/1775 train_time:692403ms step_avg:766.78ms
step:904/1775 train_time:693182ms step_avg:766.79ms
step:905/1775 train_time:693959ms step_avg:766.81ms
step:906/1775 train_time:694739ms step_avg:766.82ms
step:907/1775 train_time:695512ms step_avg:766.83ms
step:908/1775 train_time:696289ms step_avg:766.84ms
step:909/1775 train_time:697064ms step_avg:766.85ms
step:910/1775 train_time:697844ms step_avg:766.86ms
step:911/1775 train_time:698622ms step_avg:766.87ms
step:912/1775 train_time:699403ms step_avg:766.89ms
step:913/1775 train_time:700179ms step_avg:766.90ms
step:914/1775 train_time:700957ms step_avg:766.91ms
step:915/1775 train_time:701730ms step_avg:766.92ms
step:916/1775 train_time:702507ms step_avg:766.93ms
step:917/1775 train_time:703281ms step_avg:766.94ms
step:918/1775 train_time:704061ms step_avg:766.95ms
step:919/1775 train_time:705019ms step_avg:767.16ms
step:920/1775 train_time:705810ms step_avg:767.18ms
step:921/1775 train_time:706582ms step_avg:767.19ms
step:922/1775 train_time:707366ms step_avg:767.21ms
step:923/1775 train_time:708149ms step_avg:767.23ms
step:924/1775 train_time:708934ms step_avg:767.24ms
step:925/1775 train_time:709712ms step_avg:767.26ms
step:926/1775 train_time:710496ms step_avg:767.27ms
step:927/1775 train_time:711279ms step_avg:767.29ms
step:928/1775 train_time:712064ms step_avg:767.31ms
step:929/1775 train_time:712843ms step_avg:767.32ms
step:930/1775 train_time:713629ms step_avg:767.34ms
step:931/1775 train_time:714407ms step_avg:767.35ms
step:932/1775 train_time:715190ms step_avg:767.37ms
step:933/1775 train_time:715971ms step_avg:767.39ms
step:934/1775 train_time:716753ms step_avg:767.40ms
step:935/1775 train_time:717529ms step_avg:767.41ms
step:936/1775 train_time:718318ms step_avg:767.43ms
step:937/1775 train_time:719097ms step_avg:767.45ms
step:938/1775 train_time:719883ms step_avg:767.47ms
step:939/1775 train_time:720662ms step_avg:767.48ms
step:940/1775 train_time:721447ms step_avg:767.50ms
step:941/1775 train_time:722223ms step_avg:767.51ms
step:942/1775 train_time:723009ms step_avg:767.53ms
step:943/1775 train_time:723786ms step_avg:767.54ms
step:944/1775 train_time:724572ms step_avg:767.56ms
step:945/1775 train_time:725349ms step_avg:767.57ms
step:946/1775 train_time:726132ms step_avg:767.58ms
step:947/1775 train_time:726916ms step_avg:767.60ms
step:948/1775 train_time:727698ms step_avg:767.61ms
step:949/1775 train_time:728476ms step_avg:767.63ms
step:950/1775 train_time:729263ms step_avg:767.65ms
step:951/1775 train_time:730041ms step_avg:767.66ms
step:952/1775 train_time:730827ms step_avg:767.68ms
step:953/1775 train_time:731606ms step_avg:767.69ms
step:954/1775 train_time:732390ms step_avg:767.70ms
step:955/1775 train_time:733170ms step_avg:767.72ms
step:956/1775 train_time:733955ms step_avg:767.73ms
step:957/1775 train_time:734733ms step_avg:767.75ms
step:958/1775 train_time:735519ms step_avg:767.77ms
step:959/1775 train_time:736304ms step_avg:767.78ms
step:960/1775 train_time:737083ms step_avg:767.79ms
step:961/1775 train_time:737863ms step_avg:767.81ms
step:962/1775 train_time:738648ms step_avg:767.83ms
step:963/1775 train_time:739426ms step_avg:767.84ms
step:964/1775 train_time:740208ms step_avg:767.85ms
step:965/1775 train_time:740987ms step_avg:767.86ms
step:966/1775 train_time:741770ms step_avg:767.88ms
step:967/1775 train_time:742545ms step_avg:767.89ms
step:968/1775 train_time:743332ms step_avg:767.91ms
step:969/1775 train_time:744111ms step_avg:767.92ms
step:970/1775 train_time:744894ms step_avg:767.93ms
step:971/1775 train_time:745670ms step_avg:767.94ms
step:972/1775 train_time:746453ms step_avg:767.96ms
step:973/1775 train_time:747232ms step_avg:767.97ms
step:974/1775 train_time:748014ms step_avg:767.98ms
step:975/1775 train_time:748793ms step_avg:767.99ms
step:976/1775 train_time:749575ms step_avg:768.01ms
step:977/1775 train_time:750354ms step_avg:768.02ms
step:978/1775 train_time:751136ms step_avg:768.03ms
step:979/1775 train_time:751910ms step_avg:768.04ms
step:980/1775 train_time:752695ms step_avg:768.06ms
step:981/1775 train_time:753474ms step_avg:768.07ms
step:982/1775 train_time:754258ms step_avg:768.08ms
step:983/1775 train_time:755035ms step_avg:768.09ms
step:984/1775 train_time:755824ms step_avg:768.11ms
step:985/1775 train_time:756602ms step_avg:768.12ms
step:986/1775 train_time:757386ms step_avg:768.14ms
step:987/1775 train_time:758164ms step_avg:768.15ms
step:988/1775 train_time:758948ms step_avg:768.17ms
step:989/1775 train_time:759726ms step_avg:768.18ms
step:990/1775 train_time:760510ms step_avg:768.19ms
step:991/1775 train_time:761285ms step_avg:768.20ms
step:992/1775 train_time:762071ms step_avg:768.22ms
step:993/1775 train_time:762849ms step_avg:768.23ms
step:994/1775 train_time:763633ms step_avg:768.24ms
step:995/1775 train_time:764410ms step_avg:768.25ms
step:996/1775 train_time:765192ms step_avg:768.27ms
step:997/1775 train_time:765969ms step_avg:768.27ms
step:998/1775 train_time:766750ms step_avg:768.29ms
step:999/1775 train_time:767527ms step_avg:768.30ms
step:1000/1775 train_time:768312ms step_avg:768.31ms
step:1000/1775 val_loss:3.7333 val_malbo_loss:3.7591 train_time:768341ms step_avg:768.34ms
step:1001/1775 train_time:769089ms step_avg:768.32ms
step:1002/1775 train_time:769872ms step_avg:768.34ms
step:1003/1775 train_time:770649ms step_avg:768.34ms
step:1004/1775 train_time:771435ms step_avg:768.36ms
step:1005/1775 train_time:772209ms step_avg:768.37ms
step:1006/1775 train_time:772989ms step_avg:768.38ms
step:1007/1775 train_time:773772ms step_avg:768.39ms
step:1008/1775 train_time:774549ms step_avg:768.40ms
step:1009/1775 train_time:775326ms step_avg:768.41ms
step:1010/1775 train_time:776110ms step_avg:768.43ms
step:1011/1775 train_time:776886ms step_avg:768.43ms
step:1012/1775 train_time:777670ms step_avg:768.45ms
step:1013/1775 train_time:778444ms step_avg:768.45ms
step:1014/1775 train_time:779229ms step_avg:768.47ms
step:1015/1775 train_time:780004ms step_avg:768.48ms
step:1016/1775 train_time:780790ms step_avg:768.49ms
step:1017/1775 train_time:781569ms step_avg:768.50ms
step:1018/1775 train_time:782352ms step_avg:768.52ms
step:1019/1775 train_time:783127ms step_avg:768.53ms
step:1020/1775 train_time:783914ms step_avg:768.54ms
step:1021/1775 train_time:784686ms step_avg:768.55ms
step:1022/1775 train_time:785470ms step_avg:768.56ms
step:1023/1775 train_time:786249ms step_avg:768.57ms
step:1024/1775 train_time:787033ms step_avg:768.59ms
step:1025/1775 train_time:787810ms step_avg:768.60ms
step:1026/1775 train_time:788591ms step_avg:768.61ms
step:1027/1775 train_time:789370ms step_avg:768.62ms
step:1028/1775 train_time:790152ms step_avg:768.63ms
step:1029/1775 train_time:790927ms step_avg:768.64ms
step:1030/1775 train_time:791708ms step_avg:768.65ms
step:1031/1775 train_time:792484ms step_avg:768.66ms
step:1032/1775 train_time:793269ms step_avg:768.67ms
step:1033/1775 train_time:794049ms step_avg:768.68ms
step:1034/1775 train_time:794833ms step_avg:768.70ms
step:1035/1775 train_time:795607ms step_avg:768.70ms
step:1036/1775 train_time:796390ms step_avg:768.72ms
step:1037/1775 train_time:797169ms step_avg:768.73ms
step:1038/1775 train_time:797950ms step_avg:768.74ms
step:1039/1775 train_time:798727ms step_avg:768.75ms
step:1040/1775 train_time:799509ms step_avg:768.76ms
step:1041/1775 train_time:800286ms step_avg:768.77ms
step:1042/1775 train_time:801071ms step_avg:768.78ms
step:1043/1775 train_time:801848ms step_avg:768.79ms
step:1044/1775 train_time:802632ms step_avg:768.80ms
step:1045/1775 train_time:803410ms step_avg:768.81ms
step:1046/1775 train_time:804189ms step_avg:768.82ms
step:1047/1775 train_time:804968ms step_avg:768.83ms
step:1048/1775 train_time:805752ms step_avg:768.85ms
step:1049/1775 train_time:806524ms step_avg:768.85ms
step:1050/1775 train_time:807315ms step_avg:768.87ms
step:1051/1775 train_time:808088ms step_avg:768.88ms
step:1052/1775 train_time:808872ms step_avg:768.89ms
step:1053/1775 train_time:809646ms step_avg:768.89ms
step:1054/1775 train_time:810431ms step_avg:768.91ms
step:1055/1775 train_time:811206ms step_avg:768.92ms
step:1056/1775 train_time:811990ms step_avg:768.93ms
step:1057/1775 train_time:812767ms step_avg:768.94ms
step:1058/1775 train_time:813551ms step_avg:768.95ms
step:1059/1775 train_time:814327ms step_avg:768.96ms
step:1060/1775 train_time:815111ms step_avg:768.97ms
step:1061/1775 train_time:815888ms step_avg:768.98ms
step:1062/1775 train_time:816672ms step_avg:768.99ms
step:1063/1775 train_time:817445ms step_avg:769.00ms
step:1064/1775 train_time:818229ms step_avg:769.01ms
step:1065/1775 train_time:819005ms step_avg:769.02ms
step:1066/1775 train_time:819791ms step_avg:769.03ms
step:1067/1775 train_time:820565ms step_avg:769.04ms
step:1068/1775 train_time:821351ms step_avg:769.06ms
step:1069/1775 train_time:822129ms step_avg:769.06ms
step:1070/1775 train_time:822911ms step_avg:769.08ms
step:1071/1775 train_time:823691ms step_avg:769.09ms
step:1072/1775 train_time:824471ms step_avg:769.10ms
step:1073/1775 train_time:825246ms step_avg:769.10ms
step:1074/1775 train_time:826033ms step_avg:769.12ms
step:1075/1775 train_time:826809ms step_avg:769.12ms
step:1076/1775 train_time:827588ms step_avg:769.13ms
step:1077/1775 train_time:828366ms step_avg:769.14ms
step:1078/1775 train_time:829153ms step_avg:769.16ms
step:1079/1775 train_time:829930ms step_avg:769.17ms
step:1080/1775 train_time:830712ms step_avg:769.18ms
step:1081/1775 train_time:831490ms step_avg:769.19ms
step:1082/1775 train_time:832274ms step_avg:769.20ms
step:1083/1775 train_time:833047ms step_avg:769.20ms
step:1084/1775 train_time:833832ms step_avg:769.22ms
step:1085/1775 train_time:834609ms step_avg:769.23ms
step:1086/1775 train_time:835390ms step_avg:769.24ms
step:1087/1775 train_time:836173ms step_avg:769.25ms
step:1088/1775 train_time:836950ms step_avg:769.26ms
step:1089/1775 train_time:837729ms step_avg:769.26ms
step:1090/1775 train_time:838511ms step_avg:769.28ms
step:1091/1775 train_time:839284ms step_avg:769.28ms
step:1092/1775 train_time:840070ms step_avg:769.29ms
step:1093/1775 train_time:840847ms step_avg:769.30ms
step:1094/1775 train_time:841631ms step_avg:769.31ms
step:1095/1775 train_time:842406ms step_avg:769.32ms
step:1096/1775 train_time:843191ms step_avg:769.33ms
step:1097/1775 train_time:843969ms step_avg:769.34ms
step:1098/1775 train_time:844753ms step_avg:769.36ms
step:1099/1775 train_time:845529ms step_avg:769.36ms
step:1100/1775 train_time:846308ms step_avg:769.37ms
step:1101/1775 train_time:847087ms step_avg:769.38ms
step:1102/1775 train_time:847870ms step_avg:769.39ms
step:1103/1775 train_time:848646ms step_avg:769.40ms
step:1104/1775 train_time:849428ms step_avg:769.41ms
step:1105/1775 train_time:850209ms step_avg:769.42ms
step:1106/1775 train_time:850991ms step_avg:769.43ms
step:1107/1775 train_time:851768ms step_avg:769.44ms
step:1108/1775 train_time:852549ms step_avg:769.45ms
step:1109/1775 train_time:853325ms step_avg:769.45ms
step:1110/1775 train_time:854108ms step_avg:769.47ms
step:1111/1775 train_time:854886ms step_avg:769.47ms
step:1112/1775 train_time:855671ms step_avg:769.49ms
step:1113/1775 train_time:856446ms step_avg:769.49ms
step:1114/1775 train_time:857232ms step_avg:769.51ms
step:1115/1775 train_time:858006ms step_avg:769.51ms
step:1116/1775 train_time:858792ms step_avg:769.53ms
step:1117/1775 train_time:859564ms step_avg:769.53ms
step:1118/1775 train_time:860348ms step_avg:769.54ms
step:1119/1775 train_time:861125ms step_avg:769.55ms
step:1120/1775 train_time:861908ms step_avg:769.56ms
step:1121/1775 train_time:862684ms step_avg:769.57ms
step:1122/1775 train_time:863470ms step_avg:769.58ms
step:1123/1775 train_time:864243ms step_avg:769.58ms
step:1124/1775 train_time:865026ms step_avg:769.60ms
step:1125/1775 train_time:865809ms step_avg:769.61ms
step:1126/1775 train_time:866591ms step_avg:769.62ms
step:1127/1775 train_time:867366ms step_avg:769.62ms
step:1128/1775 train_time:868149ms step_avg:769.64ms
step:1129/1775 train_time:868930ms step_avg:769.65ms
step:1130/1775 train_time:869714ms step_avg:769.66ms
step:1131/1775 train_time:870489ms step_avg:769.66ms
step:1132/1775 train_time:871270ms step_avg:769.67ms
step:1133/1775 train_time:872048ms step_avg:769.68ms
step:1134/1775 train_time:872827ms step_avg:769.69ms
step:1135/1775 train_time:873608ms step_avg:769.70ms
step:1136/1775 train_time:874389ms step_avg:769.71ms
step:1137/1775 train_time:875166ms step_avg:769.71ms
step:1138/1775 train_time:875948ms step_avg:769.73ms
step:1139/1775 train_time:876725ms step_avg:769.73ms
step:1140/1775 train_time:877506ms step_avg:769.74ms
step:1141/1775 train_time:878284ms step_avg:769.75ms
step:1142/1775 train_time:879072ms step_avg:769.77ms
step:1143/1775 train_time:879845ms step_avg:769.77ms
step:1144/1775 train_time:880629ms step_avg:769.78ms
step:1145/1775 train_time:881406ms step_avg:769.79ms
step:1146/1775 train_time:882188ms step_avg:769.80ms
step:1147/1775 train_time:882967ms step_avg:769.81ms
step:1148/1775 train_time:883749ms step_avg:769.82ms
step:1149/1775 train_time:884527ms step_avg:769.82ms
step:1150/1775 train_time:885309ms step_avg:769.83ms
step:1151/1775 train_time:886084ms step_avg:769.84ms
step:1152/1775 train_time:886867ms step_avg:769.85ms
step:1153/1775 train_time:887643ms step_avg:769.85ms
step:1154/1775 train_time:888428ms step_avg:769.87ms
step:1155/1775 train_time:889205ms step_avg:769.87ms
step:1156/1775 train_time:889989ms step_avg:769.89ms
step:1157/1775 train_time:890767ms step_avg:769.89ms
step:1158/1775 train_time:1033148ms step_avg:892.18ms
step:1159/1775 train_time:1172884ms step_avg:1011.98ms
step:1160/1775 train_time:1174002ms step_avg:1012.07ms
step:1161/1775 train_time:1175104ms step_avg:1012.15ms
step:1162/1775 train_time:1176219ms step_avg:1012.24ms
step:1163/1775 train_time:1177328ms step_avg:1012.32ms
step:1164/1775 train_time:1178442ms step_avg:1012.41ms
step:1165/1775 train_time:1179552ms step_avg:1012.49ms
step:1166/1775 train_time:1180669ms step_avg:1012.58ms
step:1167/1775 train_time:1181777ms step_avg:1012.66ms
step:1168/1775 train_time:1182892ms step_avg:1012.75ms
step:1169/1775 train_time:1183997ms step_avg:1012.83ms
step:1170/1775 train_time:1185117ms step_avg:1012.92ms
step:1171/1775 train_time:1186226ms step_avg:1013.00ms
step:1172/1775 train_time:1187350ms step_avg:1013.10ms
step:1173/1775 train_time:1188458ms step_avg:1013.18ms
step:1174/1775 train_time:1189577ms step_avg:1013.27ms
step:1175/1775 train_time:1190686ms step_avg:1013.35ms
step:1176/1775 train_time:1191809ms step_avg:1013.44ms
step:1177/1775 train_time:1192921ms step_avg:1013.53ms
step:1178/1775 train_time:1194040ms step_avg:1013.62ms
step:1179/1775 train_time:1195155ms step_avg:1013.70ms
step:1180/1775 train_time:1196273ms step_avg:1013.79ms
step:1181/1775 train_time:1197385ms step_avg:1013.87ms
step:1182/1775 train_time:1198505ms step_avg:1013.96ms
step:1183/1775 train_time:1199616ms step_avg:1014.05ms
step:1184/1775 train_time:1200735ms step_avg:1014.13ms
step:1185/1775 train_time:1201849ms step_avg:1014.22ms
step:1186/1775 train_time:1202975ms step_avg:1014.31ms
step:1187/1775 train_time:1204091ms step_avg:1014.40ms
step:1188/1775 train_time:1205214ms step_avg:1014.49ms
step:1189/1775 train_time:1206331ms step_avg:1014.58ms
step:1190/1775 train_time:1207450ms step_avg:1014.66ms
step:1191/1775 train_time:1208565ms step_avg:1014.75ms
step:1192/1775 train_time:1209690ms step_avg:1014.84ms
step:1193/1775 train_time:1210808ms step_avg:1014.93ms
step:1194/1775 train_time:1211927ms step_avg:1015.01ms
step:1195/1775 train_time:1213037ms step_avg:1015.09ms
step:1196/1775 train_time:1214167ms step_avg:1015.19ms
step:1197/1775 train_time:1215279ms step_avg:1015.27ms
step:1198/1775 train_time:1216400ms step_avg:1015.36ms
step:1199/1775 train_time:1217516ms step_avg:1015.44ms
step:1200/1775 train_time:1218637ms step_avg:1015.53ms
step:1201/1775 train_time:1219755ms step_avg:1015.62ms
step:1202/1775 train_time:1220880ms step_avg:1015.71ms
step:1203/1775 train_time:1221994ms step_avg:1015.79ms
step:1204/1775 train_time:1223121ms step_avg:1015.88ms
step:1205/1775 train_time:1224236ms step_avg:1015.96ms
step:1206/1775 train_time:1225360ms step_avg:1016.05ms
step:1207/1775 train_time:1226479ms step_avg:1016.14ms
step:1208/1775 train_time:1227598ms step_avg:1016.22ms
step:1209/1775 train_time:1228710ms step_avg:1016.30ms
step:1210/1775 train_time:1229839ms step_avg:1016.40ms
step:1211/1775 train_time:1230956ms step_avg:1016.48ms
step:1212/1775 train_time:1232180ms step_avg:1016.65ms
step:1213/1775 train_time:1233386ms step_avg:1016.81ms
step:1214/1775 train_time:1234490ms step_avg:1016.88ms
step:1215/1775 train_time:1235610ms step_avg:1016.96ms
step:1216/1775 train_time:1236737ms step_avg:1017.05ms
step:1217/1775 train_time:1237853ms step_avg:1017.13ms
step:1218/1775 train_time:1238985ms step_avg:1017.23ms
step:1219/1775 train_time:1240096ms step_avg:1017.31ms
step:1220/1775 train_time:1241229ms step_avg:1017.40ms
step:1221/1775 train_time:1242350ms step_avg:1017.49ms
step:1222/1775 train_time:1243481ms step_avg:1017.58ms
step:1223/1775 train_time:1244592ms step_avg:1017.66ms
step:1224/1775 train_time:1245730ms step_avg:1017.75ms
step:1225/1775 train_time:1246844ms step_avg:1017.83ms
step:1226/1775 train_time:1247968ms step_avg:1017.92ms
step:1227/1775 train_time:1249090ms step_avg:1018.00ms
step:1228/1775 train_time:1250223ms step_avg:1018.10ms
step:1229/1775 train_time:1251341ms step_avg:1018.18ms
step:1230/1775 train_time:1252474ms step_avg:1018.27ms
step:1231/1775 train_time:1253594ms step_avg:1018.35ms
step:1232/1775 train_time:1254720ms step_avg:1018.44ms
step:1233/1775 train_time:1255840ms step_avg:1018.52ms
step:1234/1775 train_time:1256968ms step_avg:1018.61ms
step:1235/1775 train_time:1258083ms step_avg:1018.69ms
step:1236/1775 train_time:1259208ms step_avg:1018.78ms
step:1237/1775 train_time:1260325ms step_avg:1018.86ms
step:1238/1775 train_time:1261456ms step_avg:1018.95ms
step:1239/1775 train_time:1262579ms step_avg:1019.03ms
step:1240/1775 train_time:1263711ms step_avg:1019.12ms
step:1241/1775 train_time:1264824ms step_avg:1019.20ms
step:1242/1775 train_time:1265950ms step_avg:1019.28ms
step:1243/1775 train_time:1267070ms step_avg:1019.36ms
step:1244/1775 train_time:1268198ms step_avg:1019.45ms
step:1245/1775 train_time:1269320ms step_avg:1019.53ms
step:1246/1775 train_time:1270453ms step_avg:1019.62ms
step:1247/1775 train_time:1271578ms step_avg:1019.71ms
step:1248/1775 train_time:1272696ms step_avg:1019.79ms
step:1249/1775 train_time:1273825ms step_avg:1019.88ms
step:1250/1775 train_time:1274953ms step_avg:1019.96ms
step:1250/1775 val_loss:3.5059 val_malbo_loss:3.5318 train_time:1275008ms step_avg:1020.01ms
step:1251/1775 train_time:1276064ms step_avg:1020.04ms
step:1252/1775 train_time:1277181ms step_avg:1020.11ms
step:1253/1775 train_time:1278288ms step_avg:1020.18ms
step:1254/1775 train_time:1279403ms step_avg:1020.26ms
step:1255/1775 train_time:1280516ms step_avg:1020.33ms
step:1256/1775 train_time:1281632ms step_avg:1020.41ms
step:1257/1775 train_time:1282740ms step_avg:1020.48ms
step:1258/1775 train_time:1283856ms step_avg:1020.55ms
step:1259/1775 train_time:1284967ms step_avg:1020.63ms
step:1260/1775 train_time:1286083ms step_avg:1020.70ms
step:1261/1775 train_time:1287194ms step_avg:1020.77ms
step:1262/1775 train_time:1288316ms step_avg:1020.85ms
step:1263/1775 train_time:1289424ms step_avg:1020.92ms
step:1264/1775 train_time:1290547ms step_avg:1021.00ms
step:1265/1775 train_time:1291657ms step_avg:1021.07ms
step:1266/1775 train_time:1292784ms step_avg:1021.16ms
step:1267/1775 train_time:1293895ms step_avg:1021.23ms
step:1268/1775 train_time:1295016ms step_avg:1021.31ms
step:1269/1775 train_time:1296125ms step_avg:1021.38ms
step:1270/1775 train_time:1297251ms step_avg:1021.46ms
step:1271/1775 train_time:1298364ms step_avg:1021.53ms
step:1272/1775 train_time:1299485ms step_avg:1021.61ms
step:1273/1775 train_time:1300598ms step_avg:1021.68ms
step:1274/1775 train_time:1301720ms step_avg:1021.76ms
step:1275/1775 train_time:1302836ms step_avg:1021.83ms
step:1276/1775 train_time:1303962ms step_avg:1021.91ms
step:1277/1775 train_time:1305074ms step_avg:1021.98ms
step:1278/1775 train_time:1306197ms step_avg:1022.06ms
step:1279/1775 train_time:1307314ms step_avg:1022.14ms
step:1280/1775 train_time:1308430ms step_avg:1022.21ms
step:1281/1775 train_time:1309546ms step_avg:1022.28ms
step:1282/1775 train_time:1310671ms step_avg:1022.36ms
step:1283/1775 train_time:1311787ms step_avg:1022.44ms
step:1284/1775 train_time:1312908ms step_avg:1022.51ms
step:1285/1775 train_time:1314024ms step_avg:1022.59ms
step:1286/1775 train_time:1315143ms step_avg:1022.66ms
step:1287/1775 train_time:1316262ms step_avg:1022.74ms
step:1288/1775 train_time:1317383ms step_avg:1022.81ms
step:1289/1775 train_time:1318498ms step_avg:1022.88ms
step:1290/1775 train_time:1319622ms step_avg:1022.96ms
step:1291/1775 train_time:1320739ms step_avg:1023.04ms
step:1292/1775 train_time:1321863ms step_avg:1023.11ms
step:1293/1775 train_time:1322979ms step_avg:1023.19ms
step:1294/1775 train_time:1324104ms step_avg:1023.26ms
step:1295/1775 train_time:1325220ms step_avg:1023.34ms
step:1296/1775 train_time:1326343ms step_avg:1023.41ms
step:1297/1775 train_time:1327461ms step_avg:1023.49ms
step:1298/1775 train_time:1328583ms step_avg:1023.56ms
step:1299/1775 train_time:1329702ms step_avg:1023.64ms
step:1300/1775 train_time:1330820ms step_avg:1023.71ms
step:1301/1775 train_time:1331938ms step_avg:1023.78ms
step:1302/1775 train_time:1333061ms step_avg:1023.86ms
step:1303/1775 train_time:1334178ms step_avg:1023.93ms
step:1304/1775 train_time:1335303ms step_avg:1024.01ms
step:1305/1775 train_time:1336422ms step_avg:1024.08ms
step:1306/1775 train_time:1337548ms step_avg:1024.16ms
step:1307/1775 train_time:1338660ms step_avg:1024.22ms
step:1308/1775 train_time:1339792ms step_avg:1024.31ms
step:1309/1775 train_time:1340905ms step_avg:1024.37ms
step:1310/1775 train_time:1342031ms step_avg:1024.45ms
step:1311/1775 train_time:1343154ms step_avg:1024.53ms
step:1312/1775 train_time:1344280ms step_avg:1024.60ms
step:1313/1775 train_time:1345396ms step_avg:1024.67ms
step:1314/1775 train_time:1346525ms step_avg:1024.75ms
step:1315/1775 train_time:1347644ms step_avg:1024.82ms
step:1316/1775 train_time:1348771ms step_avg:1024.90ms
step:1317/1775 train_time:1349882ms step_avg:1024.97ms
step:1318/1775 train_time:1351016ms step_avg:1025.05ms
step:1319/1775 train_time:1352134ms step_avg:1025.12ms
step:1320/1775 train_time:1353262ms step_avg:1025.20ms
step:1321/1775 train_time:1354386ms step_avg:1025.27ms
step:1322/1775 train_time:1355510ms step_avg:1025.35ms
step:1323/1775 train_time:1356637ms step_avg:1025.42ms
step:1324/1775 train_time:1357765ms step_avg:1025.50ms
step:1325/1775 train_time:1358884ms step_avg:1025.57ms
step:1326/1775 train_time:1360010ms step_avg:1025.65ms
step:1327/1775 train_time:1361127ms step_avg:1025.72ms
step:1328/1775 train_time:1362257ms step_avg:1025.80ms
step:1329/1775 train_time:1363382ms step_avg:1025.87ms
step:1330/1775 train_time:1364509ms step_avg:1025.95ms
step:1331/1775 train_time:1365627ms step_avg:1026.02ms
step:1332/1775 train_time:1366752ms step_avg:1026.09ms
step:1333/1775 train_time:1367878ms step_avg:1026.17ms
step:1334/1775 train_time:1369001ms step_avg:1026.24ms
step:1335/1775 train_time:1370121ms step_avg:1026.31ms
step:1336/1775 train_time:1371248ms step_avg:1026.38ms
step:1337/1775 train_time:1372368ms step_avg:1026.45ms
step:1338/1775 train_time:1373496ms step_avg:1026.53ms
step:1339/1775 train_time:1374617ms step_avg:1026.60ms
step:1340/1775 train_time:1375744ms step_avg:1026.67ms
step:1341/1775 train_time:1376864ms step_avg:1026.74ms
step:1342/1775 train_time:1377993ms step_avg:1026.82ms
step:1343/1775 train_time:1379107ms step_avg:1026.89ms
step:1344/1775 train_time:1380246ms step_avg:1026.97ms
step:1345/1775 train_time:1381364ms step_avg:1027.04ms
step:1346/1775 train_time:1382493ms step_avg:1027.11ms
step:1347/1775 train_time:1383609ms step_avg:1027.18ms
step:1348/1775 train_time:1384742ms step_avg:1027.26ms
step:1349/1775 train_time:1385857ms step_avg:1027.32ms
step:1350/1775 train_time:1386985ms step_avg:1027.40ms
step:1351/1775 train_time:1388107ms step_avg:1027.47ms
step:1352/1775 train_time:1389233ms step_avg:1027.54ms
step:1353/1775 train_time:1390355ms step_avg:1027.61ms
step:1354/1775 train_time:1391485ms step_avg:1027.68ms
step:1355/1775 train_time:1392607ms step_avg:1027.75ms
step:1356/1775 train_time:1393734ms step_avg:1027.83ms
step:1357/1775 train_time:1394859ms step_avg:1027.90ms
step:1358/1775 train_time:1395989ms step_avg:1027.97ms
step:1359/1775 train_time:1397105ms step_avg:1028.04ms
step:1360/1775 train_time:1398232ms step_avg:1028.11ms
step:1361/1775 train_time:1399346ms step_avg:1028.18ms
step:1362/1775 train_time:1400469ms step_avg:1028.24ms
step:1363/1775 train_time:1401592ms step_avg:1028.31ms
step:1364/1775 train_time:1402723ms step_avg:1028.39ms
step:1365/1775 train_time:1403839ms step_avg:1028.45ms
step:1366/1775 train_time:1404967ms step_avg:1028.53ms
step:1367/1775 train_time:1406090ms step_avg:1028.60ms
step:1368/1775 train_time:1407216ms step_avg:1028.67ms
step:1369/1775 train_time:1408337ms step_avg:1028.73ms
step:1370/1775 train_time:1409468ms step_avg:1028.81ms
step:1371/1775 train_time:1410592ms step_avg:1028.88ms
step:1372/1775 train_time:1411718ms step_avg:1028.95ms
step:1373/1775 train_time:1412839ms step_avg:1029.02ms
step:1374/1775 train_time:1413963ms step_avg:1029.09ms
step:1375/1775 train_time:1415089ms step_avg:1029.16ms
step:1376/1775 train_time:1416212ms step_avg:1029.22ms
step:1377/1775 train_time:1417335ms step_avg:1029.29ms
step:1378/1775 train_time:1418462ms step_avg:1029.36ms
step:1379/1775 train_time:1419583ms step_avg:1029.43ms
step:1380/1775 train_time:1420712ms step_avg:1029.50ms
step:1381/1775 train_time:1421838ms step_avg:1029.57ms
step:1382/1775 train_time:1422964ms step_avg:1029.64ms
step:1383/1775 train_time:1424084ms step_avg:1029.71ms
step:1384/1775 train_time:1425212ms step_avg:1029.78ms
step:1385/1775 train_time:1426329ms step_avg:1029.84ms
step:1386/1775 train_time:1427463ms step_avg:1029.92ms
step:1387/1775 train_time:1428580ms step_avg:1029.98ms
step:1388/1775 train_time:1429705ms step_avg:1030.05ms
step:1389/1775 train_time:1430826ms step_avg:1030.11ms
step:1390/1775 train_time:1431956ms step_avg:1030.18ms
step:1391/1775 train_time:1433076ms step_avg:1030.25ms
step:1392/1775 train_time:1434206ms step_avg:1030.32ms
step:1393/1775 train_time:1435324ms step_avg:1030.38ms
step:1394/1775 train_time:1436451ms step_avg:1030.45ms
step:1395/1775 train_time:1437578ms step_avg:1030.52ms
step:1396/1775 train_time:1438711ms step_avg:1030.60ms
step:1397/1775 train_time:1439831ms step_avg:1030.66ms
step:1398/1775 train_time:1440962ms step_avg:1030.73ms
step:1399/1775 train_time:1442081ms step_avg:1030.79ms
step:1400/1775 train_time:1443214ms step_avg:1030.87ms
step:1401/1775 train_time:1444332ms step_avg:1030.93ms
step:1402/1775 train_time:1445461ms step_avg:1031.00ms
step:1403/1775 train_time:1446583ms step_avg:1031.06ms
step:1404/1775 train_time:1447710ms step_avg:1031.13ms
step:1405/1775 train_time:1448829ms step_avg:1031.20ms
step:1406/1775 train_time:1449955ms step_avg:1031.26ms
step:1407/1775 train_time:1451082ms step_avg:1031.33ms
step:1408/1775 train_time:1452206ms step_avg:1031.40ms
step:1409/1775 train_time:1453331ms step_avg:1031.46ms
step:1410/1775 train_time:1454459ms step_avg:1031.53ms
step:1411/1775 train_time:1455569ms step_avg:1031.59ms
step:1412/1775 train_time:1456716ms step_avg:1031.67ms
step:1413/1775 train_time:1457841ms step_avg:1031.73ms
step:1414/1775 train_time:1458965ms step_avg:1031.80ms
step:1415/1775 train_time:1460088ms step_avg:1031.86ms
step:1416/1775 train_time:1461225ms step_avg:1031.94ms
step:1417/1775 train_time:1462339ms step_avg:1032.00ms
step:1418/1775 train_time:1463471ms step_avg:1032.07ms
step:1419/1775 train_time:1464590ms step_avg:1032.13ms
step:1420/1775 train_time:1465718ms step_avg:1032.20ms
step:1421/1775 train_time:1466842ms step_avg:1032.26ms
step:1422/1775 train_time:1467968ms step_avg:1032.33ms
step:1423/1775 train_time:1469088ms step_avg:1032.39ms
step:1424/1775 train_time:1470381ms step_avg:1032.57ms
step:1425/1775 train_time:1471497ms step_avg:1032.63ms
step:1426/1775 train_time:1472624ms step_avg:1032.70ms
step:1427/1775 train_time:1473750ms step_avg:1032.76ms
step:1428/1775 train_time:1474878ms step_avg:1032.83ms
step:1429/1775 train_time:1476004ms step_avg:1032.89ms
step:1430/1775 train_time:1477131ms step_avg:1032.96ms
step:1431/1775 train_time:1478251ms step_avg:1033.02ms
step:1432/1775 train_time:1479384ms step_avg:1033.09ms
step:1433/1775 train_time:1480504ms step_avg:1033.15ms
step:1434/1775 train_time:1481632ms step_avg:1033.22ms
step:1435/1775 train_time:1482750ms step_avg:1033.28ms
step:1436/1775 train_time:1483888ms step_avg:1033.35ms
step:1437/1775 train_time:1485010ms step_avg:1033.41ms
step:1438/1775 train_time:1486134ms step_avg:1033.47ms
step:1439/1775 train_time:1487252ms step_avg:1033.53ms
step:1440/1775 train_time:1488381ms step_avg:1033.60ms
step:1441/1775 train_time:1489507ms step_avg:1033.66ms
step:1442/1775 train_time:1490633ms step_avg:1033.73ms
step:1443/1775 train_time:1491755ms step_avg:1033.79ms
step:1444/1775 train_time:1492885ms step_avg:1033.85ms
step:1445/1775 train_time:1494008ms step_avg:1033.92ms
step:1446/1775 train_time:1495133ms step_avg:1033.98ms
step:1447/1775 train_time:1496251ms step_avg:1034.04ms
step:1448/1775 train_time:1497384ms step_avg:1034.10ms
step:1449/1775 train_time:1498498ms step_avg:1034.16ms
step:1450/1775 train_time:1499632ms step_avg:1034.23ms
step:1451/1775 train_time:1500749ms step_avg:1034.29ms
step:1452/1775 train_time:1501879ms step_avg:1034.35ms
step:1453/1775 train_time:1503004ms step_avg:1034.41ms
step:1454/1775 train_time:1504127ms step_avg:1034.48ms
step:1455/1775 train_time:1505250ms step_avg:1034.54ms
step:1456/1775 train_time:1506381ms step_avg:1034.60ms
step:1457/1775 train_time:1507502ms step_avg:1034.66ms
step:1458/1775 train_time:1508628ms step_avg:1034.72ms
step:1459/1775 train_time:1509744ms step_avg:1034.78ms
step:1460/1775 train_time:1510873ms step_avg:1034.84ms
step:1461/1775 train_time:1512002ms step_avg:1034.91ms
step:1462/1775 train_time:1513131ms step_avg:1034.97ms
step:1463/1775 train_time:1514250ms step_avg:1035.03ms
step:1464/1775 train_time:1515385ms step_avg:1035.10ms
step:1465/1775 train_time:1516509ms step_avg:1035.16ms
step:1466/1775 train_time:1517635ms step_avg:1035.22ms
step:1467/1775 train_time:1518758ms step_avg:1035.28ms
step:1468/1775 train_time:1519888ms step_avg:1035.35ms
step:1469/1775 train_time:1521006ms step_avg:1035.40ms
step:1470/1775 train_time:1522134ms step_avg:1035.47ms
step:1471/1775 train_time:1523258ms step_avg:1035.53ms
step:1472/1775 train_time:1524386ms step_avg:1035.59ms
step:1473/1775 train_time:1525509ms step_avg:1035.65ms
step:1474/1775 train_time:1526638ms step_avg:1035.71ms
step:1475/1775 train_time:1527757ms step_avg:1035.77ms
step:1476/1775 train_time:1528885ms step_avg:1035.83ms
step:1477/1775 train_time:1530009ms step_avg:1035.89ms
step:1478/1775 train_time:1531136ms step_avg:1035.95ms
step:1479/1775 train_time:1532259ms step_avg:1036.01ms
step:1480/1775 train_time:1533391ms step_avg:1036.07ms
step:1481/1775 train_time:1534507ms step_avg:1036.13ms
step:1482/1775 train_time:1535638ms step_avg:1036.19ms
step:1483/1775 train_time:1536763ms step_avg:1036.25ms
step:1484/1775 train_time:1537895ms step_avg:1036.32ms
step:1485/1775 train_time:1539012ms step_avg:1036.37ms
step:1486/1775 train_time:1540147ms step_avg:1036.44ms
step:1487/1775 train_time:1541272ms step_avg:1036.50ms
step:1488/1775 train_time:1542403ms step_avg:1036.56ms
step:1489/1775 train_time:1543526ms step_avg:1036.62ms
step:1490/1775 train_time:1544657ms step_avg:1036.68ms
step:1491/1775 train_time:1545778ms step_avg:1036.74ms
step:1492/1775 train_time:1546905ms step_avg:1036.80ms
step:1493/1775 train_time:1548032ms step_avg:1036.86ms
step:1494/1775 train_time:1549160ms step_avg:1036.92ms
step:1495/1775 train_time:1550279ms step_avg:1036.98ms
step:1496/1775 train_time:1551411ms step_avg:1037.04ms
step:1497/1775 train_time:1552533ms step_avg:1037.10ms
step:1498/1775 train_time:1553661ms step_avg:1037.16ms
step:1499/1775 train_time:1554786ms step_avg:1037.22ms
step:1500/1775 train_time:1555912ms step_avg:1037.27ms
step:1500/1775 val_loss:3.3782 val_malbo_loss:3.4044 train_time:1555970ms step_avg:1037.31ms
step:1501/1775 train_time:1557034ms step_avg:1037.33ms
step:1502/1775 train_time:1558161ms step_avg:1037.39ms
step:1503/1775 train_time:1559280ms step_avg:1037.45ms
step:1504/1775 train_time:1560405ms step_avg:1037.50ms
step:1505/1775 train_time:1561527ms step_avg:1037.56ms
step:1506/1775 train_time:1562657ms step_avg:1037.62ms
step:1507/1775 train_time:1563775ms step_avg:1037.67ms
step:1508/1775 train_time:1564902ms step_avg:1037.73ms
step:1509/1775 train_time:1566021ms step_avg:1037.79ms
step:1510/1775 train_time:1567146ms step_avg:1037.85ms
step:1511/1775 train_time:1568265ms step_avg:1037.90ms
step:1512/1775 train_time:1569392ms step_avg:1037.96ms
step:1513/1775 train_time:1570509ms step_avg:1038.01ms
step:1514/1775 train_time:1571640ms step_avg:1038.07ms
step:1515/1775 train_time:1572757ms step_avg:1038.12ms
step:1516/1775 train_time:1573889ms step_avg:1038.19ms
step:1517/1775 train_time:1575004ms step_avg:1038.24ms
step:1518/1775 train_time:1576126ms step_avg:1038.29ms
step:1519/1775 train_time:1577251ms step_avg:1038.35ms
step:1520/1775 train_time:1578376ms step_avg:1038.41ms
step:1521/1775 train_time:1579501ms step_avg:1038.46ms
step:1522/1775 train_time:1580634ms step_avg:1038.52ms
step:1523/1775 train_time:1581755ms step_avg:1038.58ms
step:1524/1775 train_time:1582884ms step_avg:1038.64ms
step:1525/1775 train_time:1584001ms step_avg:1038.69ms
step:1526/1775 train_time:1585127ms step_avg:1038.75ms
step:1527/1775 train_time:1586244ms step_avg:1038.80ms
step:1528/1775 train_time:1587371ms step_avg:1038.86ms
step:1529/1775 train_time:1588508ms step_avg:1038.92ms
step:1530/1775 train_time:1589637ms step_avg:1038.98ms
step:1531/1775 train_time:1590760ms step_avg:1039.03ms
step:1532/1775 train_time:1591885ms step_avg:1039.09ms
step:1533/1775 train_time:1593008ms step_avg:1039.14ms
step:1534/1775 train_time:1594138ms step_avg:1039.20ms
step:1535/1775 train_time:1595257ms step_avg:1039.26ms
step:1536/1775 train_time:1596390ms step_avg:1039.32ms
step:1537/1775 train_time:1597508ms step_avg:1039.37ms
step:1538/1775 train_time:1598637ms step_avg:1039.43ms
step:1539/1775 train_time:1599754ms step_avg:1039.48ms
step:1540/1775 train_time:1600883ms step_avg:1039.53ms
step:1541/1775 train_time:1602009ms step_avg:1039.59ms
step:1542/1775 train_time:1603138ms step_avg:1039.65ms
step:1543/1775 train_time:1604261ms step_avg:1039.70ms
step:1544/1775 train_time:1605388ms step_avg:1039.76ms
step:1545/1775 train_time:1606510ms step_avg:1039.81ms
step:1546/1775 train_time:1607639ms step_avg:1039.87ms
step:1547/1775 train_time:1608759ms step_avg:1039.92ms
step:1548/1775 train_time:1609888ms step_avg:1039.98ms
step:1549/1775 train_time:1611013ms step_avg:1040.03ms
step:1550/1775 train_time:1612143ms step_avg:1040.09ms
step:1551/1775 train_time:1613262ms step_avg:1040.14ms
step:1552/1775 train_time:1614393ms step_avg:1040.20ms
step:1553/1775 train_time:1615517ms step_avg:1040.26ms
step:1554/1775 train_time:1616647ms step_avg:1040.31ms
step:1555/1775 train_time:1617772ms step_avg:1040.37ms
step:1556/1775 train_time:1618900ms step_avg:1040.42ms
step:1557/1775 train_time:1620023ms step_avg:1040.48ms
step:1558/1775 train_time:1621155ms step_avg:1040.54ms
step:1559/1775 train_time:1622275ms step_avg:1040.59ms
step:1560/1775 train_time:1623404ms step_avg:1040.64ms
step:1561/1775 train_time:1624529ms step_avg:1040.70ms
step:1562/1775 train_time:1625660ms step_avg:1040.76ms
step:1563/1775 train_time:1626780ms step_avg:1040.81ms
step:1564/1775 train_time:1627900ms step_avg:1040.86ms
step:1565/1775 train_time:1629024ms step_avg:1040.91ms
step:1566/1775 train_time:1630148ms step_avg:1040.96ms
step:1567/1775 train_time:1631268ms step_avg:1041.01ms
step:1568/1775 train_time:1632396ms step_avg:1041.07ms
step:1569/1775 train_time:1633513ms step_avg:1041.12ms
step:1570/1775 train_time:1634642ms step_avg:1041.17ms
step:1571/1775 train_time:1635758ms step_avg:1041.22ms
step:1572/1775 train_time:1636891ms step_avg:1041.28ms
step:1573/1775 train_time:1638011ms step_avg:1041.33ms
step:1574/1775 train_time:1639142ms step_avg:1041.39ms
step:1575/1775 train_time:1640263ms step_avg:1041.44ms
step:1576/1775 train_time:1641387ms step_avg:1041.49ms
step:1577/1775 train_time:1642513ms step_avg:1041.54ms
step:1578/1775 train_time:1643641ms step_avg:1041.60ms
step:1579/1775 train_time:1644759ms step_avg:1041.65ms
step:1580/1775 train_time:1645888ms step_avg:1041.70ms
step:1581/1775 train_time:1647009ms step_avg:1041.75ms
step:1582/1775 train_time:1648142ms step_avg:1041.81ms
step:1583/1775 train_time:1649260ms step_avg:1041.86ms
step:1584/1775 train_time:1650387ms step_avg:1041.91ms
step:1585/1775 train_time:1651509ms step_avg:1041.96ms
step:1586/1775 train_time:1652638ms step_avg:1042.02ms
step:1587/1775 train_time:1653756ms step_avg:1042.06ms
step:1588/1775 train_time:1654887ms step_avg:1042.12ms
step:1589/1775 train_time:1656014ms step_avg:1042.17ms
step:1590/1775 train_time:1657138ms step_avg:1042.23ms
step:1591/1775 train_time:1658262ms step_avg:1042.28ms
step:1592/1775 train_time:1659396ms step_avg:1042.33ms
step:1593/1775 train_time:1660516ms step_avg:1042.38ms
step:1594/1775 train_time:1661644ms step_avg:1042.44ms
step:1595/1775 train_time:1662764ms step_avg:1042.49ms
step:1596/1775 train_time:1663890ms step_avg:1042.54ms
step:1597/1775 train_time:1665012ms step_avg:1042.59ms
step:1598/1775 train_time:1666142ms step_avg:1042.64ms
step:1599/1775 train_time:1667264ms step_avg:1042.69ms
step:1600/1775 train_time:1668386ms step_avg:1042.74ms
step:1601/1775 train_time:1669507ms step_avg:1042.79ms
step:1602/1775 train_time:1670638ms step_avg:1042.85ms
step:1603/1775 train_time:1671755ms step_avg:1042.89ms
step:1604/1775 train_time:1672887ms step_avg:1042.95ms
step:1605/1775 train_time:1674005ms step_avg:1042.99ms
step:1606/1775 train_time:1675132ms step_avg:1043.05ms
step:1607/1775 train_time:1676251ms step_avg:1043.09ms
step:1608/1775 train_time:1677379ms step_avg:1043.15ms
step:1609/1775 train_time:1678500ms step_avg:1043.19ms
step:1610/1775 train_time:1679628ms step_avg:1043.25ms
step:1611/1775 train_time:1680752ms step_avg:1043.30ms
step:1612/1775 train_time:1681878ms step_avg:1043.35ms
step:1613/1775 train_time:1682997ms step_avg:1043.40ms
step:1614/1775 train_time:1684126ms step_avg:1043.45ms
step:1615/1775 train_time:1685242ms step_avg:1043.49ms
step:1616/1775 train_time:1686378ms step_avg:1043.55ms
step:1617/1775 train_time:1687494ms step_avg:1043.60ms
step:1618/1775 train_time:1688621ms step_avg:1043.65ms
step:1619/1775 train_time:1689741ms step_avg:1043.69ms
step:1620/1775 train_time:1690874ms step_avg:1043.75ms
step:1621/1775 train_time:1691995ms step_avg:1043.80ms
step:1622/1775 train_time:1693121ms step_avg:1043.85ms
step:1623/1775 train_time:1694238ms step_avg:1043.89ms
step:1624/1775 train_time:1695367ms step_avg:1043.94ms
step:1625/1775 train_time:1696486ms step_avg:1043.99ms
step:1626/1775 train_time:1697614ms step_avg:1044.04ms
step:1627/1775 train_time:1698737ms step_avg:1044.09ms
step:1628/1775 train_time:1699861ms step_avg:1044.14ms
step:1629/1775 train_time:1700980ms step_avg:1044.19ms
step:1630/1775 train_time:1702113ms step_avg:1044.24ms
step:1631/1775 train_time:1703232ms step_avg:1044.29ms
step:1632/1775 train_time:1704361ms step_avg:1044.34ms
step:1633/1775 train_time:1705480ms step_avg:1044.38ms
step:1634/1775 train_time:1706607ms step_avg:1044.44ms
step:1635/1775 train_time:1707910ms step_avg:1044.59ms
step:1636/1775 train_time:1709039ms step_avg:1044.64ms
step:1637/1775 train_time:1710160ms step_avg:1044.69ms
step:1638/1775 train_time:1711281ms step_avg:1044.74ms
step:1639/1775 train_time:1712401ms step_avg:1044.78ms
step:1640/1775 train_time:1713534ms step_avg:1044.84ms
step:1641/1775 train_time:1714659ms step_avg:1044.89ms
step:1642/1775 train_time:1715783ms step_avg:1044.93ms
step:1643/1775 train_time:1716903ms step_avg:1044.98ms
step:1644/1775 train_time:1718030ms step_avg:1045.03ms
step:1645/1775 train_time:1719153ms step_avg:1045.08ms
step:1646/1775 train_time:1720284ms step_avg:1045.13ms
step:1647/1775 train_time:1721402ms step_avg:1045.17ms
step:1648/1775 train_time:1722529ms step_avg:1045.22ms
step:1649/1775 train_time:1723651ms step_avg:1045.27ms
step:1650/1775 train_time:1724781ms step_avg:1045.32ms
step:1651/1775 train_time:1725898ms step_avg:1045.37ms
step:1652/1775 train_time:1727024ms step_avg:1045.41ms
step:1653/1775 train_time:1728147ms step_avg:1045.46ms
step:1654/1775 train_time:1729274ms step_avg:1045.51ms
step:1655/1775 train_time:1730394ms step_avg:1045.56ms
step:1656/1775 train_time:1731523ms step_avg:1045.61ms
step:1657/1775 train_time:1732637ms step_avg:1045.65ms
step:1658/1775 train_time:1733766ms step_avg:1045.70ms
step:1659/1775 train_time:1734881ms step_avg:1045.74ms
step:1660/1775 train_time:1736009ms step_avg:1045.79ms
step:1661/1775 train_time:1737130ms step_avg:1045.83ms
step:1662/1775 train_time:1738264ms step_avg:1045.89ms
step:1663/1775 train_time:1739386ms step_avg:1045.93ms
step:1664/1775 train_time:1740518ms step_avg:1045.98ms
step:1665/1775 train_time:1741640ms step_avg:1046.03ms
step:1666/1775 train_time:1742764ms step_avg:1046.08ms
step:1667/1775 train_time:1743891ms step_avg:1046.13ms
step:1668/1775 train_time:1745015ms step_avg:1046.17ms
step:1669/1775 train_time:1746135ms step_avg:1046.22ms
step:1670/1775 train_time:1747263ms step_avg:1046.27ms
step:1671/1775 train_time:1748379ms step_avg:1046.31ms
step:1672/1775 train_time:1749506ms step_avg:1046.36ms
step:1673/1775 train_time:1750630ms step_avg:1046.40ms
step:1674/1775 train_time:1751758ms step_avg:1046.45ms
step:1675/1775 train_time:1752880ms step_avg:1046.50ms
step:1676/1775 train_time:1754004ms step_avg:1046.54ms
step:1677/1775 train_time:1755123ms step_avg:1046.58ms
step:1678/1775 train_time:1756249ms step_avg:1046.63ms
step:1679/1775 train_time:1757376ms step_avg:1046.68ms
step:1680/1775 train_time:1758501ms step_avg:1046.73ms
step:1681/1775 train_time:1759618ms step_avg:1046.77ms
step:1682/1775 train_time:1760747ms step_avg:1046.82ms
step:1683/1775 train_time:1761864ms step_avg:1046.86ms
step:1684/1775 train_time:1762994ms step_avg:1046.91ms
step:1685/1775 train_time:1764113ms step_avg:1046.95ms
step:1686/1775 train_time:1765242ms step_avg:1047.00ms
step:1687/1775 train_time:1766361ms step_avg:1047.04ms
step:1688/1775 train_time:1767489ms step_avg:1047.09ms
step:1689/1775 train_time:1768608ms step_avg:1047.13ms
step:1690/1775 train_time:1769736ms step_avg:1047.18ms
step:1691/1775 train_time:1770854ms step_avg:1047.22ms
step:1692/1775 train_time:1771986ms step_avg:1047.27ms
step:1693/1775 train_time:1773100ms step_avg:1047.31ms
step:1694/1775 train_time:1774228ms step_avg:1047.36ms
step:1695/1775 train_time:1775356ms step_avg:1047.41ms
step:1696/1775 train_time:1776479ms step_avg:1047.45ms
step:1697/1775 train_time:1777595ms step_avg:1047.49ms
step:1698/1775 train_time:1778724ms step_avg:1047.54ms
step:1699/1775 train_time:1779848ms step_avg:1047.59ms
step:1700/1775 train_time:1780980ms step_avg:1047.64ms
step:1701/1775 train_time:1782103ms step_avg:1047.68ms
step:1702/1775 train_time:1783230ms step_avg:1047.73ms
step:1703/1775 train_time:1784350ms step_avg:1047.77ms
step:1704/1775 train_time:1785483ms step_avg:1047.82ms
step:1705/1775 train_time:1786599ms step_avg:1047.86ms
step:1706/1775 train_time:1787727ms step_avg:1047.91ms
step:1707/1775 train_time:1788851ms step_avg:1047.95ms
step:1708/1775 train_time:1789981ms step_avg:1048.00ms
step:1709/1775 train_time:1791102ms step_avg:1048.04ms
step:1710/1775 train_time:1792231ms step_avg:1048.09ms
step:1711/1775 train_time:1793357ms step_avg:1048.13ms
step:1712/1775 train_time:1794479ms step_avg:1048.18ms
step:1713/1775 train_time:1795601ms step_avg:1048.22ms
step:1714/1775 train_time:1796726ms step_avg:1048.26ms
step:1715/1775 train_time:1797850ms step_avg:1048.31ms
step:1716/1775 train_time:1798975ms step_avg:1048.35ms
step:1717/1775 train_time:1800099ms step_avg:1048.40ms
step:1718/1775 train_time:1801229ms step_avg:1048.45ms
step:1719/1775 train_time:1802345ms step_avg:1048.48ms
step:1720/1775 train_time:1803480ms step_avg:1048.53ms
step:1721/1775 train_time:1804598ms step_avg:1048.58ms
step:1722/1775 train_time:1805720ms step_avg:1048.62ms
step:1723/1775 train_time:1806842ms step_avg:1048.66ms
step:1724/1775 train_time:1807968ms step_avg:1048.71ms
step:1725/1775 train_time:1809085ms step_avg:1048.75ms
step:1726/1775 train_time:1810212ms step_avg:1048.79ms
step:1727/1775 train_time:1811338ms step_avg:1048.84ms
step:1728/1775 train_time:1812466ms step_avg:1048.88ms
step:1729/1775 train_time:1813585ms step_avg:1048.92ms
step:1730/1775 train_time:1814718ms step_avg:1048.97ms
step:1731/1775 train_time:1815835ms step_avg:1049.01ms
step:1732/1775 train_time:1816964ms step_avg:1049.06ms
step:1733/1775 train_time:1818081ms step_avg:1049.09ms
step:1734/1775 train_time:1819211ms step_avg:1049.14ms
step:1735/1775 train_time:1820332ms step_avg:1049.18ms
step:1736/1775 train_time:1941824ms step_avg:1118.56ms
step:1737/1775 train_time:1942926ms step_avg:1118.55ms
step:1738/1775 train_time:1944042ms step_avg:1118.55ms
step:1739/1775 train_time:1945150ms step_avg:1118.55ms
step:1740/1775 train_time:1946272ms step_avg:1118.55ms
step:1741/1775 train_time:1947388ms step_avg:1118.55ms
step:1742/1775 train_time:1948509ms step_avg:1118.55ms
step:1743/1775 train_time:1949617ms step_avg:1118.54ms
step:1744/1775 train_time:1950738ms step_avg:1118.54ms
step:1745/1775 train_time:1951852ms step_avg:1118.54ms
step:1746/1775 train_time:1952978ms step_avg:1118.54ms
step:1747/1775 train_time:1954092ms step_avg:1118.54ms
step:1748/1775 train_time:1955211ms step_avg:1118.54ms
step:1749/1775 train_time:1956323ms step_avg:1118.54ms
step:1750/1775 train_time:1957446ms step_avg:1118.54ms
step:1750/1775 val_loss:3.2872 val_malbo_loss:3.3139 train_time:1957503ms step_avg:1118.57ms
step:1751/1775 train_time:1958560ms step_avg:1118.54ms
step:1752/1775 train_time:1959677ms step_avg:1118.54ms
step:1753/1775 train_time:1960788ms step_avg:1118.53ms
step:1754/1775 train_time:1961906ms step_avg:1118.53ms
step:1755/1775 train_time:1963020ms step_avg:1118.53ms
step:1756/1775 train_time:1964141ms step_avg:1118.53ms
step:1757/1775 train_time:1965254ms step_avg:1118.53ms
step:1758/1775 train_time:1966378ms step_avg:1118.53ms
step:1759/1775 train_time:1967498ms step_avg:1118.53ms
step:1760/1775 train_time:1968619ms step_avg:1118.53ms
step:1761/1775 train_time:1969739ms step_avg:1118.53ms
step:1762/1775 train_time:1970862ms step_avg:1118.54ms
step:1763/1775 train_time:1971980ms step_avg:1118.54ms
step:1764/1775 train_time:1973105ms step_avg:1118.54ms
step:1765/1775 train_time:1974222ms step_avg:1118.54ms
step:1766/1775 train_time:1975347ms step_avg:1118.54ms
step:1767/1775 train_time:1976468ms step_avg:1118.54ms
step:1768/1775 train_time:1977591ms step_avg:1118.55ms
step:1769/1775 train_time:1978707ms step_avg:1118.55ms
step:1770/1775 train_time:1979833ms step_avg:1118.55ms
step:1771/1775 train_time:1980951ms step_avg:1118.55ms
step:1772/1775 train_time:1982076ms step_avg:1118.55ms
step:1773/1775 train_time:1983195ms step_avg:1118.55ms
step:1774/1775 train_time:1984321ms step_avg:1118.56ms
step:1775/1775 train_time:1985438ms step_avg:1118.56ms
step:1775/1775 val_loss:3.2809 val_malbo_loss:3.3076 train_time:1985499ms step_avg:1118.59ms
peak memory allocated: 30840 MiB reserved: 46520 MiB
