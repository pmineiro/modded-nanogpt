import os
import sys

with open(sys.argv[0]) as f:
    code = f.read()  # read the code of this file ASAP, for logging
import copy
import glob
import math
import threading
import time
import uuid
from dataclasses import dataclass
from collections import defaultdict
from itertools import accumulate
from pathlib import Path
import gc

from malbo import compute_malbo_parameters

os.environ["PYTORCH_ALLOC_CONF"] = "expandable_segments:True"
import torch

torch.empty(
    1, device=f"cuda:{os.environ['LOCAL_RANK']}", requires_grad=True
).backward()  # prevents a bug on some systems
import torch._dynamo as dynamo
import torch.distributed as dist
import torch.nn.functional as F

# torch._inductor.config.coordinate_descent_tuning = True # we have banned this flag for new records because it causes compilation to take 30min
import triton
import triton.language as tl
from triton.tools.tensor_descriptor import TensorDescriptor
from kernels import get_kernel
from torch import Tensor, nn

dynamo.config.recompile_limit = 64

# -----------------------------------------------------------------------------
# Custom operators: FP8 matmul by @YouJiacheng

@torch.library.custom_op("nanogpt::mm", mutates_args=())
def mm_op(x: Tensor, w: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor, Tensor]:
    @torch.compile
    def impl(x: Tensor, w: Tensor):
        assert x.is_contiguous() and w.is_contiguous()
        x_f8 = x.div(x_s).to(torch.float8_e4m3fn)
        w_f8 = w.div(w_s).to(torch.float8_e4m3fn)
        out = torch._scaled_mm(
            x_f8,
            w_f8.T,
            out_dtype=torch.bfloat16,
            scale_a=x.new_tensor(x_s, dtype=torch.float32),
            scale_b=x.new_tensor(w_s, dtype=torch.float32),
            use_fast_accum=True,
        )
        return out, x_f8, w_f8

    return impl(x, w)

@mm_op.register_fake
def _(x: Tensor, w: Tensor, *_):
    assert x.ndim == w.ndim == 2
    assert x.shape[1] == w.shape[1]
    assert x.device == w.device
    assert x.is_contiguous() and w.is_contiguous()
    return x @ w.T, x.to(torch.float8_e4m3fn), w.to(torch.float8_e4m3fn)

@torch.library.custom_op("nanogpt::mm_backward", mutates_args=())
def mm_backward_op(g: Tensor, x_f8: Tensor, w_f8: Tensor, x_s: float, w_s: float, grad_s: float) -> tuple[Tensor, Tensor]:
    @torch.compile
    def impl(grad: Tensor, x_f8: Tensor, w_f8: Tensor):
        assert grad.is_contiguous()
        x_inv_s = grad.new_tensor(x_s, dtype=torch.float32)
        w_inv_s = grad.new_tensor(w_s, dtype=torch.float32)
        grad_inv_s = grad.new_tensor(grad_s, dtype=torch.float32)
        grad_f8 = grad.div(grad_s).to(torch.float8_e5m2)
        grad_x = torch._scaled_mm(
            grad_f8,
            w_f8.T.contiguous().T,
            out_dtype=torch.bfloat16,
            scale_a=grad_inv_s,
            scale_b=w_inv_s,
            use_fast_accum=False,
        )
        # faster than grad_f8_t @ x_f8, for (d_out, d_in) == (50304, 768)
        grad_w = torch._scaled_mm(
            x_f8.T.contiguous(),
            grad_f8.T.contiguous().T,
            out_dtype=torch.float32,
            scale_a=x_inv_s,
            scale_b=grad_inv_s,
            use_fast_accum=False,
        ).T
        return grad_x, grad_w

    return impl(g, x_f8, w_f8)

@mm_backward_op.register_fake
def _(g: Tensor, x_f8: Tensor, w_f8: Tensor, *_):
    return x_f8.to(torch.bfloat16), w_f8.T.contiguous().T.to(torch.float32)

def backward(ctx, grad_out: Tensor, *_):
    x_f8, w_f8 = ctx.saved_tensors
    x_s, w_s, grad_s = ctx.scales
    grad_x, grad_w = torch.ops.nanogpt.mm_backward(
        grad_out, x_f8, w_f8, x_s, w_s, grad_s
    )
    return grad_x, grad_w, None, None, None

def setup_context(ctx: torch.autograd.function.FunctionCtx, inputs, output):
    *_, x_s, w_s, grad_s = inputs
    _, x_f8, w_f8 = output
    ctx.save_for_backward(x_f8, w_f8)
    ctx.scales = x_s, w_s, grad_s
    ctx.set_materialize_grads(False)

mm_op.register_autograd(backward, setup_context=setup_context)

# -----------------------------------------------------------------------------
# Triton kernel for symmetric matrix multiplication by @byronxu99

def _get_autotune_configs():
    return [
        triton.Config(
            {
                "BLOCK_SIZE_M": bm,
                "BLOCK_SIZE_N": bn,
                "BLOCK_SIZE_K": bk,
                "GROUP_SIZE_M": 8,
                "LOWER_UPPER": 1,
            },
            num_stages=stages,
            num_warps=warps,
        )
        for bm in [64, 128]
        for bn in [64, 128, 256]
        for bk in [64, 128]
        for stages, warps in [(3, 4), (3, 8), (4, 4)]
        if bm // bn <= 2 and bn // bm <= 2
    ]

@triton.jit
def _pid_to_block(
    pid,
    M,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
):
    # Split output matrix into blocks of size (BLOCK_SIZE_M, BLOCK_SIZE_N)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(M, BLOCK_SIZE_N)

    # Map PID to a single matrix in batch
    batch_idx = pid // (num_pid_m * num_pid_n)
    pid = pid % (num_pid_m * num_pid_n)

    # Map PID to 2D grid of blocks
    pid_m = pid // num_pid_n
    pid_n = pid % num_pid_n
    pid_m, pid_n = tl.swizzle2d(pid_m, pid_n, num_pid_m, num_pid_n, GROUP_SIZE_M)

    m_idx = pid_m * BLOCK_SIZE_M
    n_idx = pid_n * BLOCK_SIZE_N
    return batch_idx, m_idx, n_idx

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "K", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def XXT_kernel(
    A_ptr, C_ptr,
    M, K,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(K, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def XXT(A: torch.Tensor, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = A @ A.T
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert out.size(-2) == M, "Output matrix has incorrect shape"
    assert out.size(-1) == M, "Output matrix has incorrect shape"

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    XXT_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        K=K,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
    )
    return out

@triton.autotune(
    configs=_get_autotune_configs(),
    key=["M", "a_stride_r", "a_stride_c", "c_stride_r", "c_stride_c"],
)
@triton.jit
def ba_plus_cAA_kernel(
    A_ptr, C_ptr,
    M,
    a_stride_b, a_stride_r, a_stride_c,
    c_stride_b, c_stride_r, c_stride_c,
    alpha, beta,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    LOWER_UPPER: tl.constexpr,
):
    # This is mostly duplicated from XXT_kernel, but also loads and adds a block of A
    # Performance is slightly slower than XXT_kernel, so we use two separate kernels
    pid = tl.program_id(axis=0)
    batch_idx, m_idx, n_idx = _pid_to_block(
        pid, M, BLOCK_SIZE_M, BLOCK_SIZE_N, GROUP_SIZE_M
    )

    # Skip blocks that don't need to be computed
    skip_block_below_diag = (LOWER_UPPER == 0) and (n_idx + BLOCK_SIZE_N <= m_idx)
    skip_block_above_diag = (LOWER_UPPER != 0) and (m_idx + BLOCK_SIZE_M <= n_idx)
    if skip_block_below_diag or skip_block_above_diag:
        return

    # Index into one matrix of batch
    A_ptr += batch_idx * a_stride_b
    C_ptr += batch_idx * c_stride_b

    # Create pointer arrays for A and A.T
    offs_m = (m_idx + tl.arange(0, BLOCK_SIZE_M)) % M
    offs_n = (n_idx + tl.arange(0, BLOCK_SIZE_N)) % M
    offs_k = tl.arange(0, BLOCK_SIZE_K)
    a_ptrs = A_ptr + (offs_m[:, None] * a_stride_r + offs_k[None, :] * a_stride_c)
    at_ptrs = A_ptr + (offs_k[:, None] * a_stride_c + offs_n[None, :] * a_stride_r)

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

    # Accumulate over blocks of K
    for k in tl.range(0, tl.cdiv(M, BLOCK_SIZE_K)):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < M - k * BLOCK_SIZE_K, other=0.0)
        at = tl.load(at_ptrs, mask=offs_k[:, None] < M - k * BLOCK_SIZE_K, other=0.0)
        accumulator = tl.dot(a, at, accumulator)
        a_ptrs += BLOCK_SIZE_K * a_stride_c
        at_ptrs += BLOCK_SIZE_K * a_stride_c

    # Load block of A to add (corresponds to the current block of C)
    offs_am = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_an = n_idx + tl.arange(0, BLOCK_SIZE_N)
    a_add_ptrs = A_ptr + (offs_am[:, None] * a_stride_r + offs_an[None, :] * a_stride_c)
    a_add_mask = (offs_am[:, None] < M) & (offs_an[None, :] < M)
    a_add = tl.load(a_add_ptrs, mask=a_add_mask, other=0.0).to(tl.float32)

    # Apply alpha and beta
    accumulator *= alpha
    accumulator += a_add * beta

    out_dtype = C_ptr.dtype.element_ty
    output = accumulator.to(out_dtype)

    # Store block of C
    offs_cm = m_idx + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = n_idx + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = C_ptr + (offs_cm[:, None] * c_stride_r + offs_cn[None, :] * c_stride_c)
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < M)
    tl.store(c_ptrs, output, mask=c_mask)

    # Store block of C mirrored across the diagonal
    c_ptrs_t = C_ptr + (offs_cn[:, None] * c_stride_r + offs_cm[None, :] * c_stride_c)
    c_mask_t = (offs_cn[:, None] < M) & (offs_cm[None, :] < M)
    tl.store(c_ptrs_t, output.T, mask=c_mask_t)

def ba_plus_cAA(A: torch.Tensor, alpha: float, beta: float, out: torch.Tensor):
    """
    Launch Triton kernel to compute C = alpha * A @ A.T + beta * A
    """
    assert A.ndim == 2 or A.ndim == 3
    M, K = A.shape[-2:]
    assert M == K, "Input matrix must be square"
    assert out.size(-2) == M
    assert out.size(-1) == M

    batch_size = A.size(0) if A.ndim == 3 else 1
    input_batch_stride = A.stride(0) if A.ndim == 3 else 0
    output_batch_stride = out.stride(0) if out.ndim == 3 else 0

    grid = lambda meta: (
        batch_size * triton.cdiv(M, meta["BLOCK_SIZE_M"]) * triton.cdiv(M, meta["BLOCK_SIZE_N"]),
    )
    ba_plus_cAA_kernel[grid](
        A_ptr=A,
        C_ptr=out,
        M=M,
        a_stride_b=input_batch_stride,
        a_stride_r=A.stride(-2),
        a_stride_c=A.stride(-1),
        c_stride_b=output_batch_stride,
        c_stride_r=out.stride(-2),
        c_stride_c=out.stride(-1),
        alpha=alpha,
        beta=beta,
    )
    return out

# Computed for num_iters=5, safety_factor=2e-2, cushion=2
polar_express_coeffs = [
    (8.156554524902461, -22.48329292557795, 15.878769915207462),
    (4.042929935166739, -2.808917465908714, 0.5000178451051316),
    (3.8916678022926607, -2.772484153217685, 0.5060648178503393),
    (3.285753657755655, -2.3681294933425376, 0.46449024233003106),
    (2.3465413258596377, -1.7097828382687081, 0.42323551169305323)
]

@torch.compile(dynamic=False, fullgraph=True) # Must use dynamic=False or else it's much slower
def polar_express(G: torch.Tensor, split_baddbmm: bool = False):
    """
    Polar Express Sign Method: https://arxiv.org/pdf/2505.16932
    by Noah Amsel, David Persson, Christopher Musco, Robert M. Gower.
    """
    X = G.bfloat16()
    if G.size(-2) > G.size(-1):
        X = X.mT

    # Ensure spectral norm is at most 1
    X = X / (X.norm(dim=(-2, -1), keepdim=True) * (1 + 2e-2) + 1e-6)

    # Allocate buffers
    X = X.contiguous()
    A = torch.empty((*X.shape[:-1], X.size(-2)), device=X.device, dtype=X.dtype)
    B = torch.empty_like(A)
    C = torch.empty_like(X)

    # Select batched vs unbatched
    if split_baddbmm:
        BX_matmul = torch.bmm if X.ndim > 2 else torch.mm
    else:
        aX_plus_BX = torch.baddbmm if X.ndim > 2 else torch.addmm

    # Perform the iterations
    for a, b, c in polar_express_coeffs:
        XXT(X, out=A)  # A = X @ X.mT
        ba_plus_cAA(A, alpha=c, beta=b, out=B)  # B = b * A + c * A @ A

        # Referencing X twice causes pytorch to make a defensive copy,
        # resulting in a cudaMemcpyAsync in baddbmm.
        # For large matrices (i.e., the mlp weights), it's faster to split
        # the operation into two kernels to avoid this.
        if split_baddbmm:
            BX_matmul(B, X, out=C)  # C = B @ X
            C.add_(X, alpha=a)      # C = C + a*X  (in-place, X only read)
        else:
            aX_plus_BX(X, B, X, beta=a, out=C)  # C = a * X + B @ X

        X, C = C, X  # Swap references to avoid unnecessary copies

    if G.size(-2) > G.size(-1):
        X = X.mT
    return X


# -----------------------------------------------------------------------------
# Compiled helpers for NorMuon by @chrisjmccormick

@torch.compile(dynamic=False, fullgraph=True)
def cautious_wd_and_update_inplace(p, mantissa, grad, wd_tensor, lr_tensor):
    """
    Cautious weight decay + parameter update. wd_tensor and lr_tensor are 0-D CPU tensors.
    Mantissa is tracked to enable higher precision updates on bfloat16 parameters.
    bfloat16 format: 1 sign bit + 8 exponent bits + 7 mantissa bits = 16 bits total
    float32 format: 1 sign bit + 8 exponent bits + 23 mantissa bits = 32 bits total
    """
    assert p.dtype == mantissa.dtype == torch.uint16
    grad = grad.float()
    wd_factor = wd_tensor.to(torch.float32)
    lr_factor = lr_tensor.to(torch.float32)
    p_precise_raw = (p.to(torch.uint32) << 16) | mantissa.to(torch.uint32)
    p_precise = p_precise_raw.view(torch.float32)
    mask = (grad * p_precise) >= 0
    p_precise.copy_(p_precise - (p_precise * mask * wd_factor * lr_factor) - (grad * lr_factor))
    p.copy_((p_precise_raw >> 16).to(torch.uint16))
    mantissa.copy_(p_precise_raw.to(torch.uint16))

@torch.compile(dynamic=False, fullgraph=True)
def apply_normuon_variance_reduction(v_chunk, second_momentum_buffer, beta2, red_dim):
    """NorMuon variance reduction. Algebraically fuses the normalization steps to minimize memory ops."""
    v_mean = v_chunk.float().square().mean(dim=red_dim, keepdim=True)
    red_dim_size = v_chunk.size(red_dim)
    v_norm_sq = v_mean.sum(dim=(-2, -1), keepdim=True).mul_(red_dim_size)
    v_norm = v_norm_sq.sqrt_()
    second_momentum_buffer.lerp_(v_mean.to(dtype=second_momentum_buffer.dtype), 1 - beta2)
    step_size = second_momentum_buffer.clamp_min(1e-10).rsqrt_()
    scaled_sq_sum = (v_mean * red_dim_size) * step_size.float().square()
    v_norm_new = scaled_sq_sum.sum(dim=(-2, -1), keepdim=True).sqrt_()
    final_scale = step_size * (v_norm / v_norm_new.clamp_min_(1e-10))
    return v_chunk.mul_(final_scale.type_as(v_chunk))


# -----------------------------------------------------------------------------
# NorMuon optimizer

class NorMuon(torch.optim.Optimizer):
    """
    Muon - MomentUm Orthogonalized by Newton-schulz

    https://kellerjordan.github.io/posts/muon/

    Muon internally runs standard SGD-momentum, and then performs an orthogonalization post-
    processing step, in which each 2D parameter's update is replaced with the nearest orthogonal
    matrix. To efficiently orthogonalize each update, we use a Newton-Schulz iteration, which has
    the advantage that it can be stably run in bfloat16 on the GPU.

    Warning: This optimizer should not be used for the embedding layer, the final fully connected layer,
    or any {0,1}-D parameters; those should all be optimized by a standard method (e.g., AdamW).

    Differences from standard Muon:
    - Newton-Shulz is replaced with Polar Express for the orthogonalization step
    - NorMuon adds a low-rank variance estimator similar to Adafactor. https://arxiv.org/pdf/2510.05491
    - small 1D parameters handled here instead of in Adam
    - Cautious weight decay, a gated version of decoupled weight decay
    - Custom distributed sizing:
    The model stores all attn and mlp weights in the same shape, and then updates the view as
    needed on the forward pass. This enables attn and mlp weights to be contained within the same
    dist.reduce_scatter_tensor() call. The model architecture has been customized to enable
    (n_attn_layers+n_mlp_layers*2)%8==0 for batching across 8 GPUs with zero padding on mlp and attn.
    The scheduling is:
        1. reduce scatter attn/mlp round 1 (10 attn params 6 mlp params)
        2. reduce scatter attn/mlp round 2 (16 mlp params)
        3. wait on step 1, then compute update of 1 and schedule all gather
        4. wait on step 2, then compute update of 2 and schedule all gather
            GPUs receive [2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 ATTN, 2 MLP, 2 MLP, 2 MLP]
            GPUs that receive params of type attn reshape before computing update
        5. wait for each all gather to complete and update params
    """
    def __init__(self, params, lr=0.02, weight_decay=0.01, momentum=0.95, beta2=0.95, custom_sizing=True):
        defaults = dict(lr=lr, weight_decay=weight_decay, momentum=momentum, beta2=beta2)
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        # custom sizing requires 8 GPUs
        if custom_sizing and dist.get_world_size()==8:
            param_groups = self.generate_custom_param_groups(params)
        else:
            param_groups = self.generate_standard_param_groups(params)
        super().__init__(param_groups, defaults)

    def reset(self):
        # expose a reset for clearing buffers
        for group in self.param_groups:
            if "momentum_buffer" in group:
                group["momentum_buffer"].zero_()
                group["mantissa"].zero_()
                group["second_momentum_buffer"].zero_()

    def generate_standard_param_groups(self, params):
        """
        Use this method if running on less than 8 GPU or experimenting with additional attn or mlp modules.
        Creates one param group per module.
        """
        groups = defaultdict(list)
        for param in params:
            groups[param.label].append(param)

        param_groups = []
        for module_name, group_params in groups.items():
            chunk_size = (len(group_params) + self.world_size - 1) // self.world_size
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))

        return param_groups

    def generate_custom_param_groups(self, params):
        """
        Implementation requires that a single GPU does not receive both attn
        and mlp params when a param group is split across GPUs.
        """
        params_list = list(params)
        module_group_order = ['attn', 'mlp']
        group_sizes = [16, 16]  # 10 attn + 6 mlp, then 16 mlp
        params_list.sort(key=lambda x: module_group_order.index(x.label))

        idx = 0
        assert len(params_list) == sum(group_sizes)
        param_groups = []
        for size in group_sizes:
            chunk_size = (size + self.world_size - 1) // self.world_size
            group_params = params_list[idx: idx + size]
            param_groups.append(dict(params=group_params, chunk_size=chunk_size))
            idx += size

        return param_groups

    def step(self):
        self.step_p1()
        self.step_p2()
        self.step_p3()
        
    @torch.no_grad()
    def step_p1(self):
        """
        Part 1: Launch distributed reduce_scatter operations for parameter groups.
        """
        rank = dist.get_rank()
        self.group_infos = []
        for group in self.param_groups:
            params: list[Tensor] = group["params"]
            if not params:
                continue

            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            stacked_grads = torch.empty(
                (padded_num_params, *params[0].shape),
                dtype=params[0].dtype,
                device=params[0].device
            )
            for i, p in enumerate(params):
                stacked_grads[i].copy_(p.grad, non_blocking=True)
            if len(params) < padded_num_params:
                stacked_grads[len(params):].zero_()

            grad_chunk = torch.empty_like(stacked_grads[:chunk_size])

            reduce_future = dist.reduce_scatter_tensor(
                grad_chunk, stacked_grads, op=dist.ReduceOp.AVG, async_op=True
            ).get_future()

            self.group_infos.append(dict(grad_chunk=grad_chunk, reduce_future=reduce_future))
    
    @torch.no_grad()
    def step_p2(self):
        """
        Part 2: Compute gradient updates and launch all_gather operations
        Wait for all gather to complete for all parameter groups except final one
        """
        # Efficient distributed step by @YouJiacheng, @KonstantinWilleke, @alexrgilbert,
        # @adricarda, @tuttyfrutyee, @vdlad, @ryanyang0, @vagrawal, @varunneal, @chrisjmccormick
        rank = dist.get_rank()
        group_infos = self.group_infos
        
        self.all_gather_infos = []
        # Second pass: wait for gradients, compute updates for the local shard of parameters,
        # and launch all async all_gather operations.
        for group, info in zip(self.param_groups, group_infos):
            info["reduce_future"].wait()

            params = group["params"]
            grad_chunk = info["grad_chunk"].float()
            chunk_size = group["chunk_size"]
            padded_num_params = chunk_size * self.world_size

            start_idx = rank * chunk_size
            module_idx = start_idx if start_idx < len(params) else 0

            num_params = min(chunk_size, max(0, len(params) - start_idx))  # num params for this rank

            if "momentum_buffer" not in group:
                group["momentum_buffer"]  = torch.zeros_like(grad_chunk[:num_params], dtype=torch.float32)
                
            momentum_buffer = group["momentum_buffer"]
            # Apply momentum update to the persistent momentum buffer in-place
            momentum_buffer.lerp_(grad_chunk[:num_params], 1 - group["momentum"])
            updated_grads = grad_chunk[:num_params].lerp_(momentum_buffer, group["momentum"])

            grad_shape = updated_grads.shape
            if params[module_idx].label == 'attn':
                for p in params[module_idx:module_idx + num_params]:
                    assert p.label == 'attn'
                updated_grads = updated_grads.view(4 * grad_shape[0], grad_shape[1] // 4, grad_shape[2])

            ref_param = params[module_idx]
            param_shape = ref_param.shape

            if "second_momentum_buffer" not in group:
                group["second_momentum_buffer"] = (torch.zeros_like(updated_grads[..., :, :1], dtype=torch.float32)
                    if param_shape[-2] >= param_shape[-1] else torch.zeros_like(updated_grads[..., :1, :])
                )
            second_momentum_buffer = group["second_momentum_buffer"]

            if "param_lr_cpu" not in group:
                # Define multipliers for ALL params in this group (global, not per-shard)
                lr_mults = []
                wd_mults = []
                for p in params:
                    # Increase learning rate for modules with larger inputs than outputs.
                    # This shape check also assumes rows=input, columns=output, so take care
                    # when changing memory layouts. @chrisjmccormick
                    shape = p.shape
                    if len(shape) >= 2:
                        shape_mult = max(1.0, shape[-2] / shape[-1]) ** 0.5
                    else:
                        shape_mult = 1.0
                    lr_mults.append(shape_mult * getattr(p, "lr_mul", 1.0))
                    wd_mults.append(getattr(p, "wd_mul", 1.0))
                # Define as cpu tensors to enable Inductor constant folding
                group["param_lr_cpu"] = torch.tensor(lr_mults, dtype=torch.float32, device="cpu")
                group["param_wd_cpu"] = torch.tensor(wd_mults, dtype=torch.float32, device="cpu")

            eff_lr_all = group["param_lr_cpu"] * group["lr"]
            eff_wd_all = group["param_wd_cpu"] * group["weight_decay"] * group["lr"]

            # Slice the portion corresponding to this rank's shard
            eff_lr_cpu = eff_lr_all[module_idx:module_idx + num_params]
            eff_wd_cpu = eff_wd_all[module_idx:module_idx + num_params]

            # Compute zeropower for the entire chunk in a single, batched call.
            if num_params == 0:
                v_chunk = updated_grads
            else:
                v_chunk = polar_express(updated_grads, split_baddbmm=(ref_param.label == 'mlp'))

            # Note that the head orientation in O is transposed relative to QKV, so red_dim
            # is 'incorrect' for O. However, correcting this showed no improvement. @chrisjmccormick
            red_dim = -1 if param_shape[-2] >= param_shape[-1] else -2

            v_chunk = apply_normuon_variance_reduction(
                v_chunk, second_momentum_buffer, group["beta2"], red_dim
            )

            v_chunk = v_chunk.view(grad_shape)

            # # "Cautious" weight decay (https://arxiv.org/abs/2510.12402)
            updated_params = torch.empty_like(grad_chunk, dtype=torch.bfloat16)
            if num_params > 0:
                # Work on a stacked copy to avoid touching original params
                param_chunk = torch.stack(params[module_idx:module_idx + num_params])

                if "mantissa" not in group:
                    group["mantissa"] = torch.zeros_like(param_chunk, dtype=torch.uint16)
                mantissa = group["mantissa"]

                for local_idx in range(num_params):
                    cautious_wd_and_update_inplace(
                        param_chunk[local_idx].view(torch.uint16),
                        mantissa[local_idx],
                        v_chunk[local_idx],
                        eff_wd_cpu[local_idx],
                        eff_lr_cpu[local_idx]
                    )
            else:
                param_chunk = torch.zeros_like(v_chunk)

            updated_params[:num_params].copy_(param_chunk)
            if num_params < chunk_size:
                updated_params[num_params:].zero_()

            stacked_params = torch.empty(
                (padded_num_params, *param_shape),
                dtype=updated_params.dtype,
                device=updated_params.device,
            )

            gather_future = dist.all_gather_into_tensor(
                stacked_params, updated_params, async_op=True
            ).get_future()

            self.all_gather_infos.append(
                {
                    "gather_future": gather_future,
                    "stacked_params": stacked_params,
                    "orig_params": params,
                }
            )

        # Final pass: wait for all_gather to complete for all except final and copy results back into original parameter tensors.
        for info in self.all_gather_infos[:-1]:
            info["gather_future"].wait()
            stacked_params = info["stacked_params"]
            orig_params = info["orig_params"]

            unstacked_params = torch.unbind(stacked_params)
            for i, p in enumerate(orig_params):
                p.copy_(unstacked_params[i], non_blocking=True)

    @torch.no_grad()
    def step_p3(self):
        """
        Part 3: Wait for final all gather to complete and copy results back into original parameter tensors
        """
        info = self.all_gather_infos[-1]
        info["gather_future"].wait()
        stacked_params = info["stacked_params"]
        orig_params = info["orig_params"]

        unstacked_params = torch.unbind(stacked_params)
        for i, p in enumerate(orig_params):
            p.copy_(unstacked_params[i], non_blocking=True)


class DistAdam(torch.optim.Optimizer):
    def __init__(self, params, label_order: list[str], betas: list[list[float]], lr: float = 1e-3, eps: float = 1e-8, weight_decay: float = 0.01):
        self.world_size = dist.get_world_size() if dist.is_initialized() else 1
        defaults = dict(lr=lr, eps=eps, weight_decay=weight_decay)
        params = list(params)
        # Group by label, with explicit ordering for execution control.
        params_by_label = defaultdict(list)
        for p in params:
            params_by_label[getattr(p, 'label', None)].append(p)
        param_groups = []
        for idx, label in enumerate(label_order):
            if label in params_by_label:
                param_groups.append(dict(params=params_by_label[label], betas=betas[idx]))
        # include any unlabeled params at the end (processed last)
        if None in params_by_label:
            param_groups.append(dict(params=params_by_label[None]))
        super().__init__(param_groups, defaults)
        # init state: small params (numel < 1024) use full-sized state, others use sharded
        for p in params:
            chunk = p if p.numel() < 1024 else p[:p.size(0) // self.world_size]
            exp_avg = torch.zeros_like(chunk, dtype=torch.float32, device=p.device)
            self.state[p] = dict(step=0, exp_avg=exp_avg, exp_avg_sq=torch.zeros_like(exp_avg))

        # tag the final param for optimizer pipelining, run all gather after muon copy
        param_groups[-1]['params'][-1].is_final_param = True
        
        # DistributedAdam implementation by @vagrawal, @akash5474
        self.should_sync = False
        self._reduce_scatter_hooks = []
        self._reduce_scatter_futures = {}
        # 0-D CPU tensors to avoid recompilation in _update_step
        self._step_size_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self._eff_wd_t = torch.tensor(0.0, dtype=torch.float32, device="cpu")
        self.register_backward_hooks()

    def register_backward_hooks(self):
        for group in self.param_groups:
            for param in group["params"]:
                self._reduce_scatter_hooks.append(param.register_post_accumulate_grad_hook(self._sync_gradient))

    def load_state_dict(self, state_dict):
        """Override to preserve optimizer state dtypes (avoid BFloat16->Float32 cast that causes recompilation)."""
        # Save original state dtypes before loading
        original_dtypes = {}
        for p, s in self.state.items():
            original_dtypes[p] = {k: v.dtype for k, v in s.items() if isinstance(v, torch.Tensor)}
        
        # Call parent load_state_dict (which may cast dtypes to match param dtype)
        super().load_state_dict(state_dict)
        
        # Restore original dtypes
        for p, s in self.state.items():
            if p in original_dtypes:
                for k, v in s.items():
                    if isinstance(v, torch.Tensor) and k in original_dtypes[p]:
                        if v.dtype != original_dtypes[p][k]:
                            s[k] = v.to(original_dtypes[p][k])

    @torch.no_grad()
    def _sync_gradient(self, param):
        if not self.should_sync:
            return

        grad = param.grad
        if param.numel() < 1024:
            # Small params: use all_reduce (no scatter/gather needed)
            self._reduce_scatter_futures[param] = (
                dist.all_reduce(grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                grad
            )
        else:
            rank_size = grad.shape[0] // self.world_size
            if grad is not None:
                grad_slice = torch.empty_like(grad[:rank_size])
                self._reduce_scatter_futures[param] = (
                    dist.reduce_scatter_tensor(grad_slice, grad, op=dist.ReduceOp.AVG, async_op=True).get_future(),
                    grad_slice
                )

    def copy_lm_to_embed(self):
        # run at 2/3 of training
        lm_head = self.param_groups[0]['params'][0]
        embed = self.param_groups[-2]['params'][0]
        lm_head_state = self.state[lm_head]
        embed_state = self.state[embed]
        embed_state['step'] = lm_head_state['step']
        embed_state['exp_avg'] = lm_head_state['exp_avg'].clone()
        embed_state['exp_avg_sq'] = lm_head_state['exp_avg_sq'].clone()
        embed.data.copy_(lm_head.data)

    @staticmethod
    @torch.compile(dynamic=False, fullgraph=True)
    def _update_step(p_slice, g_slice, exp_avg, exp_avg_sq, beta1, beta2, eps, step_size_t, eff_wd_t):
        """Compiled Adam update step. step_size_t and eff_wd_t are 0-D CPU tensors to avoid recompilation."""
        exp_avg.mul_(beta1).add_(g_slice, alpha=1 - beta1)  # exp_avg = beta1 * exp_avg + (1 - beta1) * g_slice
        exp_avg_sq.mul_(beta2).addcmul_(g_slice, g_slice, value=1 - beta2)  # exp_avg_sq = beta2 * exp_avg_sq + (1 - beta2) * g_slice^2
        # compute step
        update = exp_avg.div(exp_avg_sq.sqrt().add_(eps)).mul_(step_size_t)  # update = (exp_avg / (sqrt(exp_avg_sq) + eps)) * step_size
        # cautious weight decay
        mask = (update * p_slice) > 0
        update.addcmul_(p_slice, mask, value=eff_wd_t)  # update += eff_wd_t * p_slice * mask
        p_slice.add_(other=update, alpha=-1.0)  # p_slice -= update

    @torch.no_grad()
    def step(self, muon_opt):
        muon_opt.step_p1()
        rank = dist.get_rank()
        all_gather_futures: list[torch.Future] = []

        last_param = None
        last_p_slice = None
        for group in self.param_groups:      
            beta1, beta2 = group['betas']
            eps = group['eps']
            wd = group['weight_decay']
            for param in group['params']:
                if param not in self._reduce_scatter_futures:
                    continue

                fut, g_slice = self._reduce_scatter_futures[param]
                fut.wait()

                is_small = param.numel() < 1024
                if is_small:
                    # Small params: g_slice is actually full grad, p_slice is full param
                    p_slice = param
                else:
                    rank_size = param.shape[0] // self.world_size
                    p_slice = param[rank * rank_size:(rank + 1) * rank_size]

                lr = group['lr'] * getattr(param, "lr_mul", 1.0)
                state = self.state[param]
                state["step"] += 1
                t = state["step"]
                
                # Pre-compute changing values as 0-D CPU tensors to avoid recompilation.
                # `.fill_(value)` is the same as "= value", but doesn't change the tensor object.
                bias1, bias2 = 1 - beta1 ** t, 1 - beta2 ** t
                self._step_size_t.fill_(lr * (bias2 ** 0.5 / bias1))
                self._eff_wd_t.fill_(lr * lr * wd * getattr(param, "wd_mul", 1.0)) # `lr` included twice to serve as weight decay schedule.

                DistAdam._update_step(p_slice, g_slice, state["exp_avg"], state["exp_avg_sq"],
                                      beta1, beta2, eps, self._step_size_t, self._eff_wd_t)

                if not is_small:
                    if getattr(param, "is_final_param", False):
                        last_param = param
                        last_p_slice = p_slice
                    else:
                        all_gather_futures.append(dist.all_gather_into_tensor(param, p_slice, async_op=True).get_future())
        self._reduce_scatter_futures.clear()

        muon_opt.step_p2()
        torch.futures.collect_all(all_gather_futures).wait()

        if last_param is not None:
            last_all_gather_future = dist.all_gather_into_tensor(last_param, last_p_slice, async_op=True).get_future()
        muon_opt.step_p3()
        torch.futures.collect_all([last_all_gather_future]).wait()

# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

def norm(x: Tensor):
    return F.rms_norm(x, (x.size(-1),))

class CastedLinear(nn.Linear):
    def __init__(self, in_features: int, out_features: int, use_fp8=False, x_s=1.0, w_s=1.0, grad_s=1.0):
        super().__init__(in_features, out_features, bias=False)
        self.use_fp8 = use_fp8
        self.x_s = x_s
        self.w_s = w_s
        self.grad_s = grad_s

    def reset_parameters(self) -> None:
        with torch.no_grad():
            self.weight.zero_()  # @Grad62304977 and others

    def forward(self, x: Tensor):
        if self.use_fp8 and self.training:
            _x = x.flatten(0, -2)
            out: Tensor = torch.ops.nanogpt.mm(_x, self.weight, x_s=self.x_s, w_s=self.w_s, grad_s=self.grad_s)[0]
            return out.reshape(*x.shape[:-1], -1)
        else:
            return F.linear(x, self.weight.type_as(x))


# -----------------------------------------------------------------------------
# PyTorch nn.Module definitions for the model

class Yarn(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        # half-truncate RoPE by @YouJiacheng (w/ base freq tuning)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        theta = torch.outer(t, angular_freq)
        self.factor1 = nn.Buffer(
            theta.cos().to(torch.bfloat16), persistent=False
        )
        self.factor2 = nn.Buffer(
            theta.sin().to(torch.bfloat16), persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        theta = torch.outer(t, self.angular_freq)
        self.factor1.copy_(theta.cos())
        self.factor2.copy_(theta.sin())
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

class YarnPairedHead(nn.Module):
    def __init__(self, head_dim, max_seq_len):
        super().__init__()
        self.head_dim = head_dim
        self.max_seq_len = max_seq_len
        self.reset()

    def rotary(self, x_BTHD):
        assert self.factor1.size(0) >= x_BTHD.size(-3)
        factor1, factor2 = (
            self.factor1[None, : x_BTHD.size(-3), None, :],
            self.factor2[None, : x_BTHD.size(-3), None, :],
        )
        x_flip = x_BTHD.view(*x_BTHD.shape[:-1], x_BTHD.shape[-1] // 2, 2).flip(-1).view(x_BTHD.shape)
        return factor1 * x_BTHD + factor2 * x_flip

    def reset(self):
        angular_freq = (1 / 1024) ** torch.linspace(0, 1, steps=self.head_dim//4, dtype=torch.float32, device=device)
        angular_freq = angular_freq.repeat_interleave(2)
        angular_freq = torch.cat([angular_freq, angular_freq.new_zeros(self.head_dim//2)])
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, angular_freq)
        theta2 = torch.outer(t_odd, angular_freq)
        self.factor1 = nn.Buffer(
            torch.cat((theta1.cos(),theta2.cos()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2 = nn.Buffer(
            torch.cat((theta1.sin(),theta2.sin()), dim=-1).to(torch.bfloat16), 
            persistent=False
        )
        self.factor2[..., 1::2] *= -1
        self.angular_freq = angular_freq
        # start with 0.1, inspired by 0.12 from @leloykun and learnable scalars used by @brendanh0gan https://x.com/hi_tysam/status/1879693583898591283
        self.attn_scale = 0.1

    def apply(self, old_window: int, new_window: int, alpha: int=1, beta: int=32):
        rotations = args.block_size * old_window * self.angular_freq / (2 * torch.pi)
        scaling_factor = old_window / new_window
        interpolation_weight = torch.clamp((rotations - alpha) / (beta - alpha), 0, 1)
        self.angular_freq *= scaling_factor + interpolation_weight * (1 - scaling_factor)
        t = torch.arange(2*self.max_seq_len, dtype=torch.float32, device=self.angular_freq.device)
        t_even = 2 * t
        t_odd = 2 * t + 1
        theta1 = torch.outer(t_even, self.angular_freq)
        theta2 = torch.outer(t_odd, self.angular_freq)
        self.factor1.copy_(torch.cat((theta1.cos(),theta2.cos()), dim=-1))
        self.factor2.copy_( torch.cat((theta1.sin(),theta2.sin()), dim=-1))
        self.factor2[..., 1::2] *= -1
        self.attn_scale *= 0.2 * math.log(new_window / old_window) + 1

@dataclass
class AttnArgs:
    ve: torch.Tensor
    sa_lambdas: torch.Tensor
    seqlens: torch.Tensor
    bm_size: int
    yarn: Yarn
    key_offset: bool
    attn_gate_w: torch.Tensor
    ve_gate_w: torch.Tensor

flash_attn_interface = get_kernel('varunneal/flash-attention-3').flash_attn_interface

class CausalSelfAttention(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        # merged QKVO weights: suggested by many, implemented by @fernbear.bsky.social, and further improved by @YouJiacheng
        # https://x.com/hi_tysam/status/1879699187107033311
        # Simplified layout by @chrisjmccormick
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)  # init QKV weights
            self.qkvo_w[self.dim * 3:].zero_()  # init O weights to zero

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas, key_offset = attn_args.ve, attn_args.sa_lambdas, attn_args.key_offset
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        # sparse gated attention to enable context based no-op by @classiclarryd
        # only include gates on layers with value embeds used on forward pass
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k) # QK norm @Grad62304977
        q, k = yarn.rotary(q), yarn.rotary(k)
        if key_offset:
            # shift keys forward for the stationary head dims. Enables 1-layer induction.
            k[:, 1:, :, self.head_dim // 4:self.head_dim // 2] = k[:, :-1, :, self.head_dim // 4:self.head_dim // 2]
            k[:, 1:, :, 3 * self.head_dim // 4:] = k[:, :-1, :, 3 * self.head_dim // 4:]
        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T, self.num_heads, 1)
            v = v + ve_gate_out * ve.view_as(v) # @ KoszarskyB & @Grad62304977

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # use flash_attn over flex_attn @varunneal. flash_attn_varlen suggested by @YouJiacheng
        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim) # re-assemble all head outputs side by side
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))  # sa_lambdas[1] pre-multiplied to O @shenberg
        return y

class PairedHeadCausalSelfAttention(nn.Module):
    """
    Pairs up attention heads such that queries from head 1 can attend to keys in head 2, and vice-versa.
    Implemented by interleaving the k, q, and v for pairs of heads to form twice as long sequences
    EG [k1_h1, k2_h1, k3_h1], [k1_h2, k2_h2, k3_h2] -> [k1_h1, k1_h2, k2_h1, k2_h2, k3_h1, k3_h2], repeat for q and v
    """
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = head_dim
        self.dim = dim
        self.hdim = num_heads * head_dim

        assert self.hdim == self.dim, "num_heads * head_dim must equal model_dim"
        std = self.dim ** -0.5
        bound = (3 ** 0.5) * std
        self.qkvo_w = nn.Parameter(torch.empty(self.dim * 4, self.hdim, dtype=torch.bfloat16))
        self.qkvo_w.label = 'attn'

        with torch.no_grad():
            self.qkvo_w[:self.dim * 3].uniform_(-bound, bound)
            self.qkvo_w[self.dim * 3:].zero_()

    def forward(self, x: Tensor, attn_args: AttnArgs):
        B, T = x.size(0), x.size(1) # batch size, sequence length
        assert B == 1, "varlen sequences requires B == 1"
        assert T % 16 == 0
        # unpack attention args
        yarn = attn_args.yarn
        ve, sa_lambdas = attn_args.ve, attn_args.sa_lambdas
        seqlens, bm_size = attn_args.seqlens, attn_args.bm_size
        attn_gate_w, ve_gate_w = attn_args.attn_gate_w, attn_args.ve_gate_w

        q, k, v = F.linear(x, sa_lambdas[0] * self.qkvo_w[:self.dim * 3].type_as(x)).view(B, T, 3 * self.num_heads, self.head_dim).chunk(3, dim=-2)
        q, k = norm(q), norm(k)

        # delay q,k reshape until rotary makes data contiguous, to enable view (non-copy)
        q = q.view(B, T, self.num_heads // 2, self.head_dim * 2)
        k = k.view(B, T, self.num_heads // 2, self.head_dim * 2)
        v = v.reshape(B, T*2, self.num_heads//2, self.head_dim)

        q, k = yarn.rotary(q), yarn.rotary(k)

        q = q.view(B, T*2, self.num_heads//2, self.head_dim)
        k = k.view(B, T*2, self.num_heads//2, self.head_dim)

        if ve is not None:
            ve_gate_out = 2 * torch.sigmoid(F.linear(x[..., :12], ve_gate_w)).view(B, T*2, self.num_heads//2, 1)
            v = v + ve_gate_out * ve.view_as(v)

        max_len = args.train_max_seq_len if self.training else (args.val_batch_size // (grad_accum_steps * world_size))

        # paired head correction
        seqlens = 2 * seqlens
        max_len = 2 * max_len

        y = flash_attn_interface.flash_attn_varlen_func(q[0], k[0], v[0], cu_seqlens_q=seqlens, cu_seqlens_k=seqlens,
                                                        max_seqlen_q=max_len, max_seqlen_k=max_len,
                                                        causal=True, softmax_scale=yarn.attn_scale, window_size=(bm_size, 0))
        y = y.view(B, T, self.num_heads, self.head_dim)
        y = y * torch.sigmoid(F.linear(x[..., :12], attn_gate_w)).view(B, T, self.num_heads, 1)
        y = y.contiguous().view(B, T, self.num_heads * self.head_dim)
        y = F.linear(y, sa_lambdas[1] * self.qkvo_w[self.dim * 3:].type_as(y))
        return y

@triton.jit
def linear_relu_square_kernel(a_desc, b_desc, c_desc, aux_desc,
                                 M, N, K,
                                 BLOCK_SIZE_M: tl.constexpr,
                                 BLOCK_SIZE_N: tl.constexpr,
                                 BLOCK_SIZE_K: tl.constexpr,
                                 GROUP_SIZE_M: tl.constexpr,
                                 NUM_SMS: tl.constexpr,
                                 FORWARD: tl.constexpr,
                                 ):
    dtype = tl.bfloat16
    start_pid = tl.program_id(axis=0)
    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
    k_tiles = tl.cdiv(K, BLOCK_SIZE_K)
    num_tiles = num_pid_m * num_pid_n

    tile_id_c = start_pid - NUM_SMS
    num_pid_in_group = GROUP_SIZE_M * num_pid_n

    for tile_id in tl.range(start_pid, num_tiles, NUM_SMS, flatten=True):
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am = pid_m * BLOCK_SIZE_M
        offs_bn = pid_n * BLOCK_SIZE_N

        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for ki in range(k_tiles):
            offs_k = ki * BLOCK_SIZE_K
            a = a_desc.load([offs_am, offs_k])
            b = b_desc.load([offs_bn, offs_k])
            accumulator = tl.dot(a, b.T, accumulator)

        tile_id_c += NUM_SMS
        pid_m = tile_id // num_pid_n
        pid_n = tile_id % num_pid_n
        offs_am_c = pid_m * BLOCK_SIZE_M
        offs_bn_c = pid_n * BLOCK_SIZE_N

        acc = tl.reshape(accumulator, (BLOCK_SIZE_M, 2, BLOCK_SIZE_N // 2))
        acc = tl.permute(acc, (0, 2, 1))
        acc0, acc1 = tl.split(acc)

        c0 = acc0.to(dtype)
        if not FORWARD:
            c0_pre = aux_desc.load([offs_am_c, offs_bn_c])
            c0 = 2 * c0 * tl.where(c0_pre > 0, c0_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c], c0)

        if FORWARD:
            c0_post = tl.maximum(c0, 0)
            c0_post = c0_post * c0_post
            aux_desc.store([offs_am_c, offs_bn_c], c0_post)

        c1 = acc1.to(dtype)
        if not FORWARD:
            c1_pre = aux_desc.load([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2])
            c1 = 2 * c1 * tl.where(c1_pre > 0, c1_pre, 0)

        c_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1)

        if FORWARD:
            c1_post = tl.maximum(c1, 0)
            c1_post = c1_post * c1_post
            aux_desc.store([offs_am_c, offs_bn_c + BLOCK_SIZE_N // 2], c1_post)


def linear_relu_square(a, b, aux=None):
    M, K = a.shape
    N, K = b.shape
    dtype = a.dtype

    c = torch.empty((M, N), device=a.device, dtype=dtype)

    FORWARD = False
    if aux is None:
        FORWARD = True
        aux = torch.empty((M, N), device=a.device, dtype=dtype)

    NUM_SMS = torch.cuda.get_device_properties("cuda").multi_processor_count

    BLOCK_SIZE_M = 128
    BLOCK_SIZE_N = 256
    BLOCK_SIZE_K = 64
    num_stages = 4 if FORWARD else 3
    num_warps = 8

    a_desc = TensorDescriptor.from_tensor(a, [BLOCK_SIZE_M, BLOCK_SIZE_K])
    b_desc = TensorDescriptor.from_tensor(b, [BLOCK_SIZE_N, BLOCK_SIZE_K])
    c_desc = TensorDescriptor.from_tensor(c, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])
    aux_desc = TensorDescriptor.from_tensor(aux, [BLOCK_SIZE_M, BLOCK_SIZE_N // 2])

    def grid(META):
        return (min(
            NUM_SMS,
            triton.cdiv(M, BLOCK_SIZE_M) * triton.cdiv(N, BLOCK_SIZE_N),
        ), )

    linear_relu_square_kernel[grid](
        a_desc, b_desc, c_desc, aux_desc,
        M, N, K,
        BLOCK_SIZE_M=BLOCK_SIZE_M,
        BLOCK_SIZE_N=BLOCK_SIZE_N,
        BLOCK_SIZE_K=BLOCK_SIZE_K,
        GROUP_SIZE_M=1,
        NUM_SMS=NUM_SMS,
        FORWARD=FORWARD,
        num_stages=num_stages,
        num_warps=num_warps
    )

    if FORWARD:
        return c, aux
    else:
        return c

class FusedLinearReLUSquareFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, W1, W2):
        pre, post = linear_relu_square(x.view((-1, x.shape[-1])), W1)
        x3 = post @ W2
        ctx.save_for_backward(x, W1, W2, pre, post)
        return x3.view(x.shape)

    @staticmethod
    def backward(ctx, grad_output):
        x, W1, W2, pre, post = ctx.saved_tensors
        dW2 = post.T @ grad_output
        dpre = linear_relu_square(grad_output.view((-1, grad_output.shape[-1])), W2, aux=pre)
        dW1 = dpre.T @ x
        dx = dpre @ W1
        return dx.view(x.shape), dW1, dW2

class MLP(nn.Module):
    def __init__(self, dim: int):
        super().__init__()
        hdim = 4 * dim
        # Transposed layout to match attention weights
        self.c_fc = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        self.c_proj = nn.Parameter(torch.empty(hdim, dim, dtype=torch.bfloat16))
        # label all modules for explicit optimizer grouping
        self.c_fc.label = 'mlp'
        self.c_proj.label = 'mlp'
        self.c_proj.lr_mul = 2.

        std = 0.5 * (dim ** -0.5)
        bound = (3 ** 0.5) * std # improved init scale by @YouJiacheng
        with torch.no_grad():
            self.c_fc.uniform_(-bound, bound)
            self.c_proj.zero_() # zero init suggested by @Grad62304977

    def forward(self, x: Tensor):
        # relu(x)^2:
        # https://arxiv.org/abs/2109.08668v2; ~1-2% better than GELU; suggested by @SKYLINEZ007 and @Grad62304977

        # This call computes relu(x @ W1.T)^2 @ W2.T
        return FusedLinearReLUSquareFunction.apply(x, self.c_fc, self.c_proj)


class Block(nn.Module):
    def __init__(self, dim: int, head_dim: int, num_heads: int, layer_idx: int, use_paired_head: bool):
        super().__init__()
        # skip attention of blocks.6 (the 7th layer) by @YouJiacheng
        if use_paired_head:
            self.attn = PairedHeadCausalSelfAttention(dim, head_dim, num_heads, layer_idx)
        else:
            self.attn = CausalSelfAttention(dim, head_dim, num_heads, layer_idx) if layer_idx != 6 else None
        # skip MLP blocks for first MLP layer by @EmelyanenkoK
        self.mlp = MLP(dim)

    def forward(self, x: Tensor, attn_args: AttnArgs):
        if self.attn is not None:
            x = x + self.attn(norm(x), attn_args)
        if self.mlp is not None:
            x = x + self.mlp(norm(x))
        return x

# -----------------------------------------------------------------------------
# Fused Softcapped Cross Entropy

@triton.jit
def fused_softcapped_entropy_fwd_kernel(
    logits_ptr, losses_ptr, lse_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)
    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    
    max_val = -float('inf')
    sum_exp = 0.0
    
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=-float('inf')).to(tl.float32)
        z = A * tl.sigmoid((val + B) / C)
        z = tl.where(mask, z, -float('inf'))
        curr_max = tl.max(z, axis=0)
        new_max = tl.maximum(max_val, curr_max)
        sum_exp = sum_exp * tl.exp(max_val - new_max) + tl.sum(tl.exp(z - new_max), axis=0)
        max_val = new_max
    
    lse = max_val + tl.log(sum_exp)
    tl.store(lse_ptr + row_idx, lse)
    
    total_loss = 0.0
    for k in range(n_predict):
        target_idx = row_idx + k
        if target_idx < n_rows:
            weight = tl.load(mtp_weights_ptr + k)
            if weight > 0:
                target = tl.load(targets_ptr + target_idx).to(tl.int32)
                if target >= 0 and target < n_cols:
                    val_target = tl.load(logits_row_ptr + target).to(tl.float32)
                    z_target = A * tl.sigmoid((val_target + B) / C)
                    total_loss += weight * (lse - z_target)
    
    tl.store(losses_ptr + row_idx, total_loss)

@triton.jit
def fused_softcapped_entropy_bwd_kernel(
    grad_input_ptr, grad_output_ptr, lse_ptr, logits_ptr, targets_ptr, mtp_weights_ptr,
    stride_logits_n, stride_logits_v, stride_grad_n, stride_grad_v,
    n_rows, n_cols, n_predict,
    A, B, C,
    BLOCK_SIZE: tl.constexpr
):
    row_idx = tl.program_id(0).to(tl.int64)

    logits_row_ptr = logits_ptr + row_idx * stride_logits_n
    grad_row_ptr = grad_input_ptr + row_idx * stride_grad_n
    
    lse = tl.load(lse_ptr + row_idx)
    grad_loss = tl.load(grad_output_ptr + row_idx)
    
    S_w = 0.0
    for k in range(n_predict):
        if row_idx + k < n_rows:
            S_w += tl.load(mtp_weights_ptr + k)
            
    for off in range(0, n_cols, BLOCK_SIZE):
        cols = off + tl.arange(0, BLOCK_SIZE)
        mask = cols < n_cols
        val = tl.load(logits_row_ptr + cols, mask=mask, other=0.0).to(tl.float32)
        u = (val + B) / C
        sigmoid_u = tl.sigmoid(u)
        z = A * sigmoid_u
        p = tl.exp(z - lse)
        
        term1 = S_w * p
        term2 = tl.zeros([BLOCK_SIZE], dtype=tl.float32)
        for k in range(n_predict):
            if row_idx + k < n_rows:
                target = tl.load(targets_ptr + row_idx + k).to(tl.int32)
                weight = tl.load(mtp_weights_ptr + k)
                term2 += tl.where(cols == target, weight, 0.0)
        
        grad_z = grad_loss * (term1 - term2)
        dz_dx = (1.0 / C) * z * (1.0 - sigmoid_u)
        grad_x = grad_z * dz_dx
        tl.store(grad_row_ptr + cols, grad_x.to(tl.bfloat16), mask=mask)

class FusedSoftcappedCrossEntropy(torch.autograd.Function):
    @staticmethod
    def forward(ctx, logits, targets, mtp_weights, A=23.0, B=5.0, C=7.5):
        n_rows, n_cols = logits.shape
        if mtp_weights is None:
             mtp_weights = torch.tensor([1.0], device=logits.device, dtype=torch.float32)
        n_predict = mtp_weights.shape[0]

        losses = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        lse = torch.empty(n_rows, dtype=torch.float32, device=logits.device)
        
        logits = logits.contiguous()
        targets = targets.contiguous()
        mtp_weights = mtp_weights.contiguous()

        grid = (n_rows,)
        fused_softcapped_entropy_fwd_kernel[grid](
            logits, losses, lse, targets, mtp_weights,
            logits.stride(0), logits.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        
        ctx.save_for_backward(logits, targets, mtp_weights, lse)
        ctx.params = (A, B, C)
        return losses

    @staticmethod
    def backward(ctx, grad_output):
        logits, targets, mtp_weights, lse = ctx.saved_tensors
        A, B, C = ctx.params
        n_rows, n_cols = logits.shape
        n_predict = mtp_weights.shape[0]
        
        grad_input = torch.empty((n_rows, n_cols), dtype=torch.bfloat16, device=logits.device)
        grad_output = grad_output.contiguous()
        
        grid = (n_rows,)
        fused_softcapped_entropy_bwd_kernel[grid](
            grad_input, grad_output, lse, logits, targets, mtp_weights,
            logits.stride(0), logits.stride(1), grad_input.stride(0), grad_input.stride(1),
            n_rows, n_cols, n_predict,
            A, B, C,
            BLOCK_SIZE=1024,
            num_warps=8,
            num_stages=4
        )
        return grad_input, None, None, None, None, None

# -----------------------------------------------------------------------------
# The main model

def next_multiple_of_n(v: float | int, *, n: int):
    return next(x for x in range(n, int(v) + 1 + n, n) if x >= v)

@dataclass
class ForwardScheduleConfig:
    mtp_weights: torch.Tensor
    ws_short: int
    ws_long: int

class GPT(nn.Module):
    def __init__(self, vocab_size: int, num_layers: int, num_heads: int, head_dim: int, model_dim: int, max_seq_len: int, use_malbo: bool):
        super().__init__()
        self.num_layers = num_layers
        vocab_size = next_multiple_of_n(vocab_size, n=128)

        self.smear_gate = CastedLinear(12, 1)
        self.smear_gate.weight.label = 'smear_gate'
        self.smear_gate.weight.lr_mul = 0.01
        self.smear_gate.weight.wd_mul = 0.0

        self.skip_gate = CastedLinear(12, 1)
        self.skip_gate.weight.label = 'skip_gate'
        self.skip_gate.weight.lr_mul = 0.05
        self.skip_gate.weight.wd_mul = 0.0

        # token value embeddings by @KoszarskyB - inspired by @Grad62304977's value residual implementation following https://arxiv.org/abs/2410.17897
        # value embedding code simplification inspired by @ragulpr https://github.com/KellerJordan/modded-nanogpt/pull/78
        self.value_embeds = nn.ModuleList([nn.Embedding(vocab_size, model_dim) for _ in range(3)])
        for embed in self.value_embeds:
            nn.init.zeros_(embed.weight)
        for ve in self.value_embeds:
            ve.weight.label = 'value_embed'
        
        # parameter banks for attention and value embedding gate weights
        self.attn_gate_bank = nn.Parameter(torch.zeros(10, num_heads, 12)) # 10 layers
        self.attn_gate_bank.label = 'attn_gate_bank'
        self.ve_gate_bank = nn.Parameter(torch.zeros(5, num_heads, 12)) # 5 layers
        self.ve_gate_bank.label = 've_gate_bank'

        self.paired_head_layers = [0, 2, 5, 9]
        self.blocks = nn.ModuleList([Block(model_dim, head_dim, num_heads, i, i in self.paired_head_layers) for i in range(num_layers)])
        self.yarn = Yarn(head_dim, max_seq_len)
        self.yarn_paired_head = YarnPairedHead(head_dim, max_seq_len)
        # there are only 50257 unique GPT-2 tokens; we extend to nearest multiple of 128 for efficiency.
        # suggested to me by @Grad62304977. this originates from Karpathy's experiments.
        use_fp8 = not os.environ.get("DISABLE_FP8", False)

        self.lm_head = CastedLinear(model_dim, vocab_size, use_fp8=use_fp8, x_s=100/448, w_s=1.6/448, grad_s=0.75/448)
        nn.init.normal_(self.lm_head.weight, mean=0, std=0.005)
        self.lm_head.weight.label = 'lm_head'

        self.embed = nn.Embedding(vocab_size, model_dim)
        self.embed.weight.label = 'embed'

        # x0_lambdas separated out for different optimizer treatment (no beta smoothing)
        self.x0_lambdas = nn.Parameter(torch.zeros(num_layers))
        self.x0_lambdas.label = 'x0_lambdas'
        self.x0_lambdas.lr_mul = 5.0
        self.x0_lambdas.wd_mul = 0.0

        pad = (-num_layers * 3 - 3) % dist.get_world_size()  # updated: 3*num_layers instead of 4*
        self.scalars = nn.Parameter(
            torch.cat(
                [
                    1.1 * torch.ones(num_layers),  # resid lambdas. 1.1 init such that layer i weight is i^(num_layers-i).
                    *[torch.tensor([0.5, 1.0]) for _ in range(num_layers)],  # SA lambdas
                    torch.zeros(1), # smear_lambda
                    0.5*torch.ones(1), # backout_lambda
                    -1.5 * torch.ones(1),  # skip_lambda -> (-1.5)  0.18
                    torch.ones(pad),
                ]
            )
        )

        self.scalars.label = 'scalars'
        # set learning rates
        for param in self.value_embeds.parameters():
            param.lr_mul = 75.
            param.wd_mul = 5.
        for param in self.embed.parameters():
            param.wd_mul = 150.
        for param in self.lm_head.parameters():
            param.wd_mul = 150.
        self.scalars.lr_mul = 5.0
        self.scalars.wd_mul = 0.0

        self.split_embed = False

        self.use_malbo = use_malbo

    def forward(self, input_seq: Tensor, target_seq: Tensor, seqlens: Tensor, schedule_cfg: ForwardScheduleConfig):
        assert input_seq.ndim == 1

        # unpack schedule_cfg
        mtp_weights, ws_short, ws_long = schedule_cfg.mtp_weights, schedule_cfg.ws_short, schedule_cfg.ws_long

        # set configs
        skip_connections = []
        skip_in = [3] # long attention window on layer 3
        skip_out = [6] # no attn op on layer 6
        x_backout = None
        backout_layer = 7

        # set lambdas
        resid_lambdas = self.scalars[: 1 * self.num_layers]
        x0_lambdas = self.x0_lambdas
        sa_lambdas = self.scalars[1 * self.num_layers: 3 * self.num_layers].view(-1, 2)
        smear_lambda = self.scalars[3 * self.num_layers]
        backout_lambda = self.scalars[3 * self.num_layers+1]
        skip_lambda = self.scalars[3 * self.num_layers+2]

        # set block masks and key shift
        short_bm = ws_short * args.block_size
        long_bm = ws_long * args.block_size
        bm_sizes = [short_bm, short_bm, short_bm, long_bm, short_bm, short_bm, None, short_bm, short_bm, short_bm, long_bm]
        assert len(bm_sizes) == self.num_layers
        key_offset = [b==long_bm for b in bm_sizes] # apply partial key offset to long windows

        # weight-tied: use lm_head.weight for embedding lookup (or separate embed after split)
        if self.split_embed:
            x = self.embed(input_seq)
        else:
            x = F.embedding(input_seq, self.lm_head.weight)
        ve = [value_embed(input_seq) for value_embed in self.value_embeds]
        # 012 ... 012 structure on token value embeddings by @YouJiacheng, improved on @leloykun's U-net structure
        # dropping first layer updates this to .12 ... 012
        ve = [ve[1], ve[2]] + [None] * (self.num_layers - 5) + [ve[0], ve[1], ve[2]]
        assert len(ve) == self.num_layers

        # smear token embed forward 1 position @classiclarryd
        smear_gate_out = smear_lambda * torch.sigmoid(self.smear_gate(x[1:, :self.smear_gate.weight.size(-1)]))
        x = torch.cat([x[:1], x[1:] + smear_gate_out * x[:-1]])
        x = x0 = norm(x[None])

        # unbind gate banks to avoid select_backwards kernel
        ag = [w.bfloat16() for w in self.attn_gate_bank.unbind(0)] 
        veg = [w.bfloat16() for w in self.ve_gate_bank.unbind(0)]
        attn_gates = ag[:6] + [None] + ag[6:]
        ve_gates = [veg[0], veg[1]] + [None] * (self.num_layers - 5) + [veg[2], veg[3], veg[4]]
        assert len(attn_gates) == self.num_layers
        assert len(ve_gates) == self.num_layers

        for i in range(self.num_layers):
            yarn = self.yarn_paired_head if i in self.paired_head_layers else self.yarn
            attn_args = AttnArgs(
                ve=ve[i],
                sa_lambdas=sa_lambdas[i],
                seqlens=seqlens,
                bm_size=bm_sizes[i],
                yarn=yarn,
                key_offset=key_offset[i],
                attn_gate_w=attn_gates[i],
                ve_gate_w=ve_gates[i]
            )
            if i in skip_out:
                skip_gate_out = torch.sigmoid(skip_lambda) * 2 * torch.sigmoid(self.skip_gate(x0[..., :self.skip_gate.weight.size(-1)]))
                x = x + skip_gate_out * skip_connections.pop()
            if i == 0:
                x = (resid_lambdas[0] + x0_lambdas[0]) * x
            else:
                x = resid_lambdas[i] * x + x0_lambdas[i] * x0
            x = self.blocks[i](x, attn_args)
            if i in skip_in:
                skip_connections.append(x)
            if i == backout_layer:
                x_backout = x

        # back out contributions from first 7 layers that are only required for downstream context and not direct prediction
        x -= backout_lambda * x_backout
        x = norm(x)
        logits = self.lm_head(x)
        # @Grad62304977 added tanh softcapping following Gemma 2 paper, @KoszarskyB reduced it from 30 to 15
        # @YouJiacheng shifted it by +15 (2*sigmoid(2*x)=tanh(x)+1). @classiclarryd updated to 23*sigmoid((logits+5)/7.5)
        if self.training:
            losses = FusedSoftcappedCrossEntropy.apply(logits.view(-1, logits.size(-1)), target_seq, mtp_weights)
            loss = losses.sum()
            if self.use_malbo:
                T, K = logits.view(-1, logits.size(-1)).shape
                with torch.no_grad():
                    vhat, kappa, gamma = compute_malbo_parameters((-losses).float().exp().unsqueeze(0), K)
                    weights = (vhat * kappa * gamma).squeeze(0)
                malbo_loss = T * (weights * losses).sum()
            else:
                malbo_loss = loss
        else:
            logits = 23 * torch.sigmoid((logits + 5) / 7.5)
            logits_for_loss = logits.float()
            losses = F.cross_entropy(logits_for_loss.view(-1, logits_for_loss.size(-1)), target_seq, reduction="none")
            loss = losses.mean()
            if self.use_malbo:
                T, K = logits.view(-1, logits.size(-1)).shape
                with torch.no_grad():
                    vhat, kappa, gamma = compute_malbo_parameters((-losses).float().exp().unsqueeze(0), K)
                    weights = (vhat * kappa * gamma).squeeze(0)
                malbo_loss = (weights * losses).sum()
            else:
                malbo_loss = loss
        return loss, malbo_loss

# -----------------------------------------------------------------------------
# Distributed data loader

def _load_data_shard(file: Path):
    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32
    assert header[0] == 20240520, "magic number mismatch in the data .bin file"
    assert header[1] == 1, "unsupported version"
    num_tokens = int(header[2]) # number of tokens (claimed)
    with file.open("rb", buffering=0) as f:
        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng
        f.seek(256 * 4)
        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng
        assert nbytes == 2 * num_tokens, "number of tokens read does not match header"
    return tokens

BOS_ID = 50256

class BOSFinder:
    # Helper for getting sequences that start at the beginning of documents by @varunneal based on work by @classiclarryd
    def __init__(self, tokens: Tensor, world_size: int = 1, quickload: bool = False):
        # Precompute BOS positions once per shard
        self.tokens=tokens
        self.size = tokens.numel()
        self.quickload = quickload
        if quickload:
            # only scan first 4 million tokens, then kickoff async thread to scan rest
            self.bos_idx = (tokens[:4_000_000] == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
            self.thread = None
            self.ready = threading.Event()
            self.start()
        else:
            self.bos_idx = (tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.i = 0
        self.world_size = world_size
        self.batch_iter = 0

    def _load(self):
        self.bos_idx_async = (self.tokens == BOS_ID).nonzero(as_tuple=True)[0].to(torch.int64).cpu().numpy()
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        self.bos_idx = self.bos_idx_async

    def next_batch(self, num_tokens_local: int, max_seq_len: int):
        # if quickload was used, repoint to the full dataset after 5 batches
        if self.quickload and self.batch_iter==5:
            self.get()
        n = len(self.bos_idx)
        starts = [[] for _ in range(self.world_size)]
        ends = [[] for _ in range(self.world_size)]

        idx = self.i
        for r in range(self.world_size):
            cur_len = 0
            while cur_len <= num_tokens_local:
                if idx >= n:
                    raise StopIteration(f"Insufficient BOS ahead; hit tail of shard.")
                cur = self.bos_idx[idx]
                starts[r].append(cur)
                end = min(self.bos_idx[idx + 1] if idx + 1 < n else self.size,
                          cur + max_seq_len,
                          cur + num_tokens_local - cur_len + 1)
                ends[r].append(end)
                cur_len += end - cur
                idx += 1

            assert cur_len == num_tokens_local + 1
        self.i = idx
        self.batch_iter+=1
        return starts, ends

class DataPreloader:
    # Helper for asynchronously loading next shard and indexing bos tokens
    def __init__(self, file_iter, world_size: int = 1):
        self.file_iter = file_iter
        self.world_size = world_size
        self.thread = None
        self.data = None
        self.ready = threading.Event()

    def _load(self):
        tokens = _load_data_shard(next(self.file_iter))
        self.data = (tokens, BOSFinder(tokens, self.world_size))
        self.ready.set()

    def start(self):
        self.ready.clear()
        self.thread = threading.Thread(target=self._load)
        self.thread.start()

    def get(self):
        if self.thread:
            self.ready.wait()
            self.thread.join()
        return self.data

def distributed_data_generator(filename_pattern: str, num_tokens: int, max_seq_len: int, grad_accum_steps: int = 1, align_to_bos: bool = True):
    # align_to_bos: each sequence begins with Beginning of Sequence token, sequences truncated to max_seq_len
    rank = dist.get_rank() if dist.is_initialized() else 0
    world_size = dist.get_world_size() if dist.is_initialized() else 1
    assert num_tokens % (world_size * grad_accum_steps) == 0, "Batch size must be divisible by world size"
    num_tokens = num_tokens // grad_accum_steps

    files = [Path(file) for file in sorted(glob.glob(filename_pattern))]
    if not files:
        raise FileNotFoundError(f"No files found for pattern: {filename_pattern}")

    file_iter = iter(files)  # Use itertools.cycle(files) for multi-epoch training
    tokens = _load_data_shard(next(file_iter))
    if align_to_bos:
        finder = BOSFinder(tokens, world_size=world_size, quickload=True)
        preloader = DataPreloader(file_iter, world_size)
        preloader.start()
    else:
        pos = 0  # for unaligned case

    while True:
        num_tokens_local = num_tokens // world_size
        max_num_docs = next_multiple_of_n(num_tokens_local // 300, n=128)  # median doc length is ~400

        if align_to_bos:
            try:
                seq_starts, seq_ends = finder.next_batch(num_tokens_local, max_seq_len)
                start_idxs, end_idxs = torch.tensor(seq_starts[rank]), torch.tensor(seq_ends[rank])
            except StopIteration:
                # This shard is exhausted, load the next one in the next loop iteration.
                tokens, finder = preloader.get()
                preloader.start()
                continue

            buf = torch.cat([tokens[i:j] for i, j in zip(start_idxs, end_idxs)])
            _inputs = buf[:-1]
            _targets = buf[1:]
            end_idxs[-1] -= 1  # last document was too long to account for _targets offset
            cum_lengths = (end_idxs - start_idxs).cumsum(0)

        else:
            if pos + num_tokens + 1 >= len(tokens):  # should not occur for val data
                tokens, pos = _load_data_shard(next(file_iter)), 0

            pos_local = pos + rank * num_tokens_local
            buf = tokens[pos_local: pos_local + num_tokens_local + 1]
            _inputs = buf[:-1].view(num_tokens_local, )
            _targets = buf[1:].view(num_tokens_local, )

            cum_lengths = torch.nonzero(_inputs == BOS_ID)[:, 0]
            pos += num_tokens


        _cum_lengths = torch.full((max_num_docs,), num_tokens_local)
        _cum_lengths[0] = 0
        _cum_lengths[1:len(cum_lengths) + 1] = cum_lengths

        # Cast to int32 on CPU before transfer to avoid dtype conversion during .to()
        _inputs = _inputs.to(dtype=torch.int32)
        _targets = _targets.to(dtype=torch.int64)
        _cum_lengths = _cum_lengths.to(dtype=torch.int32)

        new_params = yield (
            _inputs.to(device="cuda", non_blocking=True),
            _targets.to(device="cuda", non_blocking=True),
            _cum_lengths.to(device="cuda", non_blocking=True)
        )

        if new_params is not None:
            # makes it possible for generator to receive new (num_tokens, max_seq_len, grad_accum_steps) via .send()
            new_num_tokens, new_max_seq_len, new_grad_accum_steps = new_params
            assert new_num_tokens % (world_size * new_grad_accum_steps) == 0, "Num tokens must be divisible by world size"
            num_tokens = new_num_tokens // new_grad_accum_steps
            max_seq_len = new_max_seq_len

# -----------------------------------------------------------------------------
# Training Management
 
def get_bs(step: int):
    if step >= args.num_scheduled_iterations:
        return args.train_bs_extension
    x = step / args.num_scheduled_iterations
    bs_idx = int(len(args.train_bs_schedule) * x)
    return args.train_bs_schedule[bs_idx]

def get_ws(step: int):
    # set short window size to half of long window size
    # Higher ws on "extension" steps
    if step >= args.num_scheduled_iterations:
        return args.ws_final // 2, args.ws_final
    x = step / args.num_scheduled_iterations
    assert 0 <= x < 1
    ws_idx = int(len(args.ws_schedule) * x)
    return args.ws_schedule[ws_idx] // 2, args.ws_schedule[ws_idx]

# learning rate schedule: tied to batch size schedule, with cooldown at the end.
def get_lr(step: int):
    if step > args.num_scheduled_iterations:
        return 0.1
    lr_max = 1.0
    x = step / args.num_scheduled_iterations
    if x > 1/3:
       lr_max = 1.52  # (16/8)**0.6
    if x > 2/3:
        lr_max = 1.73  # (24/8)**0.5
    if x >= 1 - args.cooldown_frac:
        w = (1 - x) / args.cooldown_frac
        lr = lr_max * w + (1 - w) * 0.1
        return lr
    return lr_max

def get_muon_momentum(step: int, muon_warmup_steps=300, muon_cooldown_steps=50, momentum_min=0.85, momentum_max=0.95):
    # warmup phase: linearly increase momentum from min to max
    # cooldown phase: linearly decrease momentum from max to min
    momentum_cd_start = args.num_iterations - muon_cooldown_steps
    if step < muon_warmup_steps:
        frac = step / muon_warmup_steps
        momentum = momentum_min + frac * (momentum_max - momentum_min)
    elif step > momentum_cd_start:
        frac = (step - momentum_cd_start) / muon_cooldown_steps
        momentum = momentum_max - frac * (momentum_max - momentum_min)
    else:
        momentum = momentum_max
    return momentum

class TrainingManager():
    """
    Manages three optimizers for Adam embed/lm_head, Adam scalars, and Muon weight matrices.
    Notable Features:
        1. Scalars are given higher momentum terms to smooth learning @ChrisJMcCormick
        2. Scalar weights are temporarily frozen during batch size or window size updates @ChrisJMcCormick
        3. Adam optimizers are only stepped on odd steps @classiclarryd
        4. Adam optimizers have hooks to start gradient communication during backwards pass @akash5474
        5. Muon has a linear momentum warmup and cooldown schedule
        6. Learning rates follow a linear decay schedule
        7. Embed/lm_head weights and optimizer state splits at 2/3 of training @classiclarryd

    Manages model architecture, data, and target that changes during training
    Notable Features:
        1. Multi Token Prediction schedule of [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1] @varunneal
        2. Sliding Attention window schedule of [1,3] -> [3,7] -> [5,11] -> [6,13]
        3. YaRN updates to RoPE on window changes
        4. Split embed and lm head at 2/3 of training
        5. Batch size schedule of 8 -> 16 -> 24
        6. Post training extension of long windows from 13 to 20
    """
    def __init__(self, model):
        self.mtp_weights_schedule = self._build_mtp_schedule()
        self.model = model
        adam_betas = {
            'lm_head': [0.5, 0.95],
            'smear_gate': [0.9, 0.99],
            'attn_gate_bank': [0.9, 0.99],
            've_gate_bank': [0.9, 0.99],
            'skip_gate': [0.9, 0.99],
            'x0_lambdas': [0.65, 0.95],
            'scalars': [0.9, 0.99],
            'embed': [0.5, 0.95],
            'value_embed': [0.75, 0.95]
        }
        adam_labels = list(adam_betas.keys())
        adam_beta_values = list(adam_betas.values())
        muon_labels = ['attn', 'mlp']
        adam_params = [p for p in model.parameters() if getattr(p, 'label', None) in adam_labels]
        muon_params = [p for p in model.parameters() if getattr(p, 'label', None) in muon_labels]
        assert set(getattr(p, 'label', None) for p in model.parameters()) == set(adam_labels + muon_labels), "All params must have label"

        self.adam_opt = DistAdam(adam_params, adam_labels, adam_beta_values, lr=0.008, eps=1e-10, weight_decay=0.005)
        self.muon_opt = NorMuon(muon_params, lr=0.023, momentum=0.95, beta2=0.95, weight_decay=1.2)
        self.optimizers = [self.adam_opt, self.muon_opt]

        # split after odd number step
        self.split_step = math.ceil(args.split_embed_frac * args.num_scheduled_iterations) | 1

        # set defaults
        for opt in self.optimizers:
            opt.odd_step_only = False 
            opt.should_sync = True
            for group in opt.param_groups:
                group["initial_lr"] = group["lr"]

        # on even steps, only step Muon params
        self.adam_opt.odd_step_only = True

        self.reset()

    def _build_mtp_schedule(self):
        # Precompute MTP weights for all steps to avoid tensor allocation during training
        # Schedule: [1, 0.5, 0.25->0] -> [1, 0.5->0] -> [1]
        mtp_weights_schedule = []
        for s in range(args.num_iterations + 1):
            x = s / args.num_scheduled_iterations
            if x < 1/3:
                w = [1.0, 0.5, 0.25 * (1 - 3*x)]
            elif x < 2/3:
                w = [1.0, 0.5 * (1 - (3*x - 1))]
            else:
                w = [1.0]
            mtp_weights_schedule.append(torch.tensor(w, device=device))
        return mtp_weights_schedule

    def apply_final_ws_ext(self):
        self.ws_long = args.ws_validate_post_yarn_ext

    def get_forward_args(self):
        return ForwardScheduleConfig(
            mtp_weights = self.mtp_weights,
            ws_short = self.ws_short,
            ws_long = self.ws_long
        )
    
    def _is_active_step(self, opt, step: int):
        return (opt.odd_step_only and step%2==1) or not opt.odd_step_only

    def get_transition_steps(self):
        transition_steps = []
        ws_short, ws_long = get_ws(0)
        for step in range(1, args.num_iterations):
            ws_short, new_ws_long = get_ws(step)
            if new_ws_long != ws_long:
                transition_steps.append(step)
                ws_long = new_ws_long
        return transition_steps

    def advance_schedule(self, step: int):
        self.ws_short, new_ws_long = get_ws(step)
        if new_ws_long != self.ws_long:
            self.model.yarn.apply(self.ws_long, new_ws_long)
            self.model.yarn_paired_head.apply(self.ws_long, new_ws_long)

        new_batch_size = get_bs(step)
        if new_batch_size != self.batch_size:
            self.train_loader_send_args = (new_batch_size, args.train_max_seq_len, grad_accum_steps)
        else:
            self.train_loader_send_args = None

        self.ws_long = new_ws_long
        self.mtp_weights = self.mtp_weights_schedule[step]
    
    def step_optimizers(self, step: int):                
        step_lr = get_lr(step)
        muon_momentum = get_muon_momentum(step)
        for group in self.muon_opt.param_groups:
            group["momentum"] = muon_momentum

        for opt in self.optimizers:
            for group in opt.param_groups:
                group["lr"] = group["initial_lr"] * step_lr
                
        if self._is_active_step(self.adam_opt, step):
            # adam will interleave calls to muon step
            self.adam_opt.step(self.muon_opt)
            self.model.zero_grad(set_to_none=True)
            self.adam_opt.should_sync = False
        else:
            self.muon_opt.step()
            self.muon_opt.zero_grad(set_to_none=True)
            
        if step == self.split_step:
            self.adam_opt.copy_lm_to_embed()
            self.model.split_embed = True
    
    def activate_hooks(self, step: int):
        for opt in self.optimizers:
            if self._is_active_step(opt, step):
                opt.should_sync = True

    def reset(self, state=None):
        if state is not None:
            for opt, opt_state in zip(self.optimizers, state):
                opt.should_sync = False
                opt.load_state_dict(opt_state)

        # muon momentum buffers not in state dict
        self.muon_opt.reset()
        self.model.split_embed = False

        self.ws_short, self.ws_long = get_ws(0)
        self.batch_size = get_bs(0)
        self.model.yarn.reset()
        self.model.yarn_paired_head.reset()

    def get_state(self):
        return [copy.deepcopy(opt.state_dict()) for opt in self.optimizers]

# -----------------------------------------------------------------------------
# int main

@dataclass
class Hyperparameters:
    # data
    train_files: str = "data/fineweb10B/fineweb_train_*.bin" # input .bin to train on
    val_files: str = "data/fineweb10B/fineweb_val_*.bin" # input .bin to eval validation loss on
    val_tokens: int = 10485760 # how many tokens of validation data? it's important to keep this fixed for consistent comparisons
    # batch sizes
    train_bs_schedule: tuple = (8 * 2048 * 8, 16 * 2048 * 8, 24 * 2048 * 8)
    train_bs_extension: int = 24 * 2048 * 8
    train_max_seq_len: int = 128 * 16
    val_batch_size: int = 4 * 64 * 1024 * 1 # scales with world size
    # optimization
    num_scheduled_iterations: int = 1735  # number of steps to complete lr and ws schedule
    num_extension_iterations: int = 40  # number of steps to continue training at final lr and ws
    num_iterations: int = num_scheduled_iterations + num_extension_iterations
    cooldown_frac: float = 0.50  # fraction of num_scheduled_iterations spent cooling down the learning rate
    split_embed_frac: float = 2/3  # fraction of training when embeddings split from lm_head
    # evaluation and logging
    run_id: str = f"{uuid.uuid4()}"
    val_loss_every: int = 250  # every how many steps to evaluate val loss? 0 for only at the end
    save_checkpoint: bool = False
    # attention masking
    block_size: int = 128
    ws_schedule: tuple = (3, 7, 11)
    ws_final: int = 13 # increase final validation ws, used for YaRN extension and short window size @classiclarryd
    ws_validate_post_yarn_ext: int = 20 # extend long windows out even further after applying YaRN

args = Hyperparameters()

data_path = os.environ.get("DATA_PATH", ".")
args.train_files = os.path.join(data_path, args.train_files)
args.val_files = os.path.join(data_path, args.val_files)

# torchrun sets these env variables
rank = int(os.environ["RANK"])
world_size = int(os.environ["WORLD_SIZE"])
assert 8 % world_size == 0, "world_size must be a divisor of 8"
grad_accum_steps = 8 // world_size
assert torch.cuda.is_available()
device = torch.device("cuda", int(os.environ["LOCAL_RANK"]))
torch.cuda.set_device(device)
dist.init_process_group(backend="nccl", device_id=device)
dist.barrier()
master_process = (rank == 0) # this process will do logging, checkpointing etc.

# begin logging
logfile = None
if master_process:
    run_id = args.run_id
    os.makedirs("logs", exist_ok=True)
    logfile = f"logs/{run_id}.txt"
    print(logfile)
def print0(s, console=False):
    if master_process:
        with open(logfile, "a") as f:
            if console:
                print(s)
            print(s, file=f)

# begin by printing this file (the Python code)
print0(code)
print0("="*100)
# log information about the hardware/software environment this is running on
print0(f"Running Python {sys.version}")
print0(f"Running PyTorch {torch.version.__version__} compiled for CUDA {torch.version.cuda}")
print0(f"Running Triton version {triton.__version__}")

def nvidia_smi():
    import subprocess  # avoid top level import
    return subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True).stdout
print0(nvidia_smi())
print0("="*100)

model: nn.Module = GPT(
    vocab_size=50257,
    num_layers=11,
    num_heads=6,
    head_dim=128,
    model_dim=768,
    max_seq_len=args.val_batch_size // (grad_accum_steps * world_size),
    use_malbo=os.environ.get('use_malbo', 'True') == 'True',
).cuda()
for m in model.modules():
    if isinstance(m, (nn.Embedding, nn.Linear)):
        m.weight.data = m.weight.data.bfloat16()
model.attn_gate_bank.data = model.attn_gate_bank.data.bfloat16()
model.ve_gate_bank.data = model.ve_gate_bank.data.bfloat16()
for param in model.parameters():
    dist.broadcast(param.detach(), 0)

model: nn.Module = torch.compile(model, dynamic=False, fullgraph=True)
training_manager = TrainingManager(model)

########################################
#            Warmup kernels            #
########################################
print0("Compiling model and warming up kernels (~7 minutes on first execution)", console=True)
# Warmup the training kernels, then re-initialize the state so we aren't cheating
initial_state = dict(model=copy.deepcopy(model.state_dict()),
                     optimizers=training_manager.get_state()) # save the initial state
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)
val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)

transition_steps = training_manager.get_transition_steps()
# first few steps plus transitions
warmup_steps = sorted({0, 1, 2} | set(s + offset for s in transition_steps for offset in [-1, 0, 1] if s + offset >= 0)) 
print0(f"Sampling steps {warmup_steps} for warmup", console=True)
for step in warmup_steps:
    training_manager.advance_schedule(step)
    model.eval()
    with torch.no_grad():
        inputs, targets, cum_seqlens = next(val_loader)
        model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
    model.train()
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        (model(inputs, targets, cum_seqlens, training_manager.get_forward_args())[1] / grad_accum_steps).backward()
    training_manager.step_optimizers(step)
    break
print0("Resetting Model", console=True)
model.zero_grad(set_to_none=True)
model.load_state_dict(initial_state["model"])
training_manager.reset(initial_state["optimizers"])
del val_loader, train_loader, initial_state
model.train()

########################################
#        Training and validation       #
########################################
train_loader = distributed_data_generator(args.train_files, args.train_bs_schedule[0], args.train_max_seq_len, grad_accum_steps=grad_accum_steps)

gc.collect()

training_time_ms = 0
# start the clock
torch.cuda.synchronize()
t0 = time.perf_counter()
# begin training
train_steps = args.num_iterations
for step in range(train_steps + 1):
    last_step = (step == train_steps)
    training_manager.advance_schedule(step)
    # --------------- VALIDATION SECTION -----------------
    if last_step or (args.val_loss_every > 0 and step % args.val_loss_every == 0):
        if last_step:
            training_manager.apply_final_ws_ext()
        # stop the clock
        torch.cuda.synchronize()
        training_time_ms += 1000 * (time.perf_counter() - t0)
        model.eval()
        assert args.val_tokens % args.val_batch_size == 0
        val_steps = grad_accum_steps * args.val_tokens // args.val_batch_size
        val_loader = distributed_data_generator(args.val_files, args.val_batch_size, -1, grad_accum_steps=grad_accum_steps, align_to_bos=False)
        val_loss, val_malbo_loss = 0, 0
        with torch.no_grad():
            for _ in range(val_steps):
                inputs, targets, cum_seqlens = next(val_loader)
                this_val_loss, this_val_malbo_loss = model(inputs, targets, cum_seqlens, training_manager.get_forward_args())
                val_loss += this_val_loss
                val_malbo_loss += this_val_malbo_loss
        val_loss /= val_steps
        val_malbo_loss /= val_steps
        del val_loader
        dist.reduce(val_loss, 0, op=dist.ReduceOp.AVG)
        dist.reduce(val_malbo_loss, 0, op=dist.ReduceOp.AVG)
        n_predict = training_manager.mtp_weights_schedule[step].size(0)
        lr = get_lr(step)
        bs = get_bs(step)
        print0(f"step:{step}/{train_steps} {lr=:.4f} {bs=} {n_predict=} val_loss:{val_loss:.4f} val_malbo_loss:{val_malbo_loss:.4f} train_time:{training_time_ms:.0f}ms step_avg:{training_time_ms/max(step, 1):.2f}ms", console=True)
        model.train()
        # start the clock again
        torch.cuda.synchronize()
        t0 = time.perf_counter()

    if last_step:
        if master_process and args.save_checkpoint:
            log = dict(step=step, code=code, model=model.state_dict(), optimizers=[opt.state_dict() for opt in optimizers])
            os.makedirs(f"logs/{run_id}", exist_ok=True)
            torch.save(log, f"logs/{run_id}/state_step{step:06d}.pt")
        # the last step only has the validation loop, so break to avoid training
        break

    # --------------- TRAINING SECTION -----------------
    for idx in range(grad_accum_steps):
        # enable gradient sync for the DistAdam optimizers on the last iteration before we step them
        if idx == grad_accum_steps - 1:
            training_manager.activate_hooks(step)
        send_args = training_manager.train_loader_send_args
        inputs, targets, cum_seqlens = train_loader.send(send_args)
        loss = model(inputs, targets, cum_seqlens, training_manager.get_forward_args())[1] / grad_accum_steps
        loss.backward()
    training_manager.step_optimizers(step)

    # logging
    approx_training_time_ms = training_time_ms + 1000 * (time.perf_counter() - t0)
    n_predict = training_manager.mtp_weights_schedule[step].size(0)
    lr = get_lr(step)
    bs = get_bs(step)
    print0(f"step:{step+1}/{train_steps} {loss.item()=} {n_predict=} {lr=:.4f} {bs=} train_time:{approx_training_time_ms:.0f}ms step_avg:{approx_training_time_ms/(step + 1):.2f}ms", console=True)

print0(f"peak memory allocated: {torch.cuda.max_memory_allocated() // 1024 // 1024} MiB "
       f"reserved: {torch.cuda.max_memory_reserved() // 1024 // 1024} MiB", console=True)
dist.destroy_process_group()

====================================================================================================
Running Python 3.10.19 (main, Oct 21 2025, 16:43:05) [GCC 11.2.0]
Running PyTorch 2.10.0.dev20251210+cu126 compiled for CUDA 12.6
Running Triton version 3.6.0
Fri Jan 23 02:21:18 2026       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 570.133.20             Driver Version: 570.133.20     CUDA Version: 12.8     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA H100 PCIe               On  |   00000000:A1:00.0 Off |                    0 |
| N/A   24C    P0             74W /  310W |    1103MiB /  81559MiB |      7%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A            1669      C   .../envs/speedrun/bin/python3.10       1094MiB |
+-----------------------------------------------------------------------------------------+

====================================================================================================
Compiling model and warming up kernels (~7 minutes on first execution)
Sampling steps [0, 1, 2, 578, 579, 580, 1156, 1157, 1158, 1734, 1735, 1736] for warmup
Resetting Model
step:0/1775 lr=1.0000 bs=131072 n_predict=3 val_loss:10.8303 val_malbo_loss:10.8303 train_time:0ms step_avg:0.05ms
step:1/1775 loss.item()=38782.671875 n_predict=3 lr=1.0000 bs=131072 train_time:386ms step_avg:385.98ms
step:2/1775 loss.item()=38550.2734375 n_predict=3 lr=1.0000 bs=131072 train_time:4078ms step_avg:2038.94ms
step:3/1775 loss.item()=32732.25390625 n_predict=3 lr=1.0000 bs=131072 train_time:4387ms step_avg:1462.25ms
step:4/1775 loss.item()=31412.775390625 n_predict=3 lr=1.0000 bs=131072 train_time:4769ms step_avg:1192.18ms
step:5/1775 loss.item()=30377.8125 n_predict=3 lr=1.0000 bs=131072 train_time:5162ms step_avg:1032.43ms
step:6/1775 loss.item()=30180.787109375 n_predict=3 lr=1.0000 bs=131072 train_time:5545ms step_avg:924.14ms
step:7/1775 loss.item()=30090.296875 n_predict=3 lr=1.0000 bs=131072 train_time:5931ms step_avg:847.26ms
step:8/1775 loss.item()=27638.392578125 n_predict=3 lr=1.0000 bs=131072 train_time:6320ms step_avg:790.00ms
step:9/1775 loss.item()=28203.86328125 n_predict=3 lr=1.0000 bs=131072 train_time:6704ms step_avg:744.91ms
step:10/1775 loss.item()=26399.521484375 n_predict=3 lr=1.0000 bs=131072 train_time:7096ms step_avg:709.55ms
step:11/1775 loss.item()=27209.47265625 n_predict=3 lr=1.0000 bs=131072 train_time:7484ms step_avg:680.39ms
step:12/1775 loss.item()=26773.68359375 n_predict=3 lr=1.0000 bs=131072 train_time:7871ms step_avg:655.91ms
step:13/1775 loss.item()=26616.28125 n_predict=3 lr=1.0000 bs=131072 train_time:8262ms step_avg:635.54ms
step:14/1775 loss.item()=26080.001953125 n_predict=3 lr=1.0000 bs=131072 train_time:8651ms step_avg:617.91ms
step:15/1775 loss.item()=26038.80859375 n_predict=3 lr=1.0000 bs=131072 train_time:9040ms step_avg:602.67ms
step:16/1775 loss.item()=25323.748046875 n_predict=3 lr=1.0000 bs=131072 train_time:9431ms step_avg:589.44ms
step:17/1775 loss.item()=25048.69921875 n_predict=3 lr=1.0000 bs=131072 train_time:9818ms step_avg:577.55ms
step:18/1775 loss.item()=26223.216796875 n_predict=3 lr=1.0000 bs=131072 train_time:10208ms step_avg:567.14ms
step:19/1775 loss.item()=26075.595703125 n_predict=3 lr=1.0000 bs=131072 train_time:10598ms step_avg:557.81ms
step:20/1775 loss.item()=26223.9765625 n_predict=3 lr=1.0000 bs=131072 train_time:10987ms step_avg:549.34ms
step:21/1775 loss.item()=25511.44921875 n_predict=3 lr=1.0000 bs=131072 train_time:11378ms step_avg:541.81ms
step:22/1775 loss.item()=25269.53515625 n_predict=3 lr=1.0000 bs=131072 train_time:11766ms step_avg:534.81ms
step:23/1775 loss.item()=24759.08203125 n_predict=3 lr=1.0000 bs=131072 train_time:12155ms step_avg:528.50ms
step:24/1775 loss.item()=24916.3203125 n_predict=3 lr=1.0000 bs=131072 train_time:12546ms step_avg:522.76ms
step:25/1775 loss.item()=25619.91796875 n_predict=3 lr=1.0000 bs=131072 train_time:12935ms step_avg:517.41ms
step:26/1775 loss.item()=25005.390625 n_predict=3 lr=1.0000 bs=131072 train_time:13326ms step_avg:512.55ms
step:27/1775 loss.item()=24739.5 n_predict=3 lr=1.0000 bs=131072 train_time:13717ms step_avg:508.03ms
step:28/1775 loss.item()=24699.84765625 n_predict=3 lr=1.0000 bs=131072 train_time:14105ms step_avg:503.76ms
step:29/1775 loss.item()=25525.03515625 n_predict=3 lr=1.0000 bs=131072 train_time:14495ms step_avg:499.84ms
step:30/1775 loss.item()=24519.91796875 n_predict=3 lr=1.0000 bs=131072 train_time:14886ms step_avg:496.19ms
step:31/1775 loss.item()=23929.123046875 n_predict=3 lr=1.0000 bs=131072 train_time:15275ms step_avg:492.74ms
step:32/1775 loss.item()=23957.115234375 n_predict=3 lr=1.0000 bs=131072 train_time:15666ms step_avg:489.55ms
step:33/1775 loss.item()=24663.16015625 n_predict=3 lr=1.0000 bs=131072 train_time:16055ms step_avg:486.51ms
step:34/1775 loss.item()=23964.322265625 n_predict=3 lr=1.0000 bs=131072 train_time:16445ms step_avg:483.67ms
step:35/1775 loss.item()=24381.005859375 n_predict=3 lr=1.0000 bs=131072 train_time:16833ms step_avg:480.94ms
step:36/1775 loss.item()=24219.88671875 n_predict=3 lr=1.0000 bs=131072 train_time:17220ms step_avg:478.32ms
step:37/1775 loss.item()=23955.265625 n_predict=3 lr=1.0000 bs=131072 train_time:17609ms step_avg:475.93ms
step:38/1775 loss.item()=23372.6875 n_predict=3 lr=1.0000 bs=131072 train_time:17999ms step_avg:473.67ms
step:39/1775 loss.item()=24035.55078125 n_predict=3 lr=1.0000 bs=131072 train_time:18390ms step_avg:471.53ms
step:40/1775 loss.item()=23512.23046875 n_predict=3 lr=1.0000 bs=131072 train_time:18778ms step_avg:469.45ms
step:41/1775 loss.item()=23672.54296875 n_predict=3 lr=1.0000 bs=131072 train_time:19166ms step_avg:467.46ms
step:42/1775 loss.item()=23736.703125 n_predict=3 lr=1.0000 bs=131072 train_time:19556ms step_avg:465.62ms
step:43/1775 loss.item()=23754.603515625 n_predict=3 lr=1.0000 bs=131072 train_time:19945ms step_avg:463.84ms
step:44/1775 loss.item()=23257.02734375 n_predict=3 lr=1.0000 bs=131072 train_time:20335ms step_avg:462.17ms
step:45/1775 loss.item()=22688.095703125 n_predict=3 lr=1.0000 bs=131072 train_time:20724ms step_avg:460.53ms
step:46/1775 loss.item()=23123.056640625 n_predict=3 lr=1.0000 bs=131072 train_time:21114ms step_avg:458.99ms
step:47/1775 loss.item()=23025.884765625 n_predict=3 lr=1.0000 bs=131072 train_time:21502ms step_avg:457.50ms
step:48/1775 loss.item()=22468.869140625 n_predict=3 lr=1.0000 bs=131072 train_time:21891ms step_avg:456.07ms
step:49/1775 loss.item()=24014.19921875 n_predict=3 lr=1.0000 bs=131072 train_time:22280ms step_avg:454.69ms
step:50/1775 loss.item()=22669.56640625 n_predict=3 lr=1.0000 bs=131072 train_time:22668ms step_avg:453.36ms
step:51/1775 loss.item()=22423.9921875 n_predict=3 lr=1.0000 bs=131072 train_time:23059ms step_avg:452.14ms
step:52/1775 loss.item()=22393.970703125 n_predict=3 lr=1.0000 bs=131072 train_time:23447ms step_avg:450.90ms
step:53/1775 loss.item()=22579.53515625 n_predict=3 lr=1.0000 bs=131072 train_time:23835ms step_avg:449.71ms
step:54/1775 loss.item()=23001.57421875 n_predict=3 lr=1.0000 bs=131072 train_time:24225ms step_avg:448.62ms
step:55/1775 loss.item()=22367.77734375 n_predict=3 lr=1.0000 bs=131072 train_time:24615ms step_avg:447.54ms
step:56/1775 loss.item()=22649.142578125 n_predict=3 lr=1.0000 bs=131072 train_time:25005ms step_avg:446.53ms
step:57/1775 loss.item()=22794.73828125 n_predict=3 lr=1.0000 bs=131072 train_time:25397ms step_avg:445.56ms
step:58/1775 loss.item()=22321.9296875 n_predict=3 lr=1.0000 bs=131072 train_time:25785ms step_avg:444.57ms
step:59/1775 loss.item()=22808.259765625 n_predict=3 lr=1.0000 bs=131072 train_time:26174ms step_avg:443.62ms
step:60/1775 loss.item()=23093.517578125 n_predict=3 lr=1.0000 bs=131072 train_time:26564ms step_avg:442.73ms
step:61/1775 loss.item()=21169.25390625 n_predict=3 lr=1.0000 bs=131072 train_time:26953ms step_avg:441.85ms
step:62/1775 loss.item()=21976.158203125 n_predict=3 lr=1.0000 bs=131072 train_time:27348ms step_avg:441.10ms
step:63/1775 loss.item()=22394.01953125 n_predict=3 lr=1.0000 bs=131072 train_time:27731ms step_avg:440.18ms
step:64/1775 loss.item()=22671.369140625 n_predict=3 lr=1.0000 bs=131072 train_time:28119ms step_avg:439.36ms
step:65/1775 loss.item()=22596.71875 n_predict=3 lr=1.0000 bs=131072 train_time:28508ms step_avg:438.59ms
step:66/1775 loss.item()=22389.74609375 n_predict=3 lr=1.0000 bs=131072 train_time:28897ms step_avg:437.84ms
step:67/1775 loss.item()=22437.162109375 n_predict=3 lr=1.0000 bs=131072 train_time:29286ms step_avg:437.10ms
step:68/1775 loss.item()=22980.71875 n_predict=3 lr=1.0000 bs=131072 train_time:29676ms step_avg:436.40ms
step:69/1775 loss.item()=21889.84375 n_predict=3 lr=1.0000 bs=131072 train_time:30065ms step_avg:435.72ms
step:70/1775 loss.item()=21606.5 n_predict=3 lr=1.0000 bs=131072 train_time:30456ms step_avg:435.09ms
step:71/1775 loss.item()=21914.9765625 n_predict=3 lr=1.0000 bs=131072 train_time:30845ms step_avg:434.43ms
step:72/1775 loss.item()=21367.71484375 n_predict=3 lr=1.0000 bs=131072 train_time:31236ms step_avg:433.84ms
step:73/1775 loss.item()=21390.248046875 n_predict=3 lr=1.0000 bs=131072 train_time:31624ms step_avg:433.21ms
step:74/1775 loss.item()=21377.23046875 n_predict=3 lr=1.0000 bs=131072 train_time:32015ms step_avg:432.64ms
step:75/1775 loss.item()=21716.66796875 n_predict=3 lr=1.0000 bs=131072 train_time:32401ms step_avg:432.01ms
step:76/1775 loss.item()=21034.134765625 n_predict=3 lr=1.0000 bs=131072 train_time:32790ms step_avg:431.45ms
step:77/1775 loss.item()=21209.392578125 n_predict=3 lr=1.0000 bs=131072 train_time:33181ms step_avg:430.92ms
step:78/1775 loss.item()=21756.82421875 n_predict=3 lr=1.0000 bs=131072 train_time:33571ms step_avg:430.40ms
step:79/1775 loss.item()=21245.62109375 n_predict=3 lr=1.0000 bs=131072 train_time:33959ms step_avg:429.86ms
step:80/1775 loss.item()=21518.744140625 n_predict=3 lr=1.0000 bs=131072 train_time:34349ms step_avg:429.36ms
step:81/1775 loss.item()=21889.455078125 n_predict=3 lr=1.0000 bs=131072 train_time:34741ms step_avg:428.90ms
step:82/1775 loss.item()=21901.15234375 n_predict=3 lr=1.0000 bs=131072 train_time:35129ms step_avg:428.40ms
step:83/1775 loss.item()=21064.955078125 n_predict=3 lr=1.0000 bs=131072 train_time:35521ms step_avg:427.96ms
step:84/1775 loss.item()=20710.16015625 n_predict=3 lr=1.0000 bs=131072 train_time:35909ms step_avg:427.48ms
step:85/1775 loss.item()=20899.087890625 n_predict=3 lr=1.0000 bs=131072 train_time:36298ms step_avg:427.04ms
step:86/1775 loss.item()=21383.37109375 n_predict=3 lr=1.0000 bs=131072 train_time:36689ms step_avg:426.61ms
step:87/1775 loss.item()=21120.9453125 n_predict=3 lr=1.0000 bs=131072 train_time:37078ms step_avg:426.19ms
step:88/1775 loss.item()=21181.03515625 n_predict=3 lr=1.0000 bs=131072 train_time:37467ms step_avg:425.76ms
step:89/1775 loss.item()=21082.3515625 n_predict=3 lr=1.0000 bs=131072 train_time:37857ms step_avg:425.35ms
step:90/1775 loss.item()=21071.884765625 n_predict=3 lr=1.0000 bs=131072 train_time:38246ms step_avg:424.96ms
step:91/1775 loss.item()=21076.064453125 n_predict=3 lr=1.0000 bs=131072 train_time:38636ms step_avg:424.57ms
step:92/1775 loss.item()=20699.92578125 n_predict=3 lr=1.0000 bs=131072 train_time:39025ms step_avg:424.19ms
step:93/1775 loss.item()=21288.64453125 n_predict=3 lr=1.0000 bs=131072 train_time:39416ms step_avg:423.83ms
step:94/1775 loss.item()=20962.0859375 n_predict=3 lr=1.0000 bs=131072 train_time:39805ms step_avg:423.46ms
step:95/1775 loss.item()=21331.74609375 n_predict=3 lr=1.0000 bs=131072 train_time:40195ms step_avg:423.10ms
step:96/1775 loss.item()=21051.27734375 n_predict=3 lr=1.0000 bs=131072 train_time:40584ms step_avg:422.75ms
step:97/1775 loss.item()=21180.1484375 n_predict=3 lr=1.0000 bs=131072 train_time:40974ms step_avg:422.42ms
step:98/1775 loss.item()=20019.4609375 n_predict=3 lr=1.0000 bs=131072 train_time:41363ms step_avg:422.07ms
step:99/1775 loss.item()=20524.83203125 n_predict=3 lr=1.0000 bs=131072 train_time:41753ms step_avg:421.75ms
step:100/1775 loss.item()=21324.40625 n_predict=3 lr=1.0000 bs=131072 train_time:42143ms step_avg:421.43ms
step:101/1775 loss.item()=20699.56640625 n_predict=3 lr=1.0000 bs=131072 train_time:42531ms step_avg:421.10ms
step:102/1775 loss.item()=19926.6796875 n_predict=3 lr=1.0000 bs=131072 train_time:42921ms step_avg:420.79ms
step:103/1775 loss.item()=20594.2421875 n_predict=3 lr=1.0000 bs=131072 train_time:43311ms step_avg:420.49ms
step:104/1775 loss.item()=21538.10546875 n_predict=3 lr=1.0000 bs=131072 train_time:43700ms step_avg:420.19ms
step:105/1775 loss.item()=20348.1640625 n_predict=3 lr=1.0000 bs=131072 train_time:44091ms step_avg:419.92ms
step:106/1775 loss.item()=20220.62890625 n_predict=3 lr=1.0000 bs=131072 train_time:44479ms step_avg:419.62ms
step:107/1775 loss.item()=20912.32421875 n_predict=3 lr=1.0000 bs=131072 train_time:44869ms step_avg:419.34ms
step:108/1775 loss.item()=20849.236328125 n_predict=3 lr=1.0000 bs=131072 train_time:45261ms step_avg:419.08ms
step:109/1775 loss.item()=21154.49609375 n_predict=3 lr=1.0000 bs=131072 train_time:45648ms step_avg:418.79ms
step:110/1775 loss.item()=20356.48046875 n_predict=3 lr=1.0000 bs=131072 train_time:46038ms step_avg:418.53ms
step:111/1775 loss.item()=20356.849609375 n_predict=3 lr=1.0000 bs=131072 train_time:46427ms step_avg:418.26ms
step:112/1775 loss.item()=20792.146484375 n_predict=3 lr=1.0000 bs=131072 train_time:46816ms step_avg:418.00ms
step:113/1775 loss.item()=20765.1015625 n_predict=3 lr=1.0000 bs=131072 train_time:47207ms step_avg:417.76ms
step:114/1775 loss.item()=19876.564453125 n_predict=3 lr=1.0000 bs=131072 train_time:47596ms step_avg:417.51ms
step:115/1775 loss.item()=19954.611328125 n_predict=3 lr=1.0000 bs=131072 train_time:47987ms step_avg:417.28ms
step:116/1775 loss.item()=20081.455078125 n_predict=3 lr=1.0000 bs=131072 train_time:48377ms step_avg:417.04ms
step:117/1775 loss.item()=20259.46484375 n_predict=3 lr=1.0000 bs=131072 train_time:48766ms step_avg:416.80ms
step:118/1775 loss.item()=21090.8828125 n_predict=3 lr=1.0000 bs=131072 train_time:49156ms step_avg:416.57ms
step:119/1775 loss.item()=20588.59765625 n_predict=3 lr=1.0000 bs=131072 train_time:49546ms step_avg:416.35ms
step:120/1775 loss.item()=20157.96484375 n_predict=3 lr=1.0000 bs=131072 train_time:49936ms step_avg:416.14ms
step:121/1775 loss.item()=19895.08203125 n_predict=3 lr=1.0000 bs=131072 train_time:50326ms step_avg:415.92ms
step:122/1775 loss.item()=20443.31640625 n_predict=3 lr=1.0000 bs=131072 train_time:50715ms step_avg:415.70ms
step:123/1775 loss.item()=19657.69921875 n_predict=3 lr=1.0000 bs=131072 train_time:51109ms step_avg:415.52ms
step:124/1775 loss.item()=20532.86328125 n_predict=3 lr=1.0000 bs=131072 train_time:51501ms step_avg:415.33ms
step:125/1775 loss.item()=19834.11328125 n_predict=3 lr=1.0000 bs=131072 train_time:51890ms step_avg:415.12ms
step:126/1775 loss.item()=20034.087890625 n_predict=3 lr=1.0000 bs=131072 train_time:52279ms step_avg:414.91ms
step:127/1775 loss.item()=20116.53125 n_predict=3 lr=1.0000 bs=131072 train_time:52667ms step_avg:414.70ms
step:128/1775 loss.item()=19431.09765625 n_predict=3 lr=1.0000 bs=131072 train_time:53057ms step_avg:414.51ms
step:129/1775 loss.item()=19405.4453125 n_predict=3 lr=1.0000 bs=131072 train_time:53447ms step_avg:414.32ms
step:130/1775 loss.item()=19875.3046875 n_predict=3 lr=1.0000 bs=131072 train_time:53838ms step_avg:414.14ms
step:131/1775 loss.item()=20671.69140625 n_predict=3 lr=1.0000 bs=131072 train_time:54225ms step_avg:413.93ms
step:132/1775 loss.item()=20095.853515625 n_predict=3 lr=1.0000 bs=131072 train_time:54616ms step_avg:413.75ms
step:133/1775 loss.item()=20480.29296875 n_predict=3 lr=1.0000 bs=131072 train_time:55005ms step_avg:413.57ms
step:134/1775 loss.item()=19679.0234375 n_predict=3 lr=1.0000 bs=131072 train_time:55412ms step_avg:413.52ms
step:135/1775 loss.item()=19344.654296875 n_predict=3 lr=1.0000 bs=131072 train_time:55791ms step_avg:413.26ms
step:136/1775 loss.item()=19985.8125 n_predict=3 lr=1.0000 bs=131072 train_time:56181ms step_avg:413.10ms
step:137/1775 loss.item()=19453.287109375 n_predict=3 lr=1.0000 bs=131072 train_time:56569ms step_avg:412.91ms
step:138/1775 loss.item()=19597.0546875 n_predict=3 lr=1.0000 bs=131072 train_time:56958ms step_avg:412.74ms
step:139/1775 loss.item()=19383.6640625 n_predict=3 lr=1.0000 bs=131072 train_time:57348ms step_avg:412.57ms
step:140/1775 loss.item()=20017.66015625 n_predict=3 lr=1.0000 bs=131072 train_time:57735ms step_avg:412.40ms
step:141/1775 loss.item()=19802.19921875 n_predict=3 lr=1.0000 bs=131072 train_time:58126ms step_avg:412.24ms
step:142/1775 loss.item()=19609.220703125 n_predict=3 lr=1.0000 bs=131072 train_time:58516ms step_avg:412.09ms
step:143/1775 loss.item()=19334.1875 n_predict=3 lr=1.0000 bs=131072 train_time:58904ms step_avg:411.92ms
step:144/1775 loss.item()=19680.4296875 n_predict=3 lr=1.0000 bs=131072 train_time:59295ms step_avg:411.77ms
step:145/1775 loss.item()=19366.185546875 n_predict=3 lr=1.0000 bs=131072 train_time:59684ms step_avg:411.62ms
step:146/1775 loss.item()=19530.5546875 n_predict=3 lr=1.0000 bs=131072 train_time:60074ms step_avg:411.46ms
step:147/1775 loss.item()=19224.138671875 n_predict=3 lr=1.0000 bs=131072 train_time:60463ms step_avg:411.31ms
step:148/1775 loss.item()=19879.12109375 n_predict=3 lr=1.0000 bs=131072 train_time:60852ms step_avg:411.16ms
step:149/1775 loss.item()=19657.0 n_predict=3 lr=1.0000 bs=131072 train_time:61241ms step_avg:411.01ms
step:150/1775 loss.item()=19433.5625 n_predict=3 lr=1.0000 bs=131072 train_time:61629ms step_avg:410.86ms
step:151/1775 loss.item()=19120.900390625 n_predict=3 lr=1.0000 bs=131072 train_time:62019ms step_avg:410.72ms
step:152/1775 loss.item()=19183.671875 n_predict=3 lr=1.0000 bs=131072 train_time:62409ms step_avg:410.58ms
step:153/1775 loss.item()=19085.08984375 n_predict=3 lr=1.0000 bs=131072 train_time:62798ms step_avg:410.45ms
step:154/1775 loss.item()=19517.001953125 n_predict=3 lr=1.0000 bs=131072 train_time:63187ms step_avg:410.31ms
step:155/1775 loss.item()=19433.083984375 n_predict=3 lr=1.0000 bs=131072 train_time:63576ms step_avg:410.17ms
step:156/1775 loss.item()=19657.796875 n_predict=3 lr=1.0000 bs=131072 train_time:63964ms step_avg:410.02ms
step:157/1775 loss.item()=19828.1171875 n_predict=3 lr=1.0000 bs=131072 train_time:64355ms step_avg:409.90ms
step:158/1775 loss.item()=19142.724609375 n_predict=3 lr=1.0000 bs=131072 train_time:64745ms step_avg:409.78ms
step:159/1775 loss.item()=19489.734375 n_predict=3 lr=1.0000 bs=131072 train_time:65134ms step_avg:409.65ms
step:160/1775 loss.item()=18995.9140625 n_predict=3 lr=1.0000 bs=131072 train_time:65524ms step_avg:409.53ms
step:161/1775 loss.item()=19258.732421875 n_predict=3 lr=1.0000 bs=131072 train_time:65919ms step_avg:409.43ms
step:162/1775 loss.item()=19066.203125 n_predict=3 lr=1.0000 bs=131072 train_time:66308ms step_avg:409.31ms
step:163/1775 loss.item()=19442.169921875 n_predict=3 lr=1.0000 bs=131072 train_time:66696ms step_avg:409.18ms
step:164/1775 loss.item()=19504.302734375 n_predict=3 lr=1.0000 bs=131072 train_time:67084ms step_avg:409.05ms
step:165/1775 loss.item()=19114.26171875 n_predict=3 lr=1.0000 bs=131072 train_time:67474ms step_avg:408.93ms
step:166/1775 loss.item()=19060.529296875 n_predict=3 lr=1.0000 bs=131072 train_time:67863ms step_avg:408.81ms
step:167/1775 loss.item()=19335.453125 n_predict=3 lr=1.0000 bs=131072 train_time:68252ms step_avg:408.70ms
step:168/1775 loss.item()=19663.58984375 n_predict=3 lr=1.0000 bs=131072 train_time:68642ms step_avg:408.58ms
step:169/1775 loss.item()=18902.58203125 n_predict=3 lr=1.0000 bs=131072 train_time:69029ms step_avg:408.45ms
step:170/1775 loss.item()=18915.8828125 n_predict=3 lr=1.0000 bs=131072 train_time:69419ms step_avg:408.35ms
step:171/1775 loss.item()=18501.03125 n_predict=3 lr=1.0000 bs=131072 train_time:69809ms step_avg:408.24ms
step:172/1775 loss.item()=19029.9765625 n_predict=3 lr=1.0000 bs=131072 train_time:70197ms step_avg:408.12ms
step:173/1775 loss.item()=19651.349609375 n_predict=3 lr=1.0000 bs=131072 train_time:70587ms step_avg:408.02ms
step:174/1775 loss.item()=19185.01953125 n_predict=3 lr=1.0000 bs=131072 train_time:70975ms step_avg:407.90ms
step:175/1775 loss.item()=19506.046875 n_predict=3 lr=1.0000 bs=131072 train_time:71365ms step_avg:407.80ms
step:176/1775 loss.item()=18855.271484375 n_predict=3 lr=1.0000 bs=131072 train_time:71756ms step_avg:407.70ms
step:177/1775 loss.item()=19661.697265625 n_predict=3 lr=1.0000 bs=131072 train_time:72145ms step_avg:407.60ms
step:178/1775 loss.item()=18692.00390625 n_predict=3 lr=1.0000 bs=131072 train_time:72536ms step_avg:407.51ms
step:179/1775 loss.item()=18972.8515625 n_predict=3 lr=1.0000 bs=131072 train_time:72926ms step_avg:407.41ms
step:180/1775 loss.item()=18676.5234375 n_predict=3 lr=1.0000 bs=131072 train_time:73315ms step_avg:407.31ms
step:181/1775 loss.item()=18993.412109375 n_predict=3 lr=1.0000 bs=131072 train_time:73705ms step_avg:407.21ms
step:182/1775 loss.item()=18719.876953125 n_predict=3 lr=1.0000 bs=131072 train_time:74093ms step_avg:407.11ms
step:183/1775 loss.item()=18952.25 n_predict=3 lr=1.0000 bs=131072 train_time:74481ms step_avg:407.00ms
step:184/1775 loss.item()=18374.666015625 n_predict=3 lr=1.0000 bs=131072 train_time:74871ms step_avg:406.91ms
step:185/1775 loss.item()=18464.59765625 n_predict=3 lr=1.0000 bs=131072 train_time:75259ms step_avg:406.81ms
step:186/1775 loss.item()=18889.94921875 n_predict=3 lr=1.0000 bs=131072 train_time:75649ms step_avg:406.71ms
step:187/1775 loss.item()=19292.462890625 n_predict=3 lr=1.0000 bs=131072 train_time:76039ms step_avg:406.63ms
step:188/1775 loss.item()=18579.1015625 n_predict=3 lr=1.0000 bs=131072 train_time:76431ms step_avg:406.55ms
step:189/1775 loss.item()=18594.943359375 n_predict=3 lr=1.0000 bs=131072 train_time:76819ms step_avg:406.45ms
step:190/1775 loss.item()=18615.6328125 n_predict=3 lr=1.0000 bs=131072 train_time:77208ms step_avg:406.36ms
step:191/1775 loss.item()=18689.87109375 n_predict=3 lr=1.0000 bs=131072 train_time:77597ms step_avg:406.27ms
step:192/1775 loss.item()=18990.91796875 n_predict=3 lr=1.0000 bs=131072 train_time:77988ms step_avg:406.19ms
step:193/1775 loss.item()=18480.794921875 n_predict=3 lr=1.0000 bs=131072 train_time:78377ms step_avg:406.10ms
step:194/1775 loss.item()=19610.4453125 n_predict=3 lr=1.0000 bs=131072 train_time:78765ms step_avg:406.01ms
step:195/1775 loss.item()=18656.548828125 n_predict=3 lr=1.0000 bs=131072 train_time:79154ms step_avg:405.92ms
step:196/1775 loss.item()=18435.556640625 n_predict=3 lr=1.0000 bs=131072 train_time:79544ms step_avg:405.84ms
step:197/1775 loss.item()=18036.83984375 n_predict=3 lr=1.0000 bs=131072 train_time:79933ms step_avg:405.75ms
step:198/1775 loss.item()=18788.34375 n_predict=3 lr=1.0000 bs=131072 train_time:80322ms step_avg:405.67ms
step:199/1775 loss.item()=19137.7421875 n_predict=3 lr=1.0000 bs=131072 train_time:80711ms step_avg:405.58ms
step:200/1775 loss.item()=18805.46875 n_predict=3 lr=1.0000 bs=131072 train_time:81100ms step_avg:405.50ms
step:201/1775 loss.item()=18416.72265625 n_predict=3 lr=1.0000 bs=131072 train_time:81488ms step_avg:405.41ms
step:202/1775 loss.item()=18538.29296875 n_predict=3 lr=1.0000 bs=131072 train_time:81876ms step_avg:405.33ms
step:203/1775 loss.item()=18992.322265625 n_predict=3 lr=1.0000 bs=131072 train_time:82267ms step_avg:405.26ms
step:204/1775 loss.item()=18423.43359375 n_predict=3 lr=1.0000 bs=131072 train_time:82656ms step_avg:405.17ms
step:205/1775 loss.item()=18319.1953125 n_predict=3 lr=1.0000 bs=131072 train_time:83046ms step_avg:405.10ms
step:206/1775 loss.item()=19123.59375 n_predict=3 lr=1.0000 bs=131072 train_time:83436ms step_avg:405.03ms
step:207/1775 loss.item()=18631.3046875 n_predict=3 lr=1.0000 bs=131072 train_time:83824ms step_avg:404.95ms
step:208/1775 loss.item()=17915.947265625 n_predict=3 lr=1.0000 bs=131072 train_time:84213ms step_avg:404.87ms
step:209/1775 loss.item()=19743.00390625 n_predict=3 lr=1.0000 bs=131072 train_time:84601ms step_avg:404.79ms
step:210/1775 loss.item()=18132.1640625 n_predict=3 lr=1.0000 bs=131072 train_time:84990ms step_avg:404.71ms
step:211/1775 loss.item()=18527.638671875 n_predict=3 lr=1.0000 bs=131072 train_time:85379ms step_avg:404.64ms
step:212/1775 loss.item()=18475.08203125 n_predict=3 lr=1.0000 bs=131072 train_time:85767ms step_avg:404.56ms
step:213/1775 loss.item()=18368.5 n_predict=3 lr=1.0000 bs=131072 train_time:86156ms step_avg:404.49ms
step:214/1775 loss.item()=18861.01171875 n_predict=3 lr=1.0000 bs=131072 train_time:86546ms step_avg:404.42ms
step:215/1775 loss.item()=18186.33203125 n_predict=3 lr=1.0000 bs=131072 train_time:86934ms step_avg:404.34ms
step:216/1775 loss.item()=18316.8203125 n_predict=3 lr=1.0000 bs=131072 train_time:87323ms step_avg:404.27ms
step:217/1775 loss.item()=18403.3203125 n_predict=3 lr=1.0000 bs=131072 train_time:87711ms step_avg:404.20ms
step:218/1775 loss.item()=19142.73828125 n_predict=3 lr=1.0000 bs=131072 train_time:88100ms step_avg:404.13ms
step:219/1775 loss.item()=19188.06640625 n_predict=3 lr=1.0000 bs=131072 train_time:88488ms step_avg:404.06ms
step:220/1775 loss.item()=18710.701171875 n_predict=3 lr=1.0000 bs=131072 train_time:88877ms step_avg:403.99ms
step:221/1775 loss.item()=18671.91015625 n_predict=3 lr=1.0000 bs=131072 train_time:89266ms step_avg:403.92ms
step:222/1775 loss.item()=19526.416015625 n_predict=3 lr=1.0000 bs=131072 train_time:89654ms step_avg:403.85ms
step:223/1775 loss.item()=18031.408203125 n_predict=3 lr=1.0000 bs=131072 train_time:90043ms step_avg:403.78ms
step:224/1775 loss.item()=17758.650390625 n_predict=3 lr=1.0000 bs=131072 train_time:90432ms step_avg:403.71ms
step:225/1775 loss.item()=18905.31640625 n_predict=3 lr=1.0000 bs=131072 train_time:90821ms step_avg:403.65ms
step:226/1775 loss.item()=18164.392578125 n_predict=3 lr=1.0000 bs=131072 train_time:91210ms step_avg:403.58ms
step:227/1775 loss.item()=18250.92578125 n_predict=3 lr=1.0000 bs=131072 train_time:91599ms step_avg:403.52ms
step:228/1775 loss.item()=18112.404296875 n_predict=3 lr=1.0000 bs=131072 train_time:91987ms step_avg:403.45ms
step:229/1775 loss.item()=18313.2890625 n_predict=3 lr=1.0000 bs=131072 train_time:92378ms step_avg:403.40ms
step:230/1775 loss.item()=17863.90234375 n_predict=3 lr=1.0000 bs=131072 train_time:92766ms step_avg:403.33ms
step:231/1775 loss.item()=17877.29296875 n_predict=3 lr=1.0000 bs=131072 train_time:93156ms step_avg:403.27ms
step:232/1775 loss.item()=17882.6953125 n_predict=3 lr=1.0000 bs=131072 train_time:93549ms step_avg:403.23ms
step:233/1775 loss.item()=17354.248046875 n_predict=3 lr=1.0000 bs=131072 train_time:93938ms step_avg:403.17ms
step:234/1775 loss.item()=18080.845703125 n_predict=3 lr=1.0000 bs=131072 train_time:94327ms step_avg:403.11ms
step:235/1775 loss.item()=17879.79296875 n_predict=3 lr=1.0000 bs=131072 train_time:94718ms step_avg:403.05ms
step:236/1775 loss.item()=18640.607421875 n_predict=3 lr=1.0000 bs=131072 train_time:95106ms step_avg:402.99ms
step:237/1775 loss.item()=17954.77734375 n_predict=3 lr=1.0000 bs=131072 train_time:95496ms step_avg:402.94ms
step:238/1775 loss.item()=18173.25 n_predict=3 lr=1.0000 bs=131072 train_time:95886ms step_avg:402.88ms
step:239/1775 loss.item()=18164.330078125 n_predict=3 lr=1.0000 bs=131072 train_time:96274ms step_avg:402.82ms
step:240/1775 loss.item()=18128.3984375 n_predict=3 lr=1.0000 bs=131072 train_time:96662ms step_avg:402.76ms
step:241/1775 loss.item()=18397.62109375 n_predict=3 lr=1.0000 bs=131072 train_time:97051ms step_avg:402.70ms
step:242/1775 loss.item()=18730.73046875 n_predict=3 lr=1.0000 bs=131072 train_time:97439ms step_avg:402.64ms
step:243/1775 loss.item()=17775.451171875 n_predict=3 lr=1.0000 bs=131072 train_time:97828ms step_avg:402.58ms
step:244/1775 loss.item()=17995.31640625 n_predict=3 lr=1.0000 bs=131072 train_time:98219ms step_avg:402.54ms
step:245/1775 loss.item()=18475.26171875 n_predict=3 lr=1.0000 bs=131072 train_time:98606ms step_avg:402.48ms
step:246/1775 loss.item()=18183.65625 n_predict=3 lr=1.0000 bs=131072 train_time:98998ms step_avg:402.43ms
step:247/1775 loss.item()=18184.890625 n_predict=3 lr=1.0000 bs=131072 train_time:99388ms step_avg:402.38ms
step:248/1775 loss.item()=18017.845703125 n_predict=3 lr=1.0000 bs=131072 train_time:99775ms step_avg:402.32ms
step:249/1775 loss.item()=18520.001953125 n_predict=3 lr=1.0000 bs=131072 train_time:100164ms step_avg:402.26ms
step:250/1775 loss.item()=17752.265625 n_predict=3 lr=1.0000 bs=131072 train_time:100553ms step_avg:402.21ms
step:250/1775 lr=1.0000 bs=131072 n_predict=3 val_loss:4.6090 val_malbo_loss:4.6090 train_time:100622ms step_avg:402.49ms
step:251/1775 loss.item()=17922.71875 n_predict=3 lr=1.0000 bs=131072 train_time:100951ms step_avg:402.19ms
step:252/1775 loss.item()=17788.771484375 n_predict=3 lr=1.0000 bs=131072 train_time:101336ms step_avg:402.13ms
step:253/1775 loss.item()=17488.919921875 n_predict=3 lr=1.0000 bs=131072 train_time:101727ms step_avg:402.08ms
step:254/1775 loss.item()=17602.1015625 n_predict=3 lr=1.0000 bs=131072 train_time:102116ms step_avg:402.03ms
step:255/1775 loss.item()=18213.486328125 n_predict=3 lr=1.0000 bs=131072 train_time:102503ms step_avg:401.97ms
step:256/1775 loss.item()=17754.76171875 n_predict=3 lr=1.0000 bs=131072 train_time:102893ms step_avg:401.93ms
step:257/1775 loss.item()=18232.078125 n_predict=3 lr=1.0000 bs=131072 train_time:103280ms step_avg:401.87ms
step:258/1775 loss.item()=18234.87109375 n_predict=3 lr=1.0000 bs=131072 train_time:103669ms step_avg:401.82ms
step:259/1775 loss.item()=17522.328125 n_predict=3 lr=1.0000 bs=131072 train_time:104058ms step_avg:401.77ms
step:260/1775 loss.item()=17923.26171875 n_predict=3 lr=1.0000 bs=131072 train_time:104445ms step_avg:401.71ms
step:261/1775 loss.item()=17650.390625 n_predict=3 lr=1.0000 bs=131072 train_time:104834ms step_avg:401.66ms
step:262/1775 loss.item()=17675.52734375 n_predict=3 lr=1.0000 bs=131072 train_time:105223ms step_avg:401.62ms
step:263/1775 loss.item()=17584.12109375 n_predict=3 lr=1.0000 bs=131072 train_time:105611ms step_avg:401.56ms
step:264/1775 loss.item()=18151.890625 n_predict=3 lr=1.0000 bs=131072 train_time:106001ms step_avg:401.52ms
step:265/1775 loss.item()=17600.759765625 n_predict=3 lr=1.0000 bs=131072 train_time:106389ms step_avg:401.47ms
step:266/1775 loss.item()=17901.97265625 n_predict=3 lr=1.0000 bs=131072 train_time:106780ms step_avg:401.43ms
step:267/1775 loss.item()=18073.18359375 n_predict=3 lr=1.0000 bs=131072 train_time:107169ms step_avg:401.38ms
step:268/1775 loss.item()=17983.80078125 n_predict=3 lr=1.0000 bs=131072 train_time:107557ms step_avg:401.33ms
step:269/1775 loss.item()=18023.61328125 n_predict=3 lr=1.0000 bs=131072 train_time:107947ms step_avg:401.29ms
step:270/1775 loss.item()=17568.421875 n_predict=3 lr=1.0000 bs=131072 train_time:108336ms step_avg:401.24ms
step:271/1775 loss.item()=17280.46875 n_predict=3 lr=1.0000 bs=131072 train_time:108724ms step_avg:401.20ms
step:272/1775 loss.item()=17979.484375 n_predict=3 lr=1.0000 bs=131072 train_time:109114ms step_avg:401.16ms
step:273/1775 loss.item()=17447.220703125 n_predict=3 lr=1.0000 bs=131072 train_time:109502ms step_avg:401.11ms
step:274/1775 loss.item()=18037.080078125 n_predict=3 lr=1.0000 bs=131072 train_time:109892ms step_avg:401.07ms
step:275/1775 loss.item()=16894.2890625 n_predict=3 lr=1.0000 bs=131072 train_time:110282ms step_avg:401.03ms
step:276/1775 loss.item()=17973.70703125 n_predict=3 lr=1.0000 bs=131072 train_time:110670ms step_avg:400.98ms
step:277/1775 loss.item()=18163.9609375 n_predict=3 lr=1.0000 bs=131072 train_time:111061ms step_avg:400.94ms
step:278/1775 loss.item()=17476.52734375 n_predict=3 lr=1.0000 bs=131072 train_time:111451ms step_avg:400.90ms
step:279/1775 loss.item()=17628.6953125 n_predict=3 lr=1.0000 bs=131072 train_time:111837ms step_avg:400.85ms
step:280/1775 loss.item()=17753.79296875 n_predict=3 lr=1.0000 bs=131072 train_time:112227ms step_avg:400.81ms
step:281/1775 loss.item()=18094.4765625 n_predict=3 lr=1.0000 bs=131072 train_time:112617ms step_avg:400.77ms
step:282/1775 loss.item()=17858.94140625 n_predict=3 lr=1.0000 bs=131072 train_time:113007ms step_avg:400.73ms
step:283/1775 loss.item()=18118.455078125 n_predict=3 lr=1.0000 bs=131072 train_time:113397ms step_avg:400.70ms
step:284/1775 loss.item()=17984.21875 n_predict=3 lr=1.0000 bs=131072 train_time:113786ms step_avg:400.65ms
step:285/1775 loss.item()=17235.53125 n_predict=3 lr=1.0000 bs=131072 train_time:114172ms step_avg:400.60ms
step:286/1775 loss.item()=18403.744140625 n_predict=3 lr=1.0000 bs=131072 train_time:114563ms step_avg:400.57ms
step:287/1775 loss.item()=17088.384765625 n_predict=3 lr=1.0000 bs=131072 train_time:114951ms step_avg:400.53ms
step:288/1775 loss.item()=17527.70703125 n_predict=3 lr=1.0000 bs=131072 train_time:115341ms step_avg:400.49ms
step:289/1775 loss.item()=17949.8203125 n_predict=3 lr=1.0000 bs=131072 train_time:115730ms step_avg:400.45ms
step:290/1775 loss.item()=17712.806640625 n_predict=3 lr=1.0000 bs=131072 train_time:116121ms step_avg:400.42ms
step:291/1775 loss.item()=16999.830078125 n_predict=3 lr=1.0000 bs=131072 train_time:116512ms step_avg:400.38ms
step:292/1775 loss.item()=17473.208984375 n_predict=3 lr=1.0000 bs=131072 train_time:116900ms step_avg:400.34ms
step:293/1775 loss.item()=17472.177734375 n_predict=3 lr=1.0000 bs=131072 train_time:117291ms step_avg:400.31ms
step:294/1775 loss.item()=17804.6328125 n_predict=3 lr=1.0000 bs=131072 train_time:117680ms step_avg:400.27ms
step:295/1775 loss.item()=17260.890625 n_predict=3 lr=1.0000 bs=131072 train_time:118069ms step_avg:400.24ms
step:296/1775 loss.item()=17553.83984375 n_predict=3 lr=1.0000 bs=131072 train_time:118459ms step_avg:400.20ms
step:297/1775 loss.item()=17391.12109375 n_predict=3 lr=1.0000 bs=131072 train_time:118849ms step_avg:400.17ms
step:298/1775 loss.item()=17620.96875 n_predict=3 lr=1.0000 bs=131072 train_time:119239ms step_avg:400.13ms
step:299/1775 loss.item()=17419.2578125 n_predict=3 lr=1.0000 bs=131072 train_time:119628ms step_avg:400.09ms
step:300/1775 loss.item()=17452.0390625 n_predict=3 lr=1.0000 bs=131072 train_time:120017ms step_avg:400.06ms
step:301/1775 loss.item()=18176.453125 n_predict=3 lr=1.0000 bs=131072 train_time:120406ms step_avg:400.02ms
step:302/1775 loss.item()=17629.79296875 n_predict=3 lr=1.0000 bs=131072 train_time:120794ms step_avg:399.98ms
step:303/1775 loss.item()=17067.171875 n_predict=3 lr=1.0000 bs=131072 train_time:121182ms step_avg:399.94ms
step:304/1775 loss.item()=17822.359375 n_predict=3 lr=1.0000 bs=131072 train_time:121573ms step_avg:399.91ms
step:305/1775 loss.item()=17024.322265625 n_predict=3 lr=1.0000 bs=131072 train_time:121960ms step_avg:399.87ms
step:306/1775 loss.item()=17446.904296875 n_predict=3 lr=1.0000 bs=131072 train_time:122349ms step_avg:399.83ms
step:307/1775 loss.item()=17666.619140625 n_predict=3 lr=1.0000 bs=131072 train_time:122740ms step_avg:399.80ms
step:308/1775 loss.item()=17122.185546875 n_predict=3 lr=1.0000 bs=131072 train_time:123130ms step_avg:399.77ms
step:309/1775 loss.item()=17448.9375 n_predict=3 lr=1.0000 bs=131072 train_time:123518ms step_avg:399.73ms
step:310/1775 loss.item()=17709.482421875 n_predict=3 lr=1.0000 bs=131072 train_time:123909ms step_avg:399.71ms
step:311/1775 loss.item()=16992.8828125 n_predict=3 lr=1.0000 bs=131072 train_time:124296ms step_avg:399.67ms
step:312/1775 loss.item()=17426.27734375 n_predict=3 lr=1.0000 bs=131072 train_time:124686ms step_avg:399.63ms
step:313/1775 loss.item()=17692.89453125 n_predict=3 lr=1.0000 bs=131072 train_time:125075ms step_avg:399.60ms
step:314/1775 loss.item()=17217.578125 n_predict=3 lr=1.0000 bs=131072 train_time:125462ms step_avg:399.56ms
step:315/1775 loss.item()=17760.06640625 n_predict=3 lr=1.0000 bs=131072 train_time:125852ms step_avg:399.53ms
step:316/1775 loss.item()=17495.689453125 n_predict=3 lr=1.0000 bs=131072 train_time:126241ms step_avg:399.50ms
step:317/1775 loss.item()=17523.359375 n_predict=3 lr=1.0000 bs=131072 train_time:126629ms step_avg:399.46ms
step:318/1775 loss.item()=17848.8046875 n_predict=3 lr=1.0000 bs=131072 train_time:127018ms step_avg:399.43ms
step:319/1775 loss.item()=17243.36328125 n_predict=3 lr=1.0000 bs=131072 train_time:127406ms step_avg:399.39ms
step:320/1775 loss.item()=17841.3125 n_predict=3 lr=1.0000 bs=131072 train_time:127793ms step_avg:399.35ms
step:321/1775 loss.item()=17042.625 n_predict=3 lr=1.0000 bs=131072 train_time:128182ms step_avg:399.32ms
step:322/1775 loss.item()=16933.587890625 n_predict=3 lr=1.0000 bs=131072 train_time:128571ms step_avg:399.29ms
step:323/1775 loss.item()=17731.986328125 n_predict=3 lr=1.0000 bs=131072 train_time:128959ms step_avg:399.25ms
step:324/1775 loss.item()=17579.80859375 n_predict=3 lr=1.0000 bs=131072 train_time:129347ms step_avg:399.22ms
step:325/1775 loss.item()=17265.03515625 n_predict=3 lr=1.0000 bs=131072 train_time:129737ms step_avg:399.19ms
step:326/1775 loss.item()=17090.775390625 n_predict=3 lr=1.0000 bs=131072 train_time:130125ms step_avg:399.16ms
step:327/1775 loss.item()=16726.6640625 n_predict=3 lr=1.0000 bs=131072 train_time:130512ms step_avg:399.12ms
step:328/1775 loss.item()=17288.76171875 n_predict=3 lr=1.0000 bs=131072 train_time:130901ms step_avg:399.09ms
step:329/1775 loss.item()=17098.109375 n_predict=3 lr=1.0000 bs=131072 train_time:131290ms step_avg:399.06ms
step:330/1775 loss.item()=17629.48046875 n_predict=3 lr=1.0000 bs=131072 train_time:131678ms step_avg:399.02ms
step:331/1775 loss.item()=17350.0078125 n_predict=3 lr=1.0000 bs=131072 train_time:132066ms step_avg:398.99ms
step:332/1775 loss.item()=17142.4609375 n_predict=3 lr=1.0000 bs=131072 train_time:132454ms step_avg:398.96ms
step:333/1775 loss.item()=16738.673828125 n_predict=3 lr=1.0000 bs=131072 train_time:132843ms step_avg:398.93ms
step:334/1775 loss.item()=17267.591796875 n_predict=3 lr=1.0000 bs=131072 train_time:133231ms step_avg:398.90ms
step:335/1775 loss.item()=17101.802734375 n_predict=3 lr=1.0000 bs=131072 train_time:133620ms step_avg:398.87ms
step:336/1775 loss.item()=16750.3671875 n_predict=3 lr=1.0000 bs=131072 train_time:134008ms step_avg:398.83ms
step:337/1775 loss.item()=17295.71484375 n_predict=3 lr=1.0000 bs=131072 train_time:134397ms step_avg:398.80ms
step:338/1775 loss.item()=17154.16015625 n_predict=3 lr=1.0000 bs=131072 train_time:134787ms step_avg:398.78ms
step:339/1775 loss.item()=16755.275390625 n_predict=3 lr=1.0000 bs=131072 train_time:135175ms step_avg:398.75ms
step:340/1775 loss.item()=17156.248046875 n_predict=3 lr=1.0000 bs=131072 train_time:135564ms step_avg:398.72ms
step:341/1775 loss.item()=16407.66796875 n_predict=3 lr=1.0000 bs=131072 train_time:135951ms step_avg:398.68ms
step:342/1775 loss.item()=17727.376953125 n_predict=3 lr=1.0000 bs=131072 train_time:136340ms step_avg:398.65ms
step:343/1775 loss.item()=17067.75 n_predict=3 lr=1.0000 bs=131072 train_time:136731ms step_avg:398.63ms
step:344/1775 loss.item()=17102.888671875 n_predict=3 lr=1.0000 bs=131072 train_time:137117ms step_avg:398.60ms
step:345/1775 loss.item()=17262.578125 n_predict=3 lr=1.0000 bs=131072 train_time:137508ms step_avg:398.57ms
step:346/1775 loss.item()=17148.64453125 n_predict=3 lr=1.0000 bs=131072 train_time:137896ms step_avg:398.54ms
step:347/1775 loss.item()=15995.681640625 n_predict=3 lr=1.0000 bs=131072 train_time:138284ms step_avg:398.51ms
step:348/1775 loss.item()=17190.51171875 n_predict=3 lr=1.0000 bs=131072 train_time:138673ms step_avg:398.49ms
step:349/1775 loss.item()=17122.755859375 n_predict=3 lr=1.0000 bs=131072 train_time:139060ms step_avg:398.45ms
step:350/1775 loss.item()=17082.232421875 n_predict=3 lr=1.0000 bs=131072 train_time:139449ms step_avg:398.43ms
step:351/1775 loss.item()=17195.86328125 n_predict=3 lr=1.0000 bs=131072 train_time:139838ms step_avg:398.40ms
step:352/1775 loss.item()=17080.86328125 n_predict=3 lr=1.0000 bs=131072 train_time:140227ms step_avg:398.37ms
step:353/1775 loss.item()=17034.5625 n_predict=3 lr=1.0000 bs=131072 train_time:140617ms step_avg:398.35ms
step:354/1775 loss.item()=17398.0859375 n_predict=3 lr=1.0000 bs=131072 train_time:141006ms step_avg:398.32ms
step:355/1775 loss.item()=17260.54296875 n_predict=3 lr=1.0000 bs=131072 train_time:141392ms step_avg:398.29ms
step:356/1775 loss.item()=16706.369140625 n_predict=3 lr=1.0000 bs=131072 train_time:141781ms step_avg:398.26ms
step:357/1775 loss.item()=17025.8046875 n_predict=3 lr=1.0000 bs=131072 train_time:142170ms step_avg:398.23ms
step:358/1775 loss.item()=16853.84375 n_predict=3 lr=1.0000 bs=131072 train_time:142558ms step_avg:398.21ms
step:359/1775 loss.item()=16433.365234375 n_predict=3 lr=1.0000 bs=131072 train_time:142947ms step_avg:398.18ms
step:360/1775 loss.item()=16925.701171875 n_predict=3 lr=1.0000 bs=131072 train_time:143336ms step_avg:398.16ms
step:361/1775 loss.item()=17300.7578125 n_predict=3 lr=1.0000 bs=131072 train_time:143725ms step_avg:398.13ms
step:362/1775 loss.item()=16753.240234375 n_predict=3 lr=1.0000 bs=131072 train_time:144113ms step_avg:398.10ms
step:363/1775 loss.item()=16707.9921875 n_predict=3 lr=1.0000 bs=131072 train_time:144500ms step_avg:398.07ms
step:364/1775 loss.item()=16901.171875 n_predict=3 lr=1.0000 bs=131072 train_time:144891ms step_avg:398.05ms
step:365/1775 loss.item()=17464.47265625 n_predict=3 lr=1.0000 bs=131072 train_time:145280ms step_avg:398.03ms
step:366/1775 loss.item()=16602.017578125 n_predict=3 lr=1.0000 bs=131072 train_time:145669ms step_avg:398.00ms
step:367/1775 loss.item()=16978.416015625 n_predict=3 lr=1.0000 bs=131072 train_time:146059ms step_avg:397.98ms
step:368/1775 loss.item()=16834.34765625 n_predict=3 lr=1.0000 bs=131072 train_time:146447ms step_avg:397.96ms
step:369/1775 loss.item()=16940.76171875 n_predict=3 lr=1.0000 bs=131072 train_time:146837ms step_avg:397.93ms
step:370/1775 loss.item()=17096.177734375 n_predict=3 lr=1.0000 bs=131072 train_time:147227ms step_avg:397.91ms
step:371/1775 loss.item()=16550.787109375 n_predict=3 lr=1.0000 bs=131072 train_time:147614ms step_avg:397.88ms
step:372/1775 loss.item()=17196.8984375 n_predict=3 lr=1.0000 bs=131072 train_time:148002ms step_avg:397.86ms
step:373/1775 loss.item()=17187.048828125 n_predict=3 lr=1.0000 bs=131072 train_time:148392ms step_avg:397.83ms
step:374/1775 loss.item()=16836.125 n_predict=3 lr=1.0000 bs=131072 train_time:148779ms step_avg:397.81ms
step:375/1775 loss.item()=16887.205078125 n_predict=3 lr=1.0000 bs=131072 train_time:149167ms step_avg:397.78ms
step:376/1775 loss.item()=16699.91796875 n_predict=3 lr=1.0000 bs=131072 train_time:149555ms step_avg:397.75ms
step:377/1775 loss.item()=17182.580078125 n_predict=3 lr=1.0000 bs=131072 train_time:149944ms step_avg:397.73ms
step:378/1775 loss.item()=16842.66796875 n_predict=3 lr=1.0000 bs=131072 train_time:150332ms step_avg:397.70ms
step:379/1775 loss.item()=16757.7421875 n_predict=3 lr=1.0000 bs=131072 train_time:150720ms step_avg:397.68ms
step:380/1775 loss.item()=16559.390625 n_predict=3 lr=1.0000 bs=131072 train_time:151111ms step_avg:397.66ms
step:381/1775 loss.item()=16410.25 n_predict=3 lr=1.0000 bs=131072 train_time:151500ms step_avg:397.64ms
step:382/1775 loss.item()=16644.16015625 n_predict=3 lr=1.0000 bs=131072 train_time:151889ms step_avg:397.62ms
step:383/1775 loss.item()=16735.658203125 n_predict=3 lr=1.0000 bs=131072 train_time:152278ms step_avg:397.59ms
step:384/1775 loss.item()=16908.78125 n_predict=3 lr=1.0000 bs=131072 train_time:152666ms step_avg:397.57ms
step:385/1775 loss.item()=16548.3046875 n_predict=3 lr=1.0000 bs=131072 train_time:153052ms step_avg:397.54ms
step:386/1775 loss.item()=16758.1875 n_predict=3 lr=1.0000 bs=131072 train_time:153443ms step_avg:397.52ms
step:387/1775 loss.item()=16497.642578125 n_predict=3 lr=1.0000 bs=131072 train_time:153831ms step_avg:397.50ms
step:388/1775 loss.item()=16631.6875 n_predict=3 lr=1.0000 bs=131072 train_time:154219ms step_avg:397.47ms
step:389/1775 loss.item()=17127.16015625 n_predict=3 lr=1.0000 bs=131072 train_time:154609ms step_avg:397.45ms
step:390/1775 loss.item()=17154.16015625 n_predict=3 lr=1.0000 bs=131072 train_time:154996ms step_avg:397.43ms
step:391/1775 loss.item()=16176.0625 n_predict=3 lr=1.0000 bs=131072 train_time:155387ms step_avg:397.41ms
step:392/1775 loss.item()=16089.7490234375 n_predict=3 lr=1.0000 bs=131072 train_time:155776ms step_avg:397.39ms
step:393/1775 loss.item()=16318.1494140625 n_predict=3 lr=1.0000 bs=131072 train_time:156164ms step_avg:397.36ms
step:394/1775 loss.item()=16880.2890625 n_predict=3 lr=1.0000 bs=131072 train_time:156552ms step_avg:397.34ms
step:395/1775 loss.item()=16179.630859375 n_predict=3 lr=1.0000 bs=131072 train_time:156943ms step_avg:397.32ms
step:396/1775 loss.item()=16340.1904296875 n_predict=3 lr=1.0000 bs=131072 train_time:157332ms step_avg:397.30ms
step:397/1775 loss.item()=16116.6298828125 n_predict=3 lr=1.0000 bs=131072 train_time:157718ms step_avg:397.28ms
step:398/1775 loss.item()=16671.52734375 n_predict=3 lr=1.0000 bs=131072 train_time:158107ms step_avg:397.25ms
step:399/1775 loss.item()=16441.611328125 n_predict=3 lr=1.0000 bs=131072 train_time:158497ms step_avg:397.24ms
step:400/1775 loss.item()=17044.68359375 n_predict=3 lr=1.0000 bs=131072 train_time:158886ms step_avg:397.22ms
step:401/1775 loss.item()=16718.4296875 n_predict=3 lr=1.0000 bs=131072 train_time:159275ms step_avg:397.19ms
step:402/1775 loss.item()=16991.9609375 n_predict=3 lr=1.0000 bs=131072 train_time:159663ms step_avg:397.17ms
step:403/1775 loss.item()=16516.486328125 n_predict=3 lr=1.0000 bs=131072 train_time:160052ms step_avg:397.15ms
step:404/1775 loss.item()=16279.470703125 n_predict=3 lr=1.0000 bs=131072 train_time:160442ms step_avg:397.13ms
step:405/1775 loss.item()=16649.4375 n_predict=3 lr=1.0000 bs=131072 train_time:160830ms step_avg:397.11ms
step:406/1775 loss.item()=16170.185546875 n_predict=3 lr=1.0000 bs=131072 train_time:161219ms step_avg:397.09ms
step:407/1775 loss.item()=16702.484375 n_predict=3 lr=1.0000 bs=131072 train_time:161609ms step_avg:397.07ms
step:408/1775 loss.item()=16489.568359375 n_predict=3 lr=1.0000 bs=131072 train_time:161998ms step_avg:397.05ms
step:409/1775 loss.item()=16743.171875 n_predict=3 lr=1.0000 bs=131072 train_time:162387ms step_avg:397.03ms
step:410/1775 loss.item()=16051.810546875 n_predict=3 lr=1.0000 bs=131072 train_time:162773ms step_avg:397.01ms
step:411/1775 loss.item()=16695.2421875 n_predict=3 lr=1.0000 bs=131072 train_time:163164ms step_avg:396.99ms
step:412/1775 loss.item()=17214.72265625 n_predict=3 lr=1.0000 bs=131072 train_time:163552ms step_avg:396.97ms
step:413/1775 loss.item()=16221.71484375 n_predict=3 lr=1.0000 bs=131072 train_time:163941ms step_avg:396.95ms
step:414/1775 loss.item()=16508.84375 n_predict=3 lr=1.0000 bs=131072 train_time:164331ms step_avg:396.93ms
step:415/1775 loss.item()=15928.1796875 n_predict=3 lr=1.0000 bs=131072 train_time:164718ms step_avg:396.91ms
step:416/1775 loss.item()=16268.0869140625 n_predict=3 lr=1.0000 bs=131072 train_time:165107ms step_avg:396.89ms
step:417/1775 loss.item()=16559.541015625 n_predict=3 lr=1.0000 bs=131072 train_time:165495ms step_avg:396.87ms
step:418/1775 loss.item()=16188.048828125 n_predict=3 lr=1.0000 bs=131072 train_time:165884ms step_avg:396.85ms
step:419/1775 loss.item()=15891.95703125 n_predict=3 lr=1.0000 bs=131072 train_time:166271ms step_avg:396.83ms
step:420/1775 loss.item()=16879.73828125 n_predict=3 lr=1.0000 bs=131072 train_time:166660ms step_avg:396.81ms
step:421/1775 loss.item()=16449.9140625 n_predict=3 lr=1.0000 bs=131072 train_time:167049ms step_avg:396.79ms
step:422/1775 loss.item()=15741.4296875 n_predict=3 lr=1.0000 bs=131072 train_time:167438ms step_avg:396.77ms
step:423/1775 loss.item()=16389.2265625 n_predict=3 lr=1.0000 bs=131072 train_time:167827ms step_avg:396.75ms
step:424/1775 loss.item()=15392.4296875 n_predict=3 lr=1.0000 bs=131072 train_time:168215ms step_avg:396.73ms
step:425/1775 loss.item()=15960.267578125 n_predict=3 lr=1.0000 bs=131072 train_time:168604ms step_avg:396.72ms
step:426/1775 loss.item()=16052.287109375 n_predict=3 lr=1.0000 bs=131072 train_time:168993ms step_avg:396.70ms
step:427/1775 loss.item()=15742.2822265625 n_predict=3 lr=1.0000 bs=131072 train_time:169382ms step_avg:396.68ms
step:428/1775 loss.item()=16542.564453125 n_predict=3 lr=1.0000 bs=131072 train_time:169770ms step_avg:396.66ms
step:429/1775 loss.item()=16934.7109375 n_predict=3 lr=1.0000 bs=131072 train_time:170159ms step_avg:396.64ms
step:430/1775 loss.item()=16771.0078125 n_predict=3 lr=1.0000 bs=131072 train_time:170548ms step_avg:396.62ms
step:431/1775 loss.item()=15894.6416015625 n_predict=3 lr=1.0000 bs=131072 train_time:170937ms step_avg:396.61ms
step:432/1775 loss.item()=16126.130859375 n_predict=3 lr=1.0000 bs=131072 train_time:171326ms step_avg:396.59ms
step:433/1775 loss.item()=16363.8359375 n_predict=3 lr=1.0000 bs=131072 train_time:171713ms step_avg:396.57ms
step:434/1775 loss.item()=15942.080078125 n_predict=3 lr=1.0000 bs=131072 train_time:172101ms step_avg:396.55ms
step:435/1775 loss.item()=16738.30078125 n_predict=3 lr=1.0000 bs=131072 train_time:172491ms step_avg:396.53ms
step:436/1775 loss.item()=16210.140625 n_predict=3 lr=1.0000 bs=131072 train_time:172881ms step_avg:396.52ms
step:437/1775 loss.item()=16040.248046875 n_predict=3 lr=1.0000 bs=131072 train_time:173268ms step_avg:396.49ms
step:438/1775 loss.item()=16667.689453125 n_predict=3 lr=1.0000 bs=131072 train_time:173657ms step_avg:396.48ms
step:439/1775 loss.item()=15920.763671875 n_predict=3 lr=1.0000 bs=131072 train_time:174046ms step_avg:396.46ms
step:440/1775 loss.item()=15286.8037109375 n_predict=3 lr=1.0000 bs=131072 train_time:174433ms step_avg:396.44ms
step:441/1775 loss.item()=16403.46875 n_predict=3 lr=1.0000 bs=131072 train_time:174821ms step_avg:396.42ms
step:442/1775 loss.item()=16120.318359375 n_predict=3 lr=1.0000 bs=131072 train_time:175211ms step_avg:396.40ms
step:443/1775 loss.item()=16208.0390625 n_predict=3 lr=1.0000 bs=131072 train_time:175597ms step_avg:396.38ms
step:444/1775 loss.item()=15698.556640625 n_predict=3 lr=1.0000 bs=131072 train_time:175987ms step_avg:396.37ms
step:445/1775 loss.item()=15920.65234375 n_predict=3 lr=1.0000 bs=131072 train_time:176373ms step_avg:396.34ms
step:446/1775 loss.item()=15348.0791015625 n_predict=3 lr=1.0000 bs=131072 train_time:176760ms step_avg:396.32ms
step:447/1775 loss.item()=15900.6640625 n_predict=3 lr=1.0000 bs=131072 train_time:177152ms step_avg:396.31ms
step:448/1775 loss.item()=15724.341796875 n_predict=3 lr=1.0000 bs=131072 train_time:177538ms step_avg:396.29ms
step:449/1775 loss.item()=16398.6875 n_predict=3 lr=1.0000 bs=131072 train_time:177927ms step_avg:396.27ms
step:450/1775 loss.item()=17843.82421875 n_predict=3 lr=1.0000 bs=131072 train_time:178315ms step_avg:396.26ms
step:451/1775 loss.item()=16120.0439453125 n_predict=3 lr=1.0000 bs=131072 train_time:178701ms step_avg:396.23ms
step:452/1775 loss.item()=16649.796875 n_predict=3 lr=1.0000 bs=131072 train_time:179093ms step_avg:396.22ms
step:453/1775 loss.item()=15982.5703125 n_predict=3 lr=1.0000 bs=131072 train_time:179482ms step_avg:396.21ms
step:454/1775 loss.item()=16081.9267578125 n_predict=3 lr=1.0000 bs=131072 train_time:179870ms step_avg:396.19ms
step:455/1775 loss.item()=15871.8671875 n_predict=3 lr=1.0000 bs=131072 train_time:180260ms step_avg:396.18ms
step:456/1775 loss.item()=16785.685546875 n_predict=3 lr=1.0000 bs=131072 train_time:180647ms step_avg:396.16ms
step:457/1775 loss.item()=15983.1669921875 n_predict=3 lr=1.0000 bs=131072 train_time:181037ms step_avg:396.14ms
step:458/1775 loss.item()=15579.439453125 n_predict=3 lr=1.0000 bs=131072 train_time:181424ms step_avg:396.12ms
step:459/1775 loss.item()=16467.4140625 n_predict=3 lr=1.0000 bs=131072 train_time:181811ms step_avg:396.10ms
step:460/1775 loss.item()=16267.8486328125 n_predict=3 lr=1.0000 bs=131072 train_time:182200ms step_avg:396.09ms
step:461/1775 loss.item()=16024.18359375 n_predict=3 lr=1.0000 bs=131072 train_time:182588ms step_avg:396.07ms
step:462/1775 loss.item()=16009.2734375 n_predict=3 lr=1.0000 bs=131072 train_time:182977ms step_avg:396.05ms
step:463/1775 loss.item()=16482.865234375 n_predict=3 lr=1.0000 bs=131072 train_time:183365ms step_avg:396.04ms
step:464/1775 loss.item()=15283.158203125 n_predict=3 lr=1.0000 bs=131072 train_time:183751ms step_avg:396.02ms
step:465/1775 loss.item()=15903.60546875 n_predict=3 lr=1.0000 bs=131072 train_time:184140ms step_avg:396.00ms
step:466/1775 loss.item()=16191.83203125 n_predict=3 lr=1.0000 bs=131072 train_time:184529ms step_avg:395.99ms
step:467/1775 loss.item()=16421.07421875 n_predict=3 lr=1.0000 bs=131072 train_time:184918ms step_avg:395.97ms
step:468/1775 loss.item()=15776.337890625 n_predict=3 lr=1.0000 bs=131072 train_time:185307ms step_avg:395.96ms
step:469/1775 loss.item()=15697.7734375 n_predict=3 lr=1.0000 bs=131072 train_time:185695ms step_avg:395.94ms
step:470/1775 loss.item()=15595.001953125 n_predict=3 lr=1.0000 bs=131072 train_time:186082ms step_avg:395.92ms
step:471/1775 loss.item()=15582.7998046875 n_predict=3 lr=1.0000 bs=131072 train_time:186472ms step_avg:395.91ms
step:472/1775 loss.item()=15774.3984375 n_predict=3 lr=1.0000 bs=131072 train_time:186859ms step_avg:395.89ms
step:473/1775 loss.item()=16254.541015625 n_predict=3 lr=1.0000 bs=131072 train_time:187248ms step_avg:395.87ms
step:474/1775 loss.item()=15664.537109375 n_predict=3 lr=1.0000 bs=131072 train_time:187636ms step_avg:395.86ms
step:475/1775 loss.item()=16193.837890625 n_predict=3 lr=1.0000 bs=131072 train_time:188024ms step_avg:395.84ms
step:476/1775 loss.item()=16110.87109375 n_predict=3 lr=1.0000 bs=131072 train_time:188414ms step_avg:395.83ms
step:477/1775 loss.item()=16211.818359375 n_predict=3 lr=1.0000 bs=131072 train_time:188803ms step_avg:395.81ms
step:478/1775 loss.item()=15581.2001953125 n_predict=3 lr=1.0000 bs=131072 train_time:189192ms step_avg:395.80ms
step:479/1775 loss.item()=15767.755859375 n_predict=3 lr=1.0000 bs=131072 train_time:189581ms step_avg:395.79ms
step:480/1775 loss.item()=15544.96875 n_predict=3 lr=1.0000 bs=131072 train_time:189969ms step_avg:395.77ms
step:481/1775 loss.item()=15595.876953125 n_predict=3 lr=1.0000 bs=131072 train_time:190358ms step_avg:395.75ms
step:482/1775 loss.item()=15462.0224609375 n_predict=3 lr=1.0000 bs=131072 train_time:190747ms step_avg:395.74ms
step:483/1775 loss.item()=15530.158203125 n_predict=3 lr=1.0000 bs=131072 train_time:191133ms step_avg:395.72ms
step:484/1775 loss.item()=15972.6171875 n_predict=3 lr=1.0000 bs=131072 train_time:191521ms step_avg:395.71ms
step:485/1775 loss.item()=15559.392578125 n_predict=3 lr=1.0000 bs=131072 train_time:191912ms step_avg:395.69ms
step:486/1775 loss.item()=15769.650390625 n_predict=3 lr=1.0000 bs=131072 train_time:192298ms step_avg:395.67ms
step:487/1775 loss.item()=15232.0966796875 n_predict=3 lr=1.0000 bs=131072 train_time:192687ms step_avg:395.66ms
step:488/1775 loss.item()=15056.296875 n_predict=3 lr=1.0000 bs=131072 train_time:193075ms step_avg:395.65ms
step:489/1775 loss.item()=15363.2822265625 n_predict=3 lr=1.0000 bs=131072 train_time:193462ms step_avg:395.63ms
step:490/1775 loss.item()=15771.625 n_predict=3 lr=1.0000 bs=131072 train_time:193851ms step_avg:395.61ms
step:491/1775 loss.item()=15772.4072265625 n_predict=3 lr=1.0000 bs=131072 train_time:194238ms step_avg:395.60ms
step:492/1775 loss.item()=15625.99609375 n_predict=3 lr=1.0000 bs=131072 train_time:194626ms step_avg:395.58ms
step:493/1775 loss.item()=16293.296875 n_predict=3 lr=1.0000 bs=131072 train_time:195014ms step_avg:395.57ms
step:494/1775 loss.item()=15486.861328125 n_predict=3 lr=1.0000 bs=131072 train_time:195404ms step_avg:395.55ms
step:495/1775 loss.item()=15599.251953125 n_predict=3 lr=1.0000 bs=131072 train_time:195792ms step_avg:395.54ms
step:496/1775 loss.item()=15676.0849609375 n_predict=3 lr=1.0000 bs=131072 train_time:196182ms step_avg:395.53ms
step:497/1775 loss.item()=15663.208984375 n_predict=3 lr=1.0000 bs=131072 train_time:196569ms step_avg:395.51ms
step:498/1775 loss.item()=15939.076171875 n_predict=3 lr=1.0000 bs=131072 train_time:196959ms step_avg:395.50ms
step:499/1775 loss.item()=15220.556640625 n_predict=3 lr=1.0000 bs=131072 train_time:197347ms step_avg:395.48ms
step:500/1775 loss.item()=14822.1328125 n_predict=3 lr=1.0000 bs=131072 train_time:197734ms step_avg:395.47ms
step:500/1775 lr=1.0000 bs=131072 n_predict=3 val_loss:4.2931 val_malbo_loss:4.2931 train_time:197804ms step_avg:395.61ms
step:501/1775 loss.item()=15542.666015625 n_predict=3 lr=1.0000 bs=131072 train_time:198131ms step_avg:395.47ms
step:502/1775 loss.item()=15960.39453125 n_predict=3 lr=1.0000 bs=131072 train_time:198517ms step_avg:395.45ms
step:503/1775 loss.item()=15582.7646484375 n_predict=3 lr=1.0000 bs=131072 train_time:198906ms step_avg:395.44ms
step:504/1775 loss.item()=15207.0654296875 n_predict=3 lr=1.0000 bs=131072 train_time:199294ms step_avg:395.42ms
step:505/1775 loss.item()=15791.619140625 n_predict=3 lr=1.0000 bs=131072 train_time:199682ms step_avg:395.41ms
step:506/1775 loss.item()=15608.109375 n_predict=3 lr=1.0000 bs=131072 train_time:200071ms step_avg:395.40ms
step:507/1775 loss.item()=15011.6416015625 n_predict=3 lr=1.0000 bs=131072 train_time:200460ms step_avg:395.38ms
step:508/1775 loss.item()=15767.0712890625 n_predict=3 lr=1.0000 bs=131072 train_time:200849ms step_avg:395.37ms
step:509/1775 loss.item()=15292.9921875 n_predict=3 lr=1.0000 bs=131072 train_time:201239ms step_avg:395.36ms
step:510/1775 loss.item()=16145.455078125 n_predict=3 lr=1.0000 bs=131072 train_time:201627ms step_avg:395.35ms
step:511/1775 loss.item()=15833.3125 n_predict=3 lr=1.0000 bs=131072 train_time:202016ms step_avg:395.33ms
step:512/1775 loss.item()=15256.26171875 n_predict=3 lr=1.0000 bs=131072 train_time:202403ms step_avg:395.32ms
step:513/1775 loss.item()=14756.134765625 n_predict=3 lr=1.0000 bs=131072 train_time:202792ms step_avg:395.31ms
step:514/1775 loss.item()=15471.8515625 n_predict=3 lr=1.0000 bs=131072 train_time:203181ms step_avg:395.29ms
step:515/1775 loss.item()=15209.9873046875 n_predict=3 lr=1.0000 bs=131072 train_time:203571ms step_avg:395.28ms
step:516/1775 loss.item()=16024.2021484375 n_predict=3 lr=1.0000 bs=131072 train_time:203958ms step_avg:395.27ms
step:517/1775 loss.item()=15744.1572265625 n_predict=3 lr=1.0000 bs=131072 train_time:204348ms step_avg:395.26ms
step:518/1775 loss.item()=15345.5712890625 n_predict=3 lr=1.0000 bs=131072 train_time:204736ms step_avg:395.24ms
step:519/1775 loss.item()=14911.876953125 n_predict=3 lr=1.0000 bs=131072 train_time:205124ms step_avg:395.23ms
step:520/1775 loss.item()=15456.115234375 n_predict=3 lr=1.0000 bs=131072 train_time:205514ms step_avg:395.22ms
step:521/1775 loss.item()=15010.37890625 n_predict=3 lr=1.0000 bs=131072 train_time:205901ms step_avg:395.20ms
step:522/1775 loss.item()=15591.58203125 n_predict=3 lr=1.0000 bs=131072 train_time:206290ms step_avg:395.19ms
step:523/1775 loss.item()=15746.501953125 n_predict=3 lr=1.0000 bs=131072 train_time:206679ms step_avg:395.18ms
step:524/1775 loss.item()=15657.896484375 n_predict=3 lr=1.0000 bs=131072 train_time:207068ms step_avg:395.17ms
step:525/1775 loss.item()=15766.78515625 n_predict=3 lr=1.0000 bs=131072 train_time:207456ms step_avg:395.15ms
step:526/1775 loss.item()=15666.8359375 n_predict=3 lr=1.0000 bs=131072 train_time:207843ms step_avg:395.14ms
step:527/1775 loss.item()=15159.572265625 n_predict=3 lr=1.0000 bs=131072 train_time:208233ms step_avg:395.13ms
step:528/1775 loss.item()=15477.806640625 n_predict=3 lr=1.0000 bs=131072 train_time:208621ms step_avg:395.11ms
step:529/1775 loss.item()=15387.0126953125 n_predict=3 lr=1.0000 bs=131072 train_time:209009ms step_avg:395.10ms
step:530/1775 loss.item()=15553.7080078125 n_predict=3 lr=1.0000 bs=131072 train_time:209398ms step_avg:395.09ms
step:531/1775 loss.item()=15310.986328125 n_predict=3 lr=1.0000 bs=131072 train_time:209784ms step_avg:395.07ms
step:532/1775 loss.item()=15031.90625 n_predict=3 lr=1.0000 bs=131072 train_time:210177ms step_avg:395.07ms
step:533/1775 loss.item()=15411.87109375 n_predict=3 lr=1.0000 bs=131072 train_time:210562ms step_avg:395.05ms
step:534/1775 loss.item()=15461.52734375 n_predict=3 lr=1.0000 bs=131072 train_time:210951ms step_avg:395.04ms
step:535/1775 loss.item()=15719.953125 n_predict=3 lr=1.0000 bs=131072 train_time:211339ms step_avg:395.03ms
step:536/1775 loss.item()=15278.12890625 n_predict=3 lr=1.0000 bs=131072 train_time:211727ms step_avg:395.01ms
step:537/1775 loss.item()=15296.576171875 n_predict=3 lr=1.0000 bs=131072 train_time:212115ms step_avg:395.00ms
step:538/1775 loss.item()=15008.4697265625 n_predict=3 lr=1.0000 bs=131072 train_time:212504ms step_avg:394.99ms
step:539/1775 loss.item()=15228.5498046875 n_predict=3 lr=1.0000 bs=131072 train_time:212891ms step_avg:394.97ms
step:540/1775 loss.item()=15955.908203125 n_predict=3 lr=1.0000 bs=131072 train_time:213281ms step_avg:394.97ms
step:541/1775 loss.item()=15377.6953125 n_predict=3 lr=1.0000 bs=131072 train_time:213669ms step_avg:394.95ms
step:542/1775 loss.item()=15854.921875 n_predict=3 lr=1.0000 bs=131072 train_time:214057ms step_avg:394.94ms
step:543/1775 loss.item()=15324.7529296875 n_predict=3 lr=1.0000 bs=131072 train_time:214445ms step_avg:394.93ms
step:544/1775 loss.item()=14746.85546875 n_predict=3 lr=1.0000 bs=131072 train_time:214835ms step_avg:394.92ms
step:545/1775 loss.item()=15821.5888671875 n_predict=3 lr=1.0000 bs=131072 train_time:215222ms step_avg:394.90ms
step:546/1775 loss.item()=15705.8505859375 n_predict=3 lr=1.0000 bs=131072 train_time:215610ms step_avg:394.89ms
step:547/1775 loss.item()=16124.849609375 n_predict=3 lr=1.0000 bs=131072 train_time:215997ms step_avg:394.88ms
step:548/1775 loss.item()=14742.7822265625 n_predict=3 lr=1.0000 bs=131072 train_time:216385ms step_avg:394.86ms
step:549/1775 loss.item()=14995.0 n_predict=3 lr=1.0000 bs=131072 train_time:216774ms step_avg:394.85ms
step:550/1775 loss.item()=15174.837890625 n_predict=3 lr=1.0000 bs=131072 train_time:217162ms step_avg:394.84ms
step:551/1775 loss.item()=15215.50390625 n_predict=3 lr=1.0000 bs=131072 train_time:217550ms step_avg:394.83ms
step:552/1775 loss.item()=14590.119140625 n_predict=3 lr=1.0000 bs=131072 train_time:217939ms step_avg:394.82ms
step:553/1775 loss.item()=15021.064453125 n_predict=3 lr=1.0000 bs=131072 train_time:218327ms step_avg:394.80ms
step:554/1775 loss.item()=15250.806640625 n_predict=3 lr=1.0000 bs=131072 train_time:218715ms step_avg:394.79ms
step:555/1775 loss.item()=15480.52734375 n_predict=3 lr=1.0000 bs=131072 train_time:219103ms step_avg:394.78ms
step:556/1775 loss.item()=15587.365234375 n_predict=3 lr=1.0000 bs=131072 train_time:219489ms step_avg:394.76ms
step:557/1775 loss.item()=15002.935546875 n_predict=3 lr=1.0000 bs=131072 train_time:219878ms step_avg:394.75ms
step:558/1775 loss.item()=15320.126953125 n_predict=3 lr=1.0000 bs=131072 train_time:220267ms step_avg:394.74ms
step:559/1775 loss.item()=15468.79296875 n_predict=3 lr=1.0000 bs=131072 train_time:220655ms step_avg:394.73ms
step:560/1775 loss.item()=14905.83984375 n_predict=3 lr=1.0000 bs=131072 train_time:221043ms step_avg:394.72ms
step:561/1775 loss.item()=15727.224609375 n_predict=3 lr=1.0000 bs=131072 train_time:221430ms step_avg:394.71ms
step:562/1775 loss.item()=14557.609375 n_predict=3 lr=1.0000 bs=131072 train_time:221818ms step_avg:394.69ms
step:563/1775 loss.item()=14661.4443359375 n_predict=3 lr=1.0000 bs=131072 train_time:222207ms step_avg:394.68ms
step:564/1775 loss.item()=15160.544921875 n_predict=3 lr=1.0000 bs=131072 train_time:222595ms step_avg:394.67ms
step:565/1775 loss.item()=15398.6982421875 n_predict=3 lr=1.0000 bs=131072 train_time:222983ms step_avg:394.66ms
step:566/1775 loss.item()=14825.2607421875 n_predict=3 lr=1.0000 bs=131072 train_time:223370ms step_avg:394.65ms
step:567/1775 loss.item()=14545.9267578125 n_predict=3 lr=1.0000 bs=131072 train_time:223758ms step_avg:394.63ms
step:568/1775 loss.item()=15232.77734375 n_predict=3 lr=1.0000 bs=131072 train_time:224146ms step_avg:394.62ms
step:569/1775 loss.item()=14638.6953125 n_predict=3 lr=1.0000 bs=131072 train_time:224534ms step_avg:394.61ms
step:570/1775 loss.item()=15293.265625 n_predict=3 lr=1.0000 bs=131072 train_time:224923ms step_avg:394.60ms
step:571/1775 loss.item()=15380.068359375 n_predict=3 lr=1.0000 bs=131072 train_time:225311ms step_avg:394.59ms
step:572/1775 loss.item()=14435.419921875 n_predict=3 lr=1.0000 bs=131072 train_time:225700ms step_avg:394.58ms
step:573/1775 loss.item()=15018.67578125 n_predict=3 lr=1.0000 bs=131072 train_time:226087ms step_avg:394.57ms
step:574/1775 loss.item()=14983.255859375 n_predict=3 lr=1.0000 bs=131072 train_time:226477ms step_avg:394.56ms
step:575/1775 loss.item()=14559.701171875 n_predict=3 lr=1.0000 bs=131072 train_time:226865ms step_avg:394.55ms
step:576/1775 loss.item()=14776.923828125 n_predict=3 lr=1.0000 bs=131072 train_time:227253ms step_avg:394.54ms
step:577/1775 loss.item()=15179.9609375 n_predict=3 lr=1.0000 bs=131072 train_time:227641ms step_avg:394.53ms
step:578/1775 loss.item()=14804.080078125 n_predict=3 lr=1.0000 bs=131072 train_time:228029ms step_avg:394.51ms
step:579/1775 loss.item()=14436.5947265625 n_predict=3 lr=1.0000 bs=131072 train_time:228419ms step_avg:394.51ms
step:580/1775 loss.item()=30520.337890625 n_predict=2 lr=1.5200 bs=262144 train_time:276487ms step_avg:476.70ms
step:581/1775 loss.item()=31512.87109375 n_predict=2 lr=1.5200 bs=262144 train_time:277208ms step_avg:477.12ms
step:582/1775 loss.item()=29297.423828125 n_predict=2 lr=1.5200 bs=262144 train_time:277921ms step_avg:477.53ms
step:583/1775 loss.item()=34006.265625 n_predict=2 lr=1.5200 bs=262144 train_time:278641ms step_avg:477.94ms
step:584/1775 loss.item()=31503.630859375 n_predict=2 lr=1.5200 bs=262144 train_time:279365ms step_avg:478.37ms
step:585/1775 loss.item()=32127.00390625 n_predict=2 lr=1.5200 bs=262144 train_time:280083ms step_avg:478.77ms
step:586/1775 loss.item()=30814.859375 n_predict=2 lr=1.5200 bs=262144 train_time:280799ms step_avg:479.18ms
step:587/1775 loss.item()=31256.7578125 n_predict=2 lr=1.5200 bs=262144 train_time:281518ms step_avg:479.59ms
step:588/1775 loss.item()=31021.625 n_predict=2 lr=1.5200 bs=262144 train_time:282239ms step_avg:480.00ms
step:589/1775 loss.item()=30260.7265625 n_predict=2 lr=1.5200 bs=262144 train_time:282955ms step_avg:480.40ms
step:590/1775 loss.item()=29989.369140625 n_predict=2 lr=1.5200 bs=262144 train_time:283674ms step_avg:480.80ms
step:591/1775 loss.item()=30243.71484375 n_predict=2 lr=1.5200 bs=262144 train_time:284394ms step_avg:481.21ms
step:592/1775 loss.item()=30979.94140625 n_predict=2 lr=1.5200 bs=262144 train_time:285113ms step_avg:481.61ms
step:593/1775 loss.item()=30910.69921875 n_predict=2 lr=1.5200 bs=262144 train_time:285831ms step_avg:482.01ms
step:594/1775 loss.item()=30168.943359375 n_predict=2 lr=1.5200 bs=262144 train_time:286549ms step_avg:482.41ms
step:595/1775 loss.item()=30519.267578125 n_predict=2 lr=1.5200 bs=262144 train_time:287268ms step_avg:482.80ms
step:596/1775 loss.item()=29822.56640625 n_predict=2 lr=1.5200 bs=262144 train_time:287989ms step_avg:483.20ms
step:597/1775 loss.item()=29266.29296875 n_predict=2 lr=1.5200 bs=262144 train_time:288705ms step_avg:483.59ms
step:598/1775 loss.item()=29696.388671875 n_predict=2 lr=1.5200 bs=262144 train_time:289425ms step_avg:483.99ms
step:599/1775 loss.item()=29636.482421875 n_predict=2 lr=1.5200 bs=262144 train_time:290144ms step_avg:484.38ms
step:600/1775 loss.item()=29226.10546875 n_predict=2 lr=1.5200 bs=262144 train_time:290865ms step_avg:484.77ms
step:601/1775 loss.item()=29832.115234375 n_predict=2 lr=1.5200 bs=262144 train_time:291585ms step_avg:485.17ms
step:602/1775 loss.item()=30254.5078125 n_predict=2 lr=1.5200 bs=262144 train_time:292307ms step_avg:485.56ms
step:603/1775 loss.item()=28720.8828125 n_predict=2 lr=1.5200 bs=262144 train_time:293025ms step_avg:485.94ms
step:604/1775 loss.item()=29352.296875 n_predict=2 lr=1.5200 bs=262144 train_time:293749ms step_avg:486.34ms
step:605/1775 loss.item()=30443.41796875 n_predict=2 lr=1.5200 bs=262144 train_time:294469ms step_avg:486.73ms
step:606/1775 loss.item()=29037.2421875 n_predict=2 lr=1.5200 bs=262144 train_time:295189ms step_avg:487.11ms
step:607/1775 loss.item()=30328.58203125 n_predict=2 lr=1.5200 bs=262144 train_time:295909ms step_avg:487.49ms
step:608/1775 loss.item()=29847.40625 n_predict=2 lr=1.5200 bs=262144 train_time:296630ms step_avg:487.88ms
step:609/1775 loss.item()=28607.6875 n_predict=2 lr=1.5200 bs=262144 train_time:297350ms step_avg:488.26ms
step:610/1775 loss.item()=29267.921875 n_predict=2 lr=1.5200 bs=262144 train_time:298072ms step_avg:488.64ms
step:611/1775 loss.item()=29574.857421875 n_predict=2 lr=1.5200 bs=262144 train_time:298792ms step_avg:489.02ms
step:612/1775 loss.item()=28471.15625 n_predict=2 lr=1.5200 bs=262144 train_time:299514ms step_avg:489.40ms
step:613/1775 loss.item()=29879.86328125 n_predict=2 lr=1.5200 bs=262144 train_time:300233ms step_avg:489.78ms
step:614/1775 loss.item()=29670.0625 n_predict=2 lr=1.5200 bs=262144 train_time:300955ms step_avg:490.15ms
step:615/1775 loss.item()=28372.23046875 n_predict=2 lr=1.5200 bs=262144 train_time:301675ms step_avg:490.53ms
step:616/1775 loss.item()=29340.08203125 n_predict=2 lr=1.5200 bs=262144 train_time:302396ms step_avg:490.90ms
step:617/1775 loss.item()=28699.765625 n_predict=2 lr=1.5200 bs=262144 train_time:303119ms step_avg:491.28ms
step:618/1775 loss.item()=27943.033203125 n_predict=2 lr=1.5200 bs=262144 train_time:303842ms step_avg:491.65ms
step:619/1775 loss.item()=29344.91796875 n_predict=2 lr=1.5200 bs=262144 train_time:304564ms step_avg:492.03ms
step:620/1775 loss.item()=28810.15625 n_predict=2 lr=1.5200 bs=262144 train_time:305286ms step_avg:492.40ms
step:621/1775 loss.item()=28673.03125 n_predict=2 lr=1.5200 bs=262144 train_time:306011ms step_avg:492.77ms
step:622/1775 loss.item()=28994.080078125 n_predict=2 lr=1.5200 bs=262144 train_time:306734ms step_avg:493.14ms
step:623/1775 loss.item()=28763.3125 n_predict=2 lr=1.5200 bs=262144 train_time:307456ms step_avg:493.51ms
step:624/1775 loss.item()=29306.931640625 n_predict=2 lr=1.5200 bs=262144 train_time:308181ms step_avg:493.88ms
step:625/1775 loss.item()=30344.9296875 n_predict=2 lr=1.5200 bs=262144 train_time:308902ms step_avg:494.24ms
step:626/1775 loss.item()=28520.087890625 n_predict=2 lr=1.5200 bs=262144 train_time:309625ms step_avg:494.61ms
step:627/1775 loss.item()=28160.953125 n_predict=2 lr=1.5200 bs=262144 train_time:310345ms step_avg:494.97ms
step:628/1775 loss.item()=28861.18359375 n_predict=2 lr=1.5200 bs=262144 train_time:311069ms step_avg:495.33ms
step:629/1775 loss.item()=28970.455078125 n_predict=2 lr=1.5200 bs=262144 train_time:311789ms step_avg:495.69ms
step:630/1775 loss.item()=27990.14453125 n_predict=2 lr=1.5200 bs=262144 train_time:312511ms step_avg:496.05ms
step:631/1775 loss.item()=28365.888671875 n_predict=2 lr=1.5200 bs=262144 train_time:313233ms step_avg:496.41ms
step:632/1775 loss.item()=28564.548828125 n_predict=2 lr=1.5200 bs=262144 train_time:313952ms step_avg:496.76ms
step:633/1775 loss.item()=28250.9453125 n_predict=2 lr=1.5200 bs=262144 train_time:314673ms step_avg:497.11ms
step:634/1775 loss.item()=28336.1953125 n_predict=2 lr=1.5200 bs=262144 train_time:315396ms step_avg:497.47ms
step:635/1775 loss.item()=27990.447265625 n_predict=2 lr=1.5200 bs=262144 train_time:316119ms step_avg:497.83ms
step:636/1775 loss.item()=28619.7890625 n_predict=2 lr=1.5200 bs=262144 train_time:316843ms step_avg:498.18ms
step:637/1775 loss.item()=28268.751953125 n_predict=2 lr=1.5200 bs=262144 train_time:317566ms step_avg:498.53ms
step:638/1775 loss.item()=27955.0 n_predict=2 lr=1.5200 bs=262144 train_time:318289ms step_avg:498.89ms
step:639/1775 loss.item()=27955.111328125 n_predict=2 lr=1.5200 bs=262144 train_time:319010ms step_avg:499.23ms
step:640/1775 loss.item()=28571.416015625 n_predict=2 lr=1.5200 bs=262144 train_time:319733ms step_avg:499.58ms
step:641/1775 loss.item()=28012.32421875 n_predict=2 lr=1.5200 bs=262144 train_time:320452ms step_avg:499.93ms
step:642/1775 loss.item()=28107.94140625 n_predict=2 lr=1.5200 bs=262144 train_time:321175ms step_avg:500.27ms
step:643/1775 loss.item()=27925.70703125 n_predict=2 lr=1.5200 bs=262144 train_time:321895ms step_avg:500.61ms
step:644/1775 loss.item()=28339.84375 n_predict=2 lr=1.5200 bs=262144 train_time:322619ms step_avg:500.96ms
step:645/1775 loss.item()=27550.392578125 n_predict=2 lr=1.5200 bs=262144 train_time:323340ms step_avg:501.30ms
step:646/1775 loss.item()=28716.662109375 n_predict=2 lr=1.5200 bs=262144 train_time:324064ms step_avg:501.65ms
step:647/1775 loss.item()=28129.87890625 n_predict=2 lr=1.5200 bs=262144 train_time:324785ms step_avg:501.99ms
step:648/1775 loss.item()=28143.48046875 n_predict=2 lr=1.5200 bs=262144 train_time:325509ms step_avg:502.33ms
step:649/1775 loss.item()=29005.5 n_predict=2 lr=1.5200 bs=262144 train_time:326231ms step_avg:502.67ms
step:650/1775 loss.item()=27518.3203125 n_predict=2 lr=1.5200 bs=262144 train_time:326954ms step_avg:503.01ms
step:651/1775 loss.item()=27192.984375 n_predict=2 lr=1.5200 bs=262144 train_time:327674ms step_avg:503.34ms
step:652/1775 loss.item()=27929.748046875 n_predict=2 lr=1.5200 bs=262144 train_time:328396ms step_avg:503.67ms
step:653/1775 loss.item()=28042.72265625 n_predict=2 lr=1.5200 bs=262144 train_time:329120ms step_avg:504.01ms
step:654/1775 loss.item()=27905.15234375 n_predict=2 lr=1.5200 bs=262144 train_time:329844ms step_avg:504.35ms
step:655/1775 loss.item()=27670.96484375 n_predict=2 lr=1.5200 bs=262144 train_time:330564ms step_avg:504.68ms
step:656/1775 loss.item()=27056.779296875 n_predict=2 lr=1.5200 bs=262144 train_time:331294ms step_avg:505.02ms
step:657/1775 loss.item()=28035.3203125 n_predict=2 lr=1.5200 bs=262144 train_time:332014ms step_avg:505.35ms
step:658/1775 loss.item()=28654.18359375 n_predict=2 lr=1.5200 bs=262144 train_time:332735ms step_avg:505.68ms
step:659/1775 loss.item()=27651.615234375 n_predict=2 lr=1.5200 bs=262144 train_time:333456ms step_avg:506.00ms
step:660/1775 loss.item()=26158.50390625 n_predict=2 lr=1.5200 bs=262144 train_time:334179ms step_avg:506.33ms
step:661/1775 loss.item()=27569.10546875 n_predict=2 lr=1.5200 bs=262144 train_time:334901ms step_avg:506.66ms
step:662/1775 loss.item()=27742.265625 n_predict=2 lr=1.5200 bs=262144 train_time:335625ms step_avg:506.99ms
step:663/1775 loss.item()=27786.388671875 n_predict=2 lr=1.5200 bs=262144 train_time:336347ms step_avg:507.31ms
step:664/1775 loss.item()=28078.95703125 n_predict=2 lr=1.5200 bs=262144 train_time:337069ms step_avg:507.63ms
step:665/1775 loss.item()=27379.51171875 n_predict=2 lr=1.5200 bs=262144 train_time:337791ms step_avg:507.96ms
step:666/1775 loss.item()=27820.80859375 n_predict=2 lr=1.5200 bs=262144 train_time:338515ms step_avg:508.28ms
step:667/1775 loss.item()=27544.123046875 n_predict=2 lr=1.5200 bs=262144 train_time:339237ms step_avg:508.60ms
step:668/1775 loss.item()=27003.77734375 n_predict=2 lr=1.5200 bs=262144 train_time:339961ms step_avg:508.92ms
step:669/1775 loss.item()=27769.6328125 n_predict=2 lr=1.5200 bs=262144 train_time:340685ms step_avg:509.24ms
step:670/1775 loss.item()=27639.04296875 n_predict=2 lr=1.5200 bs=262144 train_time:341408ms step_avg:509.56ms
step:671/1775 loss.item()=26921.59375 n_predict=2 lr=1.5200 bs=262144 train_time:342128ms step_avg:509.88ms
step:672/1775 loss.item()=27641.171875 n_predict=2 lr=1.5200 bs=262144 train_time:342853ms step_avg:510.20ms
step:673/1775 loss.item()=27127.208984375 n_predict=2 lr=1.5200 bs=262144 train_time:343574ms step_avg:510.51ms
step:674/1775 loss.item()=27526.576171875 n_predict=2 lr=1.5200 bs=262144 train_time:344296ms step_avg:510.83ms
step:675/1775 loss.item()=26876.17578125 n_predict=2 lr=1.5200 bs=262144 train_time:345022ms step_avg:511.14ms
step:676/1775 loss.item()=27671.92578125 n_predict=2 lr=1.5200 bs=262144 train_time:345744ms step_avg:511.46ms
step:677/1775 loss.item()=26440.755859375 n_predict=2 lr=1.5200 bs=262144 train_time:346468ms step_avg:511.77ms
step:678/1775 loss.item()=27786.90625 n_predict=2 lr=1.5200 bs=262144 train_time:347191ms step_avg:512.08ms
step:679/1775 loss.item()=26807.169921875 n_predict=2 lr=1.5200 bs=262144 train_time:347913ms step_avg:512.39ms
step:680/1775 loss.item()=26494.61328125 n_predict=2 lr=1.5200 bs=262144 train_time:348636ms step_avg:512.70ms
step:681/1775 loss.item()=27902.681640625 n_predict=2 lr=1.5200 bs=262144 train_time:349360ms step_avg:513.01ms
step:682/1775 loss.item()=26706.5703125 n_predict=2 lr=1.5200 bs=262144 train_time:350085ms step_avg:513.32ms
step:683/1775 loss.item()=26642.388671875 n_predict=2 lr=1.5200 bs=262144 train_time:350806ms step_avg:513.63ms
step:684/1775 loss.item()=26735.689453125 n_predict=2 lr=1.5200 bs=262144 train_time:351529ms step_avg:513.93ms
step:685/1775 loss.item()=27252.064453125 n_predict=2 lr=1.5200 bs=262144 train_time:352252ms step_avg:514.24ms
step:686/1775 loss.item()=27293.001953125 n_predict=2 lr=1.5200 bs=262144 train_time:352974ms step_avg:514.54ms
step:687/1775 loss.item()=27022.107421875 n_predict=2 lr=1.5200 bs=262144 train_time:353700ms step_avg:514.85ms
step:688/1775 loss.item()=27389.96484375 n_predict=2 lr=1.5200 bs=262144 train_time:354426ms step_avg:515.15ms
step:689/1775 loss.item()=27607.421875 n_predict=2 lr=1.5200 bs=262144 train_time:355148ms step_avg:515.45ms
step:690/1775 loss.item()=26757.5 n_predict=2 lr=1.5200 bs=262144 train_time:355869ms step_avg:515.75ms
step:691/1775 loss.item()=27581.1953125 n_predict=2 lr=1.5200 bs=262144 train_time:356591ms step_avg:516.05ms
step:692/1775 loss.item()=27438.4609375 n_predict=2 lr=1.5200 bs=262144 train_time:357316ms step_avg:516.35ms
step:693/1775 loss.item()=27635.9375 n_predict=2 lr=1.5200 bs=262144 train_time:358040ms step_avg:516.65ms
step:694/1775 loss.item()=26830.77734375 n_predict=2 lr=1.5200 bs=262144 train_time:358764ms step_avg:516.95ms
step:695/1775 loss.item()=26693.986328125 n_predict=2 lr=1.5200 bs=262144 train_time:359491ms step_avg:517.25ms
step:696/1775 loss.item()=26967.97265625 n_predict=2 lr=1.5200 bs=262144 train_time:360215ms step_avg:517.55ms
step:697/1775 loss.item()=27136.2265625 n_predict=2 lr=1.5200 bs=262144 train_time:360937ms step_avg:517.84ms
step:698/1775 loss.item()=25847.50390625 n_predict=2 lr=1.5200 bs=262144 train_time:361663ms step_avg:518.14ms
step:699/1775 loss.item()=27664.263671875 n_predict=2 lr=1.5200 bs=262144 train_time:362389ms step_avg:518.44ms
step:700/1775 loss.item()=26099.109375 n_predict=2 lr=1.5200 bs=262144 train_time:363112ms step_avg:518.73ms
step:701/1775 loss.item()=26154.458984375 n_predict=2 lr=1.5200 bs=262144 train_time:363833ms step_avg:519.02ms
step:702/1775 loss.item()=25965.462890625 n_predict=2 lr=1.5200 bs=262144 train_time:364555ms step_avg:519.31ms
step:703/1775 loss.item()=27689.15625 n_predict=2 lr=1.5200 bs=262144 train_time:365279ms step_avg:519.60ms
step:704/1775 loss.item()=27223.45703125 n_predict=2 lr=1.5200 bs=262144 train_time:366003ms step_avg:519.89ms
step:705/1775 loss.item()=26725.6875 n_predict=2 lr=1.5200 bs=262144 train_time:366728ms step_avg:520.18ms
step:706/1775 loss.item()=27593.8984375 n_predict=2 lr=1.5200 bs=262144 train_time:367453ms step_avg:520.47ms
step:707/1775 loss.item()=25972.00390625 n_predict=2 lr=1.5200 bs=262144 train_time:368176ms step_avg:520.76ms
step:708/1775 loss.item()=26445.578125 n_predict=2 lr=1.5200 bs=262144 train_time:368898ms step_avg:521.04ms
step:709/1775 loss.item()=26984.345703125 n_predict=2 lr=1.5200 bs=262144 train_time:369622ms step_avg:521.33ms
step:710/1775 loss.item()=26982.61328125 n_predict=2 lr=1.5200 bs=262144 train_time:370351ms step_avg:521.62ms
step:711/1775 loss.item()=25928.583984375 n_predict=2 lr=1.5200 bs=262144 train_time:371074ms step_avg:521.90ms
step:712/1775 loss.item()=26756.55859375 n_predict=2 lr=1.5200 bs=262144 train_time:371796ms step_avg:522.19ms
step:713/1775 loss.item()=26297.13671875 n_predict=2 lr=1.5200 bs=262144 train_time:372522ms step_avg:522.47ms
step:714/1775 loss.item()=27426.365234375 n_predict=2 lr=1.5200 bs=262144 train_time:373253ms step_avg:522.76ms
step:715/1775 loss.item()=26386.384765625 n_predict=2 lr=1.5200 bs=262144 train_time:373974ms step_avg:523.04ms
step:716/1775 loss.item()=25855.474609375 n_predict=2 lr=1.5200 bs=262144 train_time:374701ms step_avg:523.33ms
step:717/1775 loss.item()=26750.078125 n_predict=2 lr=1.5200 bs=262144 train_time:375423ms step_avg:523.60ms
step:718/1775 loss.item()=25769.3828125 n_predict=2 lr=1.5200 bs=262144 train_time:376151ms step_avg:523.89ms
step:719/1775 loss.item()=25687.689453125 n_predict=2 lr=1.5200 bs=262144 train_time:376874ms step_avg:524.16ms
step:720/1775 loss.item()=26461.685546875 n_predict=2 lr=1.5200 bs=262144 train_time:377601ms step_avg:524.45ms
step:721/1775 loss.item()=25882.298828125 n_predict=2 lr=1.5200 bs=262144 train_time:378324ms step_avg:524.72ms
step:722/1775 loss.item()=25818.7265625 n_predict=2 lr=1.5200 bs=262144 train_time:379049ms step_avg:525.00ms
step:723/1775 loss.item()=26846.271484375 n_predict=2 lr=1.5200 bs=262144 train_time:379771ms step_avg:525.27ms
step:724/1775 loss.item()=26384.16015625 n_predict=2 lr=1.5200 bs=262144 train_time:380496ms step_avg:525.55ms
step:725/1775 loss.item()=25848.607421875 n_predict=2 lr=1.5200 bs=262144 train_time:381220ms step_avg:525.82ms
step:726/1775 loss.item()=26444.806640625 n_predict=2 lr=1.5200 bs=262144 train_time:381945ms step_avg:526.10ms
step:727/1775 loss.item()=26109.5703125 n_predict=2 lr=1.5200 bs=262144 train_time:382669ms step_avg:526.37ms
step:728/1775 loss.item()=25592.7265625 n_predict=2 lr=1.5200 bs=262144 train_time:383396ms step_avg:526.64ms
step:729/1775 loss.item()=26058.68359375 n_predict=2 lr=1.5200 bs=262144 train_time:384115ms step_avg:526.91ms
step:730/1775 loss.item()=25807.521484375 n_predict=2 lr=1.5200 bs=262144 train_time:384841ms step_avg:527.18ms
step:731/1775 loss.item()=25258.93359375 n_predict=2 lr=1.5200 bs=262144 train_time:385565ms step_avg:527.45ms
step:732/1775 loss.item()=25613.69921875 n_predict=2 lr=1.5200 bs=262144 train_time:386291ms step_avg:527.72ms
step:733/1775 loss.item()=26072.861328125 n_predict=2 lr=1.5200 bs=262144 train_time:387013ms step_avg:527.99ms
step:734/1775 loss.item()=26517.193359375 n_predict=2 lr=1.5200 bs=262144 train_time:387737ms step_avg:528.25ms
step:735/1775 loss.item()=25648.806640625 n_predict=2 lr=1.5200 bs=262144 train_time:388462ms step_avg:528.52ms
step:736/1775 loss.item()=26038.890625 n_predict=2 lr=1.5200 bs=262144 train_time:389187ms step_avg:528.79ms
step:737/1775 loss.item()=25384.625 n_predict=2 lr=1.5200 bs=262144 train_time:389911ms step_avg:529.05ms
step:738/1775 loss.item()=25925.91015625 n_predict=2 lr=1.5200 bs=262144 train_time:390633ms step_avg:529.31ms
step:739/1775 loss.item()=26063.138671875 n_predict=2 lr=1.5200 bs=262144 train_time:391358ms step_avg:529.58ms
step:740/1775 loss.item()=25319.6875 n_predict=2 lr=1.5200 bs=262144 train_time:392084ms step_avg:529.84ms
step:741/1775 loss.item()=25550.50390625 n_predict=2 lr=1.5200 bs=262144 train_time:392809ms step_avg:530.11ms
step:742/1775 loss.item()=24378.27734375 n_predict=2 lr=1.5200 bs=262144 train_time:393533ms step_avg:530.37ms
step:743/1775 loss.item()=24803.37890625 n_predict=2 lr=1.5200 bs=262144 train_time:394255ms step_avg:530.63ms
step:744/1775 loss.item()=25677.8515625 n_predict=2 lr=1.5200 bs=262144 train_time:394980ms step_avg:530.89ms
step:745/1775 loss.item()=25364.033203125 n_predict=2 lr=1.5200 bs=262144 train_time:395703ms step_avg:531.15ms
step:746/1775 loss.item()=24794.94140625 n_predict=2 lr=1.5200 bs=262144 train_time:396431ms step_avg:531.41ms
step:747/1775 loss.item()=24967.0703125 n_predict=2 lr=1.5200 bs=262144 train_time:397152ms step_avg:531.66ms
step:748/1775 loss.item()=25078.548828125 n_predict=2 lr=1.5200 bs=262144 train_time:397877ms step_avg:531.92ms
step:749/1775 loss.item()=25999.2421875 n_predict=2 lr=1.5200 bs=262144 train_time:398601ms step_avg:532.18ms
step:750/1775 loss.item()=25223.37109375 n_predict=2 lr=1.5200 bs=262144 train_time:399328ms step_avg:532.44ms
step:750/1775 lr=1.5200 bs=262144 n_predict=2 val_loss:4.0007 val_malbo_loss:4.0007 train_time:399445ms step_avg:532.59ms
step:751/1775 loss.item()=24659.18359375 n_predict=2 lr=1.5200 bs=262144 train_time:400049ms step_avg:532.69ms
step:752/1775 loss.item()=25018.00390625 n_predict=2 lr=1.5200 bs=262144 train_time:400770ms step_avg:532.94ms
step:753/1775 loss.item()=25324.86328125 n_predict=2 lr=1.5200 bs=262144 train_time:401488ms step_avg:533.18ms
step:754/1775 loss.item()=24459.341796875 n_predict=2 lr=1.5200 bs=262144 train_time:402209ms step_avg:533.43ms
step:755/1775 loss.item()=25150.62109375 n_predict=2 lr=1.5200 bs=262144 train_time:402928ms step_avg:533.68ms
step:756/1775 loss.item()=25731.875 n_predict=2 lr=1.5200 bs=262144 train_time:403647ms step_avg:533.93ms
step:757/1775 loss.item()=24192.677734375 n_predict=2 lr=1.5200 bs=262144 train_time:404366ms step_avg:534.17ms
step:758/1775 loss.item()=24886.419921875 n_predict=2 lr=1.5200 bs=262144 train_time:405088ms step_avg:534.42ms
step:759/1775 loss.item()=25112.048828125 n_predict=2 lr=1.5200 bs=262144 train_time:405807ms step_avg:534.66ms
step:760/1775 loss.item()=25417.12109375 n_predict=2 lr=1.5200 bs=262144 train_time:406526ms step_avg:534.90ms
step:761/1775 loss.item()=25330.494140625 n_predict=2 lr=1.5200 bs=262144 train_time:407246ms step_avg:535.15ms
step:762/1775 loss.item()=25261.74609375 n_predict=2 lr=1.5200 bs=262144 train_time:407967ms step_avg:535.39ms
step:763/1775 loss.item()=24799.8515625 n_predict=2 lr=1.5200 bs=262144 train_time:408688ms step_avg:535.63ms
step:764/1775 loss.item()=24735.4296875 n_predict=2 lr=1.5200 bs=262144 train_time:409409ms step_avg:535.88ms
step:765/1775 loss.item()=25570.6796875 n_predict=2 lr=1.5200 bs=262144 train_time:410131ms step_avg:536.12ms
step:766/1775 loss.item()=24728.9921875 n_predict=2 lr=1.5200 bs=262144 train_time:410855ms step_avg:536.36ms
step:767/1775 loss.item()=24831.998046875 n_predict=2 lr=1.5200 bs=262144 train_time:411577ms step_avg:536.61ms
step:768/1775 loss.item()=24966.29296875 n_predict=2 lr=1.5200 bs=262144 train_time:412300ms step_avg:536.85ms
step:769/1775 loss.item()=24433.92578125 n_predict=2 lr=1.5200 bs=262144 train_time:413020ms step_avg:537.09ms
step:770/1775 loss.item()=24418.98046875 n_predict=2 lr=1.5200 bs=262144 train_time:413742ms step_avg:537.33ms
step:771/1775 loss.item()=24925.0234375 n_predict=2 lr=1.5200 bs=262144 train_time:414463ms step_avg:537.57ms
step:772/1775 loss.item()=24676.58203125 n_predict=2 lr=1.5200 bs=262144 train_time:415186ms step_avg:537.81ms
step:773/1775 loss.item()=24686.568359375 n_predict=2 lr=1.5200 bs=262144 train_time:415907ms step_avg:538.04ms
step:774/1775 loss.item()=25333.70703125 n_predict=2 lr=1.5200 bs=262144 train_time:416628ms step_avg:538.28ms
step:775/1775 loss.item()=23986.490234375 n_predict=2 lr=1.5200 bs=262144 train_time:417349ms step_avg:538.51ms
step:776/1775 loss.item()=25272.140625 n_predict=2 lr=1.5200 bs=262144 train_time:418072ms step_avg:538.75ms
step:777/1775 loss.item()=24241.86328125 n_predict=2 lr=1.5200 bs=262144 train_time:418793ms step_avg:538.99ms
step:778/1775 loss.item()=25290.140625 n_predict=2 lr=1.5200 bs=262144 train_time:419517ms step_avg:539.23ms
step:779/1775 loss.item()=24762.1796875 n_predict=2 lr=1.5200 bs=262144 train_time:420240ms step_avg:539.46ms
step:780/1775 loss.item()=24608.3828125 n_predict=2 lr=1.5200 bs=262144 train_time:420962ms step_avg:539.70ms
step:781/1775 loss.item()=24654.71484375 n_predict=2 lr=1.5200 bs=262144 train_time:421685ms step_avg:539.93ms
step:782/1775 loss.item()=24902.005859375 n_predict=2 lr=1.5200 bs=262144 train_time:422408ms step_avg:540.16ms
step:783/1775 loss.item()=24853.923828125 n_predict=2 lr=1.5200 bs=262144 train_time:423128ms step_avg:540.39ms
step:784/1775 loss.item()=24966.0703125 n_predict=2 lr=1.5200 bs=262144 train_time:423848ms step_avg:540.62ms
step:785/1775 loss.item()=24054.53515625 n_predict=2 lr=1.5200 bs=262144 train_time:424571ms step_avg:540.86ms
step:786/1775 loss.item()=24863.60546875 n_predict=2 lr=1.5200 bs=262144 train_time:425296ms step_avg:541.09ms
step:787/1775 loss.item()=24127.12109375 n_predict=2 lr=1.5200 bs=262144 train_time:426019ms step_avg:541.32ms
step:788/1775 loss.item()=24987.50390625 n_predict=2 lr=1.5200 bs=262144 train_time:426744ms step_avg:541.55ms
step:789/1775 loss.item()=24420.150390625 n_predict=2 lr=1.5200 bs=262144 train_time:427466ms step_avg:541.78ms
step:790/1775 loss.item()=24646.16015625 n_predict=2 lr=1.5200 bs=262144 train_time:428188ms step_avg:542.01ms
step:791/1775 loss.item()=24624.26171875 n_predict=2 lr=1.5200 bs=262144 train_time:428913ms step_avg:542.24ms
step:792/1775 loss.item()=23758.0 n_predict=2 lr=1.5200 bs=262144 train_time:429637ms step_avg:542.47ms
step:793/1775 loss.item()=23786.69921875 n_predict=2 lr=1.5200 bs=262144 train_time:430362ms step_avg:542.70ms
step:794/1775 loss.item()=24548.671875 n_predict=2 lr=1.5200 bs=262144 train_time:431084ms step_avg:542.93ms
step:795/1775 loss.item()=23769.099609375 n_predict=2 lr=1.5200 bs=262144 train_time:431805ms step_avg:543.15ms
step:796/1775 loss.item()=24088.6875 n_predict=2 lr=1.5200 bs=262144 train_time:432527ms step_avg:543.38ms
step:797/1775 loss.item()=24424.390625 n_predict=2 lr=1.5200 bs=262144 train_time:433248ms step_avg:543.60ms
step:798/1775 loss.item()=23378.80078125 n_predict=2 lr=1.5200 bs=262144 train_time:433968ms step_avg:543.82ms
step:799/1775 loss.item()=23843.30859375 n_predict=2 lr=1.5200 bs=262144 train_time:434690ms step_avg:544.04ms
step:800/1775 loss.item()=25112.48046875 n_predict=2 lr=1.5200 bs=262144 train_time:435412ms step_avg:544.27ms
step:801/1775 loss.item()=24524.021484375 n_predict=2 lr=1.5200 bs=262144 train_time:436135ms step_avg:544.49ms
step:802/1775 loss.item()=24594.45703125 n_predict=2 lr=1.5200 bs=262144 train_time:436862ms step_avg:544.72ms
step:803/1775 loss.item()=24413.8984375 n_predict=2 lr=1.5200 bs=262144 train_time:437582ms step_avg:544.93ms
step:804/1775 loss.item()=23478.755859375 n_predict=2 lr=1.5200 bs=262144 train_time:438307ms step_avg:545.16ms
step:805/1775 loss.item()=23972.4765625 n_predict=2 lr=1.5200 bs=262144 train_time:439027ms step_avg:545.37ms
step:806/1775 loss.item()=24167.111328125 n_predict=2 lr=1.5200 bs=262144 train_time:439752ms step_avg:545.60ms
step:807/1775 loss.item()=23185.4296875 n_predict=2 lr=1.5200 bs=262144 train_time:440475ms step_avg:545.82ms
step:808/1775 loss.item()=23819.77734375 n_predict=2 lr=1.5200 bs=262144 train_time:441200ms step_avg:546.04ms
step:809/1775 loss.item()=23608.591796875 n_predict=2 lr=1.5200 bs=262144 train_time:441919ms step_avg:546.25ms
step:810/1775 loss.item()=23531.5703125 n_predict=2 lr=1.5200 bs=262144 train_time:442643ms step_avg:546.47ms
step:811/1775 loss.item()=23704.388671875 n_predict=2 lr=1.5200 bs=262144 train_time:443366ms step_avg:546.69ms
step:812/1775 loss.item()=23746.99609375 n_predict=2 lr=1.5200 bs=262144 train_time:444087ms step_avg:546.91ms
step:813/1775 loss.item()=23494.6640625 n_predict=2 lr=1.5200 bs=262144 train_time:444808ms step_avg:547.12ms
step:814/1775 loss.item()=23468.77734375 n_predict=2 lr=1.5200 bs=262144 train_time:445530ms step_avg:547.33ms
step:815/1775 loss.item()=23390.68359375 n_predict=2 lr=1.5200 bs=262144 train_time:446251ms step_avg:547.55ms
step:816/1775 loss.item()=23667.15234375 n_predict=2 lr=1.5200 bs=262144 train_time:446978ms step_avg:547.77ms
step:817/1775 loss.item()=23452.90234375 n_predict=2 lr=1.5200 bs=262144 train_time:447701ms step_avg:547.98ms
step:818/1775 loss.item()=23623.71484375 n_predict=2 lr=1.5200 bs=262144 train_time:448422ms step_avg:548.19ms
step:819/1775 loss.item()=23335.87109375 n_predict=2 lr=1.5200 bs=262144 train_time:449143ms step_avg:548.40ms
step:820/1775 loss.item()=24167.2734375 n_predict=2 lr=1.5200 bs=262144 train_time:449867ms step_avg:548.62ms
step:821/1775 loss.item()=22859.20703125 n_predict=2 lr=1.5200 bs=262144 train_time:450589ms step_avg:548.83ms
step:822/1775 loss.item()=22807.66015625 n_predict=2 lr=1.5200 bs=262144 train_time:451315ms step_avg:549.04ms
step:823/1775 loss.item()=23570.7265625 n_predict=2 lr=1.5200 bs=262144 train_time:452037ms step_avg:549.25ms
step:824/1775 loss.item()=23707.5 n_predict=2 lr=1.5200 bs=262144 train_time:452763ms step_avg:549.47ms
step:825/1775 loss.item()=24320.3984375 n_predict=2 lr=1.5200 bs=262144 train_time:453485ms step_avg:549.68ms
step:826/1775 loss.item()=23872.169921875 n_predict=2 lr=1.5200 bs=262144 train_time:454208ms step_avg:549.89ms
step:827/1775 loss.item()=22614.6953125 n_predict=2 lr=1.5200 bs=262144 train_time:454927ms step_avg:550.09ms
step:828/1775 loss.item()=24252.373046875 n_predict=2 lr=1.5200 bs=262144 train_time:455650ms step_avg:550.30ms
step:829/1775 loss.item()=23104.1328125 n_predict=2 lr=1.5200 bs=262144 train_time:456372ms step_avg:550.51ms
step:830/1775 loss.item()=23227.380859375 n_predict=2 lr=1.5200 bs=262144 train_time:457098ms step_avg:550.72ms
step:831/1775 loss.item()=22585.44921875 n_predict=2 lr=1.5200 bs=262144 train_time:457818ms step_avg:550.92ms
step:832/1775 loss.item()=23019.359375 n_predict=2 lr=1.5200 bs=262144 train_time:458541ms step_avg:551.13ms
step:833/1775 loss.item()=24258.41796875 n_predict=2 lr=1.5200 bs=262144 train_time:459263ms step_avg:551.34ms
step:834/1775 loss.item()=23282.62890625 n_predict=2 lr=1.5200 bs=262144 train_time:459988ms step_avg:551.54ms
step:835/1775 loss.item()=23682.62109375 n_predict=2 lr=1.5200 bs=262144 train_time:460710ms step_avg:551.75ms
step:836/1775 loss.item()=23093.4921875 n_predict=2 lr=1.5200 bs=262144 train_time:461433ms step_avg:551.95ms
step:837/1775 loss.item()=23001.5234375 n_predict=2 lr=1.5200 bs=262144 train_time:462156ms step_avg:552.16ms
step:838/1775 loss.item()=23432.6015625 n_predict=2 lr=1.5200 bs=262144 train_time:462879ms step_avg:552.36ms
step:839/1775 loss.item()=22236.0625 n_predict=2 lr=1.5200 bs=262144 train_time:463602ms step_avg:552.56ms
step:840/1775 loss.item()=22715.662109375 n_predict=2 lr=1.5200 bs=262144 train_time:464324ms step_avg:552.77ms
step:841/1775 loss.item()=23863.234375 n_predict=2 lr=1.5200 bs=262144 train_time:465044ms step_avg:552.97ms
step:842/1775 loss.item()=23526.76953125 n_predict=2 lr=1.5200 bs=262144 train_time:465768ms step_avg:553.17ms
step:843/1775 loss.item()=24084.7890625 n_predict=2 lr=1.5200 bs=262144 train_time:466489ms step_avg:553.37ms
step:844/1775 loss.item()=23632.224609375 n_predict=2 lr=1.5200 bs=262144 train_time:467214ms step_avg:553.57ms
step:845/1775 loss.item()=23927.05078125 n_predict=2 lr=1.5200 bs=262144 train_time:467936ms step_avg:553.77ms
step:846/1775 loss.item()=23237.01171875 n_predict=2 lr=1.5200 bs=262144 train_time:468666ms step_avg:553.98ms
step:847/1775 loss.item()=23272.455078125 n_predict=2 lr=1.5200 bs=262144 train_time:469387ms step_avg:554.18ms
step:848/1775 loss.item()=22165.384765625 n_predict=2 lr=1.5200 bs=262144 train_time:470112ms step_avg:554.38ms
step:849/1775 loss.item()=23400.4140625 n_predict=2 lr=1.5200 bs=262144 train_time:470835ms step_avg:554.58ms
step:850/1775 loss.item()=22943.46875 n_predict=2 lr=1.5200 bs=262144 train_time:471560ms step_avg:554.78ms
step:851/1775 loss.item()=22340.259765625 n_predict=2 lr=1.5200 bs=262144 train_time:472282ms step_avg:554.97ms
step:852/1775 loss.item()=23142.478515625 n_predict=2 lr=1.5200 bs=262144 train_time:473005ms step_avg:555.17ms
step:853/1775 loss.item()=22153.7890625 n_predict=2 lr=1.5200 bs=262144 train_time:473725ms step_avg:555.36ms
step:854/1775 loss.item()=23721.26171875 n_predict=2 lr=1.5200 bs=262144 train_time:474449ms step_avg:555.56ms
step:855/1775 loss.item()=23124.62890625 n_predict=2 lr=1.5200 bs=262144 train_time:475170ms step_avg:555.75ms
step:856/1775 loss.item()=22794.72265625 n_predict=2 lr=1.5200 bs=262144 train_time:475897ms step_avg:555.95ms
step:857/1775 loss.item()=22863.17578125 n_predict=2 lr=1.5200 bs=262144 train_time:476620ms step_avg:556.15ms
step:858/1775 loss.item()=23280.796875 n_predict=2 lr=1.5200 bs=262144 train_time:477340ms step_avg:556.34ms
step:859/1775 loss.item()=22047.646484375 n_predict=2 lr=1.5200 bs=262144 train_time:478063ms step_avg:556.53ms
step:860/1775 loss.item()=22569.53125 n_predict=2 lr=1.5200 bs=262144 train_time:478787ms step_avg:556.73ms
step:861/1775 loss.item()=22868.048828125 n_predict=2 lr=1.5200 bs=262144 train_time:479508ms step_avg:556.92ms
step:862/1775 loss.item()=22898.9296875 n_predict=2 lr=1.5200 bs=262144 train_time:480230ms step_avg:557.11ms
step:863/1775 loss.item()=23267.69921875 n_predict=2 lr=1.5200 bs=262144 train_time:480955ms step_avg:557.31ms
step:864/1775 loss.item()=23565.359375 n_predict=2 lr=1.5200 bs=262144 train_time:481678ms step_avg:557.50ms
step:865/1775 loss.item()=21711.67578125 n_predict=2 lr=1.5200 bs=262144 train_time:482402ms step_avg:557.69ms
step:866/1775 loss.item()=22328.3984375 n_predict=2 lr=1.5200 bs=262144 train_time:483122ms step_avg:557.88ms
step:867/1775 loss.item()=22621.041015625 n_predict=2 lr=1.5200 bs=262144 train_time:483846ms step_avg:558.07ms
step:868/1775 loss.item()=22231.98046875 n_predict=2 lr=1.5200 bs=262144 train_time:484568ms step_avg:558.26ms
step:869/1775 loss.item()=22599.33984375 n_predict=2 lr=1.5192 bs=262144 train_time:485292ms step_avg:558.45ms
step:870/1775 loss.item()=21460.537109375 n_predict=2 lr=1.5175 bs=262144 train_time:486017ms step_avg:558.64ms
step:871/1775 loss.item()=21540.697265625 n_predict=2 lr=1.5159 bs=262144 train_time:486740ms step_avg:558.83ms
step:872/1775 loss.item()=22231.625 n_predict=2 lr=1.5143 bs=262144 train_time:487463ms step_avg:559.02ms
step:873/1775 loss.item()=22399.13671875 n_predict=2 lr=1.5126 bs=262144 train_time:488186ms step_avg:559.21ms
step:874/1775 loss.item()=22581.78125 n_predict=2 lr=1.5110 bs=262144 train_time:488909ms step_avg:559.39ms
step:875/1775 loss.item()=22852.75390625 n_predict=2 lr=1.5094 bs=262144 train_time:489635ms step_avg:559.58ms
step:876/1775 loss.item()=22232.00390625 n_predict=2 lr=1.5077 bs=262144 train_time:490358ms step_avg:559.77ms
step:877/1775 loss.item()=22093.49609375 n_predict=2 lr=1.5061 bs=262144 train_time:491084ms step_avg:559.96ms
step:878/1775 loss.item()=22371.0 n_predict=2 lr=1.5044 bs=262144 train_time:491806ms step_avg:560.14ms
step:879/1775 loss.item()=21105.103515625 n_predict=2 lr=1.5028 bs=262144 train_time:492527ms step_avg:560.33ms
step:880/1775 loss.item()=23158.40234375 n_predict=2 lr=1.5012 bs=262144 train_time:493250ms step_avg:560.51ms
step:881/1775 loss.item()=21927.90625 n_predict=2 lr=1.4995 bs=262144 train_time:493972ms step_avg:560.69ms
step:882/1775 loss.item()=21758.626953125 n_predict=2 lr=1.4979 bs=262144 train_time:494696ms step_avg:560.88ms
step:883/1775 loss.item()=21398.91015625 n_predict=2 lr=1.4963 bs=262144 train_time:495419ms step_avg:561.06ms
step:884/1775 loss.item()=22167.4296875 n_predict=2 lr=1.4946 bs=262144 train_time:496144ms step_avg:561.25ms
step:885/1775 loss.item()=22989.29296875 n_predict=2 lr=1.4930 bs=262144 train_time:496865ms step_avg:561.43ms
step:886/1775 loss.item()=21864.068359375 n_predict=2 lr=1.4914 bs=262144 train_time:497589ms step_avg:561.61ms
step:887/1775 loss.item()=22539.69140625 n_predict=2 lr=1.4897 bs=262144 train_time:498313ms step_avg:561.80ms
step:888/1775 loss.item()=21820.056640625 n_predict=2 lr=1.4881 bs=262144 train_time:499038ms step_avg:561.98ms
step:889/1775 loss.item()=21505.0546875 n_predict=2 lr=1.4864 bs=262144 train_time:499761ms step_avg:562.16ms
step:890/1775 loss.item()=23114.66796875 n_predict=2 lr=1.4848 bs=262144 train_time:500484ms step_avg:562.34ms
step:891/1775 loss.item()=21117.896484375 n_predict=2 lr=1.4832 bs=262144 train_time:501207ms step_avg:562.52ms
step:892/1775 loss.item()=22418.01953125 n_predict=2 lr=1.4815 bs=262144 train_time:501928ms step_avg:562.70ms
step:893/1775 loss.item()=21464.43359375 n_predict=2 lr=1.4799 bs=262144 train_time:502653ms step_avg:562.88ms
step:894/1775 loss.item()=21723.8125 n_predict=2 lr=1.4783 bs=262144 train_time:503379ms step_avg:563.06ms
step:895/1775 loss.item()=21891.763671875 n_predict=2 lr=1.4766 bs=262144 train_time:504100ms step_avg:563.24ms
step:896/1775 loss.item()=21854.44921875 n_predict=2 lr=1.4750 bs=262144 train_time:504823ms step_avg:563.42ms
step:897/1775 loss.item()=22133.1484375 n_predict=2 lr=1.4733 bs=262144 train_time:505545ms step_avg:563.60ms
step:898/1775 loss.item()=22275.951171875 n_predict=2 lr=1.4717 bs=262144 train_time:506268ms step_avg:563.77ms
step:899/1775 loss.item()=21759.001953125 n_predict=2 lr=1.4701 bs=262144 train_time:506988ms step_avg:563.95ms
step:900/1775 loss.item()=22451.52734375 n_predict=2 lr=1.4684 bs=262144 train_time:507711ms step_avg:564.12ms
step:901/1775 loss.item()=22131.283203125 n_predict=2 lr=1.4668 bs=262144 train_time:508436ms step_avg:564.30ms
step:902/1775 loss.item()=21243.87109375 n_predict=2 lr=1.4652 bs=262144 train_time:509160ms step_avg:564.48ms
step:903/1775 loss.item()=21918.8125 n_predict=2 lr=1.4635 bs=262144 train_time:509881ms step_avg:564.65ms
step:904/1775 loss.item()=21939.65625 n_predict=2 lr=1.4619 bs=262144 train_time:510607ms step_avg:564.83ms
step:905/1775 loss.item()=21664.96484375 n_predict=2 lr=1.4603 bs=262144 train_time:511329ms step_avg:565.00ms
step:906/1775 loss.item()=21738.50390625 n_predict=2 lr=1.4586 bs=262144 train_time:512051ms step_avg:565.18ms
step:907/1775 loss.item()=21353.017578125 n_predict=2 lr=1.4570 bs=262144 train_time:512773ms step_avg:565.35ms
step:908/1775 loss.item()=21361.2890625 n_predict=2 lr=1.4553 bs=262144 train_time:513498ms step_avg:565.53ms
step:909/1775 loss.item()=22034.6015625 n_predict=2 lr=1.4537 bs=262144 train_time:514223ms step_avg:565.70ms
step:910/1775 loss.item()=20664.97265625 n_predict=2 lr=1.4521 bs=262144 train_time:514948ms step_avg:565.88ms
step:911/1775 loss.item()=21109.86328125 n_predict=2 lr=1.4504 bs=262144 train_time:515672ms step_avg:566.05ms
step:912/1775 loss.item()=21414.24609375 n_predict=2 lr=1.4488 bs=262144 train_time:516396ms step_avg:566.22ms
step:913/1775 loss.item()=20872.22265625 n_predict=2 lr=1.4472 bs=262144 train_time:517121ms step_avg:566.40ms
step:914/1775 loss.item()=21615.556640625 n_predict=2 lr=1.4455 bs=262144 train_time:517840ms step_avg:566.56ms
step:915/1775 loss.item()=21448.58984375 n_predict=2 lr=1.4439 bs=262144 train_time:518563ms step_avg:566.74ms
step:916/1775 loss.item()=20729.79296875 n_predict=2 lr=1.4422 bs=262144 train_time:519288ms step_avg:566.91ms
step:917/1775 loss.item()=21606.5625 n_predict=2 lr=1.4406 bs=262144 train_time:520010ms step_avg:567.08ms
step:918/1775 loss.item()=22161.744140625 n_predict=2 lr=1.4390 bs=262144 train_time:520734ms step_avg:567.25ms
step:919/1775 loss.item()=21047.287109375 n_predict=2 lr=1.4373 bs=262144 train_time:521457ms step_avg:567.42ms
step:920/1775 loss.item()=21459.189453125 n_predict=2 lr=1.4357 bs=262144 train_time:522186ms step_avg:567.59ms
step:921/1775 loss.item()=21299.84765625 n_predict=2 lr=1.4341 bs=262144 train_time:522907ms step_avg:567.76ms
step:922/1775 loss.item()=20987.552734375 n_predict=2 lr=1.4324 bs=262144 train_time:523630ms step_avg:567.93ms
step:923/1775 loss.item()=21108.283203125 n_predict=2 lr=1.4308 bs=262144 train_time:524352ms step_avg:568.10ms
step:924/1775 loss.item()=20918.494140625 n_predict=2 lr=1.4292 bs=262144 train_time:525079ms step_avg:568.27ms
step:925/1775 loss.item()=21157.2578125 n_predict=2 lr=1.4275 bs=262144 train_time:525801ms step_avg:568.43ms
step:926/1775 loss.item()=20946.689453125 n_predict=2 lr=1.4259 bs=262144 train_time:526525ms step_avg:568.60ms
step:927/1775 loss.item()=20906.9296875 n_predict=2 lr=1.4242 bs=262144 train_time:527248ms step_avg:568.77ms
step:928/1775 loss.item()=21122.375 n_predict=2 lr=1.4226 bs=262144 train_time:527970ms step_avg:568.93ms
step:929/1775 loss.item()=21189.8359375 n_predict=2 lr=1.4210 bs=262144 train_time:528691ms step_avg:569.10ms
step:930/1775 loss.item()=20156.01953125 n_predict=2 lr=1.4193 bs=262144 train_time:529416ms step_avg:569.26ms
step:931/1775 loss.item()=21524.6953125 n_predict=2 lr=1.4177 bs=262144 train_time:530139ms step_avg:569.43ms
step:932/1775 loss.item()=20417.8515625 n_predict=2 lr=1.4161 bs=262144 train_time:530864ms step_avg:569.60ms
step:933/1775 loss.item()=20518.93359375 n_predict=2 lr=1.4144 bs=262144 train_time:531586ms step_avg:569.76ms
step:934/1775 loss.item()=21044.072265625 n_predict=2 lr=1.4128 bs=262144 train_time:532310ms step_avg:569.92ms
step:935/1775 loss.item()=20684.55859375 n_predict=2 lr=1.4111 bs=262144 train_time:533034ms step_avg:570.09ms
step:936/1775 loss.item()=21448.271484375 n_predict=2 lr=1.4095 bs=262144 train_time:533758ms step_avg:570.25ms
step:937/1775 loss.item()=20456.96484375 n_predict=2 lr=1.4079 bs=262144 train_time:534482ms step_avg:570.42ms
step:938/1775 loss.item()=21071.87890625 n_predict=2 lr=1.4062 bs=262144 train_time:535207ms step_avg:570.58ms
step:939/1775 loss.item()=20916.390625 n_predict=2 lr=1.4046 bs=262144 train_time:535929ms step_avg:570.74ms
step:940/1775 loss.item()=21500.703125 n_predict=2 lr=1.4030 bs=262144 train_time:536652ms step_avg:570.91ms
step:941/1775 loss.item()=20267.33203125 n_predict=2 lr=1.4013 bs=262144 train_time:537376ms step_avg:571.07ms
step:942/1775 loss.item()=19942.15234375 n_predict=2 lr=1.3997 bs=262144 train_time:538100ms step_avg:571.23ms
step:943/1775 loss.item()=20668.673828125 n_predict=2 lr=1.3981 bs=262144 train_time:538819ms step_avg:571.39ms
step:944/1775 loss.item()=19720.517578125 n_predict=2 lr=1.3964 bs=262144 train_time:539547ms step_avg:571.55ms
step:945/1775 loss.item()=20966.33203125 n_predict=2 lr=1.3948 bs=262144 train_time:540268ms step_avg:571.71ms
step:946/1775 loss.item()=20398.767578125 n_predict=2 lr=1.3931 bs=262144 train_time:540990ms step_avg:571.87ms
step:947/1775 loss.item()=20480.931640625 n_predict=2 lr=1.3915 bs=262144 train_time:541715ms step_avg:572.03ms
step:948/1775 loss.item()=20255.2265625 n_predict=2 lr=1.3899 bs=262144 train_time:542439ms step_avg:572.19ms
step:949/1775 loss.item()=20285.3359375 n_predict=2 lr=1.3882 bs=262144 train_time:543159ms step_avg:572.35ms
step:950/1775 loss.item()=19951.57421875 n_predict=2 lr=1.3866 bs=262144 train_time:543883ms step_avg:572.51ms
step:951/1775 loss.item()=20065.033203125 n_predict=2 lr=1.3850 bs=262144 train_time:544606ms step_avg:572.67ms
step:952/1775 loss.item()=20260.873046875 n_predict=2 lr=1.3833 bs=262144 train_time:545330ms step_avg:572.83ms
step:953/1775 loss.item()=19731.583984375 n_predict=2 lr=1.3817 bs=262144 train_time:546050ms step_avg:572.98ms
step:954/1775 loss.item()=19956.15234375 n_predict=2 lr=1.3800 bs=262144 train_time:546776ms step_avg:573.14ms
step:955/1775 loss.item()=19836.74609375 n_predict=2 lr=1.3784 bs=262144 train_time:547500ms step_avg:573.30ms
step:956/1775 loss.item()=20725.587890625 n_predict=2 lr=1.3768 bs=262144 train_time:548227ms step_avg:573.46ms
step:957/1775 loss.item()=20539.330078125 n_predict=2 lr=1.3751 bs=262144 train_time:548948ms step_avg:573.61ms
step:958/1775 loss.item()=19804.0859375 n_predict=2 lr=1.3735 bs=262144 train_time:549670ms step_avg:573.77ms
step:959/1775 loss.item()=20508.24609375 n_predict=2 lr=1.3719 bs=262144 train_time:550395ms step_avg:573.93ms
step:960/1775 loss.item()=21022.177734375 n_predict=2 lr=1.3702 bs=262144 train_time:551119ms step_avg:574.08ms
step:961/1775 loss.item()=19855.4921875 n_predict=2 lr=1.3686 bs=262144 train_time:551840ms step_avg:574.24ms
step:962/1775 loss.item()=19802.05859375 n_predict=2 lr=1.3670 bs=262144 train_time:552564ms step_avg:574.39ms
step:963/1775 loss.item()=20302.8984375 n_predict=2 lr=1.3653 bs=262144 train_time:553286ms step_avg:574.54ms
step:964/1775 loss.item()=19980.328125 n_predict=2 lr=1.3637 bs=262144 train_time:554009ms step_avg:574.70ms
step:965/1775 loss.item()=20238.578125 n_predict=2 lr=1.3620 bs=262144 train_time:554731ms step_avg:574.85ms
step:966/1775 loss.item()=19358.23046875 n_predict=2 lr=1.3604 bs=262144 train_time:555456ms step_avg:575.01ms
step:967/1775 loss.item()=19351.05078125 n_predict=2 lr=1.3588 bs=262144 train_time:556178ms step_avg:575.16ms
step:968/1775 loss.item()=19667.779296875 n_predict=2 lr=1.3571 bs=262144 train_time:556909ms step_avg:575.32ms
step:969/1775 loss.item()=19324.783203125 n_predict=2 lr=1.3555 bs=262144 train_time:557628ms step_avg:575.47ms
step:970/1775 loss.item()=19655.009765625 n_predict=2 lr=1.3539 bs=262144 train_time:558350ms step_avg:575.62ms
step:971/1775 loss.item()=20013.51953125 n_predict=2 lr=1.3522 bs=262144 train_time:559074ms step_avg:575.77ms
step:972/1775 loss.item()=19623.34375 n_predict=2 lr=1.3506 bs=262144 train_time:559798ms step_avg:575.92ms
step:973/1775 loss.item()=19678.0859375 n_predict=2 lr=1.3489 bs=262144 train_time:560521ms step_avg:576.08ms
step:974/1775 loss.item()=19678.888671875 n_predict=2 lr=1.3473 bs=262144 train_time:561243ms step_avg:576.23ms
step:975/1775 loss.item()=19351.59375 n_predict=2 lr=1.3457 bs=262144 train_time:561966ms step_avg:576.38ms
step:976/1775 loss.item()=20162.5078125 n_predict=2 lr=1.3440 bs=262144 train_time:562690ms step_avg:576.53ms
step:977/1775 loss.item()=19761.392578125 n_predict=2 lr=1.3424 bs=262144 train_time:563410ms step_avg:576.67ms
step:978/1775 loss.item()=20070.90625 n_predict=2 lr=1.3408 bs=262144 train_time:564131ms step_avg:576.82ms
step:979/1775 loss.item()=20034.345703125 n_predict=2 lr=1.3391 bs=262144 train_time:564852ms step_avg:576.97ms
step:980/1775 loss.item()=19235.259765625 n_predict=2 lr=1.3375 bs=262144 train_time:565576ms step_avg:577.12ms
step:981/1775 loss.item()=20719.2421875 n_predict=2 lr=1.3359 bs=262144 train_time:566299ms step_avg:577.27ms
step:982/1775 loss.item()=19377.013671875 n_predict=2 lr=1.3342 bs=262144 train_time:567025ms step_avg:577.42ms
step:983/1775 loss.item()=19724.546875 n_predict=2 lr=1.3326 bs=262144 train_time:567745ms step_avg:577.56ms
step:984/1775 loss.item()=19493.390625 n_predict=2 lr=1.3309 bs=262144 train_time:568469ms step_avg:577.71ms
step:985/1775 loss.item()=19553.6796875 n_predict=2 lr=1.3293 bs=262144 train_time:569192ms step_avg:577.86ms
step:986/1775 loss.item()=19348.919921875 n_predict=2 lr=1.3277 bs=262144 train_time:569916ms step_avg:578.01ms
step:987/1775 loss.item()=19093.64453125 n_predict=2 lr=1.3260 bs=262144 train_time:570637ms step_avg:578.15ms
step:988/1775 loss.item()=19077.34765625 n_predict=2 lr=1.3244 bs=262144 train_time:571361ms step_avg:578.30ms
step:989/1775 loss.item()=19868.82421875 n_predict=2 lr=1.3228 bs=262144 train_time:572081ms step_avg:578.44ms
step:990/1775 loss.item()=18943.3828125 n_predict=2 lr=1.3211 bs=262144 train_time:572806ms step_avg:578.59ms
step:991/1775 loss.item()=18942.591796875 n_predict=2 lr=1.3195 bs=262144 train_time:573528ms step_avg:578.74ms
step:992/1775 loss.item()=19450.3125 n_predict=2 lr=1.3178 bs=262144 train_time:574251ms step_avg:578.88ms
step:993/1775 loss.item()=19122.484375 n_predict=2 lr=1.3162 bs=262144 train_time:574975ms step_avg:579.03ms
step:994/1775 loss.item()=18524.908203125 n_predict=2 lr=1.3146 bs=262144 train_time:575700ms step_avg:579.18ms
step:995/1775 loss.item()=19342.5 n_predict=2 lr=1.3129 bs=262144 train_time:576425ms step_avg:579.32ms
step:996/1775 loss.item()=19658.33984375 n_predict=2 lr=1.3113 bs=262144 train_time:577149ms step_avg:579.47ms
step:997/1775 loss.item()=18938.01953125 n_predict=2 lr=1.3097 bs=262144 train_time:577870ms step_avg:579.61ms
step:998/1775 loss.item()=19053.923828125 n_predict=2 lr=1.3080 bs=262144 train_time:578596ms step_avg:579.76ms
step:999/1775 loss.item()=18854.873046875 n_predict=2 lr=1.3064 bs=262144 train_time:579318ms step_avg:579.90ms
step:1000/1775 loss.item()=18471.36328125 n_predict=2 lr=1.3047 bs=262144 train_time:580041ms step_avg:580.04ms
step:1000/1775 lr=1.3031 bs=262144 n_predict=2 val_loss:3.7414 val_malbo_loss:3.7414 train_time:580160ms step_avg:580.16ms
step:1001/1775 loss.item()=19527.09765625 n_predict=2 lr=1.3031 bs=262144 train_time:580763ms step_avg:580.18ms
step:1002/1775 loss.item()=18399.9140625 n_predict=2 lr=1.3015 bs=262144 train_time:581489ms step_avg:580.33ms
step:1003/1775 loss.item()=18833.96484375 n_predict=2 lr=1.2998 bs=262144 train_time:582211ms step_avg:580.47ms
step:1004/1775 loss.item()=19184.1484375 n_predict=2 lr=1.2982 bs=262144 train_time:582934ms step_avg:580.61ms
step:1005/1775 loss.item()=19093.455078125 n_predict=2 lr=1.2966 bs=262144 train_time:583658ms step_avg:580.75ms
step:1006/1775 loss.item()=18673.109375 n_predict=2 lr=1.2949 bs=262144 train_time:584383ms step_avg:580.90ms
step:1007/1775 loss.item()=19258.5703125 n_predict=2 lr=1.2933 bs=262144 train_time:585106ms step_avg:581.04ms
step:1008/1775 loss.item()=18583.640625 n_predict=2 lr=1.2917 bs=262144 train_time:585829ms step_avg:581.18ms
step:1009/1775 loss.item()=19553.802734375 n_predict=2 lr=1.2900 bs=262144 train_time:586551ms step_avg:581.32ms
step:1010/1775 loss.item()=18976.5859375 n_predict=2 lr=1.2884 bs=262144 train_time:587274ms step_avg:581.46ms
step:1011/1775 loss.item()=19142.919921875 n_predict=2 lr=1.2867 bs=262144 train_time:587995ms step_avg:581.60ms
step:1012/1775 loss.item()=19595.671875 n_predict=2 lr=1.2851 bs=262144 train_time:588720ms step_avg:581.74ms
step:1013/1775 loss.item()=18261.890625 n_predict=2 lr=1.2835 bs=262144 train_time:589443ms step_avg:581.88ms
step:1014/1775 loss.item()=18934.0703125 n_predict=2 lr=1.2818 bs=262144 train_time:590166ms step_avg:582.02ms
step:1015/1775 loss.item()=18721.884765625 n_predict=2 lr=1.2802 bs=262144 train_time:590886ms step_avg:582.15ms
step:1016/1775 loss.item()=18793.94921875 n_predict=2 lr=1.2786 bs=262144 train_time:591610ms step_avg:582.29ms
step:1017/1775 loss.item()=17533.966796875 n_predict=2 lr=1.2769 bs=262144 train_time:592333ms step_avg:582.43ms
step:1018/1775 loss.item()=18663.65625 n_predict=2 lr=1.2753 bs=262144 train_time:593055ms step_avg:582.57ms
step:1019/1775 loss.item()=18564.642578125 n_predict=2 lr=1.2736 bs=262144 train_time:593776ms step_avg:582.70ms
step:1020/1775 loss.item()=19064.876953125 n_predict=2 lr=1.2720 bs=262144 train_time:594502ms step_avg:582.85ms
step:1021/1775 loss.item()=18671.59375 n_predict=2 lr=1.2704 bs=262144 train_time:595224ms step_avg:582.98ms
step:1022/1775 loss.item()=18809.345703125 n_predict=2 lr=1.2687 bs=262144 train_time:595952ms step_avg:583.12ms
step:1023/1775 loss.item()=18875.50390625 n_predict=2 lr=1.2671 bs=262144 train_time:596671ms step_avg:583.26ms
step:1024/1775 loss.item()=18341.32421875 n_predict=2 lr=1.2655 bs=262144 train_time:597394ms step_avg:583.39ms
step:1025/1775 loss.item()=19054.86328125 n_predict=2 lr=1.2638 bs=262144 train_time:598118ms step_avg:583.53ms
step:1026/1775 loss.item()=18751.6171875 n_predict=2 lr=1.2622 bs=262144 train_time:598843ms step_avg:583.67ms
step:1027/1775 loss.item()=18954.9453125 n_predict=2 lr=1.2606 bs=262144 train_time:599565ms step_avg:583.80ms
step:1028/1775 loss.item()=18306.7578125 n_predict=2 lr=1.2589 bs=262144 train_time:600288ms step_avg:583.94ms
step:1029/1775 loss.item()=18723.17578125 n_predict=2 lr=1.2573 bs=262144 train_time:601011ms step_avg:584.07ms
step:1030/1775 loss.item()=18300.1484375 n_predict=2 lr=1.2556 bs=262144 train_time:601734ms step_avg:584.21ms
step:1031/1775 loss.item()=18528.1953125 n_predict=2 lr=1.2540 bs=262144 train_time:602455ms step_avg:584.34ms
step:1032/1775 loss.item()=18294.5546875 n_predict=2 lr=1.2524 bs=262144 train_time:603182ms step_avg:584.48ms
step:1033/1775 loss.item()=18308.25390625 n_predict=2 lr=1.2507 bs=262144 train_time:603904ms step_avg:584.61ms
step:1034/1775 loss.item()=18346.330078125 n_predict=2 lr=1.2491 bs=262144 train_time:604629ms step_avg:584.75ms
step:1035/1775 loss.item()=17827.98828125 n_predict=2 lr=1.2475 bs=262144 train_time:605352ms step_avg:584.88ms
step:1036/1775 loss.item()=18890.259765625 n_predict=2 lr=1.2458 bs=262144 train_time:606074ms step_avg:585.01ms
step:1037/1775 loss.item()=18266.06640625 n_predict=2 lr=1.2442 bs=262144 train_time:606795ms step_avg:585.14ms
step:1038/1775 loss.item()=17701.17578125 n_predict=2 lr=1.2425 bs=262144 train_time:607520ms step_avg:585.28ms
step:1039/1775 loss.item()=17643.41796875 n_predict=2 lr=1.2409 bs=262144 train_time:608242ms step_avg:585.41ms
step:1040/1775 loss.item()=18336.2734375 n_predict=2 lr=1.2393 bs=262144 train_time:608964ms step_avg:585.54ms
step:1041/1775 loss.item()=19558.462890625 n_predict=2 lr=1.2376 bs=262144 train_time:609688ms step_avg:585.68ms
step:1042/1775 loss.item()=17792.119140625 n_predict=2 lr=1.2360 bs=262144 train_time:610416ms step_avg:585.81ms
step:1043/1775 loss.item()=18275.013671875 n_predict=2 lr=1.2344 bs=262144 train_time:611142ms step_avg:585.95ms
step:1044/1775 loss.item()=17891.9765625 n_predict=2 lr=1.2327 bs=262144 train_time:611865ms step_avg:586.08ms
step:1045/1775 loss.item()=17509.595703125 n_predict=2 lr=1.2311 bs=262144 train_time:612588ms step_avg:586.21ms
step:1046/1775 loss.item()=17648.99609375 n_predict=2 lr=1.2295 bs=262144 train_time:613313ms step_avg:586.34ms
step:1047/1775 loss.item()=17376.4296875 n_predict=2 lr=1.2278 bs=262144 train_time:614035ms step_avg:586.47ms
step:1048/1775 loss.item()=18695.896484375 n_predict=2 lr=1.2262 bs=262144 train_time:614756ms step_avg:586.60ms
step:1049/1775 loss.item()=17898.056640625 n_predict=2 lr=1.2245 bs=262144 train_time:615480ms step_avg:586.73ms
step:1050/1775 loss.item()=18165.42578125 n_predict=2 lr=1.2229 bs=262144 train_time:616206ms step_avg:586.86ms
step:1051/1775 loss.item()=18106.890625 n_predict=2 lr=1.2213 bs=262144 train_time:616928ms step_avg:586.99ms
step:1052/1775 loss.item()=18428.697265625 n_predict=2 lr=1.2196 bs=262144 train_time:617650ms step_avg:587.12ms
step:1053/1775 loss.item()=17451.44140625 n_predict=2 lr=1.2180 bs=262144 train_time:618372ms step_avg:587.25ms
step:1054/1775 loss.item()=17577.08203125 n_predict=2 lr=1.2164 bs=262144 train_time:619094ms step_avg:587.38ms
step:1055/1775 loss.item()=17082.673828125 n_predict=2 lr=1.2147 bs=262144 train_time:619815ms step_avg:587.50ms
step:1056/1775 loss.item()=17391.142578125 n_predict=2 lr=1.2131 bs=262144 train_time:620538ms step_avg:587.63ms
step:1057/1775 loss.item()=17634.66796875 n_predict=2 lr=1.2114 bs=262144 train_time:621262ms step_avg:587.76ms
step:1058/1775 loss.item()=17911.359375 n_predict=2 lr=1.2098 bs=262144 train_time:621986ms step_avg:587.89ms
step:1059/1775 loss.item()=17397.205078125 n_predict=2 lr=1.2082 bs=262144 train_time:622709ms step_avg:588.02ms
step:1060/1775 loss.item()=17348.02734375 n_predict=2 lr=1.2065 bs=262144 train_time:623433ms step_avg:588.14ms
step:1061/1775 loss.item()=17491.4609375 n_predict=2 lr=1.2049 bs=262144 train_time:624154ms step_avg:588.27ms
step:1062/1775 loss.item()=17767.802734375 n_predict=2 lr=1.2033 bs=262144 train_time:624874ms step_avg:588.39ms
step:1063/1775 loss.item()=17595.66796875 n_predict=2 lr=1.2016 bs=262144 train_time:625596ms step_avg:588.52ms
step:1064/1775 loss.item()=16612.33203125 n_predict=2 lr=1.2000 bs=262144 train_time:626318ms step_avg:588.64ms
step:1065/1775 loss.item()=17875.91796875 n_predict=2 lr=1.1984 bs=262144 train_time:627039ms step_avg:588.77ms
step:1066/1775 loss.item()=17222.0703125 n_predict=2 lr=1.1967 bs=262144 train_time:627762ms step_avg:588.90ms
step:1067/1775 loss.item()=17219.205078125 n_predict=2 lr=1.1951 bs=262144 train_time:628485ms step_avg:589.02ms
step:1068/1775 loss.item()=17710.408203125 n_predict=2 lr=1.1934 bs=262144 train_time:629209ms step_avg:589.15ms
step:1069/1775 loss.item()=17834.1171875 n_predict=2 lr=1.1918 bs=262144 train_time:629932ms step_avg:589.27ms
step:1070/1775 loss.item()=17111.61328125 n_predict=2 lr=1.1902 bs=262144 train_time:630653ms step_avg:589.40ms
step:1071/1775 loss.item()=17137.66015625 n_predict=2 lr=1.1885 bs=262144 train_time:631375ms step_avg:589.52ms
step:1072/1775 loss.item()=16712.9296875 n_predict=2 lr=1.1869 bs=262144 train_time:632096ms step_avg:589.64ms
step:1073/1775 loss.item()=16937.16015625 n_predict=2 lr=1.1853 bs=262144 train_time:632820ms step_avg:589.77ms
step:1074/1775 loss.item()=17343.72265625 n_predict=2 lr=1.1836 bs=262144 train_time:633543ms step_avg:589.89ms
step:1075/1775 loss.item()=16162.01171875 n_predict=2 lr=1.1820 bs=262144 train_time:634262ms step_avg:590.01ms
step:1076/1775 loss.item()=17637.0625 n_predict=2 lr=1.1803 bs=262144 train_time:634986ms step_avg:590.14ms
step:1077/1775 loss.item()=17295.88671875 n_predict=2 lr=1.1787 bs=262144 train_time:635708ms step_avg:590.26ms
step:1078/1775 loss.item()=17054.16796875 n_predict=2 lr=1.1771 bs=262144 train_time:636430ms step_avg:590.38ms
step:1079/1775 loss.item()=17981.0 n_predict=2 lr=1.1754 bs=262144 train_time:637153ms step_avg:590.50ms
step:1080/1775 loss.item()=17410.2265625 n_predict=2 lr=1.1738 bs=262144 train_time:637873ms step_avg:590.62ms
step:1081/1775 loss.item()=16869.521484375 n_predict=2 lr=1.1722 bs=262144 train_time:638596ms step_avg:590.75ms
step:1082/1775 loss.item()=17262.04296875 n_predict=2 lr=1.1705 bs=262144 train_time:639320ms step_avg:590.87ms
step:1083/1775 loss.item()=17436.853515625 n_predict=2 lr=1.1689 bs=262144 train_time:640043ms step_avg:590.99ms
step:1084/1775 loss.item()=16512.201171875 n_predict=2 lr=1.1673 bs=262144 train_time:640767ms step_avg:591.11ms
step:1085/1775 loss.item()=16396.662109375 n_predict=2 lr=1.1656 bs=262144 train_time:641489ms step_avg:591.23ms
step:1086/1775 loss.item()=16144.876953125 n_predict=2 lr=1.1640 bs=262144 train_time:642211ms step_avg:591.35ms
step:1087/1775 loss.item()=16630.89453125 n_predict=2 lr=1.1623 bs=262144 train_time:642933ms step_avg:591.47ms
step:1088/1775 loss.item()=16980.37890625 n_predict=2 lr=1.1607 bs=262144 train_time:643655ms step_avg:591.59ms
step:1089/1775 loss.item()=16734.109375 n_predict=2 lr=1.1591 bs=262144 train_time:644378ms step_avg:591.72ms
step:1090/1775 loss.item()=16965.27734375 n_predict=2 lr=1.1574 bs=262144 train_time:645103ms step_avg:591.84ms
step:1091/1775 loss.item()=16780.390625 n_predict=2 lr=1.1558 bs=262144 train_time:645823ms step_avg:591.96ms
step:1092/1775 loss.item()=16944.203125 n_predict=2 lr=1.1542 bs=262144 train_time:646551ms step_avg:592.08ms
step:1093/1775 loss.item()=17113.83203125 n_predict=2 lr=1.1525 bs=262144 train_time:647273ms step_avg:592.20ms
step:1094/1775 loss.item()=17120.27734375 n_predict=2 lr=1.1509 bs=262144 train_time:647994ms step_avg:592.32ms
step:1095/1775 loss.item()=15752.015625 n_predict=2 lr=1.1492 bs=262144 train_time:648714ms step_avg:592.43ms
step:1096/1775 loss.item()=16251.3076171875 n_predict=2 lr=1.1476 bs=262144 train_time:649436ms step_avg:592.55ms
step:1097/1775 loss.item()=15916.830078125 n_predict=2 lr=1.1460 bs=262144 train_time:650161ms step_avg:592.67ms
step:1098/1775 loss.item()=17426.64453125 n_predict=2 lr=1.1443 bs=262144 train_time:650885ms step_avg:592.79ms
step:1099/1775 loss.item()=16307.49609375 n_predict=2 lr=1.1427 bs=262144 train_time:651609ms step_avg:592.91ms
step:1100/1775 loss.item()=15915.296875 n_predict=2 lr=1.1411 bs=262144 train_time:652333ms step_avg:593.03ms
step:1101/1775 loss.item()=16604.0859375 n_predict=2 lr=1.1394 bs=262144 train_time:653050ms step_avg:593.14ms
step:1102/1775 loss.item()=16808.25390625 n_predict=2 lr=1.1378 bs=262144 train_time:653773ms step_avg:593.26ms
step:1103/1775 loss.item()=16604.9375 n_predict=2 lr=1.1361 bs=262144 train_time:654493ms step_avg:593.38ms
step:1104/1775 loss.item()=16270.2431640625 n_predict=2 lr=1.1345 bs=262144 train_time:655214ms step_avg:593.49ms
step:1105/1775 loss.item()=16118.68359375 n_predict=2 lr=1.1329 bs=262144 train_time:655935ms step_avg:593.61ms
step:1106/1775 loss.item()=16320.4326171875 n_predict=2 lr=1.1312 bs=262144 train_time:656659ms step_avg:593.72ms
step:1107/1775 loss.item()=16777.796875 n_predict=2 lr=1.1296 bs=262144 train_time:657381ms step_avg:593.84ms
step:1108/1775 loss.item()=15587.8642578125 n_predict=2 lr=1.1280 bs=262144 train_time:658104ms step_avg:593.96ms
step:1109/1775 loss.item()=15831.6455078125 n_predict=2 lr=1.1263 bs=262144 train_time:658827ms step_avg:594.07ms
step:1110/1775 loss.item()=15954.42578125 n_predict=2 lr=1.1247 bs=262144 train_time:659551ms step_avg:594.19ms
step:1111/1775 loss.item()=15974.623046875 n_predict=2 lr=1.1231 bs=262144 train_time:660273ms step_avg:594.31ms
step:1112/1775 loss.item()=15876.3134765625 n_predict=2 lr=1.1214 bs=262144 train_time:660994ms step_avg:594.42ms
step:1113/1775 loss.item()=16006.720703125 n_predict=2 lr=1.1198 bs=262144 train_time:661714ms step_avg:594.53ms
step:1114/1775 loss.item()=15644.92578125 n_predict=2 lr=1.1181 bs=262144 train_time:662436ms step_avg:594.65ms
step:1115/1775 loss.item()=16881.857421875 n_predict=2 lr=1.1165 bs=262144 train_time:663162ms step_avg:594.76ms
step:1116/1775 loss.item()=15959.169921875 n_predict=2 lr=1.1149 bs=262144 train_time:663884ms step_avg:594.88ms
step:1117/1775 loss.item()=16461.375 n_predict=2 lr=1.1132 bs=262144 train_time:664605ms step_avg:594.99ms
step:1118/1775 loss.item()=16074.4326171875 n_predict=2 lr=1.1116 bs=262144 train_time:665332ms step_avg:595.11ms
step:1119/1775 loss.item()=15681.0849609375 n_predict=2 lr=1.1100 bs=262144 train_time:666054ms step_avg:595.22ms
step:1120/1775 loss.item()=15714.732421875 n_predict=2 lr=1.1083 bs=262144 train_time:666774ms step_avg:595.33ms
step:1121/1775 loss.item()=15417.349609375 n_predict=2 lr=1.1067 bs=262144 train_time:667495ms step_avg:595.45ms
step:1122/1775 loss.item()=15509.09765625 n_predict=2 lr=1.1050 bs=262144 train_time:668219ms step_avg:595.56ms
step:1123/1775 loss.item()=15965.1572265625 n_predict=2 lr=1.1034 bs=262144 train_time:668942ms step_avg:595.67ms
step:1124/1775 loss.item()=15597.21875 n_predict=2 lr=1.1018 bs=262144 train_time:669665ms step_avg:595.79ms
step:1125/1775 loss.item()=15660.154296875 n_predict=2 lr=1.1001 bs=262144 train_time:670389ms step_avg:595.90ms
step:1126/1775 loss.item()=15369.0322265625 n_predict=2 lr=1.0985 bs=262144 train_time:671112ms step_avg:596.01ms
step:1127/1775 loss.item()=15992.90234375 n_predict=2 lr=1.0969 bs=262144 train_time:671834ms step_avg:596.13ms
step:1128/1775 loss.item()=15330.1875 n_predict=2 lr=1.0952 bs=262144 train_time:672554ms step_avg:596.24ms
step:1129/1775 loss.item()=15660.439453125 n_predict=2 lr=1.0936 bs=262144 train_time:673274ms step_avg:596.35ms
step:1130/1775 loss.item()=15429.2919921875 n_predict=2 lr=1.0920 bs=262144 train_time:673996ms step_avg:596.46ms
step:1131/1775 loss.item()=15811.189453125 n_predict=2 lr=1.0903 bs=262144 train_time:674720ms step_avg:596.57ms
step:1132/1775 loss.item()=15656.421875 n_predict=2 lr=1.0887 bs=262144 train_time:675441ms step_avg:596.68ms
step:1133/1775 loss.item()=15555.640625 n_predict=2 lr=1.0870 bs=262144 train_time:676167ms step_avg:596.79ms
step:1134/1775 loss.item()=15806.568359375 n_predict=2 lr=1.0854 bs=262144 train_time:676887ms step_avg:596.90ms
step:1135/1775 loss.item()=15727.369140625 n_predict=2 lr=1.0838 bs=262144 train_time:677607ms step_avg:597.01ms
step:1136/1775 loss.item()=14993.4208984375 n_predict=2 lr=1.0821 bs=262144 train_time:678329ms step_avg:597.12ms
step:1137/1775 loss.item()=15192.919921875 n_predict=2 lr=1.0805 bs=262144 train_time:679051ms step_avg:597.23ms
step:1138/1775 loss.item()=15398.345703125 n_predict=2 lr=1.0789 bs=262144 train_time:679771ms step_avg:597.34ms
step:1139/1775 loss.item()=15035.2880859375 n_predict=2 lr=1.0772 bs=262144 train_time:680493ms step_avg:597.45ms
step:1140/1775 loss.item()=16111.595703125 n_predict=2 lr=1.0756 bs=262144 train_time:681213ms step_avg:597.56ms
step:1141/1775 loss.item()=15023.564453125 n_predict=2 lr=1.0739 bs=262144 train_time:681934ms step_avg:597.66ms
step:1142/1775 loss.item()=15289.083984375 n_predict=2 lr=1.0723 bs=262144 train_time:682655ms step_avg:597.77ms
step:1143/1775 loss.item()=14817.677734375 n_predict=2 lr=1.0707 bs=262144 train_time:683377ms step_avg:597.88ms
step:1144/1775 loss.item()=14359.349609375 n_predict=2 lr=1.0690 bs=262144 train_time:684101ms step_avg:597.99ms
step:1145/1775 loss.item()=15692.935546875 n_predict=2 lr=1.0674 bs=262144 train_time:684824ms step_avg:598.10ms
step:1146/1775 loss.item()=14629.90234375 n_predict=2 lr=1.0658 bs=262144 train_time:685549ms step_avg:598.21ms
step:1147/1775 loss.item()=15167.576171875 n_predict=2 lr=1.0641 bs=262144 train_time:686271ms step_avg:598.32ms
step:1148/1775 loss.item()=15413.8447265625 n_predict=2 lr=1.0625 bs=262144 train_time:686992ms step_avg:598.43ms
step:1149/1775 loss.item()=14201.8359375 n_predict=2 lr=1.0609 bs=262144 train_time:687712ms step_avg:598.53ms
step:1150/1775 loss.item()=14387.615234375 n_predict=2 lr=1.0592 bs=262144 train_time:688434ms step_avg:598.64ms
step:1151/1775 loss.item()=15133.873046875 n_predict=2 lr=1.0576 bs=262144 train_time:689156ms step_avg:598.75ms
step:1152/1775 loss.item()=15117.84375 n_predict=2 lr=1.0559 bs=262144 train_time:689876ms step_avg:598.85ms
step:1153/1775 loss.item()=14578.3974609375 n_predict=2 lr=1.0543 bs=262144 train_time:690597ms step_avg:598.96ms
step:1154/1775 loss.item()=15027.0947265625 n_predict=2 lr=1.0527 bs=262144 train_time:691321ms step_avg:599.07ms
step:1155/1775 loss.item()=14830.701171875 n_predict=2 lr=1.0510 bs=262144 train_time:692044ms step_avg:599.17ms
step:1156/1775 loss.item()=15330.1669921875 n_predict=2 lr=1.0494 bs=262144 train_time:692766ms step_avg:599.28ms
step:1157/1775 loss.item()=14814.546875 n_predict=2 lr=1.0478 bs=262144 train_time:693484ms step_avg:599.38ms
step:1158/1775 loss.item()=22197.052734375 n_predict=1 lr=1.1860 bs=393216 train_time:743806ms step_avg:642.32ms
step:1159/1775 loss.item()=21939.53125 n_predict=1 lr=1.1842 bs=393216 train_time:786077ms step_avg:678.24ms
step:1160/1775 loss.item()=21412.40625 n_predict=1 lr=1.1823 bs=393216 train_time:787126ms step_avg:678.56ms
step:1161/1775 loss.item()=22623.05859375 n_predict=1 lr=1.1804 bs=393216 train_time:788171ms step_avg:678.87ms
step:1162/1775 loss.item()=21876.46875 n_predict=1 lr=1.1785 bs=393216 train_time:789218ms step_avg:679.19ms
step:1163/1775 loss.item()=21880.62109375 n_predict=1 lr=1.1766 bs=393216 train_time:790261ms step_avg:679.50ms
step:1164/1775 loss.item()=22437.267578125 n_predict=1 lr=1.1748 bs=393216 train_time:791306ms step_avg:679.82ms
step:1165/1775 loss.item()=22001.537109375 n_predict=1 lr=1.1729 bs=393216 train_time:792350ms step_avg:680.13ms
step:1166/1775 loss.item()=22376.953125 n_predict=1 lr=1.1710 bs=393216 train_time:793397ms step_avg:680.44ms
step:1167/1775 loss.item()=21559.4921875 n_predict=1 lr=1.1691 bs=393216 train_time:794443ms step_avg:680.76ms
step:1168/1775 loss.item()=22429.9921875 n_predict=1 lr=1.1673 bs=393216 train_time:795490ms step_avg:681.07ms
step:1169/1775 loss.item()=22287.4296875 n_predict=1 lr=1.1654 bs=393216 train_time:796533ms step_avg:681.38ms
step:1170/1775 loss.item()=21656.3515625 n_predict=1 lr=1.1635 bs=393216 train_time:797579ms step_avg:681.69ms
step:1171/1775 loss.item()=21825.40625 n_predict=1 lr=1.1616 bs=393216 train_time:798625ms step_avg:682.00ms
step:1172/1775 loss.item()=22219.119140625 n_predict=1 lr=1.1597 bs=393216 train_time:799675ms step_avg:682.32ms
step:1173/1775 loss.item()=22864.986328125 n_predict=1 lr=1.1579 bs=393216 train_time:800724ms step_avg:682.63ms
step:1174/1775 loss.item()=21315.06640625 n_predict=1 lr=1.1560 bs=393216 train_time:801774ms step_avg:682.94ms
step:1175/1775 loss.item()=21607.205078125 n_predict=1 lr=1.1541 bs=393216 train_time:802823ms step_avg:683.25ms
step:1176/1775 loss.item()=21873.6484375 n_predict=1 lr=1.1522 bs=393216 train_time:803867ms step_avg:683.56ms
step:1177/1775 loss.item()=22261.1328125 n_predict=1 lr=1.1503 bs=393216 train_time:804913ms step_avg:683.87ms
step:1178/1775 loss.item()=21257.15625 n_predict=1 lr=1.1485 bs=393216 train_time:805960ms step_avg:684.18ms
step:1179/1775 loss.item()=22522.73046875 n_predict=1 lr=1.1466 bs=393216 train_time:807004ms step_avg:684.48ms
step:1180/1775 loss.item()=21931.427734375 n_predict=1 lr=1.1447 bs=393216 train_time:808053ms step_avg:684.79ms
step:1181/1775 loss.item()=21516.859375 n_predict=1 lr=1.1428 bs=393216 train_time:809102ms step_avg:685.10ms
step:1182/1775 loss.item()=21648.86328125 n_predict=1 lr=1.1409 bs=393216 train_time:810154ms step_avg:685.41ms
step:1183/1775 loss.item()=22061.80078125 n_predict=1 lr=1.1391 bs=393216 train_time:811200ms step_avg:685.71ms
step:1184/1775 loss.item()=22375.24609375 n_predict=1 lr=1.1372 bs=393216 train_time:812249ms step_avg:686.02ms
step:1185/1775 loss.item()=21075.755859375 n_predict=1 lr=1.1353 bs=393216 train_time:813296ms step_avg:686.33ms
step:1186/1775 loss.item()=21190.61328125 n_predict=1 lr=1.1334 bs=393216 train_time:814348ms step_avg:686.63ms
step:1187/1775 loss.item()=22244.6015625 n_predict=1 lr=1.1316 bs=393216 train_time:815393ms step_avg:686.94ms
step:1188/1775 loss.item()=21209.041015625 n_predict=1 lr=1.1297 bs=393216 train_time:816445ms step_avg:687.24ms
step:1189/1775 loss.item()=21419.34765625 n_predict=1 lr=1.1278 bs=393216 train_time:817494ms step_avg:687.55ms
step:1190/1775 loss.item()=21578.41015625 n_predict=1 lr=1.1259 bs=393216 train_time:818547ms step_avg:687.85ms
step:1191/1775 loss.item()=21728.869140625 n_predict=1 lr=1.1240 bs=393216 train_time:819596ms step_avg:688.16ms
step:1192/1775 loss.item()=21675.40234375 n_predict=1 lr=1.1222 bs=393216 train_time:820647ms step_avg:688.46ms
step:1193/1775 loss.item()=21887.265625 n_predict=1 lr=1.1203 bs=393216 train_time:821695ms step_avg:688.76ms
step:1194/1775 loss.item()=20797.142578125 n_predict=1 lr=1.1184 bs=393216 train_time:822746ms step_avg:689.07ms
step:1195/1775 loss.item()=22202.203125 n_predict=1 lr=1.1165 bs=393216 train_time:823795ms step_avg:689.37ms
step:1196/1775 loss.item()=21708.3203125 n_predict=1 lr=1.1146 bs=393216 train_time:824848ms step_avg:689.67ms
step:1197/1775 loss.item()=21627.32421875 n_predict=1 lr=1.1128 bs=393216 train_time:825896ms step_avg:689.97ms
step:1198/1775 loss.item()=23225.31640625 n_predict=1 lr=1.1109 bs=393216 train_time:826950ms step_avg:690.28ms
step:1199/1775 loss.item()=21954.5625 n_predict=1 lr=1.1090 bs=393216 train_time:827996ms step_avg:690.57ms
step:1200/1775 loss.item()=22078.8671875 n_predict=1 lr=1.1071 bs=393216 train_time:829049ms step_avg:690.87ms
step:1201/1775 loss.item()=21721.78125 n_predict=1 lr=1.1052 bs=393216 train_time:830096ms step_avg:691.17ms
step:1202/1775 loss.item()=21368.890625 n_predict=1 lr=1.1034 bs=393216 train_time:831148ms step_avg:691.47ms
step:1203/1775 loss.item()=21571.046875 n_predict=1 lr=1.1015 bs=393216 train_time:832198ms step_avg:691.77ms
step:1204/1775 loss.item()=21762.0234375 n_predict=1 lr=1.0996 bs=393216 train_time:833251ms step_avg:692.07ms
step:1205/1775 loss.item()=21620.98046875 n_predict=1 lr=1.0977 bs=393216 train_time:834300ms step_avg:692.36ms
step:1206/1775 loss.item()=21810.986328125 n_predict=1 lr=1.0959 bs=393216 train_time:835353ms step_avg:692.66ms
step:1207/1775 loss.item()=21992.02734375 n_predict=1 lr=1.0940 bs=393216 train_time:836407ms step_avg:692.96ms
step:1208/1775 loss.item()=22590.2109375 n_predict=1 lr=1.0921 bs=393216 train_time:837458ms step_avg:693.26ms
step:1209/1775 loss.item()=22351.146484375 n_predict=1 lr=1.0902 bs=393216 train_time:838507ms step_avg:693.55ms
step:1210/1775 loss.item()=21357.59375 n_predict=1 lr=1.0883 bs=393216 train_time:839561ms step_avg:693.85ms
step:1211/1775 loss.item()=22116.91796875 n_predict=1 lr=1.0865 bs=393216 train_time:840614ms step_avg:694.15ms
step:1212/1775 loss.item()=21192.419921875 n_predict=1 lr=1.0846 bs=393216 train_time:841669ms step_avg:694.45ms
step:1213/1775 loss.item()=20752.75390625 n_predict=1 lr=1.0827 bs=393216 train_time:842722ms step_avg:694.74ms
step:1214/1775 loss.item()=21974.0703125 n_predict=1 lr=1.0808 bs=393216 train_time:843781ms step_avg:695.04ms
step:1215/1775 loss.item()=21866.2734375 n_predict=1 lr=1.0789 bs=393216 train_time:844836ms step_avg:695.34ms
step:1216/1775 loss.item()=21986.912109375 n_predict=1 lr=1.0771 bs=393216 train_time:845893ms step_avg:695.64ms
step:1217/1775 loss.item()=21070.474609375 n_predict=1 lr=1.0752 bs=393216 train_time:846952ms step_avg:695.93ms
step:1218/1775 loss.item()=21901.0546875 n_predict=1 lr=1.0733 bs=393216 train_time:848006ms step_avg:696.23ms
step:1219/1775 loss.item()=21741.76953125 n_predict=1 lr=1.0714 bs=393216 train_time:849058ms step_avg:696.52ms
step:1220/1775 loss.item()=21636.951171875 n_predict=1 lr=1.0695 bs=393216 train_time:850113ms step_avg:696.81ms
step:1221/1775 loss.item()=21187.033203125 n_predict=1 lr=1.0677 bs=393216 train_time:851168ms step_avg:697.11ms
step:1222/1775 loss.item()=22485.75 n_predict=1 lr=1.0658 bs=393216 train_time:852220ms step_avg:697.40ms
step:1223/1775 loss.item()=22171.931640625 n_predict=1 lr=1.0639 bs=393216 train_time:853276ms step_avg:697.69ms
step:1224/1775 loss.item()=21817.63671875 n_predict=1 lr=1.0620 bs=393216 train_time:854329ms step_avg:697.98ms
step:1225/1775 loss.item()=21774.625 n_predict=1 lr=1.0601 bs=393216 train_time:855387ms step_avg:698.27ms
step:1226/1775 loss.item()=21931.3203125 n_predict=1 lr=1.0583 bs=393216 train_time:856442ms step_avg:698.57ms
step:1227/1775 loss.item()=22133.2421875 n_predict=1 lr=1.0564 bs=393216 train_time:857496ms step_avg:698.86ms
step:1228/1775 loss.item()=21276.8359375 n_predict=1 lr=1.0545 bs=393216 train_time:858551ms step_avg:699.15ms
step:1229/1775 loss.item()=21668.41796875 n_predict=1 lr=1.0526 bs=393216 train_time:859605ms step_avg:699.43ms
step:1230/1775 loss.item()=21863.48828125 n_predict=1 lr=1.0508 bs=393216 train_time:860661ms step_avg:699.72ms
step:1231/1775 loss.item()=21762.5390625 n_predict=1 lr=1.0489 bs=393216 train_time:861717ms step_avg:700.01ms
step:1232/1775 loss.item()=21148.34765625 n_predict=1 lr=1.0470 bs=393216 train_time:862774ms step_avg:700.30ms
step:1233/1775 loss.item()=21995.75 n_predict=1 lr=1.0451 bs=393216 train_time:863828ms step_avg:700.59ms
step:1234/1775 loss.item()=21941.99609375 n_predict=1 lr=1.0432 bs=393216 train_time:864886ms step_avg:700.88ms
step:1235/1775 loss.item()=21178.103515625 n_predict=1 lr=1.0414 bs=393216 train_time:865938ms step_avg:701.16ms
step:1236/1775 loss.item()=21604.9765625 n_predict=1 lr=1.0395 bs=393216 train_time:866998ms step_avg:701.45ms
step:1237/1775 loss.item()=21490.232421875 n_predict=1 lr=1.0376 bs=393216 train_time:868052ms step_avg:701.74ms
step:1238/1775 loss.item()=22164.2109375 n_predict=1 lr=1.0357 bs=393216 train_time:869110ms step_avg:702.03ms
step:1239/1775 loss.item()=21457.09765625 n_predict=1 lr=1.0338 bs=393216 train_time:870167ms step_avg:702.31ms
step:1240/1775 loss.item()=22561.0390625 n_predict=1 lr=1.0320 bs=393216 train_time:871222ms step_avg:702.60ms
step:1241/1775 loss.item()=22033.93359375 n_predict=1 lr=1.0301 bs=393216 train_time:872277ms step_avg:702.88ms
step:1242/1775 loss.item()=22356.88671875 n_predict=1 lr=1.0282 bs=393216 train_time:873332ms step_avg:703.17ms
step:1243/1775 loss.item()=21683.4375 n_predict=1 lr=1.0263 bs=393216 train_time:874388ms step_avg:703.45ms
step:1244/1775 loss.item()=22116.51171875 n_predict=1 lr=1.0244 bs=393216 train_time:875445ms step_avg:703.73ms
step:1245/1775 loss.item()=21773.984375 n_predict=1 lr=1.0226 bs=393216 train_time:876499ms step_avg:704.02ms
step:1246/1775 loss.item()=21503.439453125 n_predict=1 lr=1.0207 bs=393216 train_time:877555ms step_avg:704.30ms
step:1247/1775 loss.item()=21648.63671875 n_predict=1 lr=1.0188 bs=393216 train_time:878612ms step_avg:704.58ms
step:1248/1775 loss.item()=22191.728515625 n_predict=1 lr=1.0169 bs=393216 train_time:879669ms step_avg:704.86ms
step:1249/1775 loss.item()=21609.65234375 n_predict=1 lr=1.0151 bs=393216 train_time:880726ms step_avg:705.15ms
step:1250/1775 loss.item()=21008.0625 n_predict=1 lr=1.0132 bs=393216 train_time:881783ms step_avg:705.43ms
step:1250/1775 lr=1.0113 bs=393216 n_predict=1 val_loss:3.5127 val_malbo_loss:3.5127 train_time:881944ms step_avg:705.56ms
step:1251/1775 loss.item()=21614.19140625 n_predict=1 lr=1.0113 bs=393216 train_time:882834ms step_avg:705.70ms
step:1252/1775 loss.item()=21473.837890625 n_predict=1 lr=1.0094 bs=393216 train_time:883888ms step_avg:705.98ms
step:1253/1775 loss.item()=21126.16796875 n_predict=1 lr=1.0075 bs=393216 train_time:884936ms step_avg:706.25ms
step:1254/1775 loss.item()=21470.4453125 n_predict=1 lr=1.0057 bs=393216 train_time:885988ms step_avg:706.53ms
step:1255/1775 loss.item()=21500.37109375 n_predict=1 lr=1.0038 bs=393216 train_time:887041ms step_avg:706.81ms
step:1256/1775 loss.item()=22177.486328125 n_predict=1 lr=1.0019 bs=393216 train_time:888094ms step_avg:707.08ms
step:1257/1775 loss.item()=21246.62890625 n_predict=1 lr=1.0000 bs=393216 train_time:889145ms step_avg:707.35ms
step:1258/1775 loss.item()=22580.8828125 n_predict=1 lr=0.9981 bs=393216 train_time:890198ms step_avg:707.63ms
step:1259/1775 loss.item()=21172.890625 n_predict=1 lr=0.9963 bs=393216 train_time:891246ms step_avg:707.90ms
step:1260/1775 loss.item()=20954.53125 n_predict=1 lr=0.9944 bs=393216 train_time:892298ms step_avg:708.17ms
step:1261/1775 loss.item()=21694.9765625 n_predict=1 lr=0.9925 bs=393216 train_time:893350ms step_avg:708.45ms
step:1262/1775 loss.item()=21371.8984375 n_predict=1 lr=0.9906 bs=393216 train_time:894407ms step_avg:708.72ms
step:1263/1775 loss.item()=21844.76171875 n_predict=1 lr=0.9887 bs=393216 train_time:895460ms step_avg:708.99ms
step:1264/1775 loss.item()=21172.177734375 n_predict=1 lr=0.9869 bs=393216 train_time:896515ms step_avg:709.27ms
step:1265/1775 loss.item()=21301.046875 n_predict=1 lr=0.9850 bs=393216 train_time:897568ms step_avg:709.54ms
step:1266/1775 loss.item()=21700.130859375 n_predict=1 lr=0.9831 bs=393216 train_time:898626ms step_avg:709.81ms
step:1267/1775 loss.item()=21931.6484375 n_predict=1 lr=0.9812 bs=393216 train_time:899678ms step_avg:710.09ms
step:1268/1775 loss.item()=21040.01953125 n_predict=1 lr=0.9794 bs=393216 train_time:900735ms step_avg:710.36ms
step:1269/1775 loss.item()=22095.4453125 n_predict=1 lr=0.9775 bs=393216 train_time:901787ms step_avg:710.63ms
step:1270/1775 loss.item()=21360.35546875 n_predict=1 lr=0.9756 bs=393216 train_time:902842ms step_avg:710.90ms
step:1271/1775 loss.item()=22002.09375 n_predict=1 lr=0.9737 bs=393216 train_time:903895ms step_avg:711.17ms
step:1272/1775 loss.item()=21133.912109375 n_predict=1 lr=0.9718 bs=393216 train_time:904951ms step_avg:711.44ms
step:1273/1775 loss.item()=21302.998046875 n_predict=1 lr=0.9700 bs=393216 train_time:906005ms step_avg:711.71ms
step:1274/1775 loss.item()=22100.01953125 n_predict=1 lr=0.9681 bs=393216 train_time:907062ms step_avg:711.98ms
step:1275/1775 loss.item()=21217.8359375 n_predict=1 lr=0.9662 bs=393216 train_time:908115ms step_avg:712.25ms
step:1276/1775 loss.item()=21777.42578125 n_predict=1 lr=0.9643 bs=393216 train_time:909168ms step_avg:712.51ms
step:1277/1775 loss.item()=20756.015625 n_predict=1 lr=0.9624 bs=393216 train_time:910217ms step_avg:712.78ms
step:1278/1775 loss.item()=21072.1171875 n_predict=1 lr=0.9606 bs=393216 train_time:911274ms step_avg:713.05ms
step:1279/1775 loss.item()=20987.60546875 n_predict=1 lr=0.9587 bs=393216 train_time:912327ms step_avg:713.31ms
step:1280/1775 loss.item()=22240.033203125 n_predict=1 lr=0.9568 bs=393216 train_time:913381ms step_avg:713.58ms
step:1281/1775 loss.item()=20972.607421875 n_predict=1 lr=0.9549 bs=393216 train_time:914436ms step_avg:713.85ms
step:1282/1775 loss.item()=21373.17578125 n_predict=1 lr=0.9530 bs=393216 train_time:915488ms step_avg:714.11ms
step:1283/1775 loss.item()=21538.1484375 n_predict=1 lr=0.9512 bs=393216 train_time:916544ms step_avg:714.38ms
step:1284/1775 loss.item()=21233.751953125 n_predict=1 lr=0.9493 bs=393216 train_time:917600ms step_avg:714.64ms
step:1285/1775 loss.item()=21409.6015625 n_predict=1 lr=0.9474 bs=393216 train_time:918654ms step_avg:714.91ms
step:1286/1775 loss.item()=21485.166015625 n_predict=1 lr=0.9455 bs=393216 train_time:919711ms step_avg:715.17ms
step:1287/1775 loss.item()=21935.42578125 n_predict=1 lr=0.9437 bs=393216 train_time:920766ms step_avg:715.44ms
step:1288/1775 loss.item()=21658.91015625 n_predict=1 lr=0.9418 bs=393216 train_time:921822ms step_avg:715.70ms
step:1289/1775 loss.item()=21289.486328125 n_predict=1 lr=0.9399 bs=393216 train_time:922874ms step_avg:715.96ms
step:1290/1775 loss.item()=22250.765625 n_predict=1 lr=0.9380 bs=393216 train_time:923930ms step_avg:716.22ms
step:1291/1775 loss.item()=21191.44921875 n_predict=1 lr=0.9361 bs=393216 train_time:924980ms step_avg:716.48ms
step:1292/1775 loss.item()=21740.39453125 n_predict=1 lr=0.9343 bs=393216 train_time:926039ms step_avg:716.75ms
step:1293/1775 loss.item()=21567.82421875 n_predict=1 lr=0.9324 bs=393216 train_time:927093ms step_avg:717.01ms
step:1294/1775 loss.item()=21220.1484375 n_predict=1 lr=0.9305 bs=393216 train_time:928150ms step_avg:717.27ms
step:1295/1775 loss.item()=21696.72265625 n_predict=1 lr=0.9286 bs=393216 train_time:929204ms step_avg:717.53ms
step:1296/1775 loss.item()=21617.40234375 n_predict=1 lr=0.9267 bs=393216 train_time:930259ms step_avg:717.79ms
step:1297/1775 loss.item()=21389.322265625 n_predict=1 lr=0.9249 bs=393216 train_time:931315ms step_avg:718.05ms
step:1298/1775 loss.item()=21120.66796875 n_predict=1 lr=0.9230 bs=393216 train_time:932374ms step_avg:718.32ms
step:1299/1775 loss.item()=21462.8125 n_predict=1 lr=0.9211 bs=393216 train_time:933430ms step_avg:718.58ms
step:1300/1775 loss.item()=20934.890625 n_predict=1 lr=0.9192 bs=393216 train_time:934488ms step_avg:718.84ms
step:1301/1775 loss.item()=20949.8046875 n_predict=1 lr=0.9173 bs=393216 train_time:935545ms step_avg:719.10ms
step:1302/1775 loss.item()=21250.2109375 n_predict=1 lr=0.9155 bs=393216 train_time:936601ms step_avg:719.36ms
step:1303/1775 loss.item()=22230.73828125 n_predict=1 lr=0.9136 bs=393216 train_time:937658ms step_avg:719.61ms
step:1304/1775 loss.item()=21591.201171875 n_predict=1 lr=0.9117 bs=393216 train_time:938717ms step_avg:719.88ms
step:1305/1775 loss.item()=21518.34765625 n_predict=1 lr=0.9098 bs=393216 train_time:939771ms step_avg:720.13ms
step:1306/1775 loss.item()=21605.33203125 n_predict=1 lr=0.9080 bs=393216 train_time:940834ms step_avg:720.39ms
step:1307/1775 loss.item()=21499.62109375 n_predict=1 lr=0.9061 bs=393216 train_time:941889ms step_avg:720.65ms
step:1308/1775 loss.item()=21012.25390625 n_predict=1 lr=0.9042 bs=393216 train_time:942948ms step_avg:720.91ms
step:1309/1775 loss.item()=21259.953125 n_predict=1 lr=0.9023 bs=393216 train_time:944006ms step_avg:721.17ms
step:1310/1775 loss.item()=21276.9453125 n_predict=1 lr=0.9004 bs=393216 train_time:945064ms step_avg:721.42ms
step:1311/1775 loss.item()=21348.18359375 n_predict=1 lr=0.8986 bs=393216 train_time:946118ms step_avg:721.68ms
step:1312/1775 loss.item()=21608.2578125 n_predict=1 lr=0.8967 bs=393216 train_time:947177ms step_avg:721.93ms
step:1313/1775 loss.item()=21376.525390625 n_predict=1 lr=0.8948 bs=393216 train_time:948234ms step_avg:722.19ms
step:1314/1775 loss.item()=21271.150390625 n_predict=1 lr=0.8929 bs=393216 train_time:949290ms step_avg:722.44ms
step:1315/1775 loss.item()=20873.029296875 n_predict=1 lr=0.8910 bs=393216 train_time:950346ms step_avg:722.70ms
step:1316/1775 loss.item()=21974.71484375 n_predict=1 lr=0.8892 bs=393216 train_time:951405ms step_avg:722.95ms
step:1317/1775 loss.item()=21879.529296875 n_predict=1 lr=0.8873 bs=393216 train_time:952463ms step_avg:723.21ms
step:1318/1775 loss.item()=21561.76171875 n_predict=1 lr=0.8854 bs=393216 train_time:953519ms step_avg:723.46ms
step:1319/1775 loss.item()=21492.42578125 n_predict=1 lr=0.8835 bs=393216 train_time:954576ms step_avg:723.71ms
step:1320/1775 loss.item()=21122.1015625 n_predict=1 lr=0.8816 bs=393216 train_time:955634ms step_avg:723.97ms
step:1321/1775 loss.item()=22284.5703125 n_predict=1 lr=0.8798 bs=393216 train_time:956689ms step_avg:724.22ms
step:1322/1775 loss.item()=21846.98046875 n_predict=1 lr=0.8779 bs=393216 train_time:957746ms step_avg:724.47ms
step:1323/1775 loss.item()=21567.03125 n_predict=1 lr=0.8760 bs=393216 train_time:958806ms step_avg:724.72ms
step:1324/1775 loss.item()=21843.3203125 n_predict=1 lr=0.8741 bs=393216 train_time:959866ms step_avg:724.97ms
step:1325/1775 loss.item()=20747.466796875 n_predict=1 lr=0.8723 bs=393216 train_time:960923ms step_avg:725.22ms
step:1326/1775 loss.item()=21862.662109375 n_predict=1 lr=0.8704 bs=393216 train_time:961982ms step_avg:725.48ms
step:1327/1775 loss.item()=21246.4765625 n_predict=1 lr=0.8685 bs=393216 train_time:963039ms step_avg:725.73ms
step:1328/1775 loss.item()=21428.57421875 n_predict=1 lr=0.8666 bs=393216 train_time:964096ms step_avg:725.98ms
step:1329/1775 loss.item()=21684.234375 n_predict=1 lr=0.8647 bs=393216 train_time:965152ms step_avg:726.22ms
step:1330/1775 loss.item()=21119.19140625 n_predict=1 lr=0.8629 bs=393216 train_time:966213ms step_avg:726.48ms
step:1331/1775 loss.item()=21024.484375 n_predict=1 lr=0.8610 bs=393216 train_time:967273ms step_avg:726.73ms
step:1332/1775 loss.item()=21641.736328125 n_predict=1 lr=0.8591 bs=393216 train_time:968331ms step_avg:726.97ms
step:1333/1775 loss.item()=21147.7265625 n_predict=1 lr=0.8572 bs=393216 train_time:969387ms step_avg:727.22ms
step:1334/1775 loss.item()=21206.08984375 n_predict=1 lr=0.8553 bs=393216 train_time:970447ms step_avg:727.47ms
step:1335/1775 loss.item()=21056.66796875 n_predict=1 lr=0.8535 bs=393216 train_time:971500ms step_avg:727.72ms
step:1336/1775 loss.item()=21433.943359375 n_predict=1 lr=0.8516 bs=393216 train_time:972560ms step_avg:727.96ms
step:1337/1775 loss.item()=21590.00390625 n_predict=1 lr=0.8497 bs=393216 train_time:973620ms step_avg:728.21ms
step:1338/1775 loss.item()=21546.2265625 n_predict=1 lr=0.8478 bs=393216 train_time:974681ms step_avg:728.46ms
step:1339/1775 loss.item()=21564.646484375 n_predict=1 lr=0.8459 bs=393216 train_time:975739ms step_avg:728.71ms
step:1340/1775 loss.item()=21392.5703125 n_predict=1 lr=0.8441 bs=393216 train_time:976797ms step_avg:728.95ms
step:1341/1775 loss.item()=21409.51953125 n_predict=1 lr=0.8422 bs=393216 train_time:977854ms step_avg:729.20ms
step:1342/1775 loss.item()=20953.125 n_predict=1 lr=0.8403 bs=393216 train_time:978911ms step_avg:729.44ms
step:1343/1775 loss.item()=21877.916015625 n_predict=1 lr=0.8384 bs=393216 train_time:979967ms step_avg:729.69ms
step:1344/1775 loss.item()=21268.9921875 n_predict=1 lr=0.8366 bs=393216 train_time:981030ms step_avg:729.93ms
step:1345/1775 loss.item()=20667.642578125 n_predict=1 lr=0.8347 bs=393216 train_time:982087ms step_avg:730.18ms
step:1346/1775 loss.item()=21459.59375 n_predict=1 lr=0.8328 bs=393216 train_time:983146ms step_avg:730.42ms
step:1347/1775 loss.item()=21583.06640625 n_predict=1 lr=0.8309 bs=393216 train_time:984201ms step_avg:730.66ms
step:1348/1775 loss.item()=22066.302734375 n_predict=1 lr=0.8290 bs=393216 train_time:985259ms step_avg:730.90ms
step:1349/1775 loss.item()=20871.5 n_predict=1 lr=0.8272 bs=393216 train_time:986316ms step_avg:731.15ms
step:1350/1775 loss.item()=21435.46484375 n_predict=1 lr=0.8253 bs=393216 train_time:987373ms step_avg:731.39ms
step:1351/1775 loss.item()=22410.18359375 n_predict=1 lr=0.8234 bs=393216 train_time:988429ms step_avg:731.63ms
step:1352/1775 loss.item()=21669.52734375 n_predict=1 lr=0.8215 bs=393216 train_time:989489ms step_avg:731.87ms
step:1353/1775 loss.item()=21528.251953125 n_predict=1 lr=0.8196 bs=393216 train_time:990548ms step_avg:732.11ms
step:1354/1775 loss.item()=20771.603515625 n_predict=1 lr=0.8178 bs=393216 train_time:991607ms step_avg:732.35ms
step:1355/1775 loss.item()=20867.619140625 n_predict=1 lr=0.8159 bs=393216 train_time:992668ms step_avg:732.60ms
step:1356/1775 loss.item()=21490.265625 n_predict=1 lr=0.8140 bs=393216 train_time:993728ms step_avg:732.84ms
step:1357/1775 loss.item()=21537.28125 n_predict=1 lr=0.8121 bs=393216 train_time:994784ms step_avg:733.08ms
step:1358/1775 loss.item()=21320.3671875 n_predict=1 lr=0.8102 bs=393216 train_time:995842ms step_avg:733.32ms
step:1359/1775 loss.item()=21032.40234375 n_predict=1 lr=0.8084 bs=393216 train_time:996901ms step_avg:733.55ms
step:1360/1775 loss.item()=20866.173828125 n_predict=1 lr=0.8065 bs=393216 train_time:997961ms step_avg:733.80ms
step:1361/1775 loss.item()=20946.703125 n_predict=1 lr=0.8046 bs=393216 train_time:999019ms step_avg:734.03ms
step:1362/1775 loss.item()=21125.85546875 n_predict=1 lr=0.8027 bs=393216 train_time:1000080ms step_avg:734.27ms
step:1363/1775 loss.item()=21659.16015625 n_predict=1 lr=0.8009 bs=393216 train_time:1001137ms step_avg:734.51ms
step:1364/1775 loss.item()=21524.27734375 n_predict=1 lr=0.7990 bs=393216 train_time:1002194ms step_avg:734.75ms
step:1365/1775 loss.item()=20540.3203125 n_predict=1 lr=0.7971 bs=393216 train_time:1003251ms step_avg:734.98ms
step:1366/1775 loss.item()=20745.515625 n_predict=1 lr=0.7952 bs=393216 train_time:1004310ms step_avg:735.22ms
step:1367/1775 loss.item()=21027.513671875 n_predict=1 lr=0.7933 bs=393216 train_time:1005368ms step_avg:735.46ms
step:1368/1775 loss.item()=21293.103515625 n_predict=1 lr=0.7915 bs=393216 train_time:1006428ms step_avg:735.69ms
step:1369/1775 loss.item()=21062.328125 n_predict=1 lr=0.7896 bs=393216 train_time:1007484ms step_avg:735.93ms
step:1370/1775 loss.item()=21748.580078125 n_predict=1 lr=0.7877 bs=393216 train_time:1008543ms step_avg:736.16ms
step:1371/1775 loss.item()=21054.4296875 n_predict=1 lr=0.7858 bs=393216 train_time:1009597ms step_avg:736.39ms
step:1372/1775 loss.item()=21027.609375 n_predict=1 lr=0.7839 bs=393216 train_time:1010658ms step_avg:736.63ms
step:1373/1775 loss.item()=21383.95703125 n_predict=1 lr=0.7821 bs=393216 train_time:1011714ms step_avg:736.86ms
step:1374/1775 loss.item()=21899.0078125 n_predict=1 lr=0.7802 bs=393216 train_time:1012775ms step_avg:737.10ms
step:1375/1775 loss.item()=20895.84765625 n_predict=1 lr=0.7783 bs=393216 train_time:1013831ms step_avg:737.33ms
step:1376/1775 loss.item()=21747.76953125 n_predict=1 lr=0.7764 bs=393216 train_time:1014889ms step_avg:737.56ms
step:1377/1775 loss.item()=21705.57421875 n_predict=1 lr=0.7745 bs=393216 train_time:1015944ms step_avg:737.80ms
step:1378/1775 loss.item()=21527.3046875 n_predict=1 lr=0.7727 bs=393216 train_time:1017001ms step_avg:738.03ms
step:1379/1775 loss.item()=21549.5703125 n_predict=1 lr=0.7708 bs=393216 train_time:1018058ms step_avg:738.26ms
step:1380/1775 loss.item()=20937.65234375 n_predict=1 lr=0.7689 bs=393216 train_time:1019118ms step_avg:738.49ms
step:1381/1775 loss.item()=21547.3203125 n_predict=1 lr=0.7670 bs=393216 train_time:1020172ms step_avg:738.72ms
step:1382/1775 loss.item()=21527.744140625 n_predict=1 lr=0.7652 bs=393216 train_time:1021235ms step_avg:738.95ms
step:1383/1775 loss.item()=21860.412109375 n_predict=1 lr=0.7633 bs=393216 train_time:1022293ms step_avg:739.18ms
step:1384/1775 loss.item()=20549.15234375 n_predict=1 lr=0.7614 bs=393216 train_time:1023353ms step_avg:739.42ms
step:1385/1775 loss.item()=21074.24609375 n_predict=1 lr=0.7595 bs=393216 train_time:1024409ms step_avg:739.65ms
step:1386/1775 loss.item()=20902.35546875 n_predict=1 lr=0.7576 bs=393216 train_time:1025470ms step_avg:739.88ms
step:1387/1775 loss.item()=20835.4140625 n_predict=1 lr=0.7558 bs=393216 train_time:1026526ms step_avg:740.11ms
step:1388/1775 loss.item()=21349.2890625 n_predict=1 lr=0.7539 bs=393216 train_time:1027588ms step_avg:740.34ms
step:1389/1775 loss.item()=21591.97265625 n_predict=1 lr=0.7520 bs=393216 train_time:1028643ms step_avg:740.56ms
step:1390/1775 loss.item()=21331.953125 n_predict=1 lr=0.7501 bs=393216 train_time:1029701ms step_avg:740.79ms
step:1391/1775 loss.item()=20884.294921875 n_predict=1 lr=0.7482 bs=393216 train_time:1030759ms step_avg:741.02ms
step:1392/1775 loss.item()=22017.1640625 n_predict=1 lr=0.7464 bs=393216 train_time:1031818ms step_avg:741.25ms
step:1393/1775 loss.item()=21192.78515625 n_predict=1 lr=0.7445 bs=393216 train_time:1032876ms step_avg:741.48ms
step:1394/1775 loss.item()=20539.10546875 n_predict=1 lr=0.7426 bs=393216 train_time:1033938ms step_avg:741.71ms
step:1395/1775 loss.item()=21656.875 n_predict=1 lr=0.7407 bs=393216 train_time:1034992ms step_avg:741.93ms
step:1396/1775 loss.item()=21056.955078125 n_predict=1 lr=0.7388 bs=393216 train_time:1036052ms step_avg:742.16ms
step:1397/1775 loss.item()=21427.1171875 n_predict=1 lr=0.7370 bs=393216 train_time:1037111ms step_avg:742.38ms
step:1398/1775 loss.item()=21187.822265625 n_predict=1 lr=0.7351 bs=393216 train_time:1038171ms step_avg:742.61ms
step:1399/1775 loss.item()=21683.578125 n_predict=1 lr=0.7332 bs=393216 train_time:1039226ms step_avg:742.83ms
step:1400/1775 loss.item()=21298.291015625 n_predict=1 lr=0.7313 bs=393216 train_time:1040282ms step_avg:743.06ms
step:1401/1775 loss.item()=20906.7109375 n_predict=1 lr=0.7295 bs=393216 train_time:1041339ms step_avg:743.28ms
step:1402/1775 loss.item()=21446.63671875 n_predict=1 lr=0.7276 bs=393216 train_time:1042400ms step_avg:743.51ms
step:1403/1775 loss.item()=20957.109375 n_predict=1 lr=0.7257 bs=393216 train_time:1043458ms step_avg:743.73ms
step:1404/1775 loss.item()=20819.33203125 n_predict=1 lr=0.7238 bs=393216 train_time:1044515ms step_avg:743.96ms
step:1405/1775 loss.item()=21223.10546875 n_predict=1 lr=0.7219 bs=393216 train_time:1045573ms step_avg:744.18ms
step:1406/1775 loss.item()=20842.47265625 n_predict=1 lr=0.7201 bs=393216 train_time:1046635ms step_avg:744.41ms
step:1407/1775 loss.item()=21001.552734375 n_predict=1 lr=0.7182 bs=393216 train_time:1047695ms step_avg:744.63ms
step:1408/1775 loss.item()=20563.30078125 n_predict=1 lr=0.7163 bs=393216 train_time:1048755ms step_avg:744.85ms
step:1409/1775 loss.item()=21113.36328125 n_predict=1 lr=0.7144 bs=393216 train_time:1049815ms step_avg:745.08ms
step:1410/1775 loss.item()=20907.359375 n_predict=1 lr=0.7125 bs=393216 train_time:1050875ms step_avg:745.30ms
step:1411/1775 loss.item()=20216.04296875 n_predict=1 lr=0.7107 bs=393216 train_time:1051932ms step_avg:745.52ms
step:1412/1775 loss.item()=21165.984375 n_predict=1 lr=0.7088 bs=393216 train_time:1052993ms step_avg:745.75ms
step:1413/1775 loss.item()=21360.060546875 n_predict=1 lr=0.7069 bs=393216 train_time:1054052ms step_avg:745.97ms
step:1414/1775 loss.item()=21119.7109375 n_predict=1 lr=0.7050 bs=393216 train_time:1055113ms step_avg:746.19ms
step:1415/1775 loss.item()=20990.12890625 n_predict=1 lr=0.7031 bs=393216 train_time:1056171ms step_avg:746.41ms
step:1416/1775 loss.item()=21991.12109375 n_predict=1 lr=0.7013 bs=393216 train_time:1057231ms step_avg:746.63ms
step:1417/1775 loss.item()=20822.49609375 n_predict=1 lr=0.6994 bs=393216 train_time:1058290ms step_avg:746.85ms
step:1418/1775 loss.item()=21659.876953125 n_predict=1 lr=0.6975 bs=393216 train_time:1059349ms step_avg:747.07ms
step:1419/1775 loss.item()=20139.703125 n_predict=1 lr=0.6956 bs=393216 train_time:1060408ms step_avg:747.29ms
step:1420/1775 loss.item()=21203.45703125 n_predict=1 lr=0.6938 bs=393216 train_time:1061468ms step_avg:747.51ms
step:1421/1775 loss.item()=20343.095703125 n_predict=1 lr=0.6919 bs=393216 train_time:1062527ms step_avg:747.73ms
step:1422/1775 loss.item()=20853.15234375 n_predict=1 lr=0.6900 bs=393216 train_time:1063587ms step_avg:747.95ms
step:1423/1775 loss.item()=21327.826171875 n_predict=1 lr=0.6881 bs=393216 train_time:1064643ms step_avg:748.17ms
step:1424/1775 loss.item()=21275.37890625 n_predict=1 lr=0.6862 bs=393216 train_time:1065703ms step_avg:748.39ms
step:1425/1775 loss.item()=21321.986328125 n_predict=1 lr=0.6844 bs=393216 train_time:1066758ms step_avg:748.60ms
step:1426/1775 loss.item()=21462.3125 n_predict=1 lr=0.6825 bs=393216 train_time:1067817ms step_avg:748.82ms
step:1427/1775 loss.item()=21018.546875 n_predict=1 lr=0.6806 bs=393216 train_time:1068871ms step_avg:749.03ms
step:1428/1775 loss.item()=21268.125 n_predict=1 lr=0.6787 bs=393216 train_time:1069933ms step_avg:749.25ms
step:1429/1775 loss.item()=21613.17578125 n_predict=1 lr=0.6768 bs=393216 train_time:1070993ms step_avg:749.47ms
step:1430/1775 loss.item()=20744.603515625 n_predict=1 lr=0.6750 bs=393216 train_time:1072054ms step_avg:749.69ms
step:1431/1775 loss.item()=21121.7578125 n_predict=1 lr=0.6731 bs=393216 train_time:1073115ms step_avg:749.91ms
step:1432/1775 loss.item()=21543.66796875 n_predict=1 lr=0.6712 bs=393216 train_time:1074175ms step_avg:750.12ms
step:1433/1775 loss.item()=21642.0625 n_predict=1 lr=0.6693 bs=393216 train_time:1075236ms step_avg:750.34ms
step:1434/1775 loss.item()=21778.01171875 n_predict=1 lr=0.6674 bs=393216 train_time:1076295ms step_avg:750.55ms
step:1435/1775 loss.item()=21035.171875 n_predict=1 lr=0.6656 bs=393216 train_time:1077350ms step_avg:750.77ms
step:1436/1775 loss.item()=21228.16796875 n_predict=1 lr=0.6637 bs=393216 train_time:1078410ms step_avg:750.98ms
step:1437/1775 loss.item()=21012.375 n_predict=1 lr=0.6618 bs=393216 train_time:1079467ms step_avg:751.20ms
step:1438/1775 loss.item()=21123.92578125 n_predict=1 lr=0.6599 bs=393216 train_time:1080528ms step_avg:751.41ms
step:1439/1775 loss.item()=20592.765625 n_predict=1 lr=0.6581 bs=393216 train_time:1081585ms step_avg:751.62ms
step:1440/1775 loss.item()=21034.24609375 n_predict=1 lr=0.6562 bs=393216 train_time:1082644ms step_avg:751.84ms
step:1441/1775 loss.item()=20533.52734375 n_predict=1 lr=0.6543 bs=393216 train_time:1083701ms step_avg:752.05ms
step:1442/1775 loss.item()=21552.0703125 n_predict=1 lr=0.6524 bs=393216 train_time:1084762ms step_avg:752.26ms
step:1443/1775 loss.item()=21039.62109375 n_predict=1 lr=0.6505 bs=393216 train_time:1085818ms step_avg:752.47ms
step:1444/1775 loss.item()=20683.087890625 n_predict=1 lr=0.6487 bs=393216 train_time:1086881ms step_avg:752.69ms
step:1445/1775 loss.item()=21055.7890625 n_predict=1 lr=0.6468 bs=393216 train_time:1087939ms step_avg:752.90ms
step:1446/1775 loss.item()=20662.5390625 n_predict=1 lr=0.6449 bs=393216 train_time:1089001ms step_avg:753.11ms
step:1447/1775 loss.item()=20629.92578125 n_predict=1 lr=0.6430 bs=393216 train_time:1090058ms step_avg:753.32ms
step:1448/1775 loss.item()=20858.3671875 n_predict=1 lr=0.6411 bs=393216 train_time:1091119ms step_avg:753.54ms
step:1449/1775 loss.item()=20654.87890625 n_predict=1 lr=0.6393 bs=393216 train_time:1092176ms step_avg:753.74ms
step:1450/1775 loss.item()=21026.171875 n_predict=1 lr=0.6374 bs=393216 train_time:1093235ms step_avg:753.96ms
step:1451/1775 loss.item()=20661.4140625 n_predict=1 lr=0.6355 bs=393216 train_time:1094294ms step_avg:754.17ms
step:1452/1775 loss.item()=21528.01171875 n_predict=1 lr=0.6336 bs=393216 train_time:1095353ms step_avg:754.38ms
step:1453/1775 loss.item()=21408.48046875 n_predict=1 lr=0.6317 bs=393216 train_time:1096410ms step_avg:754.58ms
step:1454/1775 loss.item()=20519.609375 n_predict=1 lr=0.6299 bs=393216 train_time:1097468ms step_avg:754.79ms
step:1455/1775 loss.item()=21731.701171875 n_predict=1 lr=0.6280 bs=393216 train_time:1098526ms step_avg:755.00ms
step:1456/1775 loss.item()=20637.224609375 n_predict=1 lr=0.6261 bs=393216 train_time:1099585ms step_avg:755.21ms
step:1457/1775 loss.item()=22447.4609375 n_predict=1 lr=0.6242 bs=393216 train_time:1100638ms step_avg:755.41ms
step:1458/1775 loss.item()=20844.22265625 n_predict=1 lr=0.6224 bs=393216 train_time:1101701ms step_avg:755.62ms
step:1459/1775 loss.item()=20467.025390625 n_predict=1 lr=0.6205 bs=393216 train_time:1102759ms step_avg:755.83ms
step:1460/1775 loss.item()=20641.486328125 n_predict=1 lr=0.6186 bs=393216 train_time:1103820ms step_avg:756.04ms
step:1461/1775 loss.item()=21099.69921875 n_predict=1 lr=0.6167 bs=393216 train_time:1104880ms step_avg:756.25ms
step:1462/1775 loss.item()=20531.9609375 n_predict=1 lr=0.6148 bs=393216 train_time:1105941ms step_avg:756.46ms
step:1463/1775 loss.item()=20713.482421875 n_predict=1 lr=0.6130 bs=393216 train_time:1106999ms step_avg:756.66ms
step:1464/1775 loss.item()=20975.83984375 n_predict=1 lr=0.6111 bs=393216 train_time:1108060ms step_avg:756.87ms
step:1465/1775 loss.item()=20890.828125 n_predict=1 lr=0.6092 bs=393216 train_time:1109118ms step_avg:757.08ms
step:1466/1775 loss.item()=20568.677734375 n_predict=1 lr=0.6073 bs=393216 train_time:1110180ms step_avg:757.28ms
step:1467/1775 loss.item()=20488.046875 n_predict=1 lr=0.6054 bs=393216 train_time:1111239ms step_avg:757.49ms
step:1468/1775 loss.item()=21197.08984375 n_predict=1 lr=0.6036 bs=393216 train_time:1112298ms step_avg:757.70ms
step:1469/1775 loss.item()=20765.265625 n_predict=1 lr=0.6017 bs=393216 train_time:1113352ms step_avg:757.90ms
step:1470/1775 loss.item()=20780.158203125 n_predict=1 lr=0.5998 bs=393216 train_time:1114414ms step_avg:758.10ms
step:1471/1775 loss.item()=21024.4375 n_predict=1 lr=0.5979 bs=393216 train_time:1115469ms step_avg:758.31ms
step:1472/1775 loss.item()=21114.205078125 n_predict=1 lr=0.5960 bs=393216 train_time:1116530ms step_avg:758.51ms
step:1473/1775 loss.item()=20523.5078125 n_predict=1 lr=0.5942 bs=393216 train_time:1117585ms step_avg:758.71ms
step:1474/1775 loss.item()=21562.396484375 n_predict=1 lr=0.5923 bs=393216 train_time:1118641ms step_avg:758.92ms
step:1475/1775 loss.item()=20615.8984375 n_predict=1 lr=0.5904 bs=393216 train_time:1119699ms step_avg:759.12ms
step:1476/1775 loss.item()=20297.8515625 n_predict=1 lr=0.5885 bs=393216 train_time:1120757ms step_avg:759.32ms
step:1477/1775 loss.item()=20184.16015625 n_predict=1 lr=0.5867 bs=393216 train_time:1121817ms step_avg:759.52ms
step:1478/1775 loss.item()=21012.84375 n_predict=1 lr=0.5848 bs=393216 train_time:1122873ms step_avg:759.72ms
step:1479/1775 loss.item()=21209.8203125 n_predict=1 lr=0.5829 bs=393216 train_time:1123931ms step_avg:759.93ms
step:1480/1775 loss.item()=21171.873046875 n_predict=1 lr=0.5810 bs=393216 train_time:1124991ms step_avg:760.13ms
step:1481/1775 loss.item()=21273.841796875 n_predict=1 lr=0.5791 bs=393216 train_time:1126048ms step_avg:760.33ms
step:1482/1775 loss.item()=21359.203125 n_predict=1 lr=0.5773 bs=393216 train_time:1127107ms step_avg:760.53ms
step:1483/1775 loss.item()=20478.6875 n_predict=1 lr=0.5754 bs=393216 train_time:1128166ms step_avg:760.73ms
step:1484/1775 loss.item()=21258.26171875 n_predict=1 lr=0.5735 bs=393216 train_time:1129227ms step_avg:760.93ms
step:1485/1775 loss.item()=20363.267578125 n_predict=1 lr=0.5716 bs=393216 train_time:1130284ms step_avg:761.13ms
step:1486/1775 loss.item()=20939.966796875 n_predict=1 lr=0.5697 bs=393216 train_time:1131343ms step_avg:761.33ms
step:1487/1775 loss.item()=21307.09765625 n_predict=1 lr=0.5679 bs=393216 train_time:1132400ms step_avg:761.53ms
step:1488/1775 loss.item()=20968.890625 n_predict=1 lr=0.5660 bs=393216 train_time:1133459ms step_avg:761.73ms
step:1489/1775 loss.item()=21548.2578125 n_predict=1 lr=0.5641 bs=393216 train_time:1134517ms step_avg:761.93ms
step:1490/1775 loss.item()=20536.3203125 n_predict=1 lr=0.5622 bs=393216 train_time:1135574ms step_avg:762.13ms
step:1491/1775 loss.item()=22116.40234375 n_predict=1 lr=0.5603 bs=393216 train_time:1136632ms step_avg:762.33ms
step:1492/1775 loss.item()=20406.146484375 n_predict=1 lr=0.5585 bs=393216 train_time:1137695ms step_avg:762.53ms
step:1493/1775 loss.item()=20197.5234375 n_predict=1 lr=0.5566 bs=393216 train_time:1138751ms step_avg:762.73ms
step:1494/1775 loss.item()=20585.79296875 n_predict=1 lr=0.5547 bs=393216 train_time:1139810ms step_avg:762.93ms
step:1495/1775 loss.item()=20438.25 n_predict=1 lr=0.5528 bs=393216 train_time:1140868ms step_avg:763.12ms
step:1496/1775 loss.item()=20485.08984375 n_predict=1 lr=0.5510 bs=393216 train_time:1141928ms step_avg:763.32ms
step:1497/1775 loss.item()=20602.38671875 n_predict=1 lr=0.5491 bs=393216 train_time:1142982ms step_avg:763.51ms
step:1498/1775 loss.item()=20663.078125 n_predict=1 lr=0.5472 bs=393216 train_time:1144040ms step_avg:763.71ms
step:1499/1775 loss.item()=21029.765625 n_predict=1 lr=0.5453 bs=393216 train_time:1145099ms step_avg:763.91ms
step:1500/1775 loss.item()=21597.720703125 n_predict=1 lr=0.5434 bs=393216 train_time:1146158ms step_avg:764.11ms
step:1500/1775 lr=0.5416 bs=393216 n_predict=1 val_loss:3.3816 val_malbo_loss:3.3816 train_time:1146319ms step_avg:764.21ms
step:1501/1775 loss.item()=21458.130859375 n_predict=1 lr=0.5416 bs=393216 train_time:1147215ms step_avg:764.30ms
step:1502/1775 loss.item()=20460.662109375 n_predict=1 lr=0.5397 bs=393216 train_time:1148277ms step_avg:764.50ms
step:1503/1775 loss.item()=20764.75 n_predict=1 lr=0.5378 bs=393216 train_time:1149334ms step_avg:764.69ms
step:1504/1775 loss.item()=20816.1015625 n_predict=1 lr=0.5359 bs=393216 train_time:1150397ms step_avg:764.89ms
step:1505/1775 loss.item()=20626.90625 n_predict=1 lr=0.5340 bs=393216 train_time:1151454ms step_avg:765.09ms
step:1506/1775 loss.item()=20518.48046875 n_predict=1 lr=0.5322 bs=393216 train_time:1152514ms step_avg:765.28ms
step:1507/1775 loss.item()=19865.7890625 n_predict=1 lr=0.5303 bs=393216 train_time:1153572ms step_avg:765.48ms
step:1508/1775 loss.item()=20548.89453125 n_predict=1 lr=0.5284 bs=393216 train_time:1154632ms step_avg:765.67ms
step:1509/1775 loss.item()=20641.56640625 n_predict=1 lr=0.5265 bs=393216 train_time:1155687ms step_avg:765.86ms
step:1510/1775 loss.item()=20421.513671875 n_predict=1 lr=0.5246 bs=393216 train_time:1156748ms step_avg:766.06ms
step:1511/1775 loss.item()=20805.318359375 n_predict=1 lr=0.5228 bs=393216 train_time:1157807ms step_avg:766.25ms
step:1512/1775 loss.item()=20477.78125 n_predict=1 lr=0.5209 bs=393216 train_time:1158868ms step_avg:766.45ms
step:1513/1775 loss.item()=21261.32421875 n_predict=1 lr=0.5190 bs=393216 train_time:1159926ms step_avg:766.64ms
step:1514/1775 loss.item()=20642.908203125 n_predict=1 lr=0.5171 bs=393216 train_time:1160988ms step_avg:766.83ms
step:1515/1775 loss.item()=21708.80859375 n_predict=1 lr=0.5153 bs=393216 train_time:1162046ms step_avg:767.03ms
step:1516/1775 loss.item()=20784.140625 n_predict=1 lr=0.5134 bs=393216 train_time:1163106ms step_avg:767.22ms
step:1517/1775 loss.item()=19856.890625 n_predict=1 lr=0.5115 bs=393216 train_time:1164164ms step_avg:767.41ms
step:1518/1775 loss.item()=21487.310546875 n_predict=1 lr=0.5096 bs=393216 train_time:1165221ms step_avg:767.60ms
step:1519/1775 loss.item()=20492.83984375 n_predict=1 lr=0.5077 bs=393216 train_time:1166280ms step_avg:767.79ms
step:1520/1775 loss.item()=20652.92578125 n_predict=1 lr=0.5059 bs=393216 train_time:1167340ms step_avg:767.99ms
step:1521/1775 loss.item()=20931.369140625 n_predict=1 lr=0.5040 bs=393216 train_time:1168399ms step_avg:768.18ms
step:1522/1775 loss.item()=20513.7265625 n_predict=1 lr=0.5021 bs=393216 train_time:1169458ms step_avg:768.37ms
step:1523/1775 loss.item()=21682.923828125 n_predict=1 lr=0.5002 bs=393216 train_time:1170518ms step_avg:768.56ms
step:1524/1775 loss.item()=20162.09765625 n_predict=1 lr=0.4983 bs=393216 train_time:1171577ms step_avg:768.75ms
step:1525/1775 loss.item()=21098.998046875 n_predict=1 lr=0.4965 bs=393216 train_time:1172637ms step_avg:768.94ms
step:1526/1775 loss.item()=21028.615234375 n_predict=1 lr=0.4946 bs=393216 train_time:1173697ms step_avg:769.13ms
step:1527/1775 loss.item()=21438.46875 n_predict=1 lr=0.4927 bs=393216 train_time:1174753ms step_avg:769.32ms
step:1528/1775 loss.item()=20553.79296875 n_predict=1 lr=0.4908 bs=393216 train_time:1175810ms step_avg:769.51ms
step:1529/1775 loss.item()=20605.8046875 n_predict=1 lr=0.4889 bs=393216 train_time:1176867ms step_avg:769.70ms
step:1530/1775 loss.item()=21362.875 n_predict=1 lr=0.4871 bs=393216 train_time:1177927ms step_avg:769.89ms
step:1531/1775 loss.item()=20673.0703125 n_predict=1 lr=0.4852 bs=393216 train_time:1178983ms step_avg:770.07ms
step:1532/1775 loss.item()=20521.7265625 n_predict=1 lr=0.4833 bs=393216 train_time:1180040ms step_avg:770.26ms
step:1533/1775 loss.item()=20692.587890625 n_predict=1 lr=0.4814 bs=393216 train_time:1181098ms step_avg:770.45ms
step:1534/1775 loss.item()=20184.79296875 n_predict=1 lr=0.4796 bs=393216 train_time:1182159ms step_avg:770.64ms
step:1535/1775 loss.item()=20564.087890625 n_predict=1 lr=0.4777 bs=393216 train_time:1183216ms step_avg:770.82ms
step:1536/1775 loss.item()=20556.75390625 n_predict=1 lr=0.4758 bs=393216 train_time:1184275ms step_avg:771.01ms
step:1537/1775 loss.item()=20401.64453125 n_predict=1 lr=0.4739 bs=393216 train_time:1185330ms step_avg:771.20ms
step:1538/1775 loss.item()=21633.708984375 n_predict=1 lr=0.4720 bs=393216 train_time:1186393ms step_avg:771.39ms
step:1539/1775 loss.item()=21031.6796875 n_predict=1 lr=0.4702 bs=393216 train_time:1187448ms step_avg:771.57ms
step:1540/1775 loss.item()=20482.85546875 n_predict=1 lr=0.4683 bs=393216 train_time:1188510ms step_avg:771.76ms
step:1541/1775 loss.item()=20240.38671875 n_predict=1 lr=0.4664 bs=393216 train_time:1189568ms step_avg:771.95ms
step:1542/1775 loss.item()=20736.46875 n_predict=1 lr=0.4645 bs=393216 train_time:1190628ms step_avg:772.13ms
step:1543/1775 loss.item()=20784.185546875 n_predict=1 lr=0.4626 bs=393216 train_time:1191686ms step_avg:772.32ms
step:1544/1775 loss.item()=20546.203125 n_predict=1 lr=0.4608 bs=393216 train_time:1192747ms step_avg:772.50ms
step:1545/1775 loss.item()=20689.546875 n_predict=1 lr=0.4589 bs=393216 train_time:1193806ms step_avg:772.69ms
step:1546/1775 loss.item()=20521.779296875 n_predict=1 lr=0.4570 bs=393216 train_time:1194864ms step_avg:772.87ms
step:1547/1775 loss.item()=21335.12109375 n_predict=1 lr=0.4551 bs=393216 train_time:1195921ms step_avg:773.06ms
step:1548/1775 loss.item()=20474.697265625 n_predict=1 lr=0.4532 bs=393216 train_time:1196982ms step_avg:773.24ms
step:1549/1775 loss.item()=20475.048828125 n_predict=1 lr=0.4514 bs=393216 train_time:1198038ms step_avg:773.43ms
step:1550/1775 loss.item()=21244.955078125 n_predict=1 lr=0.4495 bs=393216 train_time:1199101ms step_avg:773.61ms
step:1551/1775 loss.item()=20750.71875 n_predict=1 lr=0.4476 bs=393216 train_time:1200162ms step_avg:773.80ms
step:1552/1775 loss.item()=21386.42578125 n_predict=1 lr=0.4457 bs=393216 train_time:1201223ms step_avg:773.98ms
step:1553/1775 loss.item()=21230.9609375 n_predict=1 lr=0.4439 bs=393216 train_time:1202280ms step_avg:774.17ms
step:1554/1775 loss.item()=20821.7421875 n_predict=1 lr=0.4420 bs=393216 train_time:1203342ms step_avg:774.35ms
step:1555/1775 loss.item()=20133.4765625 n_predict=1 lr=0.4401 bs=393216 train_time:1204399ms step_avg:774.53ms
step:1556/1775 loss.item()=20230.4375 n_predict=1 lr=0.4382 bs=393216 train_time:1205458ms step_avg:774.72ms
step:1557/1775 loss.item()=20656.98046875 n_predict=1 lr=0.4363 bs=393216 train_time:1206515ms step_avg:774.90ms
step:1558/1775 loss.item()=21317.822265625 n_predict=1 lr=0.4345 bs=393216 train_time:1207572ms step_avg:775.08ms
step:1559/1775 loss.item()=20820.08203125 n_predict=1 lr=0.4326 bs=393216 train_time:1208631ms step_avg:775.26ms
step:1560/1775 loss.item()=20634.181640625 n_predict=1 lr=0.4307 bs=393216 train_time:1209691ms step_avg:775.44ms
step:1561/1775 loss.item()=20570.685546875 n_predict=1 lr=0.4288 bs=393216 train_time:1210746ms step_avg:775.62ms
step:1562/1775 loss.item()=20846.92578125 n_predict=1 lr=0.4269 bs=393216 train_time:1211808ms step_avg:775.81ms
step:1563/1775 loss.item()=20679.150390625 n_predict=1 lr=0.4251 bs=393216 train_time:1212864ms step_avg:775.98ms
step:1564/1775 loss.item()=20255.03125 n_predict=1 lr=0.4232 bs=393216 train_time:1213925ms step_avg:776.17ms
step:1565/1775 loss.item()=20977.318359375 n_predict=1 lr=0.4213 bs=393216 train_time:1214986ms step_avg:776.35ms
step:1566/1775 loss.item()=20289.609375 n_predict=1 lr=0.4194 bs=393216 train_time:1216041ms step_avg:776.53ms
step:1567/1775 loss.item()=21179.119140625 n_predict=1 lr=0.4175 bs=393216 train_time:1217099ms step_avg:776.71ms
step:1568/1775 loss.item()=20185.458984375 n_predict=1 lr=0.4157 bs=393216 train_time:1218162ms step_avg:776.89ms
step:1569/1775 loss.item()=21648.13671875 n_predict=1 lr=0.4138 bs=393216 train_time:1219218ms step_avg:777.07ms
step:1570/1775 loss.item()=20441.7578125 n_predict=1 lr=0.4119 bs=393216 train_time:1220278ms step_avg:777.25ms
step:1571/1775 loss.item()=20488.3515625 n_predict=1 lr=0.4100 bs=393216 train_time:1221333ms step_avg:777.42ms
step:1572/1775 loss.item()=20700.29296875 n_predict=1 lr=0.4081 bs=393216 train_time:1222396ms step_avg:777.61ms
step:1573/1775 loss.item()=21049.87109375 n_predict=1 lr=0.4063 bs=393216 train_time:1223453ms step_avg:777.78ms
step:1574/1775 loss.item()=21836.7265625 n_predict=1 lr=0.4044 bs=393216 train_time:1224511ms step_avg:777.96ms
step:1575/1775 loss.item()=20331.640625 n_predict=1 lr=0.4025 bs=393216 train_time:1225569ms step_avg:778.14ms
step:1576/1775 loss.item()=21041.572265625 n_predict=1 lr=0.4006 bs=393216 train_time:1226631ms step_avg:778.32ms
step:1577/1775 loss.item()=21013.859375 n_predict=1 lr=0.3988 bs=393216 train_time:1227689ms step_avg:778.50ms
step:1578/1775 loss.item()=19880.8671875 n_predict=1 lr=0.3969 bs=393216 train_time:1228749ms step_avg:778.68ms
step:1579/1775 loss.item()=21139.640625 n_predict=1 lr=0.3950 bs=393216 train_time:1229806ms step_avg:778.85ms
step:1580/1775 loss.item()=20557.65234375 n_predict=1 lr=0.3931 bs=393216 train_time:1230867ms step_avg:779.03ms
step:1581/1775 loss.item()=21349.5546875 n_predict=1 lr=0.3912 bs=393216 train_time:1231921ms step_avg:779.20ms
step:1582/1775 loss.item()=20836.046875 n_predict=1 lr=0.3894 bs=393216 train_time:1232981ms step_avg:779.38ms
step:1583/1775 loss.item()=19940.716796875 n_predict=1 lr=0.3875 bs=393216 train_time:1234041ms step_avg:779.56ms
step:1584/1775 loss.item()=20272.970703125 n_predict=1 lr=0.3856 bs=393216 train_time:1235099ms step_avg:779.73ms
step:1585/1775 loss.item()=20763.484375 n_predict=1 lr=0.3837 bs=393216 train_time:1236159ms step_avg:779.91ms
step:1586/1775 loss.item()=20378.046875 n_predict=1 lr=0.3818 bs=393216 train_time:1237219ms step_avg:780.09ms
step:1587/1775 loss.item()=20892.8203125 n_predict=1 lr=0.3800 bs=393216 train_time:1238276ms step_avg:780.26ms
step:1588/1775 loss.item()=21335.34765625 n_predict=1 lr=0.3781 bs=393216 train_time:1239337ms step_avg:780.44ms
step:1589/1775 loss.item()=21510.05078125 n_predict=1 lr=0.3762 bs=393216 train_time:1240396ms step_avg:780.61ms
step:1590/1775 loss.item()=20319.06640625 n_predict=1 lr=0.3743 bs=393216 train_time:1241458ms step_avg:780.79ms
step:1591/1775 loss.item()=20317.673828125 n_predict=1 lr=0.3724 bs=393216 train_time:1242517ms step_avg:780.97ms
step:1592/1775 loss.item()=20364.775390625 n_predict=1 lr=0.3706 bs=393216 train_time:1243579ms step_avg:781.14ms
step:1593/1775 loss.item()=20331.8984375 n_predict=1 lr=0.3687 bs=393216 train_time:1244636ms step_avg:781.32ms
step:1594/1775 loss.item()=20692.986328125 n_predict=1 lr=0.3668 bs=393216 train_time:1245693ms step_avg:781.49ms
step:1595/1775 loss.item()=20536.05859375 n_predict=1 lr=0.3649 bs=393216 train_time:1246750ms step_avg:781.66ms
step:1596/1775 loss.item()=20306.1171875 n_predict=1 lr=0.3631 bs=393216 train_time:1247812ms step_avg:781.84ms
step:1597/1775 loss.item()=21133.19921875 n_predict=1 lr=0.3612 bs=393216 train_time:1248869ms step_avg:782.01ms
step:1598/1775 loss.item()=20369.48828125 n_predict=1 lr=0.3593 bs=393216 train_time:1249930ms step_avg:782.18ms
step:1599/1775 loss.item()=21521.9609375 n_predict=1 lr=0.3574 bs=393216 train_time:1250988ms step_avg:782.36ms
step:1600/1775 loss.item()=21376.681640625 n_predict=1 lr=0.3555 bs=393216 train_time:1252049ms step_avg:782.53ms
step:1601/1775 loss.item()=21249.75390625 n_predict=1 lr=0.3537 bs=393216 train_time:1253107ms step_avg:782.70ms
step:1602/1775 loss.item()=21232.9140625 n_predict=1 lr=0.3518 bs=393216 train_time:1254166ms step_avg:782.88ms
step:1603/1775 loss.item()=21639.4765625 n_predict=1 lr=0.3499 bs=393216 train_time:1255221ms step_avg:783.04ms
step:1604/1775 loss.item()=20720.767578125 n_predict=1 lr=0.3480 bs=393216 train_time:1256281ms step_avg:783.22ms
step:1605/1775 loss.item()=20277.259765625 n_predict=1 lr=0.3461 bs=393216 train_time:1257337ms step_avg:783.39ms
step:1606/1775 loss.item()=21212.046875 n_predict=1 lr=0.3443 bs=393216 train_time:1258397ms step_avg:783.56ms
step:1607/1775 loss.item()=21354.65234375 n_predict=1 lr=0.3424 bs=393216 train_time:1259453ms step_avg:783.73ms
step:1608/1775 loss.item()=20438.2890625 n_predict=1 lr=0.3405 bs=393216 train_time:1260515ms step_avg:783.90ms
step:1609/1775 loss.item()=21006.330078125 n_predict=1 lr=0.3386 bs=393216 train_time:1261577ms step_avg:784.08ms
step:1610/1775 loss.item()=21003.67578125 n_predict=1 lr=0.3367 bs=393216 train_time:1262639ms step_avg:784.25ms
step:1611/1775 loss.item()=20497.04296875 n_predict=1 lr=0.3349 bs=393216 train_time:1263698ms step_avg:784.42ms
step:1612/1775 loss.item()=20552.943359375 n_predict=1 lr=0.3330 bs=393216 train_time:1264759ms step_avg:784.59ms
step:1613/1775 loss.item()=19808.171875 n_predict=1 lr=0.3311 bs=393216 train_time:1265816ms step_avg:784.76ms
step:1614/1775 loss.item()=19990.19921875 n_predict=1 lr=0.3292 bs=393216 train_time:1266878ms step_avg:784.93ms
step:1615/1775 loss.item()=21132.576171875 n_predict=1 lr=0.3274 bs=393216 train_time:1267937ms step_avg:785.10ms
step:1616/1775 loss.item()=19514.955078125 n_predict=1 lr=0.3255 bs=393216 train_time:1269002ms step_avg:785.27ms
step:1617/1775 loss.item()=21262.388671875 n_predict=1 lr=0.3236 bs=393216 train_time:1270061ms step_avg:785.44ms
step:1618/1775 loss.item()=20879.3828125 n_predict=1 lr=0.3217 bs=393216 train_time:1271121ms step_avg:785.61ms
step:1619/1775 loss.item()=20904.88671875 n_predict=1 lr=0.3198 bs=393216 train_time:1272177ms step_avg:785.78ms
step:1620/1775 loss.item()=20549.794921875 n_predict=1 lr=0.3180 bs=393216 train_time:1273239ms step_avg:785.95ms
step:1621/1775 loss.item()=20254.876953125 n_predict=1 lr=0.3161 bs=393216 train_time:1274300ms step_avg:786.12ms
step:1622/1775 loss.item()=20367.43359375 n_predict=1 lr=0.3142 bs=393216 train_time:1275362ms step_avg:786.29ms
step:1623/1775 loss.item()=21192.982421875 n_predict=1 lr=0.3123 bs=393216 train_time:1276419ms step_avg:786.46ms
step:1624/1775 loss.item()=21259.38671875 n_predict=1 lr=0.3104 bs=393216 train_time:1277478ms step_avg:786.62ms
step:1625/1775 loss.item()=20492.779296875 n_predict=1 lr=0.3086 bs=393216 train_time:1278539ms step_avg:786.79ms
step:1626/1775 loss.item()=20859.966796875 n_predict=1 lr=0.3067 bs=393216 train_time:1279598ms step_avg:786.96ms
step:1627/1775 loss.item()=20982.6640625 n_predict=1 lr=0.3048 bs=393216 train_time:1280657ms step_avg:787.13ms
step:1628/1775 loss.item()=20861.95703125 n_predict=1 lr=0.3029 bs=393216 train_time:1281717ms step_avg:787.30ms
step:1629/1775 loss.item()=20354.640625 n_predict=1 lr=0.3010 bs=393216 train_time:1282775ms step_avg:787.46ms
step:1630/1775 loss.item()=20287.0703125 n_predict=1 lr=0.2992 bs=393216 train_time:1283837ms step_avg:787.63ms
step:1631/1775 loss.item()=20685.4765625 n_predict=1 lr=0.2973 bs=393216 train_time:1284896ms step_avg:787.80ms
step:1632/1775 loss.item()=20595.44921875 n_predict=1 lr=0.2954 bs=393216 train_time:1285957ms step_avg:787.96ms
step:1633/1775 loss.item()=20343.3515625 n_predict=1 lr=0.2935 bs=393216 train_time:1287012ms step_avg:788.13ms
step:1634/1775 loss.item()=21765.27734375 n_predict=1 lr=0.2917 bs=393216 train_time:1288073ms step_avg:788.29ms
step:1635/1775 loss.item()=21065.9921875 n_predict=1 lr=0.2898 bs=393216 train_time:1289130ms step_avg:788.46ms
step:1636/1775 loss.item()=21055.1640625 n_predict=1 lr=0.2879 bs=393216 train_time:1290192ms step_avg:788.63ms
step:1637/1775 loss.item()=21728.2109375 n_predict=1 lr=0.2860 bs=393216 train_time:1291250ms step_avg:788.79ms
step:1638/1775 loss.item()=19693.5078125 n_predict=1 lr=0.2841 bs=393216 train_time:1292310ms step_avg:788.96ms
step:1639/1775 loss.item()=20826.876953125 n_predict=1 lr=0.2823 bs=393216 train_time:1293369ms step_avg:789.12ms
step:1640/1775 loss.item()=20165.93359375 n_predict=1 lr=0.2804 bs=393216 train_time:1294429ms step_avg:789.29ms
step:1641/1775 loss.item()=19034.732421875 n_predict=1 lr=0.2785 bs=393216 train_time:1295486ms step_avg:789.45ms
step:1642/1775 loss.item()=19976.94140625 n_predict=1 lr=0.2766 bs=393216 train_time:1296545ms step_avg:789.61ms
step:1643/1775 loss.item()=21192.25 n_predict=1 lr=0.2747 bs=393216 train_time:1297603ms step_avg:789.78ms
step:1644/1775 loss.item()=20759.560546875 n_predict=1 lr=0.2729 bs=393216 train_time:1298663ms step_avg:789.94ms
step:1645/1775 loss.item()=20453.74609375 n_predict=1 lr=0.2710 bs=393216 train_time:1299721ms step_avg:790.10ms
step:1646/1775 loss.item()=20378.44921875 n_predict=1 lr=0.2691 bs=393216 train_time:1300781ms step_avg:790.27ms
step:1647/1775 loss.item()=21081.53515625 n_predict=1 lr=0.2672 bs=393216 train_time:1301837ms step_avg:790.43ms
step:1648/1775 loss.item()=20000.001953125 n_predict=1 lr=0.2653 bs=393216 train_time:1302895ms step_avg:790.59ms
step:1649/1775 loss.item()=20950.2265625 n_predict=1 lr=0.2635 bs=393216 train_time:1303952ms step_avg:790.75ms
step:1650/1775 loss.item()=21014.42578125 n_predict=1 lr=0.2616 bs=393216 train_time:1305012ms step_avg:790.92ms
step:1651/1775 loss.item()=20258.484375 n_predict=1 lr=0.2597 bs=393216 train_time:1306069ms step_avg:791.08ms
step:1652/1775 loss.item()=20848.03125 n_predict=1 lr=0.2578 bs=393216 train_time:1307130ms step_avg:791.24ms
step:1653/1775 loss.item()=20897.1015625 n_predict=1 lr=0.2560 bs=393216 train_time:1308188ms step_avg:791.40ms
step:1654/1775 loss.item()=20960.736328125 n_predict=1 lr=0.2541 bs=393216 train_time:1309248ms step_avg:791.56ms
step:1655/1775 loss.item()=20681.08203125 n_predict=1 lr=0.2522 bs=393216 train_time:1310307ms step_avg:791.73ms
step:1656/1775 loss.item()=20056.78515625 n_predict=1 lr=0.2503 bs=393216 train_time:1311367ms step_avg:791.89ms
step:1657/1775 loss.item()=20855.44140625 n_predict=1 lr=0.2484 bs=393216 train_time:1312425ms step_avg:792.05ms
step:1658/1775 loss.item()=20281.61328125 n_predict=1 lr=0.2466 bs=393216 train_time:1313483ms step_avg:792.21ms
step:1659/1775 loss.item()=20677.009765625 n_predict=1 lr=0.2447 bs=393216 train_time:1314538ms step_avg:792.37ms
step:1660/1775 loss.item()=20217.142578125 n_predict=1 lr=0.2428 bs=393216 train_time:1315595ms step_avg:792.53ms
step:1661/1775 loss.item()=20507.34765625 n_predict=1 lr=0.2409 bs=393216 train_time:1316653ms step_avg:792.69ms
step:1662/1775 loss.item()=20236.21484375 n_predict=1 lr=0.2390 bs=393216 train_time:1317712ms step_avg:792.85ms
step:1663/1775 loss.item()=19900.72265625 n_predict=1 lr=0.2372 bs=393216 train_time:1318771ms step_avg:793.01ms
step:1664/1775 loss.item()=20547.373046875 n_predict=1 lr=0.2353 bs=393216 train_time:1319830ms step_avg:793.17ms
step:1665/1775 loss.item()=20664.73828125 n_predict=1 lr=0.2334 bs=393216 train_time:1320890ms step_avg:793.33ms
step:1666/1775 loss.item()=20068.548828125 n_predict=1 lr=0.2315 bs=393216 train_time:1321950ms step_avg:793.49ms
step:1667/1775 loss.item()=21338.390625 n_predict=1 lr=0.2296 bs=393216 train_time:1323007ms step_avg:793.65ms
step:1668/1775 loss.item()=20377.359375 n_predict=1 lr=0.2278 bs=393216 train_time:1324069ms step_avg:793.81ms
step:1669/1775 loss.item()=20613.90625 n_predict=1 lr=0.2259 bs=393216 train_time:1325122ms step_avg:793.96ms
step:1670/1775 loss.item()=20789.03515625 n_predict=1 lr=0.2240 bs=393216 train_time:1326184ms step_avg:794.12ms
step:1671/1775 loss.item()=20235.96484375 n_predict=1 lr=0.2221 bs=393216 train_time:1327238ms step_avg:794.28ms
step:1672/1775 loss.item()=20559.41796875 n_predict=1 lr=0.2203 bs=393216 train_time:1328299ms step_avg:794.44ms
step:1673/1775 loss.item()=21137.8125 n_predict=1 lr=0.2184 bs=393216 train_time:1329358ms step_avg:794.60ms
step:1674/1775 loss.item()=19975.74609375 n_predict=1 lr=0.2165 bs=393216 train_time:1330421ms step_avg:794.76ms
step:1675/1775 loss.item()=20745.80078125 n_predict=1 lr=0.2146 bs=393216 train_time:1331480ms step_avg:794.91ms
step:1676/1775 loss.item()=20467.791015625 n_predict=1 lr=0.2127 bs=393216 train_time:1332540ms step_avg:795.07ms
step:1677/1775 loss.item()=20410.82421875 n_predict=1 lr=0.2109 bs=393216 train_time:1333596ms step_avg:795.23ms
step:1678/1775 loss.item()=19700.7421875 n_predict=1 lr=0.2090 bs=393216 train_time:1334656ms step_avg:795.38ms
step:1679/1775 loss.item()=20827.94140625 n_predict=1 lr=0.2071 bs=393216 train_time:1335714ms step_avg:795.54ms
step:1680/1775 loss.item()=20326.765625 n_predict=1 lr=0.2052 bs=393216 train_time:1336772ms step_avg:795.70ms
step:1681/1775 loss.item()=20042.33203125 n_predict=1 lr=0.2033 bs=393216 train_time:1337829ms step_avg:795.85ms
step:1682/1775 loss.item()=20117.97265625 n_predict=1 lr=0.2015 bs=393216 train_time:1338887ms step_avg:796.01ms
step:1683/1775 loss.item()=20855.052734375 n_predict=1 lr=0.1996 bs=393216 train_time:1339948ms step_avg:796.17ms
step:1684/1775 loss.item()=20386.767578125 n_predict=1 lr=0.1977 bs=393216 train_time:1341009ms step_avg:796.32ms
step:1685/1775 loss.item()=20000.314453125 n_predict=1 lr=0.1958 bs=393216 train_time:1342067ms step_avg:796.48ms
step:1686/1775 loss.item()=20014.53515625 n_predict=1 lr=0.1939 bs=393216 train_time:1343124ms step_avg:796.63ms
step:1687/1775 loss.item()=20587.89453125 n_predict=1 lr=0.1921 bs=393216 train_time:1344180ms step_avg:796.79ms
step:1688/1775 loss.item()=20389.54296875 n_predict=1 lr=0.1902 bs=393216 train_time:1345238ms step_avg:796.94ms
step:1689/1775 loss.item()=19727.205078125 n_predict=1 lr=0.1883 bs=393216 train_time:1346298ms step_avg:797.10ms
step:1690/1775 loss.item()=21005.208984375 n_predict=1 lr=0.1864 bs=393216 train_time:1347351ms step_avg:797.25ms
step:1691/1775 loss.item()=19994.32421875 n_predict=1 lr=0.1846 bs=393216 train_time:1348410ms step_avg:797.40ms
step:1692/1775 loss.item()=20260.40234375 n_predict=1 lr=0.1827 bs=393216 train_time:1349470ms step_avg:797.56ms
step:1693/1775 loss.item()=20721.8203125 n_predict=1 lr=0.1808 bs=393216 train_time:1350527ms step_avg:797.71ms
step:1694/1775 loss.item()=19508.82421875 n_predict=1 lr=0.1789 bs=393216 train_time:1351591ms step_avg:797.87ms
step:1695/1775 loss.item()=20106.322265625 n_predict=1 lr=0.1770 bs=393216 train_time:1352650ms step_avg:798.02ms
step:1696/1775 loss.item()=20413.21484375 n_predict=1 lr=0.1752 bs=393216 train_time:1353710ms step_avg:798.18ms
step:1697/1775 loss.item()=20122.041015625 n_predict=1 lr=0.1733 bs=393216 train_time:1354768ms step_avg:798.33ms
step:1698/1775 loss.item()=20787.7578125 n_predict=1 lr=0.1714 bs=393216 train_time:1355827ms step_avg:798.48ms
step:1699/1775 loss.item()=20454.220703125 n_predict=1 lr=0.1695 bs=393216 train_time:1356883ms step_avg:798.64ms
step:1700/1775 loss.item()=20476.05078125 n_predict=1 lr=0.1676 bs=393216 train_time:1357942ms step_avg:798.79ms
step:1701/1775 loss.item()=20304.2109375 n_predict=1 lr=0.1658 bs=393216 train_time:1359003ms step_avg:798.94ms
step:1702/1775 loss.item()=20937.6484375 n_predict=1 lr=0.1639 bs=393216 train_time:1360065ms step_avg:799.10ms
step:1703/1775 loss.item()=20617.7734375 n_predict=1 lr=0.1620 bs=393216 train_time:1361127ms step_avg:799.25ms
step:1704/1775 loss.item()=20022.90625 n_predict=1 lr=0.1601 bs=393216 train_time:1362188ms step_avg:799.41ms
step:1705/1775 loss.item()=20386.12890625 n_predict=1 lr=0.1582 bs=393216 train_time:1363251ms step_avg:799.56ms
step:1706/1775 loss.item()=20055.6328125 n_predict=1 lr=0.1564 bs=393216 train_time:1364312ms step_avg:799.71ms
step:1707/1775 loss.item()=20019.927734375 n_predict=1 lr=0.1545 bs=393216 train_time:1365369ms step_avg:799.86ms
step:1708/1775 loss.item()=20447.77734375 n_predict=1 lr=0.1526 bs=393216 train_time:1366430ms step_avg:800.02ms
step:1709/1775 loss.item()=20246.6015625 n_predict=1 lr=0.1507 bs=393216 train_time:1367492ms step_avg:800.17ms
step:1710/1775 loss.item()=20114.392578125 n_predict=1 lr=0.1489 bs=393216 train_time:1368551ms step_avg:800.32ms
step:1711/1775 loss.item()=20981.53125 n_predict=1 lr=0.1470 bs=393216 train_time:1369611ms step_avg:800.47ms
step:1712/1775 loss.item()=19716.642578125 n_predict=1 lr=0.1451 bs=393216 train_time:1370673ms step_avg:800.63ms
step:1713/1775 loss.item()=20557.5625 n_predict=1 lr=0.1432 bs=393216 train_time:1371731ms step_avg:800.78ms
step:1714/1775 loss.item()=20916.73828125 n_predict=1 lr=0.1413 bs=393216 train_time:1372793ms step_avg:800.93ms
step:1715/1775 loss.item()=20240.228515625 n_predict=1 lr=0.1395 bs=393216 train_time:1373848ms step_avg:801.08ms
step:1716/1775 loss.item()=20948.9609375 n_predict=1 lr=0.1376 bs=393216 train_time:1374910ms step_avg:801.23ms
step:1717/1775 loss.item()=19739.0390625 n_predict=1 lr=0.1357 bs=393216 train_time:1375970ms step_avg:801.38ms
step:1718/1775 loss.item()=20583.1328125 n_predict=1 lr=0.1338 bs=393216 train_time:1377029ms step_avg:801.53ms
step:1719/1775 loss.item()=19885.712890625 n_predict=1 lr=0.1319 bs=393216 train_time:1378088ms step_avg:801.68ms
step:1720/1775 loss.item()=20657.55078125 n_predict=1 lr=0.1301 bs=393216 train_time:1379147ms step_avg:801.83ms
step:1721/1775 loss.item()=20249.03125 n_predict=1 lr=0.1282 bs=393216 train_time:1380204ms step_avg:801.98ms
step:1722/1775 loss.item()=19659.046875 n_predict=1 lr=0.1263 bs=393216 train_time:1381266ms step_avg:802.13ms
step:1723/1775 loss.item()=20401.59375 n_predict=1 lr=0.1244 bs=393216 train_time:1382321ms step_avg:802.28ms
step:1724/1775 loss.item()=20095.6171875 n_predict=1 lr=0.1225 bs=393216 train_time:1383380ms step_avg:802.42ms
step:1725/1775 loss.item()=20305.5 n_predict=1 lr=0.1207 bs=393216 train_time:1384436ms step_avg:802.57ms
step:1726/1775 loss.item()=20241.302734375 n_predict=1 lr=0.1188 bs=393216 train_time:1385497ms step_avg:802.72ms
step:1727/1775 loss.item()=20506.005859375 n_predict=1 lr=0.1169 bs=393216 train_time:1386556ms step_avg:802.87ms
step:1728/1775 loss.item()=21194.421875 n_predict=1 lr=0.1150 bs=393216 train_time:1387613ms step_avg:803.02ms
step:1729/1775 loss.item()=20329.025390625 n_predict=1 lr=0.1132 bs=393216 train_time:1388669ms step_avg:803.16ms
step:1730/1775 loss.item()=20531.890625 n_predict=1 lr=0.1113 bs=393216 train_time:1389728ms step_avg:803.31ms
step:1731/1775 loss.item()=21337.775390625 n_predict=1 lr=0.1094 bs=393216 train_time:1390787ms step_avg:803.46ms
step:1732/1775 loss.item()=19899.099609375 n_predict=1 lr=0.1075 bs=393216 train_time:1391851ms step_avg:803.61ms
step:1733/1775 loss.item()=20084.943359375 n_predict=1 lr=0.1056 bs=393216 train_time:1392908ms step_avg:803.76ms
step:1734/1775 loss.item()=20473.29296875 n_predict=1 lr=0.1038 bs=393216 train_time:1393971ms step_avg:803.90ms
step:1735/1775 loss.item()=20005.34765625 n_predict=1 lr=0.1019 bs=393216 train_time:1395031ms step_avg:804.05ms
step:1736/1775 loss.item()=20500.8046875 n_predict=1 lr=0.1000 bs=393216 train_time:1424975ms step_avg:820.84ms
step:1737/1775 loss.item()=20896.091796875 n_predict=1 lr=0.1000 bs=393216 train_time:1426028ms step_avg:820.97ms
step:1738/1775 loss.item()=20339.162109375 n_predict=1 lr=0.1000 bs=393216 train_time:1427081ms step_avg:821.11ms
step:1739/1775 loss.item()=20722.078125 n_predict=1 lr=0.1000 bs=393216 train_time:1428133ms step_avg:821.24ms
step:1740/1775 loss.item()=20744.955078125 n_predict=1 lr=0.1000 bs=393216 train_time:1429187ms step_avg:821.37ms
step:1741/1775 loss.item()=19925.078125 n_predict=1 lr=0.1000 bs=393216 train_time:1430237ms step_avg:821.50ms
step:1742/1775 loss.item()=20672.484375 n_predict=1 lr=0.1000 bs=393216 train_time:1431289ms step_avg:821.64ms
step:1743/1775 loss.item()=20208.57421875 n_predict=1 lr=0.1000 bs=393216 train_time:1432339ms step_avg:821.77ms
step:1744/1775 loss.item()=20066.41796875 n_predict=1 lr=0.1000 bs=393216 train_time:1433392ms step_avg:821.90ms
step:1745/1775 loss.item()=20392.0 n_predict=1 lr=0.1000 bs=393216 train_time:1434443ms step_avg:822.03ms
step:1746/1775 loss.item()=20140.33203125 n_predict=1 lr=0.1000 bs=393216 train_time:1435498ms step_avg:822.16ms
step:1747/1775 loss.item()=20126.92578125 n_predict=1 lr=0.1000 bs=393216 train_time:1436551ms step_avg:822.30ms
step:1748/1775 loss.item()=19805.328125 n_predict=1 lr=0.1000 bs=393216 train_time:1437607ms step_avg:822.43ms
step:1749/1775 loss.item()=21305.078125 n_predict=1 lr=0.1000 bs=393216 train_time:1438661ms step_avg:822.56ms
step:1750/1775 loss.item()=19680.41796875 n_predict=1 lr=0.1000 bs=393216 train_time:1439717ms step_avg:822.70ms
step:1750/1775 lr=0.1000 bs=393216 n_predict=1 val_loss:3.2897 val_malbo_loss:3.2897 train_time:1439877ms step_avg:822.79ms
step:1751/1775 loss.item()=19995.67578125 n_predict=1 lr=0.1000 bs=393216 train_time:1440766ms step_avg:822.82ms
step:1752/1775 loss.item()=20278.203125 n_predict=1 lr=0.1000 bs=393216 train_time:1441817ms step_avg:822.95ms
step:1753/1775 loss.item()=20462.85546875 n_predict=1 lr=0.1000 bs=393216 train_time:1442871ms step_avg:823.09ms
step:1754/1775 loss.item()=21091.126953125 n_predict=1 lr=0.1000 bs=393216 train_time:1443927ms step_avg:823.22ms
step:1755/1775 loss.item()=20578.30078125 n_predict=1 lr=0.1000 bs=393216 train_time:1444979ms step_avg:823.35ms
step:1756/1775 loss.item()=20384.3515625 n_predict=1 lr=0.1000 bs=393216 train_time:1446034ms step_avg:823.48ms
step:1757/1775 loss.item()=20506.689453125 n_predict=1 lr=0.1000 bs=393216 train_time:1447085ms step_avg:823.61ms
step:1758/1775 loss.item()=20970.904296875 n_predict=1 lr=0.1000 bs=393216 train_time:1448139ms step_avg:823.74ms
step:1759/1775 loss.item()=20000.896484375 n_predict=1 lr=0.1000 bs=393216 train_time:1449194ms step_avg:823.87ms
step:1760/1775 loss.item()=20252.3828125 n_predict=1 lr=0.1000 bs=393216 train_time:1450248ms step_avg:824.00ms
step:1761/1775 loss.item()=20998.515625 n_predict=1 lr=0.1000 bs=393216 train_time:1451300ms step_avg:824.13ms
step:1762/1775 loss.item()=21394.14453125 n_predict=1 lr=0.1000 bs=393216 train_time:1452355ms step_avg:824.27ms
step:1763/1775 loss.item()=20767.1171875 n_predict=1 lr=0.1000 bs=393216 train_time:1453410ms step_avg:824.40ms
step:1764/1775 loss.item()=20645.673828125 n_predict=1 lr=0.1000 bs=393216 train_time:1454468ms step_avg:824.53ms
step:1765/1775 loss.item()=20911.54296875 n_predict=1 lr=0.1000 bs=393216 train_time:1455526ms step_avg:824.66ms
step:1766/1775 loss.item()=20323.00390625 n_predict=1 lr=0.1000 bs=393216 train_time:1456581ms step_avg:824.79ms
step:1767/1775 loss.item()=20528.546875 n_predict=1 lr=0.1000 bs=393216 train_time:1457634ms step_avg:824.92ms
step:1768/1775 loss.item()=20626.03515625 n_predict=1 lr=0.1000 bs=393216 train_time:1458689ms step_avg:825.05ms
step:1769/1775 loss.item()=20320.09375 n_predict=1 lr=0.1000 bs=393216 train_time:1459746ms step_avg:825.18ms
step:1770/1775 loss.item()=20377.046875 n_predict=1 lr=0.1000 bs=393216 train_time:1460800ms step_avg:825.31ms
step:1771/1775 loss.item()=19840.57421875 n_predict=1 lr=0.1000 bs=393216 train_time:1461854ms step_avg:825.44ms
step:1772/1775 loss.item()=18601.453125 n_predict=1 lr=0.1000 bs=393216 train_time:1462910ms step_avg:825.57ms
step:1773/1775 loss.item()=20444.125 n_predict=1 lr=0.1000 bs=393216 train_time:1463966ms step_avg:825.70ms
step:1774/1775 loss.item()=20584.20703125 n_predict=1 lr=0.1000 bs=393216 train_time:1465026ms step_avg:825.83ms
step:1775/1775 loss.item()=20742.16015625 n_predict=1 lr=0.1000 bs=393216 train_time:1466082ms step_avg:825.96ms
step:1775/1775 lr=0.1000 bs=393216 n_predict=1 val_loss:3.2833 val_malbo_loss:3.2833 train_time:1466243ms step_avg:826.05ms
peak memory allocated: 30169 MiB reserved: 30640 MiB
